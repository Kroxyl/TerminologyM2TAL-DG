Feedback-Based Tree Search for Reinforcement Learning
Daniel R. Jiang 1 Emmanuel Ekwedike 2 3 Han Liu 2 4
Abstract
Inspired by recent successes of Monte-Carlo tree
search (MCTS) in a number of artificial intelligence (AI) application domains, we propose a
model-based reinforcement learning (RL) technique that iteratively applies MCTS on batches
of small, finite-horizon versions of the original
infinite-horizon Markov decision process. The
terminal condition of the finite-horizon problems,
or the leaf-node evaluator of the decision tree generated by MCTS, is specified using a combination
of an estimated value function and an estimated
policy function. The recommendations generated
by the MCTS procedure are then provided as feedback in order to refine, through classification and
regression, the leaf-node evaluator for the next
iteration. We provide the first sample complexity
bounds for a tree search-based RL algorithm. In
addition, we show that a deep neural network implementation of the technique can create a competitive AI agent for the popular multi-player online
battle arena (MOBA) game King of Glory.
1. Introduction
Monte-Carlo tree search (MCTS), introduced in Coulom
(2006) and surveyed in detail by Browne et al. (2012), has received attention in recent years for its successes in gameplay
artificial intelligence (AI), culminating in the Go-playing
AI AlphaGo (Silver et al., 2016). MCTS seeks to iteratively
build the decision tree associated with a given Markov decision process (MDP) so that attention is focused on â€œimportantâ€ areas of the state space, assuming a given initial state
(or root node of the decision tree). The intuition behind
MCTS is that if rough estimates of state or action values are
given, then it is only necessary to expand the decision tree
in the direction of states and actions with high estimated
value. To accomplish this, MCTS utilizes the guidance of
1University of Pittsburgh 2Tencent AI Lab 3
Princeton University 4Northwestern University. Correspondence to: Daniel R. Jiang
<drjiang@pitt.edu>.
Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).
leaf-node evaluators (either a policy function (Chaslot et al.,
2006) rollout, a value function evaluation (Campbell et al.,
2002; Enzenberger, 2004), or a mixture of both (Silver et al.,
2016)) to produce estimates of downstream values once the
tree has reached a certain depth (Browne et al., 2012). The
information from the leaf-nodes are then backpropagated
up the tree. The performance of MCTS depends heavily
on the quality of the policy/value approximations (Gelly &
Silver, 2007), and at the same time, the successes of MCTS
in Go show that MCTS improves upon a given policy when
the policy is used for leaf evaluation, and in fact, it can
be viewed as a policy improvement operator (Silver et al.,
2017). In this paper, we study a new feedback-based framework, wherein MCTS updates its own leaf-node evaluators
using observations generated at the root node.
MCTS is typically viewed as an online planner, where a
decision tree is built starting from the current state as the
root node (Chaslot et al., 2006; 2008; Hingston & Masek,
2007; MaË†Ä±trepierre et al., 2008; Cazenave, 2009; Mehat & Â´
Cazenave, 2010; Gelly & Silver, 2011; Gelly et al., 2012;
Silver et al., 2016). The standard goal of MCTS is to recommend an action for the root node only. After the action is
taken, the system moves forward and a new tree is created
from the next state (statistics from the old tree may be partially saved or completely discarded). MCTS is thus a â€œlocalâ€
procedure (in that it only returns an action for a given state)
and is inherently different from value function approximation or policy function approximation approaches where a
â€œglobalâ€ policy (one that contains policy information about
all states) is built. In real-time decision-making applications,
it is more difficult to build an adequate â€œon-the-flyâ€ local
approximation than it is to use pre-trained global policy
in the short amount of time available for decision-making.
For games like Chess or Go, online planning using MCTS
may be appropriate, but in games where fast decisions are
necessary (e.g., Atari or MOBA video games), tree search
methods are too slow (Guo et al., 2014). The proposed
algorithm is intended to be used in an off-policy fashion during the reinforcement learning (RL) training phase. Once
the training is complete, the policies associated with leafnode evaluation can be implemented to make fast, real-time
decisions without any further need for tree search.
Main Contributions. These characteristics of MCTS motivate our proposed method, which attempts to leverage the
arXiv:1805.05935v1 [cs.AI] 15 May 2018
Feedback-Based Tree Search for Reinforcement Learning
local properties of MCTS into a training procedure to iteratively build global policy across all states. The idea is to
apply MCTS on batches of small, finite-horizon versions
of the original infinite-horizon Markov decision process
(MDP). A rough summary is as follows: (1) initialize an
arbitrary value function and a policy function; (2) start (possibly in parallel) a batch of MCTS instances, limited in
search-depth, initialized from a set of sampled states, while
incorporating a combination of the value and policy function
as leaf-node evaluators; (3) update both the value and policy
functions using the latest MCTS root node observations;
(4) Repeat starting from step (2). This method exploits
the idea that an MCTS policy is better than either of the
leaf-node evaluator policies alone (Silver et al., 2016), yet
improved leaf-node evaluators also improve the quality of
MCTS (Gelly & Silver, 2007). The primary contributions
of this paper are summarized below.
1. We propose a batch, MCTS-based RL method that
operates on continuous state, finite action MDPs and
exploits the idea that leaf-evaluators can be updated
to produce a stronger tree search using previous tree
search results. Function approximators are used to
track policy and value function approximations, where
the latter is used to reduce the length of the tree search
rollout (oftentimes, the rollout of the policy becomes a
computational bottle-neck in complex environments).
2. We provide a full sample complexity analysis of the
method and show that with large enough sample sizes
and sufficiently large tree search effort, the performance of the estimated policies can be made close to
optimal, up to some unavoidable approximation error.
To our knowledge, batch MCTS-based RL methods
have not been theoretically analyzed.
3. An implementation of the feedback-based tree search
algorithm using deep neural networks is tested on
the recently popular MOBA game King of Glory (a
North American version of the same game is titled
Arena of Valor). The result is a competitive AI agent
for the 1v1 mode of the game.
2. Related Work
The idea of leveraging tree search during training was first
explored by Guo et al. (2014) in the context of Atari games,
where MCTS was used to generate offline training data for a
supervised learning (classification) procedure. The authors
showed that by using the power of tree search offline, the
resulting policy was able to outperform the deep Q-network
(DQN) approach of (Mnih et al., 2013). A natural next step
is to repeatedly apply the procedure of Guo et al. (2014).
In building AlphaGo Zero, Silver et al. (2017) extends the
ideas of Guo et al. (2014) into an iterative procedure, where
the neural network policy is updated after every episode
and then reincorporated into tree search. The technique was
able to produce a superhuman Go-playing AI (and improves
upon the previous AlphaGo versions) without any human
replay data.
Our proposed algorithm is a provably near-optimal variant
(and in some respects, generalization) of the AlphaGo Zero
algorithm. The key differences are the following: (1) our
theoretical results cover a continuous, rather than finite, state
space setting, (2) the environment is a stochastic MDP rather
than a sequential deterministic two player game, (3) we use
batch updates, (4) the feedback of previous results to the
leaf-evaluator manifests as both policy and value updates
rather than just the value (as Silver et al. (2017) does not
use policy rollouts).
Anthony et al. (2017) proposes a general framework called
expert iteration that combines supervised learning with tree
search-based planning. The methods described in Guo et al.
(2014), Silver et al. (2017), and the current paper can all
be (at least loosely) expressed under the expert iteration
framework. However, no theoretical insights were given
in any of these previous works and our paper intends to
fill this gap by providing a full theoretical analysis of an
iterative, MCTS-based RL algorithm. Our analysis relies
on the concentrability coefficient idea of Munos (2007) for
approximate value iteration and builds upon the work on
classification based policy iteration (Lazaric et al., 2016),
approximate modified policy iteration (Scherrer et al., 2015),
and fitted value iteration (Munos & Szepesvari Â´ , 2008).
Sample complexity results for MCTS are relatively sparse.
Teraoka et al. (2014) gives a high probability upper bound
on the number of playouts needed to achieve -accuracy
at the root node for a stylized version of MCTS called
FindTopWinner. More recently, Kaufmann & Koolen
(2017) provided high probability bounds on the sample
complexity of two other variants of MCTS called UGapEMCTS and LUCB-MCTS. In this paper, we do not require any
particular implementation of MCTS, but make a generic
assumption on its accuracy that is inspired by these results.
3. Problem Formulation
Consider a discounted, infinite-horizon MDP with a continuous state space S and finite action space A. For all
(s, a) âˆˆ S Ã—A, the reward function r : S Ã—A â†’ R satisfies
r(s, a) âˆˆ [0, Rmax]. The transition kernel, which describes
transitions to the next state given current state s and action a,
is written p(Â·|s, a) â€” a probability measure over S. Given
a discount factor Î³ âˆˆ [0, 1), the value function V
Ï€ of a
policy Ï€ : S â†’ A starting in s = s0 âˆˆ S is given by
V
Ï€
(s) = E
"Xâˆ
t=0
Î³
t
r(st, Ï€t(st))#
, (1)
Feedback-Based Tree Search for Reinforcement Learning
where st is the state visited at time t. Let Î  be the set of
all stationary, deterministic policies (i.e., mappings from
state to action). The optimal value function is obtained by
maximizing over all policies: V
âˆ—
(s) = supÏ€âˆˆÎ  V
Ï€
(s).
Both V
Ï€
and V
âˆ—
are bounded by Vmax = Rmax/(1âˆ’Î³). We
let F be the set of bounded, real-valued functions mapping
S to [0, Vmax]. We frequently make use of the shorthand
operator TÏ€ : F â†’ F, where the quantity (TÏ€V )(s) is be
interpreted as the reward gained by taking an action according to Ï€, receiving the reward r(s, Ï€(s)), and then receiving
an expected terminal reward according to the argument V :
(TÏ€V )(s) = r(s, Ï€(s)) + Î³
Z
S
V (Ëœs) p(dsËœ|s, Ï€(s)).
It is well-known that V
Ï€
is the unique fixed-point of TÏ€,
meaning TÏ€V
Ï€ = V
Ï€
(Puterman, 2014). The Bellman operator T : F â†’ F is similarly defined using the maximizing
action:
(T V )(s) = max
aâˆˆA h
r(s, a) + Î³
Z
S
V (Ëœs) p(dsËœ|s, a)
i
.
It is also known that V
âˆ—
is the unique fixed-point of T
(Puterman, 2014) and that acting greedily with respect to
the optimal value function V
âˆ— produces an optimal policy:
Ï€
âˆ—
(s) âˆˆ arg max
aâˆˆA
h
r(s, a) + Î³
Z
S
V
âˆ—
(Ëœs) p(dsËœ|s, a)
i
.
We use the notation T
d
to mean the d compositions of the
mapping T, e.g., T
2V = T(T V ). Lastly, let V âˆˆ F and let
Î½ be a distribution over S. We define left and right versions
of an operator PÏ€:
(PÏ€V )(s) = Z
S
V (Ëœs) p(dsËœ|s, Ï€(s)),
(Î½PÏ€)(dsËœ) = Z
S
p(dsËœ|s, Ï€(s)) Î½(ds).
Note that PÏ€V âˆˆ F and ÂµPÏ€ is another distribution over S.
4. Feedback-Based Tree Search Algorithm
We now formally describe the proposed algorithm. The
parameters are as follows. Let Î Â¯ âŠ† Î  be a space of approximate policies and F âŠ† F Â¯ be a space of approximate value
functions (e.g., classes of neural network architectures). We
let Ï€k âˆˆ Î Â¯ be the policy function approximation (PFA)
and Vk âˆˆ FÂ¯ be the value function approximation (VFA) at
iteration k of the algorithm. Parameters subscripted with â€˜0â€™
are used in the value function approximation (regression)
phase and parameters subscripted with â€˜1â€™ are used in the
tree search phase. The full description of the procedure is
given in Figure 1, using the notation Ta = TÏ€a
, where Ï€a
maps all states to the action a âˆˆ A. We now summarize the
two phases, VFA (Steps 2 and 3) and MCTS (Steps 4, 5,
and 6).
VFA Phase. Given a policy Ï€k, we wish to approximate
its value by fitting a function using subroutine Regress on
N0 states sampled from a distribution Ï0. Each call to MCTS
requires repeatedly performing rollouts that are initiated
from leaf-nodes of the decision tree. Because repeating full
rollouts during tree search is expensive, the idea is that a
VFA obtained from a one-time regression on a single set
of rollouts can drastically reduce the computation needed
for MCTS. For each sampled state s, we estimate its value
using M0 full rollouts, which can be obtained using the
absorption time formulation of an infinite horizon MDP
(Puterman, 2014, Proposition 5.3.1).
MCTS Phase. On every iteration k, we sample a set of
N1 i.i.d. states from a distribution Ï1 over S. From each
state, a tree search algorithm, denoted MCTS, is executed for
M1 iterations on a search tree of maximum depth d. We
assume here that the leaf evaluator is a general function
of the PFA and VFA from the previous iteration, Ï€k and
Vk, and it is denoted as a â€œsubroutineâ€ LeafEval. The
results of the MCTS procedure are piped into a subroutine
Classify, which fits a new policy Ï€k+1 using classification
(from continuous states to discrete actions) on the new data.
As discussed more in Assumption 4, Classify uses L1
observations (one-step rollouts) to compute a loss function.
1. Sample a set of N0 i.i.d. states S0,k from Ï0 and N1 i.i.d.
states S1,k from Ï1.
2. Compute a sample average YË†k(s) of M0 independent rollouts of Ï€k for each s âˆˆ S0,k. See Assumption 1.
3. Use Regress on the set {YË†k(s) : s âˆˆ S0,k} to obtain a
value function Vk âˆˆ FÂ¯. See Assumption 1.
4. From each s âˆˆ S1,k, run MCTS with parameters M1, d,
and evaluator LeafEval. Return estimated value of each s,
denoted UË†k(s). See Assumptions 2 and 3.
5. For each s âˆˆ S1,k and a âˆˆ A, create estimate QË†k(s, a) â‰ˆ
(Ta Vk)(s) by averaging L1 transitions from p(Â·|s, a). See
Assumption 4.
6. Use Classify to solve a cost-sensitive classification problem and obtain the next policy Ï€k+1 âˆˆ Î Â¯ . Costs are measured
using {UË†k(s) : s âˆˆ S1,k} and {QË†k(s, Ï€k+1(s)) : s âˆˆ S1,k}.
See Assumption 4. Increment k and return to Step 1.
Figure 1. Feedback-Based Tree Search Algorithm
The illustration given in Figure 2 shows the interactions (and
feedback loop) of the basic components of the algorithm:
(1) a set of tree search runs initiated from a batch of sampled
states (triangles), (2) leaf evaluation using Ï€k and Vk is used
during tree search, and (3) updated PFA and VFA Ï€k+1 and
Feedback-Based Tree Search for Reinforcement Learning
S
s
1
s
2
s
N1 s Â· Â· Â·
3
leaf evaluation
update
Ï€k and
Vk
Ï€k+1 and Vk+1
tree search
Figure 2. Illustration of the Feedback Loop
Vk+1 using tree search results.
5. Assumptions
Figure 1 shows the algorithm written with general subroutines Regress, MCTS, LeafEval, and Classify, allowing
for variations in implementation suited for different problems. However, our analysis assumes specific choices and
properties of these subroutines, which we describe now. The
regression step solves a least absolute deviation problem to
minimize an empirical version of
kf âˆ’ V
Ï€k k1, Ï0 =
Z
S
|f(s) âˆ’ V
Ï€k
(s)|Ï0(ds),
as described in the first assumption.
Assumption 1 (Regress Subroutine). For each s
i âˆˆ S0,k,
define s
i = s
ij
0
for all j and for each t, the state s
ij
t+1 is
drawn from p(Â·|s
ij
t
, Ï€k(s
ij
t
)). Let YË†
k(s
i
) be an estimate of
V
Ï€k (s
i
) using M0 rollouts and Vk, the VFA resulting from
Regress, obtained via least absolute deviation regression:
YË†
k(s
i
0
) = 1
M0
X
M0
j=1
Xâˆ
t=0
Î³
t
r(s
ij
t
, Ï€k(s
ij
t
)), (2)
Vk âˆˆ arg min
fâˆˆFÂ¯
1
N0
X
N0
i=1

f(s
i
) âˆ’ YË†
k(s
i
)


. (3)
There are many ways that LeafEval may be defined. The
standard leaf evaluator for MCTS is to simulate a default
or â€œrolloutâ€ policy (Browne et al., 2012) until the end of
the game, though in related tree search techniques, authors
have also opted for a value function approximation (Campbell et al., 2002; Enzenberger, 2004). It is also possible to
combine the two approximations: Silver et al. (2016) uses
a weighted combination of a full rollout from a pre-trained
policy and a pre-trained value function approximation.
Assumption 2 (LeafEval Subroutine). Our approach uses
a partial rollout of length h â‰¥ 0 and a value estimation at
the end. LeafEval produces unbiased observations of
Jk(s) = E
"
h
Xâˆ’1
t=0
Î³
t
r(Ëœst, Ï€k(Ëœst)) + Î³
h Vk(Ëœsh)
#
, (4)
where sËœ0 = s.
Assumption 2 is motivated by our MOBA game, on which
we observed that even short rollouts (as opposed to simply
using a VFA) are immensely helpful in determining local
outcomes (e.g., dodging attacks, eliminating minions, health
regeneration). At the same time, we found that numerous
full rollouts simulated using the relatively slow and complex
game engine is far too time-consuming within tree search.
We also need to make an assumption on the sample complexity of MCTS, of which there are many possible variations
(Chaslot et al., 2006; Coulom, 2006; Kocsis & Szepesvari Â´ ,
2006; Gelly & Silver, 2007; Couetoux et al. Â¨ , 2011a;b; AlKanj et al., 2016; Jiang et al., 2017). Particularly relevant to
our continuous-state setting are tree expansion techniques
called progressive widening and double progressive widening, proposed in Couetoux et al. Â¨ (2011a), which have proven
successful in problems with continuous state/action spaces.
To our knowledge, analysis of the sample complexity is only
available for stylized versions of MCTS on finite problems,
like Teraoka et al. (2014) and Kaufmann & Koolen (2017).
Theorems from these papers show upper bounds on the number of iterations needed so that with high probability (greater
than 1 âˆ’ Î´), the value at the root node is accurate within
a tolerance of . Fortunately, there are ways to discretize
continuous state MDPs that enjoy error guarantees, such as
Bertsekas (1975), Dufour & Prieto-Rumeau (2012), or Saldi
et al. (2017). These error bounds can be combined with the
MCTS guarantees of Teraoka et al. (2014) and Kaufmann
& Koolen (2017) to produce a sample complexity bound
for MCTS on continuous problems. The next assumption
captures the essence of these results (and if desired, can
be made precise for specific implementations through the
references above).
Assumption 3 (MCTS Subroutine). Consider a d-stage,
finite-horizon subproblem of (1) with terminal value function J and initial state is s. Let the result of MCTS be denoted
UË†(s). We assume that there exists a function m(, Î´), such
that if m(, Î´) iterations of MCTS are used, the inequality
|UË†(s)âˆ’(T
d J)(s)| â‰¤  holds with probability at least 1âˆ’Î´.
Now, we are ready to discuss the Classify subroutine.
Our goal is to select a policy Ï€ âˆˆ Î Â¯ that closely mimics the
performance of the MCTS result, similar to practical implementations in existing work (Guo et al., 2014; Silver et al.,
2017; Anthony et al., 2017). The question is: given a candidate Ï€, how do we measure â€œclosenessâ€ to the MCTS policy?
We take inspiration from previous work in classificationbased RL and use a cost-based penalization of classification
Feedback-Based Tree Search for Reinforcement Learning
errors (Langford & Zadrozny, 2005; Li et al., 2007; Lazaric
et al., 2016). Since UË†(s
i
) is an approximation of the performance of the MCTS policy, we should try to select a policy
Ï€ with similar performance. To estimate the performance
of some candidate policy Ï€, we use a one-step rollout and
evaluate the downstream cost using Vk.
Assumption 4 (Classify Subroutine). For each s
i âˆˆ S1,k
and a âˆˆ A, let QË†
k(s
i
, a) be an estimate of the value of stateaction pair (s
i
, a) using L1 samples.
QË†
k(s
i
, a) = 1
L1
X
L1
j=1

r(s
i
, a) + Î³ Vk(Ëœs
j
(a))
.
Let Ï€k+1, the result of Classify, be obtained by minimizing the discrepancy between the MCTS result UË†
k and the
estimated value of the policy under approximations QË†
k:
Ï€k+1 âˆˆ arg min
Ï€âˆˆÎ Â¯
1
N1
X
N1
i=1

UË†
k(s
i
) âˆ’ QË†
k(s
i
, Ï€(s
i
))


,
where sËœ
j
(a) are i.i.d. samples from p(Â· | s
i
, a).
An issue that arises during the analysis is that even though
we can control the distribution from which states are sampled, this distribution is transformed by the transition kernel
of the policies used for rollout/lookahead. Let us now introduce the concentrability coefficient idea of Munos (2007)
(and used subsequently by many authors, including Munos
& Szepesvari Â´ (2008), Lazaric et al. (2016), Scherrer et al.
(2015), and Haskell et al. (2016)).
Assumption 5 (Concentrability). Consider any sequence
of m policies Âµ1, Âµ2, . . . , Âµm âˆˆ Î . Suppose we start in
distribution Î½ and that the state distribution attained after
applying the m policies in succession, Î½ PÂµ1 PÂµ2
Â· Â· Â· PÂµm,
is absolutely continuous with respect to Ï1. We define an
m-step concentrability coefficient
Am = sup
Âµ1, ...,Âµm








dÎ½PÂµ1 PÂµ2
Â· Â· Â· PÂµm
dÏ1








âˆ
,
and assume that Pâˆ
i,j=0 Î³
i+j Ai+j < âˆ. Similarly, we
assume Ï1PÂµ1 PÂµ2
Â· Â· Â· PÂµm, is absolutely continuous with
respect to Ï0 and assume that
A
0
m = sup
Âµ1, ...,Âµm








dÏ1PÂµ1 PÂµ2
Â· Â· Â· PÂµm
dÏ0








âˆ
is finite for any m.
The concentrability coefficient describes how the state distribution changes after m steps of arbitrary policies and how
it relates to a given reference distribution. Assumptions 1-5
are used for the remainder of the paper.
6. Sample Complexity Analysis
Before presenting the sample complexity analysis, let us
consider an algorithm that generates a sequence of policies {Ï€0, Ï€1, Ï€2, . . .} satisfying TÏ€k+1 T
dâˆ’1V
Ï€k = T
d V
Ï€k
with no error. It is proved in Bertsekas & Tsitsiklis (1996,
pp. 30-31) that Ï€k â†’ Ï€
âˆ—
in the finite state and action setting. Our proposed algorithm in Figure 1 can be viewed as
approximately satisfying this iteration in a continuous state
space setting, where MCTS plays the role of T
d
and evaluation of Ï€k uses a combination of accurate rollouts (due
to Classify) and fast VFA evaluations (due to Regress).
The sample complexity analysis requires the effects of all
errors to be systematically analyzed.
For some K â‰¥ 0, our goal is to develop a high probability
upper bound on the expected suboptimality, over an initial
state distribution Î½, of the performance of policy Ï€K, written as kV
âˆ— âˆ’ V
Ï€K k1,Î½. Because there is no requirement to
control errors with probability one, bounds in kÂ· k1,Î½ tend to
be much more useful in practice than ones in the traditional
k Â· kâˆ. Notice that:
1
N1
X
N1
i=1

UË†
k(s
i
) âˆ’ QË†
k(s
i
, Ï€k+1(s
i
))


â‰ˆ



T
d V
Ï€k âˆ’ TÏ€k+1 V
Ï€k




1,Ï1
,
(5)
where the left-hand-side is the loss function used in the
classification step from Assumption 4. It turns out that
we can relate the right-hand-side (albeit under a different
distribution) to the expected suboptimality after K iterations
kV
âˆ— âˆ’ V
Ï€K k1,Î½, as shown in the following lemma. Full
proofs of all results are given in the supplementary material.
Lemma 1 (Loss to Performance Relationship). The expected suboptimality of Ï€K can be bounded as follows:
kV
âˆ—âˆ’V
Ï€K k1,Î½ â‰¤ Î³
K d kV
âˆ— âˆ’ V
Ï€0 kâˆ
+
X
K
k=1
Î³
(Kâˆ’k)d



T
d V
Ï€kâˆ’1 âˆ’ TÏ€k V
Ï€kâˆ’1




1,Î›Î½,k
where Î›Î½,k = Î½ (PÏ€âˆ— )
(Kâˆ’k)d

I âˆ’ (Î³PÏ€k
)
âˆ’1
.
From Lemma 1, we see that the expected suboptimality at iteration K can be upper bounded by the suboptimality of the initial policy Ï€0 (in maximum norm) plus
a discounted and re-weighted version of kT
d V
Ï€kâˆ’1 âˆ’
TÏ€k V
Ï€kâˆ’1 k1,Ï1
accumulated over prior iterations. Hypothetically, if (T
d V
Ï€kâˆ’1 )(s) âˆ’ (TÏ€k V
Ï€kâˆ’1 )(s) were small
for all iterations k and all states s, then the suboptimality
of Ï€K converges linearly to zero. Hence, we may refer to
kT
d V
Ï€kâˆ’1 âˆ’ TÏ€k V
Ï€kâˆ’1 k1,Ï1
as the â€œtrue loss,â€ the target
term to be minimized at iteration k. We now have a starting
point for the analysis: if (5) can be made precise, then the
result can be combined with Lemma 1 to provide an explicit
Feedback-Based Tree Search for Reinforcement Learning
!
! T
d V
Ï€k âˆ’ TÏ€k+1 V
Ï€k
!
!
1, Ï1
!
! V
k âˆ’ V
Ï€k
!
!
1, Ï0
!
! T
dJk âˆ’ TÏ€k+1 Vk
!
!
1, Ï1
state space sampling
approximation over FÂ¯ B
â€²
Î³ min
fâˆˆFÂ¯
!f âˆ’ V
Ï€k !1, Ï0
additional error Ç«
min
Ï€âˆˆÎ Â¯
!T
d V
Ï€k âˆ’ TÏ€ V
Ï€k !1, Ï1
state/rollout sampling
approximation over Î Â¯
â€œtrue loss of Ï€k+1â€
tree search error
Figure 3. Various Errors Analyzed in Lemma 3
bound on kV
âˆ— âˆ’ V
Ï€K k1,Î½. The various errors that we incur
when relating the objective of Classify to the true loss
include the error due to regression using functions in FÂ¯; the
error due to sampling the state space according to Ï1; the
error of estimating (TÏ€ Vk)(s) using the sample average of
one-step rollouts QË†
k(s, Ï€(s)); and of course, the error due
to MCTS.
We now give a series of lemmas that help us carry out
the analysis. In the algorithmic setting, the policy Ï€k is a
random quantity that depends on the samples collected in
previous iterations; however, for simplicity, the lemmas that
follow are stated from the perspective of a fixed policy Âµ or
fixed value function approximation V rather than Ï€k or Vk.
Conditioning arguments will be used when invoking these
lemmas (see supplementary material).
Lemma 2 (Propagation of VFA Error). Consider a policy
Âµ âˆˆ Î  and value function V âˆˆ F. Analogous to (4), let
J = T
h
Âµ V . Then, under Assumption 5, we have the bounds:
(a) supÏ€âˆˆÎ Â¯ kTÏ€ V âˆ’ TÏ€ V
Âµk1,Ï1 â‰¤ Î³ A0
1 kV âˆ’ V
Âµk1,Ï0
,
(b) kT
d J âˆ’ T
d V
Âµk1,Ï1 â‰¤ Î³
d+hA0
d+h
kV âˆ’ V
Âµk1,Ï0
.
The lemma above addresses the fact that instead of using
V
Ï€k directly, Classify and MCTS only have access to the
estimates Vk and Jk = T
h
Ï€k
Vk (h steps of rollout with an
evaluation of Vk at the end), respectively. Note that propagation of the error in Vk is discounted by Î³ or Î³
d+h
and
since the lemma converts between k Â· k1,Ï1
and k Â· k1,Ï0
, it
is also impacted by the concentrability coefficients A0
1
and
A0
d+h
.
Let dÎ Â¯ be the VC-dimension of the class of binary classifiers
Î Â¯ and let dFÂ¯ be the pseudo-dimension of the function class
FÂ¯. The VC-dimension is a measure of the capacity of Î Â¯ and
the notion of a pseudo-dimension is a generalization of the
VC-dimension to real-valued functions (see, e.g., Pollard
(1990), Haussler (1992), Mohri et al. (2012) for definitions
of both). Similar to Lazaric et al. (2016) and Scherrer et al.
(2015), we will present results for the case of two actions,
i.e., |A| = 2. The extension to multiple actions is possible
by performing an analysis along the lines of Lazaric et al.
(2016, Section 6). We now quantify the error illustrated in
Figure 3. Define the quantity B0
Î³ = Î³ A0
1 + Î³
d+hA0
d+h
, the
sum of the coefficients from Lemma 2.
Lemma 3. Suppose the regression sample size N0 is
O

(VmaxB
0
Î³
)
2

âˆ’2

log(1/Î´) + dFÂ¯ log(VmaxB
0
Î³/)

and the sample size M0, for estimating the regression targets, is
O

(VmaxB
0
Î³
)
2

âˆ’2

log(N0/Î´)
 .
Furthermore, there exist constants C1, C2, C3, and C4, such
that if N1 and L1 are large enough to satisfy
N1 â‰¥ C1V
2
max 
âˆ’2

log(C2/Î´) + dÎ Â¯ log(eN1/dÎ Â¯ )

,
L1 â‰¥ C1V
2
max 
âˆ’2

log(C2N1/Î´) + dÎ Â¯ log(eL1/dÎ Â¯ )

,
and if M1 â‰¥ m(C3 , C4 Î´/N1), then
kT
d V
Ï€k âˆ’ TÏ€k+1 V
Ï€k k1,Ï1 â‰¤ B
0
Î³ min
fâˆˆFÂ¯
kf âˆ’ V
Ï€k k1,Ï0
+ min
Ï€âˆˆÎ Â¯
kT
d V
Ï€k âˆ’ TÏ€ V
Ï€k k1,Ï1 + 
with probability at least 1 âˆ’ Î´.
Sketch of Proof. By adding and subtracting terms, applying
the triangle inequality, and invoking Lemma 2, we see that:
kT
d V
Ï€k âˆ’ TÏ€k+1 V
Ï€k k1,Ï1 â‰¤ B
0
Î³ kVk âˆ’ V
Ï€k k1,Ï0
+ kT
dJk âˆ’ TÏ€k+1 Vkk1,Ï1
,
Here, the error is split into two terms. The first depends on
the sample S0,k and the history through Ï€k while the second
term depends on the sample S1,k and the history through Vk.
We can thus view Ï€k as fixed when analyzing the first term
and Vk as fixed when analyzing the second term (details in
the supplementary material). The first term kVk âˆ’V
Ï€k k1,Ï0
contributes the quantity minfâˆˆFÂ¯ kf âˆ’ V
Ï€k k1,Ï0
in the final
bound with additional estimation error contained within .
The second term kT
dJkâˆ’TÏ€k+1 Vkk1,Ï1
contributes the rest.
See Figure 3 for an illustration of the main proof steps.
The first two terms on the right-hand-side are related to the
approximation power of FÂ¯ and Î Â¯ and can be considered
unavoidable. We upper-bound these terms by maximizing
over Î Â¯, in effect removing the dependence on the random
process Ï€k in the analysis of the next theorem. We define:
D0(Î Â¯, FÂ¯) = max
Ï€âˆˆÎ Â¯
min
fâˆˆFÂ¯
kf âˆ’ V
Ï€
k1,Ï0
,
D
d
1
(Î ) = max Â¯
Ï€âˆˆÎ Â¯
min
Ï€0âˆˆÎ Â¯
kT
d V
Ï€ âˆ’ TÏ€0 V
Ï€
k1,Ï1
Feedback-Based Tree Search for Reinforcement Learning
Figure 4. Screenshot from 1v1 King of Glory
two terms that are closely related to the notion of inherent
Bellman error (Antos et al., 2008; Munos & Szepesvari Â´ ,
2008; Lazaric et al., 2016; Scherrer et al., 2015; Haskell
et al., 2017). Also, let BÎ³ =
Pâˆ
i,j=0 Î³
i+j Ai+j , which was
assumed to be finite in Assumption 5.
Theorem 1. Suppose the sample size requirements of
Lemma 3 are satisfied with /BÎ³ and Î´/K replacing  and
Î´, respectively. Then, the suboptimality of the policy Ï€K
can be bounded as follows:
kV
âˆ— âˆ’ V
Ï€K k1,Î½ â‰¤ BÎ³ [B
0
Î³ D0(Î Â¯ , FÂ¯) + D
d
1
(Î )] Â¯
+ Î³
Kd kV
âˆ— âˆ’ V
Ï€0 kâˆ + ,
with probability at least 1 âˆ’ Î´.
Search Depth. How should the search depth d be chosen?
Theorem 1 shows that as d increases, fewer iterations K
are needed to achieve a given accuracy; however, the effort
required of tree search (i.e., the function m(, Î´)) grows
exponentially in d. At the other extreme (d = 1), more iterations K are needed and the â€œfixed costâ€ of each iteration
of the algorithm (i.e., sampling, regression, and classification â€” all of the steps that do not depend on d) becomes
more prominent. For a given problem and algorithm parameters, these computational costs can each be estimated and
Theorem 1 can serve as a guide to selecting an optimal d.
7. Case Study: King of Glory MOBA AI
We implemented Feedback-Based Tree Search within a new
and challenging environment, the recently popular MOBA
game King of Glory by Tencent (the game is also known
as Honor of Kings and a North American release of the
game is titled Arena of Valor). Our implementation of the
algorithm is one of the first attempts to design an AI for the
1v1 version of this game.
Game Description. In the King of Glory, players are divided into two opposing teams and each team has a base
located on the opposite corners of the game map (similar
to other MOBA games, like League of Legends or Dota 2).
The bases are guarded by towers, which can attack the enemies when they are within a certain attack range. The goal
of each team is to overcome the towers and eventually destroy the opposing teamâ€™s â€œcrystal,â€ located at the enemyâ€™s
base. For this paper, we only consider the 1v1 mode, where
each player controls a primary â€œheroâ€ alongside less powerful game-controlled characters called â€œminions.â€ These
units guard the path to the crystal and will automatically fire
(weak) attacks at enemies within range. Figure 4 shows the
two heroes and their minions; the upper-left corner shows
the map, with the blue and red markers pinpointing the
towers and crystals.
Experimental Setup. The state variable of the system is
taken to be a 41-dimensional vector containing information
obtained directly from the game engine, including hero
locations, hero health, minion health, hero skill states, and
relative locations to various structures. There are 22 actions,
including move, attack, heal, and special skill actions, some
of which are associated with (discretized) directions. The
reward function is designed to mimic reward shaping (Ng
et al., 1999) and uses a combination of signals including
health, kills, damage dealt, and proximity to crystal. We
trained five King of Glory agents, using the hero DiRenJie:
1. The â€œFBTSâ€ agent is trained using our feedback-based
tree search algorithm for K = 7 iterations of 50 games
each. The search depth is d = 7 and rollout length is
h = 5. Each call to MCTS ran for 400 iterations.
2. The second agent is labeled â€œNRâ€ for no rollouts. It
uses the same parameters as the FBTS agent except
no rollouts are used. At a high level, this bears some
similarity to the AlphaGo Zero algorithm (Silver et al.,
2017) in a batch setting.
3. The â€œDPIâ€ agent uses the direct policy iteration technique of (Lazaric et al., 2016) for K = 10 iterations.
There is no value function and no tree search (due to
computational limitations, more iterations are possible
when tree search is not used).
4. We then have the â€œAVIâ€ agent, which implements approximate value iteration (De Farias & Van Roy, 2000;
Van Roy, 2006; Munos, 2007; Munos & Szepesvari Â´ ,
2008) for K = 10 iterations. This algorithm can be
considered a batch version of DQN (Mnih et al., 2013).
5. Lastly, we consider an â€œSLâ€ agent trained via supervised learning on a dataset of approximately 100,000
state/action pairs of human gameplay data. Notably,
the policy architecture used here is consistent with the
previous agents.
In fact, both the policy and value function approximations
are consistent across all agents; they use fully-connected
Feedback-Based Tree Search for Reinforcement Learning
neural networks with five and two hidden layers, respectively, and SELU (scaled exponential linear unit) activation
(Klambauer et al., 2017). The initial policy Ï€0 takes random
actions: move (w.p. 0.5), directional attack (w.p. 0.2), or a
special skill (w.p. 0.3). Besides biasing the move direction
toward the forward direction, no other heuristic information is used by Ï€0. MCTS was chosen to be a variant of
UCT (Kocsis & Szepesvari Â´ , 2006) that is more amenable toward parallel simulations: instead of using the argmax of the
UCB scores, we sample actions according to the distribution
obtained by applying softmax to the UCB scores.
In the practical implementation of the algorithm, Regress
uses a cosine proximity loss while Classify uses a negative log-likelihood loss, differing from the theoretical specifications. Due to the inability to â€œrewindâ€ or â€œfast-forwardâ€
the game environment to arbitrary states, the sampling distribution Ï0 is implemented by first taking random actions (for
a random number of steps) to arrive at an initial state and
then following Ï€k until the end of the game. To reduce correlation during value approximation, we discard 2/3 of the
states encountered in these trajectories. For Ï1, we follow
the MCTS policy while occasionally injecting noise (in the
form of random actions and random switches to the default
policy) to reduce correlation. During rollouts, we use the
internal AI for the hero DiRenJie as the opponent.
Results. As the game is nearly deterministic, our primary
methodology for testing to compare the agentsâ€™ effectiveness against a common set of opponents chosen from the
internal AIs. We also added the internal DiRenJie AI as a
â€œsanity checkâ€ baseline agent. To select the test opponents,
we played the internal DiRenJie AI against other internal
AIs (i.e., other heroes) and selected six heroes of the marksman type that the internal DiRenJie AI is able to defeat.
Each of our agents, including the internal DiRenJie AI, was
then played against every test opponent. Figure 5 shows the
length of time, measured in frames, for each agent to defeat
the test opponents (a value of 20,000 frames is assigned
if the opponent won). Against the set of common opponents, FBTS significantly outperforms DPI, AVI, SL, and
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of Game
0.5
1.0
1.5
2.0
Gold Ratio
vs. NR vs. DPI vs. AVI vs. SL
Figure 6. In-game Behavior
the internal AI. However, FBTS only slightly outperforms
NR on average (which is perhaps not surprising as NR is
the only other agent that also uses MCTS). Our second set
of results help to visualize head-to-head battles played between FBTS and the four baselines (all of which are won
by FBTS): Figure 6 shows the ratio of the FBTS agentâ€™s
gold to its opponentâ€™s gold as a function of time. Gold is
collected throughout the game as heroes deal damage and
defeat enemies, so a ratio above 1.0 (above the red region)
indicates good relative performance by FBTS. As the figure
shows, each game ends with FBTS achieving a gold ratio in
the range of [1.25, 1.75].
8. Conclusion & Future Work
In this paper, we provide a sample complexity analysis
for feedback-based tree search, an RL algorithm based on
repeatedly solving finite-horizon subproblems using MCTS.
Our primary methodological avenues for future work are
(1) to analyze a self-play variant of the algorithm and (2)
to consider related techniques in multi-agent domains (see,
e.g., Hu & Wellman (2003)). The implementation of the
algorithm in the 1v1 MOBA game King of Glory provided
us encouraging results against several related algorithms;
however, significant work remains for the agent to become
competitive with humans.
vs. Hero 1 vs. Hero 2 vs. Hero 3 vs. Hero 4 vs. Hero 5 vs. Hero 6 Average
0
5
10
15
Loss
Frames until Win (x 1000)
FBTS NR DPI AVI SL Internal AI
Figure 5. Number of Frames to Defeat Marksman Heroes
Feedback-Based Tree Search for Reinforcement Learning
Acknowledgements
We sincerely appreciate the helpful feedback from four
anonymous reviewers, which helped to significantly improve the paper. We also wish to thank our colleagues at
Tencent AI Lab, particularly Carson Eisenach and Xiangru
Lian, for assistance with the test environment and for providing the SL agent. The first author is very grateful for the
support from Tencent AI Lab through a faculty award.
References
Al-Kanj, Lina, Powell, Warren B, and Bouzaiene-Ayari,
Belgacem. The information-collecting vehicle routing
problem: Stochastic optimization for emergency storm
response. arXiv preprint arXiv:1605.05711, 2016.
Anthony, Thomas, Tian, Zheng, and Barber, David. Thinking fast and slow with deep learning and tree search. In
Advances in Neural Information Processing Systems, pp.
5366â€“5376, 2017.
Antos, Andras, Szepesv Â´ ari, Csaba, and Munos, R Â´ emi. Learn- Â´
ing near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path.
Machine Learning, 71(1):89â€“129, 2008.
Bertsekas, Dimitri P. Convergence of discretization procedures in dynamic programming. IEEE Transactions on
Automatic Control, 20(3):415â€“419, 1975.
Bertsekas, Dimitri P and Tsitsiklis, John N. Neuro-dynamic
Programming. Athena Scientific, Belmont, MA, 1996.
Browne, Cameron B, Powley, Edward, Whitehouse, Daniel,
Lucas, Simon M, Cowling, Peter I, Rohlfshagen, Philipp,
Tavener, Stephen, Perez, Diego, Samothrakis, Spyridon,
and Colton, Simon. A survey of monte carlo tree search
methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):1â€“43, 2012.
Campbell, Murray, Hoane Jr, A Joseph, and Hsu, Fenghsiung. Deep blue. Artificial Intelligence, 134(1-2):57â€“
83, 2002.
Cazenave, Tristan. Nested Monte-Carlo search. In International Joint Conference on Artificial Intelligence, pp.
456â€“461, 2009.
Chaslot, Guillaume, Saito, Jahn-Takeshi, Uiterwijk,
Jos WHM, Bouzy, Bruno, and van den Herik, H Jaap.
Monte-Carlo strategies for computer Go. In 18th BelgianDutch Conference on Artificial Intelligence, pp. 83â€“90,
2006.
Chaslot, Guillaume, Bakkes, Sander, Szita, Istvan, and
Spronck, Pieter. Monte-carlo tree search: A new framework for game AI. In AAAI Conference on Artificial
Intelligence and Interactive Digital Entertainment, 2008.
Couetoux, Adrien, Hoock, Jean-Baptiste, Sokolovska, Na- Â¨
taliya, Teytaud, Olivier, and Bonnard, Nicolas. Continuous upper confidence trees. In International Conference
on Learning and Intelligent Optimization, pp. 433â€“445.
Springer, 2011a.
Couetoux, Adrien, Milone, Mario, Brendel, M Â¨ aty Â´ as, Dogh- Â´
men, Hassan, Sebag, Michele, and Teytaud, Olivier. Continuous rapid action value estimates. In Asian Conference
on Machine Learning, pp. 19â€“31, 2011b.
Coulom, Remi. Efficient selectivity and backup operators Â´
in Monte-Carlo tree search. In International Conference
on Computers and Games, pp. 72â€“83, 2006.
De Farias, D Pucci and Van Roy, Benjamin. On the existence of fixed points for approximate value iteration and
temporal-difference learning. Journal of Optimization
theory and Applications, 105(3):589â€“608, 2000.
Dufour, FrancÂ¸ois and Prieto-Rumeau, Tomas. Approxi- Â´
mation of markov decision processes with general state
space. Journal of Mathematical Analysis and Applications, 388(2):1254â€“1267, 2012.
Enzenberger, Markus. Evaluation in go by a neural network using soft segmentation. In Advances in Computer
Games, pp. 97â€“108. Springer, 2004.
Gelly, Sylvain and Silver, David. Combining online and
offline knowledge in UCT. In Proceedings of the 24th
International Conference on Machine learning, pp. 273â€“
280, 2007.
Gelly, Sylvain and Silver, David. Monte-carlo tree search
and rapid action value estimation in computer Go. Artificial Intelligence, 175(11):1856â€“1875, 2011.
Gelly, Sylvain, Kocsis, Levente, Schoenauer, Marc, Sebag,
Michele, Silver, David, Szepesvari, Csaba, and Teytaud, Â´
Olivier. The grand challenge of computer Go: Monte
Carlo tree search and extensions. Communications of the
ACM, 55(3):106â€“113, 2012.
Guo, Xiaoxiao, Singh, Satinder, Lee, Honglak, Lewis,
Richard L, and Wang, Xiaoshi. Deep learning for realtime Atari game play using offline Monte-Carlo tree
search planning. In Advances in Neural Information
Processing Systems, pp. 3338â€“3346, 2014.
Haskell, William B, Jain, Rahul, and Kalathil, Dileep. Empirical dynamic programming. Mathematics of Operations Research, 41(2):402â€“429, 2016.
Haskell, William B, Jain, Rahul, Sharma, Hiteshi, and Yu,
Pengqian. An empirical dynamic programming algorithm
for continuous MDPs. arXiv preprint arXiv:1709.07506,
2017.
Feedback-Based Tree Search for Reinforcement Learning
Haussler, David. Decision theoretic generalizations of the
PAC model for neural net and other learning applications.
Information and Computation, 100(1):78â€“150, 1992.
Hingston, Philip and Masek, Martin. Experiments with
Monte Carlo Othello. In IEEE Congress on Evolutionary
Computation, pp. 4059â€“4064. IEEE, 2007.
Hu, Junling and Wellman, Michael P. Nash Q-learning
for general-sum stochastic games. Journal of Machine
Learning Research, 4(Nov):1039â€“1069, 2003.
Jiang, Daniel R, Al-Kanj, Lina, and Powell, Warren B.
Monte carlo tree search with sampled information relaxation dual bounds. arXiv preprint arXiv:1704.05963,
2017.
Kaufmann, Emilie and Koolen, Wouter. Monte-Carlo tree
search by best arm identification. In Advances in Neural
Information Processing Systems, pp. 4904â€“4913, 2017.
Klambauer, Gunter, Unterthiner, Thomas, Mayr, Andreas, Â¨
and Hochreiter, Sepp. Self-normalizing neural networks.
In Advances in Neural Information Processing Systems,
pp. 972â€“981, 2017.
Kocsis, Levente and Szepesvari, Csaba. Bandit based Â´
Monte-Carlo planning. In European Conference on Machine Learning, pp. 282â€“293, 2006.
Langford, John and Zadrozny, Bianca. Relating reinforcement learning performance to classification performance.
In Proceedings of the 22nd International Conference on
Machine Learning, pp. 473â€“480, 2005.
Lazaric, Alessandro, Ghavamzadeh, Mohammad, and
Munos, Remi. Analysis of classification-based policy Â´
iteration algorithms. Journal of Machine Learning Research, 17(19):1â€“30, 2016.
Li, Lihong, Bulitko, Vadim, and Greiner, Russell. Focus of
attention in reinforcement learning. Journal of Universal
Computer Science, 13(9):1246â€“1269, 2007.
MaË†Ä±trepierre, Raphael, Mary, J Â¨ erÂ´ emie, and Munos, R Â´ emi. Â´
Adaptative play in Texas holdâ€™em poker. In European
Conference on Artificial Intelligence, 2008.
Mehat, Jean and Cazenave, Tristan. Combining UCT and Â´
nested Monte Carlo search for single-player general game
playing. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):271â€“277, 2010.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and
Riedmiller, Martin. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of Machine Learning. MIT Press,
2012.
Munos, Remi. Performance bounds in l Â´ p-norm for approximate value iteration. SIAM Journal on Control and
Optimization, 46(2):541â€“561, 2007.
Munos, Remi and Szepesv Â´ ari, Csaba. Finite-time bounds Â´
for fitted value iteration. Journal of Machine Learning
Research, 9(May):815â€“857, 2008.
Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy
invariance under reward transformations: Theory and
application to reward shaping. In Proceedings of the
16th International Conference on Machine Learning, pp.
278â€“287, 1999.
Pollard, David. Empirical processes: Theory and applications. In NSF-CBMS Regional Conference Series in
Probability and Statistics, pp. iâ€“86. JSTOR, 1990.
Puterman, Martin L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
2014.
Saldi, Naci, Yuksel, Serdar, and Linder, Tam Â¨ as. On the Â´
asymptotic optimality of finite approximations to markov
decision processes with Borel spaces. Mathematics of
Operations Research, 42(4):945â€“978, 2017.
Scherrer, Bruno, Ghavamzadeh, Mohammad, Gabillon, Victor, Lesner, Boris, and Geist, Matthieu. Approximate
modified policy iteration and its application to the game
of tetris. Journal of Machine Learning Research, 16
(Aug):1629â€“1676, 2015.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
van den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,
D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,
Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,
D. Mastering the game of Go with deep neural networks
and tree search. Nature, 529(7587):484â€“489, 2016.
Silver, David, Schrittwieser, Julian, Simonyan, Karen,
Antonoglou, Ioannis, Huang, Aja, Guez, Arthur, Hubert,
Thomas, Baker, Lucas, Lai, Matthew, Bolton, Adrian,
et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.
Teraoka, Kazuki, Hatano, Kohei, and Takimoto, Eiji. Efficient sampling method for Monte Carlo tree search problem. IEICE Transactions on Information and Systems, 97
(3):392â€“398, 2014.
Van Roy, Benjamin. Performance loss bounds for approximate value iteration with state aggregation. Mathematics
of Operations Research, 31(2):234â€“244, 2006.
Supplementary Material for
Feedback-Based Tree Search for Reinforcement Learning
Daniel R. Jiang, Emmanuel Ekwedike, Han Liu
A Outline
Section B contains the proofs for the results stated in the main paper.
â€¢ The proofs of Lemma 1 and Lemma 2 from the main paper are given in Sections B.1
and B.2. These two lemmas are important in that they provide the main structure
for the sample complexity analysis. The bounds hold pointwise.
â€¢ In Section B.3, we provide some additional lemmas that are omitted from the main
paper.
â€¢ Section B.4 gives the proof of Lemma 3 from the main paper, which makes use of
Lemma 2 and the results from Section B.3.
â€¢ We prove the main result, Theorem 1, in Section B.5.
Lastly, in Section C, we provide additional implementation details regarding the neural
network architecture, state features, and computation.
1
Feedback-Based Tree Search for Reinforcement Learning
B Proofs
B.1 Proof of Lemma 1
This proof is a modification of arguments used in [Lazaric et al., 2016, Equation 8 and
Theorem 7]. By the fixed point property of TÏ€k+1 and the definition of the Bellman operator
T, we have V
Ï€k âˆ’ V
Ï€k+1 â‰¤ T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k+1 . Subtracting and adding TÏ€k+1 V
Ï€k :
V
Ï€k âˆ’ V
Ï€k+1 â‰¤ T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k + TÏ€k+1 V
Ï€k âˆ’ TÏ€k+1 V
Ï€k+1
â‰¤ T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k + (Î³PÏ€k+1 )

V
Ï€k âˆ’ V
Ï€k+1 
. (B.1)
Similarly, we will bound the difference between V
âˆ— and V
Ï€k+1 in terms of the distances
between V
âˆ— âˆ’ V
Ï€k and V
Ï€k âˆ’ V
Ï€k+1 :
V
âˆ— âˆ’ V
Ï€k+1 â‰¤ (Î³PÏ€âˆ— )
d

V
âˆ— âˆ’ V
Ï€k

+ T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k + (Î³PÏ€k+1 )

V
Ï€k âˆ’ V
Ï€k+1 
. (B.2)
Using the bound V
Ï€k âˆ’ V
Ï€k+1 â‰¤ [I âˆ’ (Î³PÏ€k+1 )]âˆ’1
(T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k ) from (B.1) on the
last term of the right side of (B.2) along with a power series expansion on the inverse, we
obtain:
V
âˆ— âˆ’ V
Ï€k+1 â‰¤ (Î³PÏ€âˆ— )
d
(V
âˆ— âˆ’ V
Ï€k ) + 
I + (Î³PÏ€k+1 )
Xâˆ
j=0
(Î³PÏ€k+1 )
j


T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k

= (Î³PÏ€âˆ— )
d
(V
âˆ— âˆ’ V
Ï€k ) +
I âˆ’ (Î³PÏ€k+1 )
âˆ’1

T
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k

,
which can be iterated to show:
V
âˆ— âˆ’V
Ï€K â‰¤ (Î³PÏ€âˆ— )
Kd (V
âˆ— âˆ’V
Ï€0
) +X
K
k=1
(Î³PÏ€âˆ— )
(Kâˆ’k) d

I âˆ’(Î³PÏ€k
)
âˆ’1

T
dV
Ï€kâˆ’1 âˆ’TÏ€k V
Ï€kâˆ’1

.
The statement from the lemma follows from taking absolute value, bounding by the maximum norm, and integrating.
B.2 Proof of Lemma 2
For part (a), we note the following:
kTÏ€ V âˆ’ TÏ€ V
Âµ
k1,Ï1 = Î³
Z
S

(PÏ€ V )(s) âˆ’ (PÏ€ V
Âµ
)(s)

 Ï1(ds)
â‰¤ Î³
Z
S

V (s) âˆ’ V
Âµ
(s)


d(Ï1PÏ€)
dÏ0
Ï0(ds)
â‰¤ Î³








d(Ï1PÏ€)
dÏ0








âˆ
kV âˆ’ V
Âµ
k1,Ï0
.
By the concentrability conditions of Assumption 5, the right-hand-side can be bounded by
Î³ A0
1
kV âˆ’ V
Âµk1,Ï0
. Now, we can apply the same steps with the roles of TÏ€ V and TÏ€ V
Âµ
reversed to see that the same inequality holds for kTÏ€ V âˆ’TÏ€ V
Âµk1,Ï1
and part (a) is complete.
2
Feedback-Based Tree Search for Reinforcement Le
For part (b), we partition the state space S into two sets:
S
+ =

s âˆˆ S : (T
dJ)(s) â‰¥ (T
dV
Âµ
)(s)
	
and S
- =

s âˆˆ S : (T
dJ)(s) < (T
dV
Âµ
)(s)
	
.
We start with S
+
. Consider the finite-horizon d-stage MDP with terminal value J and the
same dynamics as our infinite-horizon MDP of interest. Let Ï€
J
1
, Ï€J
2
, . . . , Ï€J
d
be the timedependent optimal policy for this MDP. Thus, we have
TÏ€
J
1
TÏ€
J
2
Â· Â· Â· TÏ€
J
d
J = T
dJ and TÏ€
J
1
TÏ€
J
2
Â· Â· Â· TÏ€
J
d
V
Âµ â‰¤ T
dV
Âµ
.
Using similar steps as for part (a), the following hold:
Z
S+

(T
dJ)(s) âˆ’ (T
dV
Âµ
)(s)

Ï1(ds) â‰¤
Z
S+

(T
dT
h
Âµ V )(s) âˆ’ (TÏ€
J
1
TÏ€
J
2
Â· Â· Â· TÏ€
J
d
T
h
Âµ V
Âµ
)(s)

Ï1(ds)
â‰¤ Î³
d+h
Z
S+

V (s) âˆ’ V
Âµ
(s)


d(Ï1PÏ€
J
1
PÏ€
J
2
Â· Â· Â· PÏ€
J
d
P
h
Âµ
)
dÏ0
Ï0(ds)
â‰¤ Î³
d+h A
0
d+h
Z
S+

V (s) âˆ’ V
Âµ
(s)

 Ï0(ds).
Now, using the optimal policy with respect to the d-stage MDP with terminal condition
V
Âµ
, we can repeat these steps to show that
Z
S-

(T
dV
Âµ
)(s) âˆ’ (T
dJ)(s)

Ï1(ds) â‰¤ Î³
d+h A
0
d+h
Z
S-

V (s) âˆ’ V
Âµ
(s)

 Ï0(ds).
Summing the two inequalities, we obtain:
kT
dJ âˆ’ T
dV
Âµ
k1,Ï1 â‰¤ Î³
d+h A
0
d+h
Z
S+

V (s) âˆ’ V
Âµ
(s)

 Ï0(ds) + Z
S-

V (s) âˆ’ V
Âµ
(s)

 Ï0(ds)

= Î³
d+h A
0
d+h
kV âˆ’ V
Âµ
k1,Ï0
,
which completes the proof.
B.3 Additional Technical Lemmas
Lemma B.1 (Section 4, Corollary 2 of Haussler [1992]). Let G be a set of functions from
X to [0, B] with pseudo-dimension dG < âˆ. Then for all 0 <  â‰¤ B, it holds that
P

sup
gâˆˆG




1
m
Xm
i=1
g(X(i)
) âˆ’ E

g(X)





> !
â‰¤ 8

32eB

log 32eB

dG
exp 
âˆ’

2m
64B2

, (B.3)
where X(i) are i.i.d. draws from the distribution of the random variable X.
Lemma B.2. Consider a policy Âµ âˆˆ Î  and suppose each s
i
is sampled i.i.d. from Ï0.
Define initial states s
ij
0 = s
i
for all j. Analogous to Step 5 of the algorithm and Assumption
1, let:
YË† (s
i
) = 1
M0
X
M0
j=1
Xâˆ
t=0
Î³
t
r(s
ij
t
, Âµ(s
ij
t
)) and V âˆˆ arg min
fâˆˆFÂ¯
1
N0
X
N0
i=1

f(s
i
) âˆ’ YË† (s
i
)


.
3
Feedback-Based Tree Search for Reinforcement Learning
For Î´ âˆˆ (0, 1) and  âˆˆ (0, Vmax), if the number of sampled states N0 satisfies the condition:
N0 â‰¥

32Vmax

2 
log 32
Î´
+ 2dFÂ¯ log 64eVmax


=: Î“a(, Î´),
and the number of rollouts performed from each state M0 satisfies:
M0 â‰¥ 8

Vmax

2
log 8N0
Î´
=: Î“b(, Î´),
then we have the following bound on the error of the value function approximation:
kV âˆ’ V
Âµ
k1,Ï0 â‰¤ min
fâˆˆFÂ¯
kf âˆ’ V
Âµ
k1,Ï0 + ,
with probabilty at least 1 âˆ’ Î´.
Proof. Recall that the estimated value function V satisfies
V âˆˆ arg min
fâˆˆFb
1
N0
X
N0
i=1





f(s
i
) âˆ’
1
M0
X
M0
j=1
h
V
Ï€
(s
i
0
) + Î¾
j
(s
i
0
)
i





,
where for each i, the terms Î¾
j
(s
i
0
) are i.i.d. mean zero error. The inner summation over j is
an equivalent way to write YË† (s
i
0
). Noting that the rollout results V
Âµ
(s
i
0
)+Î¾
j
(s
i
0
) âˆˆ [0, Vmax],
we have by Hoeffdingâ€™s inequality followed by a union bound:
P

max
i

YË† (s
i
0
) âˆ’ V
Âµ
(s
i
0
)

 > 
â‰¤ N0 âˆ†1(, M0), (B.4)
where âˆ†1(, M0) = 2 exp
âˆ’2M0 
2/V 2
max
. Define the function
âˆ†2(, N0) = 8 
32eVmax

log 32eVmax

dFÂ¯
exp 
âˆ’

2N0
64V 2
max 
,
representing the right-hand-side of the bound in Lemma B.1 with B = Vmax and m = N0.
Next, we define the loss minimizing function f
âˆ— âˆˆ arg minfâˆˆFÂ¯ kf âˆ’V
Âµk1,Ï0
. By Lemma B.1,
the probabilities of the events




kV âˆ’ V
Âµ
k1,Ï0 âˆ’
1
N0
X
N0
i=1

V (s
i
) âˆ’ V
Âµ
(s
i
)






>

4

and




kf
âˆ— âˆ’ V
Âµ
k1,Ï0 âˆ’
1
N0
X
N0
i=1

f
âˆ—
(s
i
) âˆ’ V
Âµ
(s
i
)






>

4

(B.5)
are each bounded by âˆ†2(/4, N0). Also, it follows by the definition of V that
1
N0
X
N0
i=1

V (s
i
) âˆ’ YË† (s
i
)

 â‰¤
1
N0
X
N0
i=1

f
âˆ—
(s
i
) âˆ’ YË† (s
i
)


.
Therefore, using (B.4) twice and (B.5) once, we have by a union bound that the inequality
kV âˆ’ V
Âµ
k1,Ï0 â‰¤ minfâˆˆFÂ¯ kf âˆ’ V
Âµk1,Ï0 + 
4
Feedback-Based Tree Search for Reinforcement Learnin
happens with probability greater than 1 âˆ’ 2 N0 âˆ†1(/4, M0) âˆ’ 2 âˆ†2(/4, N0). We then
choose N0 so that âˆ†2(/4, N0) = Î´/4 (following Haussler [1992], we utilize the inequality log(a log a) < 2 log(a/2) for a â‰¥ 5). To conclude, we choose M0 so that âˆ†1(/4, M0) =
Î´/(4N0).
Lemma B.3 (Sampling Error). Suppose |A| = 2 and let dÎ Â¯ be the VC-dimension of Î Â¯ .
Consider Z, V âˆˆ F and suppose each s
i
is sampled i.i.d. from Ï1. Also, let w
j
be i.i.d.
samples from the standard uniform distribution and g : S Ã— A Ã— [0, 1] â†’ S be a transition
function such that g(s, a, w) has the same distribution as p(Â·|s, a). For Î´ âˆˆ (0, 1) and
 âˆˆ (0, Vmax), if the number of sampled states N1 satisfies the condition:
N1 â‰¥ 128 
Vmax

2 
log 8
Î´
+ dÎ Â¯ log eN1
dÎ Â¯

=: Î“c(, Î´, N1),
and the number of sampled transitions L1 satisfies:
L1 â‰¥ 128 
Vmax

2 
log 8
Î´
+ dÎ Â¯ log eL1
dÎ Â¯

=: Î“d(, Î´, L1),
then we have the bounds:
(a) sup
Ï€âˆˆÎ Â¯




1
N1
X
N1
i=1
|Z(s
i
) âˆ’ (TÏ€ V )(s
i
)| âˆ’ kZ âˆ’ TÏ€ V k1,Ï1




â‰¤  w.p. at least 1 âˆ’ Î´.
(b) sup
Ï€âˆˆÎ Â¯




1
L1
X
L1
j=1
h
r(s
i
, Ï€(s
i
)) +Î³ V
g(s
i
, Ï€(s
i
), wj
)

i
âˆ’(TÏ€ V )(s
i
)




â‰¤  w.p. at least 1âˆ’Î´.
Proof. We remark that in both (a) and (b), the term within the absolute value is bounded
between 0 and Vmax. A second remark is that we reformulated the problem using w
j
to
take advantage of the fact that these random samples do not depend on the policy Ï€. Such
a property is required to invoke [GyÂ¨orfi et al., 2006, Theorem 9.1], a result that [Lazaric
et al., 2016, Lemma 3] depends on. With these two issues in mind, an argument similar to
the proof of [Lazaric et al., 2016, Lemma 3] gives the conclusion for both (a) and (b).
B.4 Proof of Lemma 3
On each iteration of the the algorithm, two random samples are used: S0,k and S1,k. From
S0,k, we obtain Vk and from S1,k we obtain Ï€k+1. Let Sk = (S0,k, S1,k) represent both of
the samples at iteration k. We define:
Gkâˆ’1 = Ïƒ{S1, S2, . . . , Skâˆ’1} and G
0
kâˆ’1 = Ïƒ{S1, S2, . . . , Skâˆ’1, S0,k}.
Due to the progression of the algorithm with two random samples per iteration, we will
analyze each iteration in two steps. We first separate the two random samples by noting
5
Feedback-Based Tree Search for Reinforcement Learnin
that:
kT
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k k1,Ï1 â‰¤ kT
dV
Ï€k âˆ’ T
dJkk1,Ï1 + kTÏ€k+1 Vk âˆ’ TÏ€k+1 V
Ï€k k1,Ï1
+ kT
dJk âˆ’ TÏ€k+1 Vkk1,Ï1
â‰¤ (Î³ A0
1 + Î³
d+hA
0
d+h
) kVk âˆ’ V
Ï€k k1,Ï0 + kT
dJk âˆ’ TÏ€k+1 Vkk1,Ï1
,
(B.6)
where the first inequality follows by adding and subtracting terms and the triangle inequality
while the second inequality follows by Lemma 2. Now, we may analyze the first term on
the right-hand-side conditional on Gkâˆ’1 and the second term conditional on G
0
kâˆ’1
.
As it is currently stated, Lemma B.2 gives an unconditional probability for a fixed
policy Âµ. However, since S0,k is independent from Gkâˆ’1 and Ï€k is Gkâˆ’1-measurable, we
can utilize Lemma B.2 in a conditional setting using a well-known property of conditional
expectations [Resnick, 2013, Property 12, Section 10.3]. This property will be repeatedly
used in this proof (without further mention). We obtain that for a sample size N0 â‰¥
Î“a(
0/(Î³ A0
1 + Î³
d+hA0
d+h
), Î´0
),
P

kVk âˆ’ V
Ï€k k1,Ï0 > min
fâˆˆFÂ¯
kf âˆ’ V
Ï€k k1,Ï0 + 
0
/(Î³ A0
1 + Î³
d+hA
0
d+h
)

 Gkâˆ’1

â‰¤ Î´
0
. (B.7)
It remains for us to analyze the error of the second term kT
dJk âˆ’ TÏ€k+1 Vkk1,Ï1
. By part (a)
of Lemma B.3 with Z = T
dJk and V = Vk, if N1 â‰¥ Î“c(
0
, Î´0
, N1) and s
i are sampled i.i.d.
from Ï1, we have
P
 



1
N1
X
N1
i=1

(T
dJk)(s
i
) âˆ’ (TÏ€k+1 Vk)(s
i
)

 âˆ’



T
dJk âˆ’ TÏ€k+1 Vk




1,Ï1




> 0


 G
0
kâˆ’1
!
â‰¤ Î´
0
. (B.8)
The term (TÏ€k+1 Vk)(s
i
) is approximated using L1 samples. Part (b) of Lemma B.3 along
with a union bound shows that if L1 â‰¥ Î“d(
0
, Î´0/N1, L1), then
P

max
i

QË†
k(s
i
, Ï€k+1(s
i
)) âˆ’ (TÏ€k+1 Vk)(s
i
)

 > 0

 G
0
kâˆ’1

â‰¤ Î´
0
. (B.9)
Similarly, by Assumption 3, if the number of iterations of MCTS M1 exceeds m(
0
, Î´0/N1),
we can take a union bound to arrive at
P

max
i

UË†
k(s
i
) âˆ’ (T
dJk)(s
i
)

 > 0


 G
0
kâˆ’1

â‰¤ Î´
0
. (B.10)
The maximum over i can be replaced with an average over the N1 samples and the conclusion
of the last two bounds would remain unchanged. Since Ï€k+1 is assumed to optimize a
quantity involving UË†
k and QË†
k, we want to relate this back to kT
dJk âˆ’TÏ€k+1 Vkk1,Ï1
. Indeed,
taking expectation of both sides of inequalities (B.8)â€“(B.10) and then combining, we obtain
that with probability at least 1 âˆ’ 3Î´
0
,



T
dJk âˆ’ TÏ€k+1 Vk




1,Ï1
â‰¤
1
N1
X
N1
i=1

UË†
k(s
i
) âˆ’ QË†
k(s
i
, Ï€k+1(s
i
))

 + 3
0
6
Feedback-Based Tree Search for Reinforcement Learning
â‰¤
1
N1
X
N1
i=1

UË†
k(s
i
) âˆ’ QË†
k(s
i
, Ï€Ëœ(s
i
))

 + 3
0
where ËœÏ€ âˆˆ arg minÏ€âˆˆÎ Â¯ kT
dV
Ï€k âˆ’TÏ€ V
Ï€k k1,Ï1
. Following the same steps in reverse , we have:



T
dJk âˆ’ TÏ€k+1 Vk




1,Ï1
â‰¤ min
Ï€âˆˆÎ Â¯
kT
dV
Ï€k âˆ’ TÏ€ V
Ï€k k1,Ï1 + 6
0
, (B.11)
with probability at least 1 âˆ’ 6 Î´
0
. Finally, we take expectation of both sides of (B.7) and
then combine with (B.6) and (B.11) while setting 
0 = /7 and Î´
0 = Î´/7 to obtain
kT
dV
Ï€k âˆ’ TÏ€k+1 V
Ï€k k1,Ï1 â‰¤ (Î³ A0
1 + Î³
d+hA
0
d+h
) min
fâˆˆFÂ¯
kf âˆ’ V
Ï€k k1,Ï0
+ min
Ï€âˆˆÎ Â¯
kT
dV
Ï€k âˆ’ TÏ€ V
Ï€k k1,Ï1 + 
with probability at least 1 âˆ’ Î´.
B.5 Proof of Theorem 1
This proof synthesizes the previous lemmas. From the definition of D0(Î Â¯ , FÂ¯) and D
d
1
(Î ) Â¯
from the main paper, we note that if the sample size assumptions of Lemma 3 are satisfied,
kT
dV
Ï€kâˆ’ TÏ€k+1 V
Ï€k k1,Ï1 â‰¤ B
0
Î³ D0(Î Â¯, FÂ¯) + D
d
1
(Î ) + Â¯ , (B.12)
with probability at least 1 âˆ’ Î´. This removes any dependence on the iteration k from the
right-hand-side. We now integrate all results with Lemma 1 in order to find a bound on
the suboptimality kV
âˆ— âˆ’ V
Ï€K k1,Î½. Consider the distribution Î›Î½,k, as defined in Lemma 1,
which needs to be related to Î½. We can use the power series expansion to write:
Î›Î½,k = Î½ (PÏ€âˆ— )
Kâˆ’k Xâˆ
i=0
(Î³PÏ€k
)
i
.
For a fixed i, the measure Î½ is transformed by applying Ï€
âˆ— a total of K âˆ’ k times and then
Ï€k a total of i times. We see that the summation term on the right-hand-side of Lemma 1
can be upper-bounded in the following way:
X
K
k=1
Î³
Kâˆ’k



T
dV
Ï€kâˆ’1 âˆ’ TÏ€k V
Ï€kâˆ’1




1,Î›Î½,k
â‰¤
K
Xâˆ’1
j=0
Xâˆ
i=0
Î³
j+i Aj+i

max
kâ‰¤K



T
dV
Ï€kâˆ’1 âˆ’ TÏ€k V
Ï€kâˆ’1




1,Ï1
,
where we use Assumption 5 with m = K âˆ’ k + i, maximize over k for the loss term, and
then re-index with j = K âˆ’ k. The coefficient in parentheses can be upper-bounded by BÎ³
(since all Aj+i are nonnegative). Finally, we use (B.12) and then a union bound over the
K iterations to conclude the statement of the theorem.
7
Feedback-Based Tree Search for Reinforcement Learning
C Implementation Details
C.1 Neural Network Architecture
The policy and value function approximations use fully-connected neural networks with five
and two hidden layers, respectively, and SELU (scaled exponential linear unit) activation
[Klambauer et al., 2017]. The policy network contains two sets of outputs: (1) one of seven
actions (no action, normal attack, move, skill 1, skill 2, skill 3, and heal) and (2) a twodimensional direction parameter used for the action. The first two hidden layers are shared
and have 120 and 100 hidden units, while each of the two outputs corresponds to a set of
three hidden layers with 80, 70, and 50 hidden units. The value function approximation
uses a fully-connected network with 128 hidden units in the first layer and 96 hidden units
in the second layer. As mentioned in the main paper, this architecture is consistent across
all agents whenever policy and/or value networks are needed.
C.2 Features of the State
As shown in Table 1, the state of the game is represented by 41-dimensional feature vector,
which was constructed using the output from the game engine and API. The features consists
of basic attributes of the two heroes, the computer-controlled units, and structures. The
feature lists also have information on the relative positions of the other units and structures
with respect to the hero controlled by algorithm.
Table 1: State Feature List
No. Feature Dimensions
1 Location of Hero 1 2
2 Location of Hero 2 2
3 HP of Hero 1 1
4 HP of Hero 2 1
5 Hero 1 skill cooldowns 5
6 Hero 2 skill cooldowns 5
7 Direction to enemy hero 3
8 Direction to enemy tower 4
9 Direction to enemy minion 3
10 Enemy tower HP 1
11 Enemy minion HP 1
12 Direction to the spring 3
13 Total HP of allied minions 1
14 Enemyâ€™s tower attacking Hero 1 3
15 Hero 1 in range of enemy towers 3
16 Hero 2 in range of enemy towers 3
8
Feedback-Based Tree Search for Reinforcement Learning
C.3 Tree Search Details
We provide some more information regarding the implementation of feedback-based tree
search. A major challenge in implementing in King of Glory is that the game engine can
only move forward, meaning that our sampled states are not i.i.d. and instead follow the
trajectory of the policy induced by MCTS. However, to decrease the correlation between
visited states, we inject random movements and random switches to the internal AI policy
in order to move to a â€œmore randomâ€ next state. Rollouts are performed on separate
processors to enable tree search in a game engine that cannot rewind. All experiments use
the c4.2xlarge instances on Amazon Web Services, and we utilized parallelization across
four cores within each call to MCTS.
References
L. GyÂ¨orfi, M. Kohler, A. Krzyzak, and H. Walk. A Distribution-free Theory of Nonparametric Regression. Springer Science & Business Media, 2006.
D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other
learning applications. Information and Computation, 100(1):78â€“150, 1992.
G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, pages 972â€“981, 2017.
A. Lazaric, M. Ghavamzadeh, and R. Munos. Analysis of classification-based policy iteration
algorithms. Journal of Machine Learning Research, 17(19):1â€“30, 2016.
S. I. Resnick. A Probability Path. Springer Science & Business Media, 2013.
9
Feedback-Based Tree Search for Reinforcement Learning