{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Terminology Project - M2 TAL - 24/11/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axel Didier - Pierre Goncalves - M2 TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (0.6.1.post1)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (4.6.2)\n",
      "Requirement already satisfied: gdown in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (4.42.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (3.1.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.1.91)\n",
      "Requirement already satisfied: tabulate in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.8.7)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (1.5.10)\n",
      "Requirement already satisfied: ftfy in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (5.8)\n",
      "Requirement already satisfied: lxml in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (4.5.0)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.22.1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.2.5)\n",
      "Requirement already satisfied: regex in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (2020.10.15)\n",
      "Requirement already satisfied: janome in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (1.2.10)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (0.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: langdetect in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (1.0.8)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (3.3.1)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from flair) (3.8.3)\n",
      "Requirement already satisfied: overrides==3.0.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.0.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from gdown->flair) (2.22.0)\n",
      "Requirement already satisfied: six in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from gdown->flair) (1.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from gdown->flair) (3.0.12)\n",
      "Requirement already satisfied: future in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from torch>=1.1.0->flair) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from torch>=1.1.0->flair) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.1.8)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (0.14.1)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->flair) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->flair) (0.8.1rc2)\n",
      "Requirement already satisfied: packaging in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->flair) (20.1)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (0.29.14)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (3.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->flair) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->flair) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->flair) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->flair) (2019.11.28)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests[socks]->gdown->flair) (1.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (45.2.0.post20200210)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.1)\n",
      "Requirement already satisfied: click in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=3.0.0->flair) (7.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (4.42.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kroxyl\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "#Packages : os, spacy, flair\n",
    "\n",
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install flair\n",
    "#!{sys.executable} -m pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based identification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a .txt lexicon with a .tsv as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moba\n",
      "avatar\n",
      "game avatar\n",
      "moba game\n",
      "toxic player\n",
      "toxic and typical player\n",
      "toxic and normal player\n",
      "typical and toxic player\n",
      "player skill\n",
      "player base skill\n",
      "player skill decomposition\n",
      "player skill composition\n",
      "player skill evaluation\n",
      "player skill formation\n",
      "player skill rating\n",
      "player skill analysis\n",
      "team composition\n",
      "dota\n",
      "dota\n",
      "creeps\n",
      "team formation\n",
      "league of legend\n",
      "online game\n",
      "hot\n",
      "dot\n",
      "typical player\n",
      "gameplay\n",
      "enemy team\n",
      "team fight\n",
      "defense\n",
      "in - game\n",
      "riot game\n",
      "blue team\n",
      "red and blue team\n",
      "enemy hero\n",
      "tactical analysis\n",
      "late game\n",
      "player behavior\n",
      "prize\n",
      "champion attribute\n",
      "opposite team\n",
      "play style\n",
      "user report\n",
      "red team\n",
      "red and blue team\n",
      "experienced role\n",
      "inexperienced role\n",
      "minion\n",
      "mana\n",
      "creep\n",
      "easy mode\n",
      "red side\n",
      "blue side\n",
      "normal player\n",
      "novice player\n",
      "lol\n",
      "lol player\n",
      "game match\n",
      "enemy base\n",
      "noob\n",
      "mage\n",
      "matchmaking\n",
      "skill rating\n",
      "win rate\n",
      "low winning rate\n",
      "enemy tower\n",
      "hard mode\n",
      "high skill player\n",
      "target selection\n",
      "gamer\n",
      "opponent team\n",
      "team role\n",
      "amount of gold\n",
      "skillful\n",
      "gameplay style\n",
      "neutral creep\n",
      "rating system\n",
      "elo rating system\n",
      "anti - mage\n",
      "real - time strategy\n",
      "high damage\n",
      "toxicity\n",
      "opposing team\n",
      "champion base skill\n",
      "team win\n",
      "red team win\n",
      "expert player\n",
      "skilled player\n",
      "unskilled player\n",
      "enemy creeps\n",
      "high skill level\n",
      "game mechanic\n",
      "health point\n",
      "win chance\n",
      "in - game period\n",
      "gank\n",
      "team base\n",
      "game experience\n",
      "good decision\n",
      "gank\n",
      "mid game\n",
      "bottom lane\n",
      "combat scenario\n",
      "rank level\n",
      "matchmaking system\n",
      "metagame\n",
      "game mode\n",
      "meta\n",
      "level of player\n",
      "red area\n",
      "target game\n",
      "ally hero\n",
      "online community\n",
      "player statistic\n",
      "team effectiveness\n",
      "griefe\n",
      "main characteristic\n",
      "trash talk\n",
      "real - time\n",
      "game state\n",
      "non - game\n",
      "rank gap\n",
      "anonymous player\n",
      "lol match\n",
      "select player\n",
      "neutral unit\n",
      "screenshot\n",
      "lol champion\n",
      "dota map\n",
      "skilled champion\n",
      "moba match\n",
      "troll\n",
      "jungler\n",
      "tower range\n",
      "allied creeps\n",
      "high damage rating\n",
      "powerful unit\n",
      "hero selection\n",
      "online team formation\n",
      "unskilled player\n",
      "offline\n",
      "tournament mode\n",
      "game highlight\n",
      "solo queue\n",
      "team skill\n",
      "base skill of player\n",
      "game map\n",
      "strategic advantage\n",
      "hit point\n",
      "low hit point\n",
      "major game\n",
      "pick and ban phase\n",
      "split push\n",
      "target selector\n",
      "easy mode\n",
      "set of action\n",
      "stfu noob\n",
      "high - level\n",
      "high level\n",
      "high skill level\n",
      "game tutorial\n",
      "assist\n",
      "tower diving\n",
      "blue area\n",
      "limited attack\n",
      "phase transition\n",
      "player skill formation\n",
      "gamer with similar skill\n",
      "incompetent player\n",
      "silencer\n",
      "special ability\n",
      "game match\n",
      "health factor\n",
      "hybrid\n",
      "killable\n",
      "agility and strength hero\n",
      "game knowledge\n",
      "theorycraft\n",
      "top tier\n",
      "middle game\n",
      "end game\n",
      "player with similar skill\n",
      "chat volume\n",
      "great escape capability\n",
      "view of red team\n",
      "large damage\n",
      "team composition analysis\n",
      "team combat\n",
      "good overall synergy\n",
      "major game update\n",
      "single hero\n",
      "mana resource\n",
      "heavy damage\n",
      "winrate\n",
      "team competition game\n",
      "mana burn skill\n",
      "griefer\n",
      "grief play\n",
      "late game carry\n",
      "player role\n",
      "average skill\n",
      "success factor\n",
      "role category\n",
      "trueskill of player\n",
      "game avatar synergy\n",
      "avatar anti - mage\n",
      "individual skill level\n",
      "elo system\n",
      "elo rating system\n",
      "set of skill\n",
      "skill set\n",
      "type of map\n",
      "middle lane\n",
      "hero status\n",
      "type of match\n",
      "allied tower\n",
      "analysis of league of legend\n",
      "stealthy play style\n",
      "competitive setting\n",
      "kda factor\n",
      "tank carry hero\n",
      "dota game\n",
      "real - time game\n",
      "high burst physical damage\n",
      "trueskill score\n",
      "list of hero\n",
      "special power\n",
      "low winning rate\n",
      "mechanic of game\n",
      "game mechanic\n",
      "hero range\n",
      "game speed\n",
      "losing team\n",
      "jungler\n",
      "specific role\n",
      "team synergy\n",
      "video highlight\n",
      "skill level of player\n",
      "target destination\n",
      "low death rate\n",
      "familiar hero\n",
      "warm - up effect\n",
      "level of synergy\n",
      "report system\n",
      "dota match\n",
      "favorite champion\n",
      "in - game avatar\n",
      "win rate\n",
      "public matchmaking\n",
      "player satisfaction\n",
      "warm - up period\n",
      "league of legend championship\n",
      "random hero\n",
      "experience point\n",
      "common technique\n",
      "strength of player\n",
      "simple game\n",
      "game goal\n",
      "full game\n",
      "ai system\n",
      "great amount of damage\n",
      "complex gameplay\n",
      "champion with high control\n",
      "neutral creep\n",
      "moba competition\n",
      "ability of champion\n",
      "reaction time\n",
      "powerful item\n",
      "game record\n",
      "active player\n",
      "official riot game\n",
      "average skill of player\n",
      "magic attacker\n",
      "jungle invasion\n",
      "champion with good control\n",
      "low control rating\n",
      "newbie\n",
      "veteran player\n",
      "weak hero\n",
      "champion level\n",
      "role attribute\n",
      "main gameplay\n",
      "good kda\n",
      "single target\n",
      "total damage\n",
      "middle of lane\n",
      "team fight strategy\n",
      "game runtime\n",
      "diverse skill\n",
      "low health\n",
      "specific champion\n",
      "small range\n",
      "team skill level\n",
      "user interface\n",
      "blind mode\n",
      "opponent player\n",
      "select match\n",
      "team cooperation\n",
      "report noob\n",
      "team push\n",
      "game style\n",
      "item building\n",
      "player -PRON- d\n",
      "cheater\n",
      "physical attacker\n",
      "game interface\n",
      "deal damage\n",
      "matchup\n",
      "aram\n",
      "movement system\n",
      "strategy\n",
      "specific skill of player\n",
      "regular attack\n",
      "player ability\n",
      "kill streak\n",
      "game time\n",
      "chat room\n",
      "average damage\n",
      "custom map\n",
      "intercept\n",
      "minion\n",
      "area - of - effect\n",
      "respawn\n",
      "top tier player\n",
      "non - player\n",
      "small number of assist\n",
      "mobile device\n",
      "powerful skill\n",
      "single layer\n",
      "fall\n",
      "low number of kill\n",
      "real - time reasoning\n",
      "time penalty\n",
      "character - level\n",
      "main hero\n",
      "low damage\n",
      "power play\n",
      "low level account\n",
      "bounty rune\n",
      "elo rating system\n",
      "balanced game\n",
      "hero unit\n",
      "low skill level\n",
      "skill set\n",
      "degrade champion status\n",
      "moba phase\n",
      "team - base role - playing game\n",
      "high number of kill\n",
      "team composition of champion\n",
      "league of legend match\n",
      "abusive language\n",
      "competitive video game\n",
      "summoner\n",
      "team performance\n",
      "early game\n",
      "teamfight\n",
      "team fight\n",
      "inexperienced role\n",
      "esports game\n",
      "late - game\n",
      "late game\n",
      "skill level\n",
      "low skill level\n",
      "similar skill level\n",
      "auto - attack\n",
      "large esport game\n",
      "co - op with ai\n",
      "opponent base\n",
      "laner\n",
      "armory\n",
      "esport journalist\n",
      "support\n",
      "mid - laner\n",
      "strategic decision - making\n",
      "high - rank\n",
      "low kda\n",
      "top - laner\n",
      "average number of kill\n",
      "matchmaking rating\n",
      "performer player\n",
      "doublekill\n",
      "last - hitting\n",
      "team - fight\n",
      "team fight\n",
      "micro - transaction\n",
      "minion behavior\n",
      "infernal shrine\n",
      "counter - picking\n",
      "solo - queue mmr\n",
      "three - lane map\n",
      "sub - genre of rts\n",
      "dragon shire\n",
      "aatrox\n",
      "massive - multiplayer online game\n",
      "leveling\n",
      "spotts\n",
      "consumable\n",
      "honor\n",
      "totaldamagetaken\n",
      "total damage\n",
      "moba community\n",
      "videogame\n",
      "video game\n",
      "champlevel\n",
      "champion level\n",
      "shen\n",
      "spamme\n",
      "lane push\n",
      "elo - like rating\n",
      "aggressive play\n",
      "competition\n",
      "base damage\n",
      "mid - game\n",
      "mid game\n",
      "highly - ranked\n",
      "small pool of health\n",
      "triplekill\n",
      "live - streaming platform\n",
      "ingame\n",
      "undead\n",
      "armor\n",
      "warrior - mage\n",
      "smurf\n",
      "minion ai\n",
      "gamebots\n",
      "base - building\n",
      "mid - season\n",
      "kill\n",
      "position\n",
      "individual player\n",
      "agility\n",
      "individual skill\n",
      "turret\n",
      "tutorial\n",
      "physical damage\n",
      "burst physical damage\n",
      "competitive game\n",
      "competitive video game\n",
      "amount of damage\n",
      "chat\n",
      "assassin\n",
      "combat\n",
      "cooperation\n",
      "team member\n",
      "visual effect\n",
      "hero\n",
      "pick\n",
      "enemy\n",
      "highlight\n",
      "interface\n",
      "mechanic\n",
      "server\n",
      "skilled\n",
      "carry\n",
      "surrender\n",
      "rune\n",
      "feeder\n",
      "passive\n",
      "user\n",
      "evolution\n",
      "melee\n",
      "mobility\n",
      "bad game\n",
      "entity\n",
      "riot\n",
      "teamwork\n",
      "lot of damage\n",
      "aggro\n",
      "skill\n",
      "game\n",
      "response time\n",
      "stat\n",
      "player\n",
      "tier\n",
      "lane\n",
      "mechanism\n",
      "dungeon\n",
      "duel\n",
      "teammate\n",
      "novice\n",
      "tower\n",
      "update\n",
      "jungle\n",
      "twitch\n",
      "real - world\n",
      "beginner\n",
      "sentinel\n",
      "input\n",
      "match\n",
      "hide\n",
      "progression\n",
      "mute\n",
      "strategy\n",
      "technique\n",
      "gold\n",
      "draft\n",
      "coop\n",
      "decision - making\n",
      "gold\n",
      "team\n",
      "main target\n",
      "animation\n",
      "glitch\n",
      "ally\n",
      "competitive\n",
      "good game\n",
      "champion\n",
      "role\n",
      "frame\n",
      "solo\n",
      "greed\n",
      "opponent\n",
      "base\n",
      "item\n",
      "rank\n",
      "experience\n",
      "battle\n",
      "potion\n",
      "communication\n",
      "spell\n",
      "control\n",
      "damage\n",
      "strength\n",
      "crystal\n",
      "ability\n",
      "obstacle\n",
      "utility\n",
      "level\n",
      "powerful\n",
      "cheat\n",
      "battlefield\n",
      "sustainability\n",
      "trap\n",
      "character\n",
      "focus\n",
      "leaver\n",
      "hacker\n",
      "wide range\n",
      "buff\n",
      "warrior\n",
      "weak\n",
      "weakness\n",
      "resource\n",
      "objective\n",
      "invisible\n",
      "fight\n",
      "capture\n",
      "arrow\n",
      "bottom\n",
      "effect\n",
      "play\n",
      "object\n",
      "impact\n",
      "advantage\n",
      "unit\n",
      "range\n",
      "farming\n",
      "split\n",
      "harassment\n",
      "session\n",
      "path\n",
      "tank\n",
      "healing\n",
      "high level\n",
      "blind\n",
      "target\n",
      "status\n",
      "early\n",
      "hard\n",
      "movement\n",
      "middle\n",
      "rift\n",
      "rank\n",
      "commentator\n",
      "tactic\n",
      "message\n",
      "death\n",
      "easy\n",
      "competition\n",
      "class\n",
      "fighter\n",
      "starter\n",
      "goal\n",
      "community\n",
      "bard\n",
      "execution\n",
      "scoreboard\n",
      "kill\n",
      "smoke\n",
      "poor\n",
      "connection\n",
      "judgment\n",
      "trigger\n",
      "rule\n",
      "world\n",
      "full\n",
      "master\n",
      "report\n",
      "greed\n",
      "distant\n",
      "support\n",
      "close\n",
      "dash\n",
      "zone\n",
      "point\n",
      "reward\n",
      "rush\n",
      "location\n",
      "stage\n",
      "tournament\n",
      "time\n",
      "attack\n",
      "script\n",
      "group\n",
      "bush\n",
      "victim\n",
      "field\n",
      "wave\n",
      "turn\n",
      "open\n",
      "green\n",
      "ping\n",
      "alert\n",
      "chance\n",
      "forest\n",
      "victory\n",
      "power\n",
      "obvious\n",
      "brain\n",
      "safe\n",
      "mistake\n",
      "hard\n",
      "train\n",
      "straight\n",
      "retreat\n",
      "area\n",
      "mark\n",
      "viewer\n",
      "clear\n",
      "piece\n",
      "disadvantage\n",
      "spirit\n",
      "protection\n",
      "host\n",
      "start\n",
      "cost\n",
      "increase\n",
      "loser\n",
      "timing\n",
      "silver\n",
      "winner\n",
      "blue\n",
      "shield\n",
      "worth\n",
      "flame\n",
      "move\n",
      "drop\n",
      "heath\n",
      "danger\n",
      "free\n",
      "energy\n",
      "coach\n",
      "equipment\n",
      "slow\n",
      "boost\n",
      "farm\n",
      "killer\n",
      "recovery\n",
      "castle\n",
      "mission\n",
      "ward\n",
      "river\n",
      "lead\n",
      "spot\n",
      "call\n",
      "season\n",
      "defeat\n",
      "farmer\n",
      "leader\n"
     ]
    }
   ],
   "source": [
    "def clean_term():\n",
    "    f = open(\"TermsTSV/MOBA-en.tsv\", \"r\")\n",
    "    raw = f.read()\n",
    "    \n",
    "    #get term raw of the tsv file\n",
    "    raw_terms = re.findall('\\:\\s(.*[a-z])', raw)\n",
    "\n",
    "    result = []\n",
    "    #loop on every terms\n",
    "    for item in raw_terms:\n",
    "        lemmatized_term = ''\n",
    "        #loop on every token of the term\n",
    "        doc = nlp(item)\n",
    "        for token in doc:\n",
    "            lemmatized_term += token.lemma_ + ' '\n",
    "            \n",
    "        print(lemmatized_term[:-1]) #-1 to remove the last white space\n",
    "        \n",
    "        #to avoid doublon\n",
    "        if lemmatized_term not in result:\n",
    "            result.append(lemmatized_term)\n",
    "            \n",
    "    #write all terms in a file\n",
    "    with open('MOBA-en-lexicon.txt', 'w') as f:\n",
    "        for item in result:\n",
    "            f.write(item+\"\\n\")\n",
    "            \n",
    "clean_term()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get functions (lexicon, article.txt, list of terms in the lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon():\n",
    "    f = open(\"MOBA-en-lexicon.txt\", \"r\")\n",
    "    lexicon = f.read().splitlines() #use splitlines to not take '\\n' char\n",
    "    #return a list of splited list (to ease the further process)\n",
    "    return [item.split() for item in lexicon]\n",
    "\n",
    "def get_doc(file_name):\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    print(file_name, '|| Text length: ',len(text))\n",
    "    return text\n",
    "\n",
    "def get_possible_term(lexicon, word):\n",
    "    possible_term = []\n",
    "    for item in lexicon:\n",
    "        if word == item[0]:\n",
    "            possible_term.append(item)\n",
    "    return possible_term\n",
    "\n",
    "lexicon = get_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to create an IOB annotated .txt file\n",
    "# Manage one joker word / don't manage syntactic variants\n",
    "def annotate_doc(txt, f):\n",
    "    \n",
    "    txt_pos=0\n",
    "    while txt_pos < len(txt):#in range(0, len(txt)):\n",
    "        word = str(txt[txt_pos].lemma_)\n",
    "        #print(\">>\", word)\n",
    "        \n",
    "        possible_terms = get_possible_term(lexicon, word)\n",
    "        match = possible_terms\n",
    "        #print(\"Possible: \",match)\n",
    "        \n",
    "        # If there is possible_terms\n",
    "        if possible_terms is not None:\n",
    "            \n",
    "            to_remove = []\n",
    "            joker = True\n",
    "            joker_pos = None\n",
    "            joker_word = None\n",
    "            # loop on them\n",
    "            for term in possible_terms:\n",
    "                #print(\">\", term)\n",
    "                \n",
    "                # Process for each term word\n",
    "                for term_pos in range(0, len(term)):\n",
    "                \n",
    "                    #check if we dont go out the text length\n",
    "                    if txt_pos+term_pos < len(txt):\n",
    "                        #check if word term are not the same of the text word\n",
    "                        if term[term_pos] != str(txt[txt_pos+term_pos].lemma_):\n",
    "                            #if not the same but the next is the same, means that still have to process (rule)\n",
    "                            if term[term_pos] == str(txt[txt_pos+term_pos + 1].lemma_) and joker is True:\n",
    "                                #joker to False because we can only do that one time\n",
    "                                joker = False\n",
    "                                joker_pos = term_pos\n",
    "                                joker_word = str(txt[txt_pos+term_pos])\n",
    "                                break\n",
    "                            else:\n",
    "                                to_remove.append(term)\n",
    "                                break\n",
    "                    else:\n",
    "                        to_remove.append(term)\n",
    "                        break\n",
    "            \n",
    "            #remove all the terms that didn't match\n",
    "            for t in to_remove:\n",
    "                match.remove(t)\n",
    "                \n",
    "            # if there is a match\n",
    "            if len(match)>0:\n",
    "                #take the longuest\n",
    "                real_match = max(match, key=len)\n",
    "                #print(\"Real Match: \", real_match)\n",
    "                \n",
    "                # and jump to the next word\n",
    "                if joker is False: \n",
    "                    f.write(str(txt[txt_pos]) + \" B\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos]) + \" B\\n\")\n",
    "                    for i in range(1, len(real_match)+1):\n",
    "                        if joker_pos == i:\n",
    "                            f.write(joker_word + \" O\\n\")\n",
    "                            #print(\"Write: \" + joker_word + \" O\\n\")\n",
    "                        else: \n",
    "                            f.write(str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                            #print(\"Write: \" + str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                    txt_pos += len(real_match)+1\n",
    "                        \n",
    "                else: \n",
    "                    \n",
    "                    f.write(str(txt[txt_pos]) + \" B\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos]) + \" B\\n\")\n",
    "                    for i in range(1, len(real_match)):\n",
    "                        f.write(str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                        #print(\"Write: \" + str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                    txt_pos += len(real_match)\n",
    "            else:\n",
    "                if word is not \"\\n\":\n",
    "                    f.write(str(txt[txt_pos])+\" O\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos])+\" O\\n\")\n",
    "                txt_pos += 1\n",
    "                if word is \".\": f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the annotated Corpus with IOB (.txt)\n",
    "#### COMMENT the last line if you don't want to run it (already train.txt already exist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_01.txt\n",
      "Articles/txt/doc_01.txt || Text length:  17603\n",
      "doc_02.txt\n",
      "Articles/txt/doc_02.txt || Text length:  29695\n",
      "doc_03.txt\n",
      "Articles/txt/doc_03.txt || Text length:  70327\n",
      "doc_04.txt\n",
      "Articles/txt/doc_04.txt || Text length:  22563\n",
      "doc_05.txt\n",
      "Articles/txt/doc_05.txt || Text length:  23758\n",
      "doc_06.txt\n",
      "Articles/txt/doc_06.txt || Text length:  32981\n",
      "doc_07.txt\n",
      "Articles/txt/doc_07.txt || Text length:  18915\n",
      "doc_08.txt\n",
      "Articles/txt/doc_08.txt || Text length:  45445\n",
      "doc_09.txt\n",
      "Articles/txt/doc_09.txt || Text length:  42509\n",
      "doc_10.txt\n",
      "Articles/txt/doc_10.txt || Text length:  43999\n",
      "doc_11.txt\n",
      "Articles/txt/doc_11.txt || Text length:  54214\n",
      "doc_12.txt\n",
      "Articles/txt/doc_12.txt || Text length:  78258\n",
      "doc_13.txt\n",
      "Articles/txt/doc_13.txt || Text length:  27403\n",
      "doc_14.txt\n",
      "Articles/txt/doc_14.txt || Text length:  25424\n",
      "doc_15.txt\n",
      "Articles/txt/doc_15.txt || Text length:  38869\n",
      "doc_16.txt\n",
      "Articles/txt/doc_16.txt || Text length:  38604\n",
      "doc_17.txt\n",
      "Articles/txt/doc_17.txt || Text length:  63251\n",
      "doc_18.txt\n",
      "Articles/txt/doc_18.txt || Text length:  51534\n",
      "doc_19.txt\n",
      "Articles/txt/doc_19.txt || Text length:  60509\n",
      "doc_20.txt\n",
      "Articles/txt/doc_20.txt || Text length:  43553\n"
     ]
    }
   ],
   "source": [
    "# Create TRAIN.TXT file\n",
    "def train():\n",
    "    f = open('Corpus/train.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[0:20]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_21.txt\n",
      "Articles/txt/doc_21.txt || Text length:  43704\n",
      "doc_22.txt\n",
      "Articles/txt/doc_22.txt || Text length:  20314\n",
      "doc_23.txt\n",
      "Articles/txt/doc_23.txt || Text length:  18202\n",
      "doc_24.txt\n",
      "Articles/txt/doc_24.txt || Text length:  42682\n",
      "doc_25.txt\n",
      "Articles/txt/doc_25.txt || Text length:  51958\n"
     ]
    }
   ],
   "source": [
    "# Create TEST.TXT file\n",
    "def test():\n",
    "    f = open('Corpus/test.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[20:25]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_26.txt\n",
      "Articles/txt/doc_26.txt || Text length:  60221\n",
      "doc_27.txt\n",
      "Articles/txt/doc_27.txt || Text length:  51510\n",
      "doc_28.txt\n",
      "Articles/txt/doc_28.txt || Text length:  92987\n",
      "doc_29.txt\n",
      "Articles/txt/doc_29.txt || Text length:  55843\n",
      "doc_30.txt\n",
      "Articles/txt/doc_30.txt || Text length:  19363\n"
     ]
    }
   ],
   "source": [
    "# Create DEV.TXT file\n",
    "def dev():\n",
    "    f = open('Corpus/dev.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[25:30]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want \n",
    "dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Your Own Sequence Labeling Dataset using FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-19 16:53:28,342 Reading data from Corpus\n",
      "2020-11-19 16:53:28,343 Train: Corpus\\train.txt\n",
      "2020-11-19 16:53:28,343 Dev: Corpus\\dev.txt\n",
      "2020-11-19 16:53:28,344 Test: Corpus\\test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'iob'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = './Corpus/'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8001"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Analysis of DOTA2 Using Game Refinement Measure .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[4].to_tagged_string('iob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Sequence Labeling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 8001 train + 2092 dev + 1577 test sentences\n",
      "Dictionary with 6 tags: <unk>, O, B, I, <START>, <STOP>\n",
      "2020-11-19 16:53:32,363 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,364 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=6, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-11-19 16:53:32,364 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,365 Corpus: \"Corpus: 8001 train + 2092 dev + 1577 test sentences\"\n",
      "2020-11-19 16:53:32,366 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,366 Parameters:\n",
      "2020-11-19 16:53:32,366  - learning_rate: \"0.1\"\n",
      "2020-11-19 16:53:32,367  - mini_batch_size: \"32\"\n",
      "2020-11-19 16:53:32,367  - patience: \"3\"\n",
      "2020-11-19 16:53:32,368  - anneal_factor: \"0.5\"\n",
      "2020-11-19 16:53:32,370  - max_epochs: \"10\"\n",
      "2020-11-19 16:53:32,370  - shuffle: \"True\"\n",
      "2020-11-19 16:53:32,370  - train_with_dev: \"False\"\n",
      "2020-11-19 16:53:32,371  - batch_growth_annealing: \"False\"\n",
      "2020-11-19 16:53:32,371 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,372 Model training base path: \"resources\\taggers\\example-pos\"\n",
      "2020-11-19 16:53:32,372 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,373 Device: cuda:0\n",
      "2020-11-19 16:53:32,373 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:32,373 Embeddings storage mode: gpu\n",
      "2020-11-19 16:53:32,376 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:53:35,695 epoch 1 - iter 25/251 - loss 9.16231195 - samples/sec: 241.29 - lr: 0.100000\n",
      "2020-11-19 16:53:38,931 epoch 1 - iter 50/251 - loss 7.69451982 - samples/sec: 247.30 - lr: 0.100000\n",
      "2020-11-19 16:53:41,702 epoch 1 - iter 75/251 - loss 6.95743553 - samples/sec: 288.89 - lr: 0.100000\n",
      "2020-11-19 16:53:44,360 epoch 1 - iter 100/251 - loss 6.54611305 - samples/sec: 301.11 - lr: 0.100000\n",
      "2020-11-19 16:53:47,437 epoch 1 - iter 125/251 - loss 6.29437189 - samples/sec: 260.10 - lr: 0.100000\n",
      "2020-11-19 16:53:50,584 epoch 1 - iter 150/251 - loss 6.06966309 - samples/sec: 254.65 - lr: 0.100000\n",
      "2020-11-19 16:53:53,437 epoch 1 - iter 175/251 - loss 5.90821736 - samples/sec: 280.32 - lr: 0.100000\n",
      "2020-11-19 16:53:56,291 epoch 1 - iter 200/251 - loss 5.78236252 - samples/sec: 280.40 - lr: 0.100000\n",
      "2020-11-19 16:53:59,239 epoch 1 - iter 225/251 - loss 5.68588953 - samples/sec: 271.61 - lr: 0.100000\n",
      "2020-11-19 16:54:02,236 epoch 1 - iter 250/251 - loss 5.58254540 - samples/sec: 267.20 - lr: 0.100000\n",
      "2020-11-19 16:54:02,257 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:54:02,258 EPOCH 1 done: loss 5.5883 - lr 0.1000000\n",
      "2020-11-19 16:54:06,780 DEV : loss 5.412528991699219 - score 0.9346\n",
      "2020-11-19 16:54:06,792 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:54:09,607 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:54:12,230 epoch 2 - iter 25/251 - loss 4.52645606 - samples/sec: 305.06 - lr: 0.100000\n",
      "2020-11-19 16:54:15,026 epoch 2 - iter 50/251 - loss 4.55441081 - samples/sec: 286.33 - lr: 0.100000\n",
      "2020-11-19 16:54:17,573 epoch 2 - iter 75/251 - loss 4.46482254 - samples/sec: 314.42 - lr: 0.100000\n",
      "2020-11-19 16:54:20,406 epoch 2 - iter 100/251 - loss 4.53466172 - samples/sec: 282.48 - lr: 0.100000\n",
      "2020-11-19 16:54:22,996 epoch 2 - iter 125/251 - loss 4.53275789 - samples/sec: 308.81 - lr: 0.100000\n",
      "2020-11-19 16:54:26,076 epoch 2 - iter 150/251 - loss 4.59962906 - samples/sec: 259.81 - lr: 0.100000\n",
      "2020-11-19 16:54:28,637 epoch 2 - iter 175/251 - loss 4.58293205 - samples/sec: 312.47 - lr: 0.100000\n",
      "2020-11-19 16:54:31,229 epoch 2 - iter 200/251 - loss 4.53865607 - samples/sec: 308.79 - lr: 0.100000\n",
      "2020-11-19 16:54:33,953 epoch 2 - iter 225/251 - loss 4.54280787 - samples/sec: 293.82 - lr: 0.100000\n",
      "2020-11-19 16:54:36,426 epoch 2 - iter 250/251 - loss 4.50920373 - samples/sec: 323.67 - lr: 0.100000\n",
      "2020-11-19 16:54:36,451 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:54:36,452 EPOCH 2 done: loss 4.5136 - lr 0.1000000\n",
      "2020-11-19 16:54:40,099 DEV : loss 3.884171962738037 - score 0.949\n",
      "2020-11-19 16:54:40,112 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:54:42,773 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:54:45,432 epoch 3 - iter 25/251 - loss 4.39472754 - samples/sec: 301.05 - lr: 0.100000\n",
      "2020-11-19 16:54:48,152 epoch 3 - iter 50/251 - loss 4.17868509 - samples/sec: 294.26 - lr: 0.100000\n",
      "2020-11-19 16:54:50,869 epoch 3 - iter 75/251 - loss 4.26784523 - samples/sec: 294.64 - lr: 0.100000\n",
      "2020-11-19 16:54:53,798 epoch 3 - iter 100/251 - loss 4.32468420 - samples/sec: 273.30 - lr: 0.100000\n",
      "2020-11-19 16:54:56,404 epoch 3 - iter 125/251 - loss 4.32837472 - samples/sec: 307.05 - lr: 0.100000\n",
      "2020-11-19 16:54:59,012 epoch 3 - iter 150/251 - loss 4.29285764 - samples/sec: 307.00 - lr: 0.100000\n",
      "2020-11-19 16:55:01,437 epoch 3 - iter 175/251 - loss 4.26684198 - samples/sec: 330.07 - lr: 0.100000\n",
      "2020-11-19 16:55:04,192 epoch 3 - iter 200/251 - loss 4.26183987 - samples/sec: 290.42 - lr: 0.100000\n",
      "2020-11-19 16:55:06,855 epoch 3 - iter 225/251 - loss 4.24946516 - samples/sec: 300.81 - lr: 0.100000\n",
      "2020-11-19 16:55:09,529 epoch 3 - iter 250/251 - loss 4.23916864 - samples/sec: 299.58 - lr: 0.100000\n",
      "2020-11-19 16:55:09,547 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:55:09,548 EPOCH 3 done: loss 4.2343 - lr 0.1000000\n",
      "2020-11-19 16:55:13,139 DEV : loss 3.7232940196990967 - score 0.951\n",
      "2020-11-19 16:55:13,151 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:55:15,741 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:55:18,144 epoch 4 - iter 25/251 - loss 4.33030399 - samples/sec: 333.06 - lr: 0.100000\n",
      "2020-11-19 16:55:20,587 epoch 4 - iter 50/251 - loss 4.13359451 - samples/sec: 327.77 - lr: 0.100000\n",
      "2020-11-19 16:55:23,407 epoch 4 - iter 75/251 - loss 4.06596039 - samples/sec: 283.85 - lr: 0.100000\n",
      "2020-11-19 16:55:26,042 epoch 4 - iter 100/251 - loss 4.08452758 - samples/sec: 303.96 - lr: 0.100000\n",
      "2020-11-19 16:55:28,789 epoch 4 - iter 125/251 - loss 4.07463614 - samples/sec: 291.47 - lr: 0.100000\n",
      "2020-11-19 16:55:31,349 epoch 4 - iter 150/251 - loss 4.06147498 - samples/sec: 312.60 - lr: 0.100000\n",
      "2020-11-19 16:55:33,893 epoch 4 - iter 175/251 - loss 4.03552038 - samples/sec: 314.69 - lr: 0.100000\n",
      "2020-11-19 16:55:36,724 epoch 4 - iter 200/251 - loss 4.07103381 - samples/sec: 282.80 - lr: 0.100000\n",
      "2020-11-19 16:55:39,306 epoch 4 - iter 225/251 - loss 4.06241265 - samples/sec: 310.02 - lr: 0.100000\n",
      "2020-11-19 16:55:41,759 epoch 4 - iter 250/251 - loss 4.03998633 - samples/sec: 326.34 - lr: 0.100000\n",
      "2020-11-19 16:55:41,795 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:55:41,796 EPOCH 4 done: loss 4.0488 - lr 0.1000000\n",
      "2020-11-19 16:55:45,421 DEV : loss 3.687502145767212 - score 0.9502\n",
      "2020-11-19 16:55:45,434 BAD EPOCHS (no improvement): 1\n",
      "2020-11-19 16:55:45,435 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:55:47,852 epoch 5 - iter 25/251 - loss 4.07924199 - samples/sec: 331.18 - lr: 0.100000\n",
      "2020-11-19 16:55:50,392 epoch 5 - iter 50/251 - loss 3.92714052 - samples/sec: 315.25 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-19 16:55:53,209 epoch 5 - iter 75/251 - loss 3.98351756 - samples/sec: 284.06 - lr: 0.100000\n",
      "2020-11-19 16:55:55,742 epoch 5 - iter 100/251 - loss 3.90487113 - samples/sec: 315.93 - lr: 0.100000\n",
      "2020-11-19 16:55:58,125 epoch 5 - iter 125/251 - loss 3.88102746 - samples/sec: 335.94 - lr: 0.100000\n",
      "2020-11-19 16:56:00,799 epoch 5 - iter 150/251 - loss 3.84390698 - samples/sec: 299.31 - lr: 0.100000\n",
      "2020-11-19 16:56:03,531 epoch 5 - iter 175/251 - loss 3.85582840 - samples/sec: 292.93 - lr: 0.100000\n",
      "2020-11-19 16:56:06,376 epoch 5 - iter 200/251 - loss 3.89857929 - samples/sec: 281.35 - lr: 0.100000\n",
      "2020-11-19 16:56:08,962 epoch 5 - iter 225/251 - loss 3.91408826 - samples/sec: 309.41 - lr: 0.100000\n",
      "2020-11-19 16:56:11,590 epoch 5 - iter 250/251 - loss 3.89550693 - samples/sec: 304.54 - lr: 0.100000\n",
      "2020-11-19 16:56:11,693 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:56:11,694 EPOCH 5 done: loss 3.9499 - lr 0.1000000\n",
      "2020-11-19 16:56:15,350 DEV : loss 3.0893001556396484 - score 0.9568\n",
      "2020-11-19 16:56:15,363 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:56:17,898 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:56:20,279 epoch 6 - iter 25/251 - loss 3.74678477 - samples/sec: 336.22 - lr: 0.100000\n",
      "2020-11-19 16:56:22,869 epoch 6 - iter 50/251 - loss 3.77511919 - samples/sec: 309.37 - lr: 0.100000\n",
      "2020-11-19 16:56:25,450 epoch 6 - iter 75/251 - loss 3.77375293 - samples/sec: 310.18 - lr: 0.100000\n",
      "2020-11-19 16:56:28,190 epoch 6 - iter 100/251 - loss 3.79860700 - samples/sec: 292.02 - lr: 0.100000\n",
      "2020-11-19 16:56:30,891 epoch 6 - iter 125/251 - loss 3.78683604 - samples/sec: 296.31 - lr: 0.100000\n",
      "2020-11-19 16:56:33,614 epoch 6 - iter 150/251 - loss 3.77348911 - samples/sec: 293.72 - lr: 0.100000\n",
      "2020-11-19 16:56:36,120 epoch 6 - iter 175/251 - loss 3.76599417 - samples/sec: 319.45 - lr: 0.100000\n",
      "2020-11-19 16:56:38,620 epoch 6 - iter 200/251 - loss 3.76593035 - samples/sec: 320.21 - lr: 0.100000\n",
      "2020-11-19 16:56:41,207 epoch 6 - iter 225/251 - loss 3.77179544 - samples/sec: 309.39 - lr: 0.100000\n",
      "2020-11-19 16:56:43,781 epoch 6 - iter 250/251 - loss 3.76600646 - samples/sec: 310.91 - lr: 0.100000\n",
      "2020-11-19 16:56:43,804 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:56:43,805 EPOCH 6 done: loss 3.7773 - lr 0.1000000\n",
      "2020-11-19 16:56:47,448 DEV : loss 3.136033296585083 - score 0.9564\n",
      "2020-11-19 16:56:47,462 BAD EPOCHS (no improvement): 1\n",
      "2020-11-19 16:56:47,463 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:56:50,392 epoch 7 - iter 25/251 - loss 3.65230149 - samples/sec: 273.39 - lr: 0.100000\n",
      "2020-11-19 16:56:52,836 epoch 7 - iter 50/251 - loss 3.58583484 - samples/sec: 327.58 - lr: 0.100000\n",
      "2020-11-19 16:56:55,714 epoch 7 - iter 75/251 - loss 3.66843181 - samples/sec: 278.02 - lr: 0.100000\n",
      "2020-11-19 16:56:58,245 epoch 7 - iter 100/251 - loss 3.71213893 - samples/sec: 316.42 - lr: 0.100000\n",
      "2020-11-19 16:57:01,292 epoch 7 - iter 125/251 - loss 3.69849759 - samples/sec: 262.74 - lr: 0.100000\n",
      "2020-11-19 16:57:03,900 epoch 7 - iter 150/251 - loss 3.68318696 - samples/sec: 306.88 - lr: 0.100000\n",
      "2020-11-19 16:57:06,525 epoch 7 - iter 175/251 - loss 3.66223755 - samples/sec: 304.88 - lr: 0.100000\n",
      "2020-11-19 16:57:09,585 epoch 7 - iter 200/251 - loss 3.66544106 - samples/sec: 261.42 - lr: 0.100000\n",
      "2020-11-19 16:57:12,019 epoch 7 - iter 225/251 - loss 3.66860411 - samples/sec: 329.04 - lr: 0.100000\n",
      "2020-11-19 16:57:14,684 epoch 7 - iter 250/251 - loss 3.65516331 - samples/sec: 300.26 - lr: 0.100000\n",
      "2020-11-19 16:57:14,705 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:57:14,705 EPOCH 7 done: loss 3.6407 - lr 0.1000000\n",
      "2020-11-19 16:57:18,500 DEV : loss 2.9495975971221924 - score 0.9593\n",
      "2020-11-19 16:57:18,514 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:57:21,110 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:57:23,654 epoch 8 - iter 25/251 - loss 3.70820907 - samples/sec: 314.61 - lr: 0.100000\n",
      "2020-11-19 16:57:26,759 epoch 8 - iter 50/251 - loss 3.65750893 - samples/sec: 257.95 - lr: 0.100000\n",
      "2020-11-19 16:57:29,476 epoch 8 - iter 75/251 - loss 3.63335808 - samples/sec: 294.53 - lr: 0.100000\n",
      "2020-11-19 16:57:32,017 epoch 8 - iter 100/251 - loss 3.56723663 - samples/sec: 314.93 - lr: 0.100000\n",
      "2020-11-19 16:57:34,472 epoch 8 - iter 125/251 - loss 3.65381723 - samples/sec: 326.07 - lr: 0.100000\n",
      "2020-11-19 16:57:37,134 epoch 8 - iter 150/251 - loss 3.63398904 - samples/sec: 300.87 - lr: 0.100000\n",
      "2020-11-19 16:57:39,879 epoch 8 - iter 175/251 - loss 3.64432695 - samples/sec: 291.61 - lr: 0.100000\n",
      "2020-11-19 16:57:42,739 epoch 8 - iter 200/251 - loss 3.62444948 - samples/sec: 279.77 - lr: 0.100000\n",
      "2020-11-19 16:57:45,350 epoch 8 - iter 225/251 - loss 3.61061714 - samples/sec: 306.68 - lr: 0.100000\n",
      "2020-11-19 16:57:47,760 epoch 8 - iter 250/251 - loss 3.59816855 - samples/sec: 332.05 - lr: 0.100000\n",
      "2020-11-19 16:57:47,783 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:57:47,784 EPOCH 8 done: loss 3.5897 - lr 0.1000000\n",
      "2020-11-19 16:57:55,435 DEV : loss 2.8404431343078613 - score 0.9599\n",
      "2020-11-19 16:57:55,449 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:57:58,050 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:58:01,030 epoch 9 - iter 25/251 - loss 3.54526067 - samples/sec: 268.68 - lr: 0.100000\n",
      "2020-11-19 16:58:03,519 epoch 9 - iter 50/251 - loss 3.50168707 - samples/sec: 321.50 - lr: 0.100000\n",
      "2020-11-19 16:58:06,185 epoch 9 - iter 75/251 - loss 3.55355115 - samples/sec: 300.28 - lr: 0.100000\n",
      "2020-11-19 16:58:08,693 epoch 9 - iter 100/251 - loss 3.54370013 - samples/sec: 319.01 - lr: 0.100000\n",
      "2020-11-19 16:58:11,189 epoch 9 - iter 125/251 - loss 3.56241125 - samples/sec: 320.73 - lr: 0.100000\n",
      "2020-11-19 16:58:14,239 epoch 9 - iter 150/251 - loss 3.56132722 - samples/sec: 262.43 - lr: 0.100000\n",
      "2020-11-19 16:58:16,791 epoch 9 - iter 175/251 - loss 3.51831393 - samples/sec: 313.58 - lr: 0.100000\n",
      "2020-11-19 16:58:19,700 epoch 9 - iter 200/251 - loss 3.50526539 - samples/sec: 275.14 - lr: 0.100000\n",
      "2020-11-19 16:58:22,469 epoch 9 - iter 225/251 - loss 3.51972185 - samples/sec: 289.13 - lr: 0.100000\n",
      "2020-11-19 16:58:24,941 epoch 9 - iter 250/251 - loss 3.48429520 - samples/sec: 323.80 - lr: 0.100000\n",
      "2020-11-19 16:58:24,983 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:58:24,983 EPOCH 9 done: loss 3.5087 - lr 0.1000000\n",
      "2020-11-19 16:58:28,803 DEV : loss 2.7531497478485107 - score 0.9609\n",
      "2020-11-19 16:58:28,816 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-19 16:58:31,362 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:58:33,874 epoch 10 - iter 25/251 - loss 3.14056262 - samples/sec: 318.82 - lr: 0.100000\n",
      "2020-11-19 16:58:36,501 epoch 10 - iter 50/251 - loss 3.29343952 - samples/sec: 304.53 - lr: 0.100000\n",
      "2020-11-19 16:58:39,045 epoch 10 - iter 75/251 - loss 3.42203161 - samples/sec: 314.73 - lr: 0.100000\n",
      "2020-11-19 16:58:41,605 epoch 10 - iter 100/251 - loss 3.41409699 - samples/sec: 312.63 - lr: 0.100000\n",
      "2020-11-19 16:58:44,055 epoch 10 - iter 125/251 - loss 3.45130079 - samples/sec: 327.07 - lr: 0.100000\n",
      "2020-11-19 16:58:46,255 epoch 10 - iter 150/251 - loss 3.42534003 - samples/sec: 363.99 - lr: 0.100000\n",
      "2020-11-19 16:58:48,760 epoch 10 - iter 175/251 - loss 3.42715728 - samples/sec: 319.81 - lr: 0.100000\n",
      "2020-11-19 16:58:51,297 epoch 10 - iter 200/251 - loss 3.43582438 - samples/sec: 315.50 - lr: 0.100000\n",
      "2020-11-19 16:58:54,128 epoch 10 - iter 225/251 - loss 3.46129667 - samples/sec: 282.78 - lr: 0.100000\n",
      "2020-11-19 16:58:56,513 epoch 10 - iter 250/251 - loss 3.45057053 - samples/sec: 335.50 - lr: 0.100000\n",
      "2020-11-19 16:58:56,541 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-19 16:58:56,542 EPOCH 10 done: loss 3.4684 - lr 0.1000000\n",
      "2020-11-19 16:59:00,091 DEV : loss 3.035252094268799 - score 0.9595\n",
      "2020-11-19 16:59:00,104 BAD EPOCHS (no improvement): 1\n",
      "2020-11-19 16:59:02,583 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-19 16:59:02,583 Testing using best model ...\n",
      "2020-11-19 16:59:02,584 loading file resources\\taggers\\example-pos\\best-model.pt\n",
      "2020-11-19 16:59:06,262 \t0.9572\n",
      "2020-11-19 16:59:06,262 \n",
      "Results:\n",
      "- F-score (micro): 0.9572\n",
      "- F-score (macro): 0.6935\n",
      "- Accuracy (incl. no class): 0.9572\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9673    0.9908    0.9789     32082\n",
      "           B     0.8189    0.5969    0.6905      2235\n",
      "           I     0.5483    0.3287    0.4110       432\n",
      "\n",
      "    accuracy                         0.9572     34749\n",
      "   macro avg     0.7782    0.6388    0.6935     34749\n",
      "weighted avg     0.9526    0.9572    0.9533     34749\n",
      "\n",
      "2020-11-19 16:59:06,263 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9572,\n",
       " 'dev_score_history': [0.9346,\n",
       "  0.949,\n",
       "  0.951,\n",
       "  0.9502,\n",
       "  0.9568,\n",
       "  0.9564,\n",
       "  0.9593,\n",
       "  0.9599,\n",
       "  0.9609,\n",
       "  0.9595],\n",
       " 'train_loss_history': [5.588340487613146,\n",
       "  4.5136235681663,\n",
       "  4.234262478779037,\n",
       "  4.048778550083418,\n",
       "  3.9499369232778054,\n",
       "  3.777298006878431,\n",
       "  3.6407367171519307,\n",
       "  3.589650133691461,\n",
       "  3.5087066048169993,\n",
       "  3.46840430635855],\n",
       " 'dev_loss_history': [5.412528991699219,\n",
       "  3.884171962738037,\n",
       "  3.7232940196990967,\n",
       "  3.687502145767212,\n",
       "  3.0893001556396484,\n",
       "  3.136033296585083,\n",
       "  2.9495975971221924,\n",
       "  2.8404431343078613,\n",
       "  2.7531497478485107,\n",
       "  3.035252094268799]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "# 1. get the corpus\n",
    "print(corpus)\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'iob'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-pos',\n",
    "              embeddings_storage_mode='gpu',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply your two identification systems on a new text of your targeted domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rule-based identification system created from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles/txt/doc_04.txt || Text length:  22563\n"
     ]
    }
   ],
   "source": [
    "def docRB():\n",
    "    f = open('test/doc4-RB.txt', 'w', encoding=\"utf8\")\n",
    "    doc = nlp(get_doc(\"Articles/txt/doc_04.txt\"))\n",
    "    annotate_doc(doc, f)\n",
    "    f.close()\n",
    "    \n",
    "docRB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/doc4-RB.txt || Text length:  32227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linguistic Analysis of Toxic Behavior in an Online Video Game Haewoon Kwak  and Jeremy Blackburn  Qatar Computing Research Institute , Doha , Qatar hkwak@qf.org.qa Telefonica Research , Barcelona , Spain jeremyb@tid.es Abstract.  In this paper we explore the linguistic components of toxic behavior by using crowdsourced data from over 590 thousand cases of accused toxic <B> players <I> in a popular match <B> - based <B> competition <B> game <B> , League of Legends.  We perform a series of linguistic analyses to gain a deeper understanding of the role <B> communication <B> plays <B> in the expression of toxic behavior.  We characterize linguistic behavior of toxic <B> players and <I> compare <I> it <I> with that of typical <B> players <I> in an online <B> competition game <I>.  We also find empirical support <B> describing how a player <B> transitions from typical to toxic behavior.  Our findings can be helpful to automatically detect and warn players <B> who may become toxic and thus insulate potential victims <B> from toxic playing in advance.  Keywords : Toxic behavior  verbal violence  Tribunal  League of Legends  cyberbullying  online <B> games <I> 1 Introduction Multiplayer games <B> provide players <B> with the thrill of true competition <B>.  Players <B> prove themselves superior to other humans that exhibit dynamic behavior far beyond that of any computer controlled <B> opponent <B>.  Additionally , some multiplayer games <B> provide another wrinkle : teamwork <B>.  Now , not only is it a test of skill <B> between two individuals , but cooperation <B> , strategy <B> , and communication <B> between teammates <B> can ensure victory <B>.  Unfortunately , the presence of teammates <B> and their influence on victory <B> and defeat <B> can result in toxic behavior.  Toxic behavior , also known as cyberbullying [ 1 ] , griefing <B> [ 4 ] , or online disinhibition [ 7 ] , is bad behavior that violates social norms , inflicts misery , continues to cause harm after it occurs , and affects an entire community <B>.  The anonymity afforded by , and ubiquity of , computer - mediated - communication <B> ( CMC ) naturally leads <B> to hostility and aggressiveness [ 3,8 ].  A major obstacle <B> in understanding toxic behavior is its subjective perception.  Unlike unethical behavior like cheating <B> , toxic behavior is nebulously defined ; toxic <B> players <I> themselves sometimes fail to recognize their behavior as toxic [ 6 ].  Nevertheless , because of the very real impact <B> toxic behavior has on our daily lives , even outside of games <B> , a deeper understanding is necessary.  arXiv:1410.5185v1 [ cs.  SI ] 20 Oct 2014 2 Haewoon Kwak and Jeremy Blackburn To further our understanding , in this paper we explore the linguistic components of toxic behavior.  Using crowdsourced data from over 590 thousand  judicial trials  of accused toxic <B> players <I> representing over 2.1 million matches <B> of a popular match <B> - based <B> competition <B> game <B> , League of Legends1 , we perform a series of linguistic analyses to gain a deeper understanding of the role <B> communication <B> plays <B> in the expression of toxic behavior.  In our previous work [ 2 ] , we found that offensive language is the most reported reason across all the three regions.  Also , in North America , verbal abuse is the second most reported reason.  In other words , linguistic components are a prime method of expressing toxicity <B>.  From our analyses we draw several findings.  First , the volume of communication <B> is not uniform throughout the length of the match <B> , instead showing a bi - modal shape with peaks at the beginning and end of a match <B>.  By comparing the distribution of frequency of communications <B> between normal <B> players <I> and toxic <B> players <I> , we find subtle differences.  Typical <B> players <I> chat <B> relatively more at the beginning of a match <B> , which is mainly for ice breaking , morale boosting , and sharing early <B> strategic information.  In contrast , toxic <B> players <I> chat <B> less at the beginning but constantly more than typical <B> players <I> after some time <B> point <B> , i.e. phase <B> transition <I>.  Next , we find discriminative uni- and bi - grams used by typical <B>   and <I> toxic <I> players <I> , as signatures of them , examine the differences , and show that certain bi - grams can be classified based <B> on when they appear in a match <B>.  Temporal patterns of the linguistic signature of toxic <B> players <I> illustrate what kind of toxic playing happens as the match <B> progresses.  Deeper analysis of temporal analysis of words used by toxic <B> and <I> typical <I> players <I> reveals a more interesting picture.  We focus <B> on how a player <B> transitions to toxic by comparing the temporal usage of popular uni - grams between typical <B> players and <I> toxic <I> players <I>.  Our contribution is two - fold.  First , we characterize linguistic behavior of toxic <B> players and <I> compare <I> it <I> with that of typical <B> players <I> in online competition <B> games <B>.  Second , we find empirical support <B> to describe how a player <B> turns <B> to be toxic.  Our findings would be helpful to automatically detect and warn players <B> who may turn <B> to be toxic and thus save potential victims <B> of toxic playing in advance.  2 Dataset The League of Legends ( LoL ) is the most popular Multiplayer Online Battle Arena out today , and suffers from a high degree of toxic behavior.  The LoL <B> Tribunal is a crowdsourced system for determining the guilt of players <B> accused of tocix behavior.  We collected 590,311 Tribunal cases from the North America region representing a total of 2,107,522 individual matches <B>.  Each Tribunal case represents a single player <B> and includes up to 5 matches <B> in which he was accused of toxic behavior.  In LoL <B> players <I> can communicate via chat <B> , which is ostensibly used to share strategic plans and other important information during the game <B>.  However , chat <B> is also a prime vector for exhibiting toxic behavior.  Thus , although 1 http://leagueoflegends.com Linguistic Analysis of Toxic Behavior in an Online Video Game 3 0.00 0.01 0.02 0.03 0 25 50 75 100 Normalized Time PDF offender typical Fig.  1.  Change of chat <B> volume <I> during a match <B>.  a variety of information is presented to Tribunal reviewers [ 2 ] , in this paper we focus <B> exclusively on the in <B> - <I> game <I> chat <B> logs.  We extract 24,039,184 messages <B> from toxic <B> players and <I> 33,252,018 <I> messages <I> from typical <B> players <I>.  Because the teammates <B> of toxic <B> players <I> are directly impacted <B> by toxic <B> playing and <I> readily <I> express <I> aggressive reactions to a toxic <B> player <I> , we define typical <B> players <I> as the set of players <B> on the opposite <B> team <I> when none of them report <B> the toxic <B> player <I>.  Before continuing , we report <B> some basic statistics about the size of vocabulary and the length of messages <B>.  We found 1,042,940 unique tokens in toxic <B> player <I> messages <B> and 1,176,356 unique tokens in typical <B> player <I> messages <B>.  While typical <B>   players <I> send 38 % more messages <B> than toxic <B> players <I> , the messages <B> are composed of only 13 % more unique tokens.  Interestingly , we find that toxic <B> players <I> send longer messages <B> than typical <B> players <I> ; the average number of words per message <B> is 3.139 and 2.732 for toxic <B> and <I> typical <I> players <I> , respectively.  3 Chat Volume over a Match <B> We begin our analysis by exploring chat <B> volume <I> over time <B>.  A LoL <B> match <I> can be broken up into logical stages <B>.  First is the early <B> game <I> ( also known as the  laning <B> phase  ) , where characters <B> are low level <B> and weak <B>.  In the early <B> game <I> , players <B> expend great effort towards  farming <B>  computer controlled <B> minions <B> to gain experience <B> and gold <B> , with aggressive <B> plays <I> against the other team <B> usually coming as the result of an over extension or other mistake <B>.  As players <B> earn gold <B> and experience <B> , they level <B> up and become stronger , and the match <B> transitions to the mid <B> game <I>.  During the mid <B> game <I> , players <B> become more aggressive and tend to group <B> up with teammates <B> to make plays <B> on their opponents <B>.  Finally , once players <B> are reaching their maximum power <B> levels <B> , the match <B> transitions into the end <B> game <I> , where teams <B> will group <B> together and make hard <B> pushes towards taking objectives <B> and winning the match <B>.  4 Haewoon Kwak and Jeremy Blackburn While these phases are not dictated by the programming of LoL , and thus there is no hard <B> cut off point <B> for when the transitions between phases occur , we suspect that each phase has an associated pattern of communication <B>.  Thus , in Figure 1 we plot the density of chat <B> messages <B> written by toxic <B> and <I> typical <I>   players <I> as a function of the normalized time <B> during a match <B>.  The plot confirms our suspicions : communication <B> is not uniform throughout the match <B>.  Instead , we see three distinct levels <B> of communication <B> , likely corresponding to the three phases of a match <B> , with relative peaks at the beginning and end of the match <B>.  This finding can be explained with a deeper understanding of how a LoL <B>   match <I> progresses.  As mentioned above , in the early <B> game <I> players <B> are relatively weak <B> and must focus <B> on farming <B> for resources <B>.  Early <B> game <I> farming <B> occurs via players <B> choosing one of three lanes <B> to spend their time <B> in.  The lanes <B> are quite far from each other on the map ( 10 + seconds or so to travel between them ) and thus players <B> on the same team <B> tend to be relatively isolated from each other.  To take advantage <B> of this isolation , and to get an early <B> lead <B> , players <B> might roam from the lane <B> they chose to play <B> in to another lane <B>.  In turn <B> , this provides their teammate <B> in the other lane <B> with a numbers advantage <B> over opposing player <B> in the lane <B>.  Colloquially , this roaming to provide a temporary numbers advantage <B> is known as a  gank <B>.   To deal with ganks <B> in the early <B> game <I> , players <B> tend to communicate via chat <B> when the opposing player <B> in their <I> lane <I> has gone missing.  As the match <B> transitions to mid <B> game <I> , teammates <B> start <B> grouping <B> up.  Since they are no longer so isolated the fear of ganks dissipates , and the need to communicate missing players <B> diminishes.  Additionally , since teammates <B> are grouped <B> together , they are seeing the same portion of the map , and there is not really that much additional information they can convey to each other.  Finally , as late <B> game <I> comes around , teams <B> must focus <B> and work together to complete objectives <B> and win the match <B>.  In practice , this might involve coming to agreement on a final  push  for an objective <B> , or agreeing on which lane <B> the team <B> should travel down.  Also , there are some customs in <B> e - <I> sports <I> , saying  gg ( good <B>   game <I> )  at the end of the game <B>.  The sharp spikes contain those messages <B> as well.  While this might explain some of the spike seen at the end of Figure 1 another , simpler explanation is that players <B> are simply communicating their ( dis)pleasure in winning or losing the match <B>.  A more interesting finding is the subtle difference in the distributions of typical <B> and <I> toxic <I> players <I>.  At the early <B> stage <B> we see more active communication <B> by normal <B> players <I>.  We suppose that it includes all the messages <B> for ice breaking or cheering ( e.g. gl ( good luck ) or hf ( have fun ) ).  However , at some point <B> after the short period , toxic <B> players <I> begin to chat <B> more than typical <B> players and <I> keep <I>   <I> such pattern until by the last stage <B>.  At the last stage <B> of the match <B> , typical <B>   players <I> again chat <B> more socially , for example , sending smile emoticons , which are :D or :) , and also saying gg , as we mentioned.  The transition point <B> , where the distribution of toxic <B> players <I> cross over that of typical <B> players <I> , is a basis of our further analysis in Section 5.  Linguistic Analysis of Toxic Behavior in an Online Video Game 5 Fig.  2.  Top 10 discriminative uni- and bi - grams 4 Discriminative Words of Toxic and Typical Players The linguistic approach to the chat <B> log characterizes toxic <B> players <I> with context.  We conduct n - gram analysis because it is intuitive and straightforward.  We filter the stopwords and then count the frequency of uni- and bi - grams from the chat <B> log involving toxic reports <B> of either verbal abuse or offensive language.  In order to find discriminative n - grams of toxic <B> players <I> we need a reference for comparison.  We conduct the same n - gram analysis from enemy <B> s chat <B> log when verbal abuse or offensive language is not reported <B> from the enemies <B>.  We consider it as a normal conversation among players <B> and call <B> those enemies <B> typical <B>   players <I>.  We create the top 1,000 uni- and bi - grams for toxic <B> and <I> typical <I> players <I> , respectively.  We find 867 uni- and 748 bi - grams in common.  Then we obtain 133 non - overlapped uni- and 252 bi - grams for toxic <B> and <I> typical <I> players <I> ; they appear only in either toxic or typical <B> players <I>.  We define them as discriminative uni- and bi - gram for toxic <B> and <I> typical <I> players <I> , respectively.  Figure 2 shows top 10 discriminative uni- and bi - grams of toxic <B> and <I> typical <I>   players <I>.  Top 10 discriminative uni- and bi - grams of toxic <B> players <I> are filled with bad words.  That is , Riot Games does not offer even the basic level <B> of bad word filtering , and such bad words can be used as the signatures of toxic <B> players <I> who used verbal abuse or offensive language.  We find that several discriminative bi - grams of typical <B> players <I> are about strategies <B> , while most of toxic <B> players <I>  bigrams are bad words.  We note that some variations of  fucking  are discriminative uni - grams but  fucking  itself is not.  It means that  fucking  is often used not only by toxic <B> players <I> but also by typical <B> players <I> as well.  This shows the difficulties of filtering bad words by a simple dictionary - based <B> approach.  As the next step of the linguistic approach , we are interested in when verbal abuse occurs from a temporal perspective during a match <B>.  We divide 252 discriminative bi - grams of toxic <B> players <I> into three classes <B> , early- , mid- , and latebi - grams , based <B> on when their highest frequencies occur.  Figure 3 presents an example of three temporal classes <B> of bi - grams.  Interestingly , 209 ( 82.94 % ) out of 252 bi - grams are late - bi - gram.  The early <B> - bi - gram  ill 6 Haewoon Kwak and Jeremy Blackburn 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 Normalized time <B> Scaled density ill feed fucking bot report <B> noob <I> Fig.  3.  Example of early- , mid- , and late - bi - gram feed  is a domain specific example of toxic behavior.  In LoL , one of the ways players <B> earn gold <B> and experience <B> during a match <B> is by killing <B> players <B> on the opposite <B> team <I>.  Intentional feeding is when a player <B> deliberately allows the other team <B> to kill <B> them , thus  feeding  the enemies <B> with gold <B> and experience <B> , in turn <B> allowing them to become quite powerful <B>.  The mid - bi - gram  fucking bot  is the toxic <B> player <I> expressing his displeasure for the performance of the bottom <B> lane <I>.  The bottom <B> lane <I> is usually manned by characters <B> that have a primarily late <B> - game <I> presence <I> , and thus being behind during the mid <B> - game <I> has <I> a significant impact <B> on the remainder of the match <B>.  Most verbal abuse of toxic <B> players <I> occurs in the late stage <B> of the game <B>.  For example ,  report <B> noob <I>  is the toxic <B> player <I> requesting that the rest of his team <B> report <B> a player <B> ( the  noob <B>  ) that he singled out for his ire.  We believe the most likely explanation for this is that verbal abuse is most likely a response to losing a game <B> , which is often not apparent until the late <B> - game <I>. <I> For example , consider a scenario where one player <B> on the team <B> has a bad <B> game <I> , perhaps making poor <B> decisions resulting in the enemy <B> team <I> becoming quite strong.  In the early- , and even mid <B> - game <I> phases <I> , a toxic <B> player <I> might still be able to hold his own , however , when the enemy <B> team <I> groups <B> up and makes coordinated pushes in the late <B> - game <I> , <I> their relative strength <B> will often result in quick and decisive victories <B> in teamfights <B>.  If toxic playing can be detected in <B> real - <I> time <I> , we could protect potential victims <B> from verbal violence , for example via alerts <B> or simply not delivering such messages <B>.  Temporal dynamics of bi - grams might help to create a mental model of toxic <B>   players <I>.  For instance , 10 bi - grams containing  bot  are divided into 1 early <B> - bigram , 5 mid - bi - grams , and 4 late - bi - grams.  Through manual inspection , we confirm that the early <B> - bi - gram (  go bot  ) is strategic and non - aggressive , the mid - bigrams are cursing , and the late - bi - grams are blaming the result of the match <B> on the bot player(s ).  This provides us with a rough idea of how toxic <B> players <I> might behave and think over time <B> : initially they have a similar mindset as typical play- Linguistic Analysis of Toxic Behavior in an Online Video Game 7 Fig.  4.  Time <B> difference of last used time <B> of uni - gram ers , but , as the game <B> plays <B> out contrary to their desires , they grow increasingly aggressive , eventually lashing out with purely abusive <B> language <I>.  We leave more sophisticated modeling of toxic <B> players <I>  thought process as future work.  5 Phase Transition of Toxic Players In the previous section we recognize which words are exclusively used by toxic <B>   and <I> normal <I> players <I>.  However , some words are used by both toxic <B> players and <I>   <I> normal <I> players <B>.  For these , the emerging patterns in a temporal sense could be quite different.  If we assume that toxic <B> players <I> exhibit toxic behavior in reaction to certain events happening during the match <B> , then the linguistic behavior of such toxic <B> players <I> should be the same as typical <B> players <I> before those events happen.  To validate the above hypothesis , we conduct the following experiment which is focused <B> on finding some words that are not used after some time <B> point <B> by toxic <B> players <I> , while they are continuously used by normal <B> players <I>.  We extract the top 30 uni - gram at every normalized time <B> unit <B> ( ranging <B> from 0 to 100 ) for toxic <B> players and <I> normal <I> players <I> , respectively.  Since top 30 uni - grams are quite stable during the match <B> , we obtain unique 80 uni - gram for toxic <B> players and <I> 91 <I>   <I> uni - grams for normal <B> players <I>.  We first observe that toxic <B> players <I> have slightly smaller vocabularies than that by normal <B> players <I>.  For each of these uni - grams , we compute the normalized time <B> of last use by toxic <B> players and <I> normal <I> players <I> , respectively.  Finally , we compute the difference of the last used time <B> between toxic <B> and <I> normal <I> players <I> for the common uni - grams.  Figure 4 lists the uni - grams with a time <B> difference greater than 30.  I.e. , words in the list are used later into the match <B> by normal <B> players <I>.  Some interesting patterns are present in the results.  First , emoticons , particularly smile emoticons , are almost never used by toxic <B>   players <I>.  Second , apologies ( e.g. ,  sorry  ) are also exclusively used by normal <B> players <I>.  Third , some words for strategic team <B> maneuvers ( e.g. ,  come  ,  ult  ,  blue <B>  ,  ward <B>  ) are used by toxic <B> players <I> , but this ceases at some point <B> during the match <B>.  Fourth , some words primarily used for communicating movements <B> with partners in the same lane <B> ( e.g.  back  ,  b  ,  brb  ( be right back ) ,  omw  ( on my way ) ,  k  8 Haewoon Kwak and Jeremy Blackburn ( okay ) ) are also used by toxic <B> players <I> , but again , after some point <B> toxic <B> players <I> stop this form of communication <B>.  Fifth , toxic <B> players <I> stop praising ( e.g. ,  gj  ( good job ) ) their teammates <B> after some point <B> in time <B>.  All these findings reveal how toxicity <B> is born during a match <B>.  It seems to be a kind of phase <B> transition <I>.  They behave the same as normal <B> players <I> during the early <B> stage <B> of the match <B> , but at some point <B> they change their behavior.  After some point <B> , they utter neither apologies nor praise to express their feelings , and also stop strategic communication <B> with team <B> members <I>.  By combining this finding with discriminative words of toxic <B> players <I> , we see the possibility for detecting a certain point <B> that a player <B> transitions to be toxic without using detailed in <B> - <I> game <I> action logs , but just chat <B> logs.  Thus , linguistic analysis of toxic <B> players <I> shows not just how different they are and when they become different as well.  6 Conclusion and Future work In this work we have examined crowdsourced data from 590 thousand cases of accused toxic <B> players <I> in a popular match <B> - based <B> competition <B> game <B> , League of Legends.  We have performed a series of linguistic analyses to gain a deeper understanding of the role <B> communication <B> plays <B> in the expression of toxic behavior.  We have several interesting findings : a bi - modal distribution of chats <B> during a match <B> , a difference between temporal chat <B> patterns between toxic <B> and <I> typical <I>   players <I> , a list of discriminative uni- and bi - grams used by typical <B> and <I> toxic <I> players <I> as signatures of them , temporal patterns of the linguistic signature of toxic <B>   players <I> , and a possible footprint of transitions from typical behavior to toxic behavior.  Our findings would be helpful to automatically detect and warn players <B> who may turn <B> to be toxic and thus save potential victims <B> of toxic playing in advance.  Finally , we suggest several directions for future work.  First , is focusing <B> on interaction between typical <B> and <I> toxic <I> players <I>.  In this work the unit <B> of our analysis is a message <B> , but we do not delve into the flow of messages <B>.  Interaction analysis could reveal more clear <B> narratives of how a player <B> transitions to toxic behavior.  Next , is building a pre - warning system to detect toxic playing earlier <B>.  The main challenge here is to build a dictionary of words that are signs of toxic playing.  As we have seen a list of discriminative uni- and bi - grams of toxic <B> and <I> typical <I>   players <I> , some bad words are also used by typical <B> players <I> as well.  This behavior is prevalent in  trash <B> talk <I>  culture , and an important factor in immersing players <B> in a competitive <B> game <I> [ 5 ].  Thus , any pre - warning system must be effective in detecting toxic playing while being flexible enough to allow for trash <B> talk <I> to avoid breaking the immersive gaming experience <B>.  We believe that the signature of toxic <B> and <I> typical <I> players <I> we found is a first step for building the dictionary for a pre - warning system.  Linguistic Analysis of Toxic Behavior in an Online Video Game 9 References 1.  J. Barlinska , A. Szuster , and M. Winiewski.  Cyberbullying among adolescent bystanders : role <B> of the communication <B> medium , form of violence , and empathy.  Journal of Community & Applied Social Psychology , 23(1):3751 , 2013.  2.  J. Blackburn and H. Kwak.  Stfu noob <B> ! : Predicting crowdsourced decisions on toxic behavior in online <B> games <I>.  In Proceedings of the 23rd International Conference on World Wide Web , WWW  14 , pages 877888 , 2014.  3.  V. H.-H. Chen , H. B.-L. Duh , and C. W. Ng.  Players <B> who play <B> to make others cry : The influence of anonymity and immersion.  In Proceedings of the International Conference on Advances in Computer Enterntainment Technology , ACE  09 , pages 341344 , 2009.  4.  T. Chesney , I. Coyne , B. Logan , and N. Madden.  Griefing <B> in virtual worlds <B> : causes , casualties and coping strategies <B>.  Information Systems Journal , 19(6):525548 , 2009.  5.  O. B. Conmy.  Trash Talk in a Competitive <B> Setting <I> : Impact <B> on Self - efficacy , Affect , and Performance.  ProQuest , 2008.  6.  H. Lin and C.-T. Sun.  The  white - eyed  player <B> culture : Grief <B> play <I> and construction of deviance in mmorpgs.  In Proceedings of DiGRA 2005 Conference , 2005.  7.  J. Suler.  The online disinhibition effect <B>.  Cyberpsychology & behavior , 7(3):321326 , 2004.  8.  P. Thompson.  What s fueling the flames <B> in cyberspace ? a social influence model.  Communication <B> and cyberspace : Social interaction in an electronic environment , pages 293311 , 1996.  '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest2 = get_doc(\"test/doc4-RB.txt\")\n",
    "doctest2 = doctest2.replace(\"\\n\", \" \")\n",
    "doctest2 = doctest2.replace(\" O \", \" \")\n",
    "doctest2 = doctest2.replace(\" B \", \" <B> \")\n",
    "doctest2 = doctest2.replace(\" I \", \" <I> \")\n",
    "doctest2 = doctest2.replace(\" .\", \".\")\n",
    "doctest2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence tagger trained model from flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-19 17:05:53,782 loading file resources/taggers/example-pos/final-model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linguistic Analysis of Toxic Behavior in an Online Video Game <B> Haewoon Kwak  and Jeremy Blackburn   Qatar Computing Research Institute , Doha , Qatar hkwak @ qf.org.qa  Telefonica Research , Barcelona , Spain jeremyb @ tid.es Abstract . In this paper we explore the linguistic components of toxic behavior by using crowdsourced data from over 590 thousand cases of accused toxic <B> players <I> in a popular match-based competition <B> game <B> , League of Legends . We perform a series of linguistic analyses to gain a deeper understanding of the role <B> communication plays <B> in the expression of toxic behavior . We characterize linguistic behavior of toxic <B> players <I> and compare it with that of typical players <B> in an online <B> competition <I> game <I> . We also find empirical support describing how a player <B> transitions from typical to toxic behavior . Our findings can be helpful to automatically detect and warn players <B> who may become toxic and thus insulate potential victims <B> from toxic <B> playing <I> in advance . Keywords : Toxic behavior  verbal violence  Tribunal  League of Legends  cyberbullying  online games <B> 1 Introduction Multiplayer games <B> provide players <B> with the thrill <B> of true competition <B> . Players <B> prove themselves superior to other humans that exhibit dynamic behavior far beyond that of any computer controlled <B> opponent <B> . Additionally , some multiplayer games <B> provide another wrinkle : teamwork <B> . Now , not only is it a test of skill <B> between two individuals , but cooperation , strategy <B> , and communication between teammates <B> can ensure victory <B> . Unfortunately , the presence of teammates <B> and their influence <B> on victory <B> and defeat <B> can result in toxic behavior . Toxic behavior , also known as cyberbullying [ 1 ] , griefing [ 4 ] , or online disinhibition [ 7 ] , is bad behavior that violates social norms , inflicts misery , continues to cause harm after it occurs , and affects an entire community . The anonymity afforded by , and ubiquity of , computer-mediated-communication ( CMC ) naturally leads <B> to hostility and aggressiveness [ 3,8 ] . A major obstacle in understanding toxic behavior is its subjective perception . Unlike unethical behavior like cheating , toxic behavior is nebulously defined ; toxic <B> players <I> themselves sometimes fail to recognize their behavior as toxic [ 6 ] . Nevertheless , because of the very real impact toxic behavior has on our daily lives , even outside of games <B> , a deeper understanding is necessary . arXiv : 1410.5185v1 [ cs.SI ] 20 Oct 2014 2 Haewoon Kwak and Jeremy Blackburn To further our understanding , in this paper we explore the linguistic components of toxic behavior . Using crowdsourced data from over 590 thousand  judicial trials  of accused toxic <B> players <I> representing over 2.1 million matches <B> of a popular match-based competition <B> game <B> , League of Legends1 , we perform a series of linguistic analyses to gain a deeper understanding of the role <B> communication plays <B> in the expression of toxic behavior . In our previous work [ 2 ] , we found that offensive <B> language <I> is the most reported <B> reason across all the three regions . Also , in North America , verbal abuse is the second most reported <B> reason . In other words , linguistic components are a prime method of expressing toxicity . From our analyses we draw <B> several findings . First , the volume of communication is not uniform throughout the length of the match <B> , instead showing a bi-modal shape with peaks at the beginning <B> and end of a match <B> . By comparing the distribution of frequency of communications between normal players <B> and toxic <B> players <I> , we find subtle differences . Typical players <B> chat <B> relatively more at the beginning of a match <B> , which is mainly for ice <B> breaking , morale boosting , and sharing early strategic information . In contrast , toxic <B> players <I> chat <B> less at the beginning but constantly more than typical players <B> after some time <B> point <B> , i.e . phase transition . Next , we find discriminative uni - and bi-grams used by typical and toxic <B> players <I> , as signatures of them , examine the differences , and show that certain bi-grams can be classified based <B> on when they appear in a match <B> . Temporal patterns of the linguistic signature of toxic <B> players <I> illustrate what kind of toxic <B> playing <I> happens as the match <B> progresses . Deeper analysis of temporal analysis of words used by toxic and typical players <B> reveals a more interesting picture . We focus <B> on how a player <B> transitions to toxic by comparing the temporal usage of popular uni-grams between typical players <B> and toxic <B> players <I> . Our contribution is two-fold . First , we characterize linguistic behavior of toxic <B> players <I> and compare it with that of typical players <B> in online competition <B> games <I> . Second , we find empirical support <B> to describe how a player <B> turns to be toxic . Our findings would be helpful to automatically detect and warn players <B> who may turn to be toxic and thus save potential victims <B> of toxic <B> playing <I> in advance . 2 Dataset The League of Legends ( LoL ) is the most popular Multiplayer Online Battle <B> Arena out today , and suffers from a high degree of toxic behavior . The LoL Tribunal is a crowdsourced system for determining the guilt of players <B> accused of tocix behavior . We collected 590,311 Tribunal cases from the North America region representing a total of 2,107,522 individual matches <B> . Each Tribunal case represents a single player <B> and includes up to 5 matches <B> in which he was accused of toxic behavior . In LoL players <B> can communicate via chat <B> , which is ostensibly used to share strategic <B> plans and other important information during the game <B> . However , chat <B> is also a prime vector for exhibiting toxic behavior . Thus , although 1 http :// leagueoflegends.com Linguistic Analysis of Toxic Behavior in an Online Video Game <B> 3 0.00 0.01 0.02 0.03 0 25 50 75 100 Normalized Time <B> PDF offender typical Fig. 1. Change of chat <B> volume during a match <B> . a variety of information is presented to Tribunal reviewers [ 2 ] , in this paper we focus exclusively on the in-game chat <B> logs . We extract 24,039,184 messages <B> from toxic <B> players <I> and 33,252,018 messages <B> from typical players <B> . Because the teammates <B> of toxic <B> players <I> are directly impacted by toxic <B> playing <I> and readily express aggressive reactions to a toxic <B> player <I> , we define typical players <B> as the set of players <B> on the opposite <B> team <B> when none of them report <B> the toxic <B> player <I> . Before continuing , we report some basic statistics about the size of vocabulary and the length of messages <B> . We found 1,042,940 unique tokens in toxic <B> player <I> messages <I> and 1,176,356 unique tokens in typical player <B> messages . While typical players <B> send 38 % more messages <B> than toxic <B> players <I> , the messages <B> are composed of only 13 % more unique tokens <B> . Interestingly , we find that toxic <B> players <I> send longer messages <B> than typical players <B> ; the average number of words per message <B> is 3.139 and 2.732 for toxic and typical players <B> , respectively . 3 Chat <B> Volume over a Match <B> We begin our analysis by exploring chat <B> volume over time <B> . A LoL match <B> can be broken up into logical stages . First is the early game <B> ( also known as the  laning phase ) , where characters <B> are low level <B> and weak . In the early game <B> , players <B> expend great effort towards  farming  computer controlled <B> minions <B> to gain experience <B> and gold <B> , with aggressive plays <B> against the other team <B> usually coming as the result of an over extension or other mistake . As players <B> earn gold <B> and experience <B> , they level <B> up and become stronger , and the match <B> transitions to the mid game <B> . During the mid game <B> , players <B> become more aggressive and tend to group <B> up with teammates <B> to make plays <B> on their opponents <B> . Finally <B> , once players <B> are reaching their maximum power levels <B> , the match <B> transitions into the end game <B> , where teams <B> will group <B> together and make hard pushes towards taking objectives and winning the match <B> . 4 Haewoon Kwak and Jeremy Blackburn While these phases are not dictated by the programming of LoL , and thus there is no hard cut off point <B> for when the transitions between phases occur , we suspect that each phase has an associated pattern of communication . Thus , in Figure 1 we plot the density of chat <B> messages <B> written by toxic and typical players <B> as a function of the normalized time <B> during a match <B> . The plot confirms our suspicions : communication is not uniform throughout the match <B> . Instead , we see three distinct levels <B> of communication , likely corresponding to the three phases of a match <B> , with relative peaks at the beginning <B> and end of the match <B> . This finding can be explained with a deeper understanding of how a LoL match <B> progresses . As mentioned above , in the early game <B> players <B> are relatively weak and must focus <B> on farming for resources <B> . Early game <B> farming occurs via players <B> choosing one of three lanes <B> to spend their time <B> in . The lanes <B> are quite far from each other on the map ( 10 + seconds <B> or so to travel <B> between them ) and thus players <B> on the same team <B> tend to be relatively isolated from each other . To take advantage <B> of this isolation , and to get an early lead <B> , players <B> might roam <B> from the lane <B> they chose to play <B> in to another lane <B> . In turn , this provides their teammate <B> in the other lane <B> with a numbers advantage <B> over opposing player <B> in the lane <B> . Colloquially , this roaming <B> to provide a temporary numbers advantage <B> is known as a  gank .  To deal with ganks in the early game <B> , players <B> tend to communicate via chat <B> when the opposing player <B> in their lane <B> has gone missing <B> . As the match <B> transitions to mid game <B> , teammates <B> start <B> grouping <B> up . Since they are no longer so isolated the fear of ganks dissipates , and the need to communicate missing players <B> diminishes . Additionally , since teammates <B> are grouped together , they are seeing the same portion of the map , and there is not really that much additional information they can convey to each other . Finally <B> , as late game <B> comes around , teams <B> must focus <B> and work together to complete objectives and win <B> the match <B> . In practice <B> , this might involve coming <B> to agreement on a final <B>  push <B>  for an objective , or agreeing on which lane <B> the team <B> should travel <B> down . Also , there are some customs in e-sports , saying  gg ( good game <B> )  at the end of the game <B> . The sharp spikes contain those messages <B> as well . While this might explain some of the spike seen at the end of Figure 1 another , simpler explanation is that players <B> are simply communicating their ( dis ) pleasure in winning or losing <B> the match <B> . A more interesting finding is the subtle difference in the distributions of typical and toxic <B> players <I> . At the early stage we see more active communication by normal players <B> . We suppose that it includes all the messages <B> for ice breaking or cheering ( e.g. gl ( good luck <B> ) or hf ( have fun <B> )) . However , at some point <B> after the short period <B> , toxic <B> players <I> begin to chat <B> more than typical players <B> and keep such pattern until by the last stage <B> . At the last stage of the match <B> , typical players <B> again chat <B> more socially , for example , sending smile emoticons , which are : D or :) , and also saying gg , as we mentioned . The transition point <B> , where the distribution of toxic <B> players <I> cross over that of typical players <B> , is a basis of our further analysis in Section 5 . Linguistic Analysis of Toxic Behavior in an Online Video Game <B> 5 Fig. 2. Top 10 discriminative uni - and bi-grams 4 Discriminative Words of Toxic and Typical Players <B> The linguistic approach to the chat <B> log characterizes toxic <B> players <I> with context . We conduct n-gram analysis because it is intuitive and straightforward . We filter the stopwords and then count the frequency of uni - and bi-grams from the chat <B> log involving toxic reports of either verbal abuse or offensive <B> language <I> . In order to find discriminative n-grams of toxic <B> players <I> we need a reference for comparison . We conduct the same n-gram analysis from enemy <B> s chat <B> log when verbal abuse or offensive <B> language <I> is not reported <B> from the enemies <B> . We consider it as a normal conversation among players <B> and call <B> those enemies <B> typical players <B> . We create the top 1,000 uni - and bi-grams for toxic and typical players <B> , respectively . We find 867 uni - and 748 bi-grams in common . Then we obtain 133 non-overlapped uni - and 252 bi-grams for toxic and typical players <B> ; they appear only in either toxic or typical players <B> . We define them as discriminative uni - and bi-gram for toxic and typical players <B> , respectively . Figure 2 shows top 10 discriminative uni - and bi-grams of toxic and typical players <B> . Top 10 discriminative uni - and bi-grams of toxic <B> players <I> are filled with bad words . That is , Riot Games <B> does not offer even the basic level <B> of bad word filtering <B> , and such bad words can be used as the signatures of toxic <B> players <I> who used verbal abuse or offensive <B> language <I> . We find that several discriminative bi-grams of typical players <B> are about strategies <B> , while most of toxic players bigrams are bad words . We note that some variations of  fucking are discriminative uni-grams but  fucking itself is not . It means that  fucking is often used not only by toxic <B> players <I> but also by typical players <B> as well . This shows the difficulties of filtering <B> bad words by a simple dictionary-based approach . As the next step of the linguistic approach , we are interested in when verbal abuse occurs from a temporal perspective during a match <B> . We divide 252 discriminative bi-grams of toxic <B> players <I> into three classes <B> , early - , mid - , and latebi-grams , based <B> on when their highest frequencies occur . Figure 3 presents an example of three temporal classes of bi-grams . Interestingly , 209 ( 82.94 %) out of 252 bi-grams are late-bi-gram . The early-bi-gram  ill 6 Haewoon Kwak and Jeremy Blackburn 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 Normalized time <B> Scaled density ill feed fucking bot report <B> noob Fig. 3. Example of early - , mid - , and late-bi-gram feed  is a domain specific example of toxic behavior . In LoL , one of the ways players <B> earn gold <B> and experience <B> during a match <B> is by killing <B> players <B> on the opposite <B> team <B> . Intentional feeding is when a player <B> deliberately allows the other team <B> to kill <B> them , thus  feeding  the enemies <B> with gold <B> and experience <B> , in turn allowing them to become quite powerful <B> . The mid-bi-gram  fucking bot  is the toxic <B> player <I> expressing his displeasure for the performance of the bottom lane <B> . The bottom lane <B> is usually manned <B> by characters <B> that have a primarily late-game presence , and thus being behind <B> during the mid-game has a significant impact on the remainder of the match <B> . Most verbal abuse of toxic <B> players <I> occurs in the late stage of the game <B> . For example ,  report <B> noob  is the toxic <B> player <I> requesting that the rest of his team <B> report <B> a player <B> ( the  noob ) that he singled out for his ire . We believe the most likely explanation for this is that verbal abuse is most likely a response to losing <B> a game <B> , which is often not apparent until the late-game . For example , consider a scenario where one player <B> on the team <B> has a bad game <B> , perhaps making poor decisions resulting in the enemy <B> team <B> becoming quite strong . In the early - , and even mid-game phases , a toxic <B> player <I> might still be able to hold his own , however , when the enemy <B> team <B> groups <B> up and makes coordinated pushes <B> in the late-game , their relative strength <B> will often result in quick and decisive victories <B> in teamfights . If toxic <B> playing <I> can be detected in real-time , we could protect potential victims <B> from verbal violence , for example via alerts or simply not delivering such messages <B> . Temporal dynamics of bi-grams might help to create a mental model of toxic <B> players <I> . For instance , 10 bi-grams containing  bot are divided into 1 early-bigram , 5 mid-bi-grams , and 4 late-bi-grams . Through manual inspection , we confirm that the early-bi-gram ( go bot ) is strategic <B> and non-aggressive , the mid-bigrams are cursing <B> , and the late-bi-grams are blaming the result of the match <B> on the bot player <B> ( s ) . This provides us with a rough idea of how toxic <B> players <I> might behave and think over time <B> : initially they have a similar mindset as typical play <B> - Linguistic Analysis of Toxic Behavior in an Online <B> Video <I> Game <I> 7 Fig. 4. Time difference of last used time <B> of uni-gram ers , but , as the game <B> plays <B> out contrary to their desires , they grow increasingly aggressive , eventually lashing out with purely abusive language . We leave more sophisticated modeling of toxic players thought process as future work . 5 Phase Transition of Toxic <B> Players <I> In the previous section we recognize which words are exclusively used by toxic and normal players <B> . However , some words are used by both toxic <B> players <I> and normal players <B> . For these , the emerging patterns in a temporal sense could be quite different . If we assume that toxic <B> players <I> exhibit toxic behavior in reaction to certain events happening during the match <B> , then the linguistic behavior of such toxic <B> players <I> should be the same as typical players <B> before those events <B> happen . To validate the above hypothesis , we conduct the following experiment which is focused on finding some words that are not used after some time <B> point <B> by toxic <B> players <I> , while they are continuously used by normal players <B> . We extract the top 30 uni-gram at every normalized time <B> unit <B> ( ranging from 0 to 100 ) for toxic <B> players <I> and normal players <B> , respectively . Since top 30 uni-grams are quite stable during the match <B> , we obtain unique 80 uni-gram for toxic <B> players <I> and 91 uni-grams for normal players <B> . We first observe that toxic <B> players <I> have slightly smaller vocabularies than that by normal players <B> . For each of these uni-grams , we compute the normalized time <B> of last use by toxic <B> players <I> and normal players <B> , respectively . Finally , we compute the difference of the last used time <B> between toxic and normal players <B> for the common uni-grams . Figure 4 lists the uni-grams with a time <B> difference greater than 30 . I.e. , words in the list are used later into the match <B> by normal players <B> . Some interesting patterns are present in the results . First , emoticons , particularly smile emoticons <B> , are almost never used by toxic <B> players <I> . Second , apologies ( e.g. ,  sorry ) are also exclusively used by normal players <B> . Third , some words for strategic team <B> maneuvers <I> ( e.g. ,  come ,  ult ,  blue ,  ward ) are used by toxic <B> players <I> , but this ceases at some point <B> during the match <B> . Fourth <B> , some words primarily used for communicating movements with partners <B> in the same lane <B> ( e.g.  back ,  b ,  brb ( be right back ) ,  omw ( on my way ) ,  k 8 Haewoon Kwak and Jeremy Blackburn ( okay )) are also used by toxic <B> players <I> , but again , after some point <B> toxic <B> players <I> stop this form of communication . Fifth , toxic <B> players <I> stop praising ( e.g. ,  gj ( good job )) their teammates <B> after some point <B> in time <B> . All these findings reveal how toxicity is born during a match <B> . It seems to be a kind of phase transition . They behave the same as normal players <B> during the early stage of the match <B> , but at some point <B> they change their behavior . After some point <B> , they utter neither apologies nor praise to express their feelings , and also stop strategic <B> communication with team <B> members . By combining this finding with discriminative words of toxic <B> players <I> , we see the possibility for detecting a certain point <B> that a player <B> transitions to be toxic without using detailed in-game <B> action logs , but just chat <B> logs . Thus , linguistic analysis of toxic <B> players <I> shows not just how different they are and when they become different as well . 6 Conclusion and Future work In this work we have examined crowdsourced data from 590 thousand cases of accused toxic <B> players <I> in a popular match-based competition <B> game <B> , League of Legends . We have performed a series of linguistic analyses to gain a deeper understanding of the role <B> communication plays <B> in the expression of toxic behavior . We have several interesting findings : a bi-modal distribution of chats <B> during a match <B> , a difference between temporal chat <B> patterns between toxic and typical players <B> , a list of discriminative uni - and bi-grams used by typical and toxic <B> players <I> as signatures of them , temporal patterns of the linguistic signature of toxic <B> players <I> , and a possible footprint of transitions from typical behavior to toxic behavior . Our findings would be helpful to automatically detect and warn players <B> who may turn to be toxic and thus save potential victims <B> of toxic <B> playing <I> in advance <B> . Finally , we suggest several directions for future work . First , is focusing <B> on interaction between typical and toxic <B> players <I> . In this work the unit <B> of our analysis is a message <B> , but we do not delve into the flow of messages <B> . Interaction analysis could reveal more clear narratives of how a player <B> transitions to toxic behavior . Next , is building a pre-warning system to detect toxic <B> playing <I> earlier . The main challenge <B> here is to build a dictionary of words that are signs of toxic <B> playing <I> . As we have seen a list of discriminative uni - and bi-grams of toxic and typical players <B> , some bad words are also used by typical players <B> as well . This behavior is prevalent in  trash talk  culture , and an important factor in immersing players <B> in a competitive game <B> [ 5 ] . Thus , any pre-warning system must be effective in detecting toxic <B> playing <I> while being flexible enough to allow for trash talk to avoid breaking the immersive gaming experience <B> . We believe that the signature of toxic and typical players <B> we found is a first step for building the dictionary for a pre-warning system . Linguistic Analysis of Toxic Behavior in an Online Video Game <B> 9 References 1. J. Barlinska , A. Szuster , and M. Winiewski . Cyberbullying among adolescent bystanders <B> : role <B> of the communication medium , form of violence , and empathy . Journal of Community & Applied Social Psychology , 23 ( 1 ) : 37  51 , 2013 . 2. J. Blackburn and H. Kwak . Stfu noob ! : Predicting crowdsourced decisions on toxic behavior in online <B> games <I> . In Proceedings of the 23rd International Conference on World Wide Web , WWW 14 , pages 877  888 , 2014 . 3. V. H. - H . Chen , H. B. - L . Duh , and C. W. Ng . Players <B> who play <B> to make others cry <B> : The influence of anonymity and immersion . In Proceedings of the International Conference on Advances in Computer Enterntainment Technology , ACE 09 , pages 341  344 , 2009 . 4. T. Chesney , I. Coyne , B. Logan , and N. Madden . Griefing in virtual worlds <B> : causes , casualties <B> and coping strategies <B> . Information Systems Journal , 19 ( 6 ) : 525  548 , 2009 . 5. O. B. Conmy . Trash Talk in a Competitive <B> Setting : Impact on Self-efficacy , Affect , and Performance . ProQuest , 2008 . 6. H. Lin and C. - T. Sun . The  white-eyed  player <B> culture : Grief play <B> and construction of deviance in mmorpgs . In Proceedings of DiGRA 2005 Conference , 2005 . 7. J. Suler . The online disinhibition effect . Cyberpsychology & behavior , 7 ( 3 ) : 321  326 , 2004 . 8. P. Thompson . Whats fueling the flames in cyberspace ? a social influence model . Communication and cyberspace : Social interaction in an electronic environment , pages 293  311 , 1996 .'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# load the model you trained\n",
    "model = SequenceTagger.load('resources/taggers/example-pos/final-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence(doctest)\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "doctest1 = sentence.to_tagged_string()\n",
    "doctest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
