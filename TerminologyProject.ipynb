{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Terminology Project - M2 TAL - 24/11/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Axel Didier - Pierre Goncalves - M2 TAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages : os, spacy, flair\n",
    "\n",
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install flair\n",
    "#!{sys.executable} -m pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based identification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a .txt lexicon with a .tsv as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moba\n",
      "avatar\n",
      "game avatar\n",
      "moba game\n",
      "toxic player\n",
      "toxic and typical player\n",
      "toxic and normal player\n",
      "typical and toxic player\n",
      "player skill\n",
      "player base skill\n",
      "player skill decomposition\n",
      "player skill composition\n",
      "player skill evaluation\n",
      "player skill formation\n",
      "player skill rating\n",
      "player skill analysis\n",
      "team composition\n",
      "dota\n",
      "dota\n",
      "creeps\n",
      "team formation\n",
      "league of legend\n",
      "online game\n",
      "hot\n",
      "dot\n",
      "typical player\n",
      "gameplay\n",
      "enemy team\n",
      "team fight\n",
      "defense\n",
      "in - game\n",
      "riot game\n",
      "blue team\n",
      "red and blue team\n",
      "enemy hero\n",
      "tactical analysis\n",
      "late game\n",
      "player behavior\n",
      "prize\n",
      "champion attribute\n",
      "opposite team\n",
      "play style\n",
      "user report\n",
      "red team\n",
      "red and blue team\n",
      "experienced role\n",
      "inexperienced role\n",
      "minion\n",
      "mana\n",
      "creep\n",
      "easy mode\n",
      "red side\n",
      "blue side\n",
      "normal player\n",
      "novice player\n",
      "lol\n",
      "lol player\n",
      "game match\n",
      "enemy base\n",
      "noob\n",
      "mage\n",
      "matchmaking\n",
      "skill rating\n",
      "win rate\n",
      "low winning rate\n",
      "enemy tower\n",
      "hard mode\n",
      "high skill player\n",
      "target selection\n",
      "gamer\n",
      "opponent team\n",
      "team role\n",
      "amount of gold\n",
      "skillful\n",
      "gameplay style\n",
      "neutral creep\n",
      "rating system\n",
      "elo rating system\n",
      "anti - mage\n",
      "real - time strategy\n",
      "high damage\n",
      "toxicity\n",
      "opposing team\n",
      "champion base skill\n",
      "team win\n",
      "red team win\n",
      "expert player\n",
      "skilled player\n",
      "unskilled player\n",
      "enemy creeps\n",
      "high skill level\n",
      "game mechanic\n",
      "health point\n",
      "win chance\n",
      "in - game period\n",
      "gank\n",
      "team base\n",
      "game experience\n",
      "good decision\n",
      "gank\n",
      "mid game\n",
      "bottom lane\n",
      "combat scenario\n",
      "rank level\n",
      "matchmaking system\n",
      "metagame\n",
      "game mode\n",
      "meta\n",
      "level of player\n",
      "red area\n",
      "target game\n",
      "ally hero\n",
      "online community\n",
      "player statistic\n",
      "team effectiveness\n",
      "griefe\n",
      "main characteristic\n",
      "trash talk\n",
      "real - time\n",
      "game state\n",
      "non - game\n",
      "rank gap\n",
      "anonymous player\n",
      "lol match\n",
      "select player\n",
      "neutral unit\n",
      "screenshot\n",
      "lol champion\n",
      "dota map\n",
      "skilled champion\n",
      "moba match\n",
      "troll\n",
      "jungler\n",
      "tower range\n",
      "allied creeps\n",
      "high damage rating\n",
      "powerful unit\n",
      "hero selection\n",
      "online team formation\n",
      "unskilled player\n",
      "offline\n",
      "tournament mode\n",
      "game highlight\n",
      "solo queue\n",
      "team skill\n",
      "base skill of player\n",
      "game map\n",
      "strategic advantage\n",
      "hit point\n",
      "low hit point\n",
      "major game\n",
      "pick and ban phase\n",
      "split push\n",
      "target selector\n",
      "easy mode\n",
      "set of action\n",
      "stfu noob\n",
      "high - level\n",
      "high level\n",
      "high skill level\n",
      "game tutorial\n",
      "assist\n",
      "tower diving\n",
      "blue area\n",
      "limited attack\n",
      "phase transition\n",
      "player skill formation\n",
      "gamer with similar skill\n",
      "incompetent player\n",
      "silencer\n",
      "special ability\n",
      "game match\n",
      "health factor\n",
      "hybrid\n",
      "killable\n",
      "agility and strength hero\n",
      "game knowledge\n",
      "theorycraft\n",
      "top tier\n",
      "middle game\n",
      "end game\n",
      "player with similar skill\n",
      "chat volume\n",
      "great escape capability\n",
      "view of red team\n",
      "large damage\n",
      "team composition analysis\n",
      "team combat\n",
      "good overall synergy\n",
      "major game update\n",
      "single hero\n",
      "mana resource\n",
      "heavy damage\n",
      "winrate\n",
      "team competition game\n",
      "mana burn skill\n",
      "griefer\n",
      "grief play\n",
      "late game carry\n",
      "player role\n",
      "average skill\n",
      "success factor\n",
      "role category\n",
      "trueskill of player\n",
      "game avatar synergy\n",
      "avatar anti - mage\n",
      "individual skill level\n",
      "elo system\n",
      "elo rating system\n",
      "set of skill\n",
      "skill set\n",
      "type of map\n",
      "middle lane\n",
      "hero status\n",
      "type of match\n",
      "allied tower\n",
      "analysis of league of legend\n",
      "stealthy play style\n",
      "competitive setting\n",
      "kda factor\n",
      "tank carry hero\n",
      "dota game\n",
      "real - time game\n",
      "high burst physical damage\n",
      "trueskill score\n",
      "list of hero\n",
      "special power\n",
      "low winning rate\n",
      "mechanic of game\n",
      "game mechanic\n",
      "hero range\n",
      "game speed\n",
      "losing team\n",
      "jungler\n",
      "specific role\n",
      "team synergy\n",
      "video highlight\n",
      "skill level of player\n",
      "target destination\n",
      "low death rate\n",
      "familiar hero\n",
      "warm - up effect\n",
      "level of synergy\n",
      "report system\n",
      "dota match\n",
      "favorite champion\n",
      "in - game avatar\n",
      "win rate\n",
      "public matchmaking\n",
      "player satisfaction\n",
      "warm - up period\n",
      "league of legend championship\n",
      "random hero\n",
      "experience point\n",
      "common technique\n",
      "strength of player\n",
      "simple game\n",
      "game goal\n",
      "full game\n",
      "ai system\n",
      "great amount of damage\n",
      "complex gameplay\n",
      "champion with high control\n",
      "neutral creep\n",
      "moba competition\n",
      "ability of champion\n",
      "reaction time\n",
      "powerful item\n",
      "game record\n",
      "active player\n",
      "official riot game\n",
      "average skill of player\n",
      "magic attacker\n",
      "jungle invasion\n",
      "champion with good control\n",
      "low control rating\n",
      "newbie\n",
      "veteran player\n",
      "weak hero\n",
      "champion level\n",
      "role attribute\n",
      "main gameplay\n",
      "good kda\n",
      "single target\n",
      "total damage\n",
      "middle of lane\n",
      "team fight strategy\n",
      "game runtime\n",
      "diverse skill\n",
      "low health\n",
      "specific champion\n",
      "small range\n",
      "team skill level\n",
      "user interface\n",
      "blind mode\n",
      "opponent player\n",
      "select match\n",
      "team cooperation\n",
      "report noob\n",
      "team push\n",
      "game style\n",
      "item building\n",
      "player -PRON- d\n",
      "cheater\n",
      "physical attacker\n",
      "game interface\n",
      "deal damage\n",
      "matchup\n",
      "aram\n",
      "movement system\n",
      "strategy\n",
      "specific skill of player\n",
      "regular attack\n",
      "player ability\n",
      "kill streak\n",
      "game time\n",
      "chat room\n",
      "average damage\n",
      "custom map\n",
      "intercept\n",
      "minion\n",
      "area - of - effect\n",
      "respawn\n",
      "top tier player\n",
      "non - player\n",
      "small number of assist\n",
      "mobile device\n",
      "powerful skill\n",
      "single layer\n",
      "fall\n",
      "low number of kill\n",
      "real - time reasoning\n",
      "time penalty\n",
      "character - level\n",
      "main hero\n",
      "low damage\n",
      "power play\n",
      "low level account\n",
      "bounty rune\n",
      "elo rating system\n",
      "balanced game\n",
      "hero unit\n",
      "low skill level\n",
      "skill set\n",
      "degrade champion status\n",
      "moba phase\n",
      "team - base role - playing game\n",
      "high number of kill\n",
      "team composition of champion\n",
      "league of legend match\n",
      "abusive language\n",
      "competitive video game\n",
      "summoner\n",
      "team performance\n",
      "early game\n",
      "teamfight\n",
      "team fight\n",
      "inexperienced role\n",
      "esports game\n",
      "late - game\n",
      "late game\n",
      "skill level\n",
      "low skill level\n",
      "similar skill level\n",
      "auto - attack\n",
      "large esport game\n",
      "co - op with ai\n",
      "opponent base\n",
      "laner\n",
      "armory\n",
      "esport journalist\n",
      "support\n",
      "mid - laner\n",
      "strategic decision - making\n",
      "high - rank\n",
      "low kda\n",
      "top - laner\n",
      "average number of kill\n",
      "matchmaking rating\n",
      "performer player\n",
      "doublekill\n",
      "last - hitting\n",
      "team - fight\n",
      "team fight\n",
      "micro - transaction\n",
      "minion behavior\n",
      "infernal shrine\n",
      "counter - picking\n",
      "solo - queue mmr\n",
      "three - lane map\n",
      "sub - genre of rts\n",
      "dragon shire\n",
      "aatrox\n",
      "massive - multiplayer online game\n",
      "leveling\n",
      "spotts\n",
      "consumable\n",
      "honor\n",
      "totaldamagetaken\n",
      "total damage\n",
      "moba community\n",
      "videogame\n",
      "video game\n",
      "champlevel\n",
      "champion level\n",
      "shen\n",
      "spamme\n",
      "lane push\n",
      "elo - like rating\n",
      "aggressive play\n",
      "competition\n",
      "base damage\n",
      "mid - game\n",
      "mid game\n",
      "highly - ranked\n",
      "small pool of health\n",
      "triplekill\n",
      "live - streaming platform\n",
      "ingame\n",
      "undead\n",
      "armor\n",
      "warrior - mage\n",
      "smurf\n",
      "minion ai\n",
      "gamebots\n",
      "base - building\n",
      "mid - season\n",
      "kill\n",
      "position\n",
      "individual player\n",
      "agility\n",
      "individual skill\n",
      "turret\n",
      "tutorial\n",
      "physical damage\n",
      "burst physical damage\n",
      "competitive game\n",
      "competitive video game\n",
      "amount of damage\n",
      "chat\n",
      "assassin\n",
      "combat\n",
      "cooperation\n",
      "team member\n",
      "visual effect\n",
      "hero\n",
      "pick\n",
      "enemy\n",
      "highlight\n",
      "interface\n",
      "mechanic\n",
      "server\n",
      "skilled\n",
      "carry\n",
      "surrender\n",
      "rune\n",
      "feeder\n",
      "passive\n",
      "user\n",
      "evolution\n",
      "melee\n",
      "mobility\n",
      "bad game\n",
      "entity\n",
      "riot\n",
      "teamwork\n",
      "lot of damage\n",
      "aggro\n",
      "skill\n",
      "game\n",
      "response time\n",
      "stat\n",
      "player\n",
      "tier\n",
      "lane\n",
      "mechanism\n",
      "dungeon\n",
      "duel\n",
      "teammate\n",
      "novice\n",
      "tower\n",
      "update\n",
      "jungle\n",
      "twitch\n",
      "real - world\n",
      "beginner\n",
      "sentinel\n",
      "input\n",
      "match\n",
      "hide\n",
      "progression\n",
      "mute\n",
      "strategy\n",
      "technique\n",
      "gold\n",
      "draft\n",
      "coop\n",
      "decision - making\n",
      "gold\n",
      "team\n",
      "main target\n",
      "animation\n",
      "glitch\n",
      "ally\n",
      "competitive\n",
      "good game\n",
      "champion\n",
      "role\n",
      "frame\n",
      "solo\n",
      "greed\n",
      "opponent\n",
      "base\n",
      "item\n",
      "rank\n",
      "experience\n",
      "battle\n",
      "potion\n",
      "communication\n",
      "spell\n",
      "control\n",
      "damage\n",
      "strength\n",
      "crystal\n",
      "ability\n",
      "obstacle\n",
      "utility\n",
      "level\n",
      "powerful\n",
      "cheat\n",
      "battlefield\n",
      "sustainability\n",
      "trap\n",
      "character\n",
      "focus\n",
      "leaver\n",
      "hacker\n",
      "wide range\n",
      "buff\n",
      "warrior\n",
      "weak\n",
      "weakness\n",
      "resource\n",
      "objective\n",
      "invisible\n",
      "fight\n",
      "capture\n",
      "arrow\n",
      "bottom\n",
      "effect\n",
      "play\n",
      "object\n",
      "impact\n",
      "advantage\n",
      "unit\n",
      "range\n",
      "farming\n",
      "split\n",
      "harassment\n",
      "session\n",
      "path\n",
      "tank\n",
      "healing\n",
      "high level\n",
      "blind\n",
      "target\n",
      "status\n",
      "early\n",
      "hard\n",
      "movement\n",
      "middle\n",
      "rift\n",
      "rank\n",
      "commentator\n",
      "tactic\n",
      "message\n",
      "death\n",
      "easy\n",
      "competition\n",
      "class\n",
      "fighter\n",
      "starter\n",
      "goal\n",
      "community\n",
      "bard\n",
      "execution\n",
      "scoreboard\n",
      "kill\n",
      "smoke\n",
      "poor\n",
      "connection\n",
      "judgment\n",
      "trigger\n",
      "rule\n",
      "world\n",
      "full\n",
      "master\n",
      "report\n",
      "greed\n",
      "distant\n",
      "support\n",
      "close\n",
      "dash\n",
      "zone\n",
      "point\n",
      "reward\n",
      "rush\n",
      "location\n",
      "stage\n",
      "tournament\n",
      "time\n",
      "attack\n",
      "script\n",
      "group\n",
      "bush\n",
      "victim\n",
      "field\n",
      "wave\n",
      "turn\n",
      "open\n",
      "green\n",
      "ping\n",
      "alert\n",
      "chance\n",
      "forest\n",
      "victory\n",
      "power\n",
      "obvious\n",
      "brain\n",
      "safe\n",
      "mistake\n",
      "hard\n",
      "train\n",
      "straight\n",
      "retreat\n",
      "area\n",
      "mark\n",
      "viewer\n",
      "clear\n",
      "piece\n",
      "disadvantage\n",
      "spirit\n",
      "protection\n",
      "host\n",
      "start\n",
      "cost\n",
      "increase\n",
      "loser\n",
      "timing\n",
      "silver\n",
      "winner\n",
      "blue\n",
      "shield\n",
      "worth\n",
      "flame\n",
      "move\n",
      "drop\n",
      "heath\n",
      "danger\n",
      "free\n",
      "energy\n",
      "coach\n",
      "equipment\n",
      "slow\n",
      "boost\n",
      "farm\n",
      "killer\n",
      "recovery\n",
      "castle\n",
      "mission\n",
      "ward\n",
      "river\n",
      "lead\n",
      "spot\n",
      "call\n",
      "season\n",
      "defeat\n",
      "farmer\n",
      "leader\n"
     ]
    }
   ],
   "source": [
    "def clean_term():\n",
    "    f = open(\"TermsTSV/MOBA-en.tsv\", \"r\")\n",
    "    raw = f.read()\n",
    "    \n",
    "    #get term raw of the tsv file\n",
    "    raw_terms = re.findall('\\:\\s(.*[a-z])', raw)\n",
    "\n",
    "    result = []\n",
    "    #loop on every terms\n",
    "    for item in raw_terms:\n",
    "        lemmatized_term = ''\n",
    "        #loop on every token of the term\n",
    "        doc = nlp(item)\n",
    "        for token in doc:\n",
    "            lemmatized_term += token.lemma_ + ' '\n",
    "            \n",
    "        print(lemmatized_term[:-1]) #-1 to remove the last white space\n",
    "        \n",
    "        #to avoid doublon\n",
    "        if lemmatized_term not in result:\n",
    "            result.append(lemmatized_term)\n",
    "            \n",
    "    #write all terms in a file\n",
    "    with open('MOBA-en-lexicon.txt', 'w') as f:\n",
    "        for item in result:\n",
    "            f.write(item+\"\\n\")\n",
    "            \n",
    "clean_term()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get functions (lexicon, article.txt, list of terms in the lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon():\n",
    "    f = open(\"MOBA-en-lexicon.txt\", \"r\")\n",
    "    lexicon = f.read().splitlines() #use splitlines to not take '\\n' char\n",
    "    #return a list of splited list (to ease the further process)\n",
    "    return [item.split() for item in lexicon]\n",
    "\n",
    "def get_doc(file_name):\n",
    "    f = open(file_name, \"r\", encoding=\"utf8\")\n",
    "    text = f.read()\n",
    "    print(file_name, '|| Text length: ',len(text))\n",
    "    return text\n",
    "\n",
    "def get_possible_term(lexicon, word):\n",
    "    possible_term = []\n",
    "    for item in lexicon:\n",
    "        if word == item[0]:\n",
    "            possible_term.append(item)\n",
    "    return possible_term\n",
    "\n",
    "lexicon = get_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to create an IOB annotated .txt file\n",
    "# Manage one joker word / don't manage syntactic variants\n",
    "def annotate_doc(txt, f):\n",
    "    \n",
    "    txt_pos=0\n",
    "    while txt_pos < len(txt):#in range(0, len(txt)):\n",
    "        word = str(txt[txt_pos].lemma_)\n",
    "        #print(\">>\", word)\n",
    "        \n",
    "        possible_terms = get_possible_term(lexicon, word)\n",
    "        match = possible_terms\n",
    "        #print(\"Possible: \",match)\n",
    "        \n",
    "        # If there is possible_terms\n",
    "        if possible_terms is not None:\n",
    "            \n",
    "            to_remove = []\n",
    "            joker = True\n",
    "            joker_pos = None\n",
    "            joker_word = None\n",
    "            # loop on them\n",
    "            for term in possible_terms:\n",
    "                #print(\">\", term)\n",
    "                \n",
    "                # Process for each term word\n",
    "                for term_pos in range(0, len(term)):\n",
    "                \n",
    "                    #check if we dont go out the text length\n",
    "                    if txt_pos+term_pos < len(txt):\n",
    "                        #check if word term are not the same of the text word\n",
    "                        if term[term_pos] != str(txt[txt_pos+term_pos].lemma_):\n",
    "                            #if not the same but the next is the same, means that still have to process (rule)\n",
    "                            if term[term_pos] == str(txt[txt_pos+term_pos + 1].lemma_) and joker is True:\n",
    "                                #joker to False because we can only do that one time\n",
    "                                joker = False\n",
    "                                joker_pos = term_pos\n",
    "                                joker_word = str(txt[txt_pos+term_pos])\n",
    "                                break\n",
    "                            else:\n",
    "                                to_remove.append(term)\n",
    "                                break\n",
    "                    else:\n",
    "                        to_remove.append(term)\n",
    "                        break\n",
    "            \n",
    "            #remove all the terms that didn't match\n",
    "            for t in to_remove:\n",
    "                match.remove(t)\n",
    "                \n",
    "            # if there is a match\n",
    "            if len(match)>0:\n",
    "                #take the longuest\n",
    "                real_match = max(match, key=len)\n",
    "                #print(\"Real Match: \", real_match)\n",
    "                \n",
    "                # and jump to the next word\n",
    "                if joker is False: \n",
    "                    f.write(str(txt[txt_pos]) + \" B\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos]) + \" B\\n\")\n",
    "                    for i in range(1, len(real_match)+1):\n",
    "                        if joker_pos == i:\n",
    "                            f.write(joker_word + \" O\\n\")\n",
    "                            #print(\"Write: \" + joker_word + \" O\\n\")\n",
    "                        else: \n",
    "                            f.write(str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                            #print(\"Write: \" + str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                    txt_pos += len(real_match)+1\n",
    "                        \n",
    "                else: \n",
    "                    \n",
    "                    f.write(str(txt[txt_pos]) + \" B\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos]) + \" B\\n\")\n",
    "                    for i in range(1, len(real_match)):\n",
    "                        f.write(str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                        #print(\"Write: \" + str(txt[txt_pos+i]) + \" I\\n\")\n",
    "                    txt_pos += len(real_match)\n",
    "            else:\n",
    "                if word is not \"\\n\":\n",
    "                    f.write(str(txt[txt_pos])+\" O\\n\")\n",
    "                    #print(\"Write: \" + str(txt[txt_pos])+\" O\\n\")\n",
    "                txt_pos += 1\n",
    "                if word is \".\": f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the annotated Corpus with IOB (.txt)\n",
    "#### COMMENT the last line if you don't want to run it (already train.txt already exist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_01.txt\n",
      "Articles/txt/doc_01.txt || Text length:  17603\n",
      "doc_02.txt\n",
      "Articles/txt/doc_02.txt || Text length:  29695\n",
      "doc_03.txt\n",
      "Articles/txt/doc_03.txt || Text length:  70327\n",
      "doc_04.txt\n",
      "Articles/txt/doc_04.txt || Text length:  22563\n",
      "doc_05.txt\n",
      "Articles/txt/doc_05.txt || Text length:  23758\n",
      "doc_06.txt\n",
      "Articles/txt/doc_06.txt || Text length:  32981\n",
      "doc_07.txt\n",
      "Articles/txt/doc_07.txt || Text length:  18915\n",
      "doc_08.txt\n",
      "Articles/txt/doc_08.txt || Text length:  45445\n",
      "doc_09.txt\n",
      "Articles/txt/doc_09.txt || Text length:  42509\n",
      "doc_10.txt\n",
      "Articles/txt/doc_10.txt || Text length:  43999\n",
      "doc_11.txt\n",
      "Articles/txt/doc_11.txt || Text length:  54214\n",
      "doc_12.txt\n",
      "Articles/txt/doc_12.txt || Text length:  78258\n",
      "doc_13.txt\n",
      "Articles/txt/doc_13.txt || Text length:  27403\n",
      "doc_14.txt\n",
      "Articles/txt/doc_14.txt || Text length:  25424\n",
      "doc_15.txt\n",
      "Articles/txt/doc_15.txt || Text length:  38869\n",
      "doc_16.txt\n",
      "Articles/txt/doc_16.txt || Text length:  38604\n",
      "doc_17.txt\n",
      "Articles/txt/doc_17.txt || Text length:  63251\n",
      "doc_18.txt\n",
      "Articles/txt/doc_18.txt || Text length:  51534\n",
      "doc_19.txt\n",
      "Articles/txt/doc_19.txt || Text length:  60509\n",
      "doc_20.txt\n",
      "Articles/txt/doc_20.txt || Text length:  43553\n"
     ]
    }
   ],
   "source": [
    "# Create TRAIN.TXT file\n",
    "def train():\n",
    "    f = open('Corpus/train.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[0:20]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_21.txt\n",
      "Articles/txt/doc_21.txt || Text length:  43704\n",
      "doc_22.txt\n",
      "Articles/txt/doc_22.txt || Text length:  20314\n",
      "doc_23.txt\n",
      "Articles/txt/doc_23.txt || Text length:  18202\n",
      "doc_24.txt\n",
      "Articles/txt/doc_24.txt || Text length:  42682\n",
      "doc_25.txt\n",
      "Articles/txt/doc_25.txt || Text length:  51958\n"
     ]
    }
   ],
   "source": [
    "# Create TEST.TXT file\n",
    "def test():\n",
    "    f = open('Corpus/test.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[20:25]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_26.txt\n",
      "Articles/txt/doc_26.txt || Text length:  60221\n",
      "doc_27.txt\n",
      "Articles/txt/doc_27.txt || Text length:  51510\n",
      "doc_28.txt\n",
      "Articles/txt/doc_28.txt || Text length:  92987\n",
      "doc_29.txt\n",
      "Articles/txt/doc_29.txt || Text length:  55843\n",
      "doc_30.txt\n",
      "Articles/txt/doc_30.txt || Text length:  19363\n"
     ]
    }
   ],
   "source": [
    "# Create DEV.TXT file\n",
    "def dev():\n",
    "    f = open('Corpus/dev.txt', 'w', encoding=\"utf8\")\n",
    "    for filename in os.listdir(\"Articles/txt/\")[25:30]:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            doc = nlp(get_doc(\"Articles/txt/\"+filename))\n",
    "            annotate_doc(doc, f)\n",
    "    f.close()\n",
    "\n",
    "# comment/uncomment if you want \n",
    "dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Your Own Sequence Labeling Dataset using FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 00:21:39,887 Reading data from Corpus\n",
      "2020-11-20 00:21:39,888 Train: Corpus\\train.txt\n",
      "2020-11-20 00:21:39,888 Dev: Corpus\\dev.txt\n",
      "2020-11-20 00:21:39,889 Test: Corpus\\test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'iob'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = './Corpus/'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a player <B> ’s hit <B> points <I> reach 0 , he dies , a kill <B> ( with associated gold <B> and experience <B> ) is awarded to his killers <B> , and he must wait a certain period of time <B> to “ respawn <B> ” and return to the fight <B> .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[800].to_tagged_string('iob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Sequence Labeling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 8001 train + 2092 dev + 1577 test sentences\n",
      "Dictionary with 6 tags: <unk>, O, B, I, <START>, <STOP>\n",
      "2020-11-20 00:21:49,461 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,462 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=6, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-11-20 00:21:49,462 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,463 Corpus: \"Corpus: 8001 train + 2092 dev + 1577 test sentences\"\n",
      "2020-11-20 00:21:49,464 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,465 Parameters:\n",
      "2020-11-20 00:21:49,465  - learning_rate: \"0.1\"\n",
      "2020-11-20 00:21:49,466  - mini_batch_size: \"32\"\n",
      "2020-11-20 00:21:49,466  - patience: \"3\"\n",
      "2020-11-20 00:21:49,466  - anneal_factor: \"0.5\"\n",
      "2020-11-20 00:21:49,467  - max_epochs: \"10\"\n",
      "2020-11-20 00:21:49,467  - shuffle: \"True\"\n",
      "2020-11-20 00:21:49,468  - train_with_dev: \"False\"\n",
      "2020-11-20 00:21:49,468  - batch_growth_annealing: \"False\"\n",
      "2020-11-20 00:21:49,468 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,469 Model training base path: \"resources\\taggers\\example-pos\"\n",
      "2020-11-20 00:21:49,469 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,470 Device: cuda:0\n",
      "2020-11-20 00:21:49,470 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:49,472 Embeddings storage mode: gpu\n",
      "2020-11-20 00:21:49,474 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:21:52,272 epoch 1 - iter 25/251 - loss 10.76600885 - samples/sec: 286.06 - lr: 0.100000\n",
      "2020-11-20 00:21:54,794 epoch 1 - iter 50/251 - loss 8.93184245 - samples/sec: 317.20 - lr: 0.100000\n",
      "2020-11-20 00:21:57,515 epoch 1 - iter 75/251 - loss 7.82494899 - samples/sec: 294.14 - lr: 0.100000\n",
      "2020-11-20 00:22:00,240 epoch 1 - iter 100/251 - loss 7.23161990 - samples/sec: 293.89 - lr: 0.100000\n",
      "2020-11-20 00:22:02,905 epoch 1 - iter 125/251 - loss 6.85742270 - samples/sec: 300.46 - lr: 0.100000\n",
      "2020-11-20 00:22:05,775 epoch 1 - iter 150/251 - loss 6.57802347 - samples/sec: 278.83 - lr: 0.100000\n",
      "2020-11-20 00:22:08,670 epoch 1 - iter 175/251 - loss 6.38518088 - samples/sec: 276.54 - lr: 0.100000\n",
      "2020-11-20 00:22:11,539 epoch 1 - iter 200/251 - loss 6.19018498 - samples/sec: 278.91 - lr: 0.100000\n",
      "2020-11-20 00:22:14,321 epoch 1 - iter 225/251 - loss 6.04222355 - samples/sec: 287.75 - lr: 0.100000\n",
      "2020-11-20 00:22:16,798 epoch 1 - iter 250/251 - loss 5.91818971 - samples/sec: 323.16 - lr: 0.100000\n",
      "2020-11-20 00:22:16,822 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:22:16,822 EPOCH 1 done: loss 5.9062 - lr 0.1000000\n",
      "2020-11-20 00:22:21,262 DEV : loss 4.2321343421936035 - score 0.9511\n",
      "2020-11-20 00:22:21,274 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:22:24,359 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:22:27,064 epoch 2 - iter 25/251 - loss 4.70182955 - samples/sec: 295.90 - lr: 0.100000\n",
      "2020-11-20 00:22:29,708 epoch 2 - iter 50/251 - loss 4.70498577 - samples/sec: 302.63 - lr: 0.100000\n",
      "2020-11-20 00:22:32,043 epoch 2 - iter 75/251 - loss 4.78881386 - samples/sec: 342.86 - lr: 0.100000\n",
      "2020-11-20 00:22:34,610 epoch 2 - iter 100/251 - loss 4.72890913 - samples/sec: 311.67 - lr: 0.100000\n",
      "2020-11-20 00:22:37,286 epoch 2 - iter 125/251 - loss 4.66848159 - samples/sec: 299.07 - lr: 0.100000\n",
      "2020-11-20 00:22:39,941 epoch 2 - iter 150/251 - loss 4.63645927 - samples/sec: 301.59 - lr: 0.100000\n",
      "2020-11-20 00:22:42,365 epoch 2 - iter 175/251 - loss 4.56765791 - samples/sec: 330.30 - lr: 0.100000\n",
      "2020-11-20 00:22:44,770 epoch 2 - iter 200/251 - loss 4.56753547 - samples/sec: 332.69 - lr: 0.100000\n",
      "2020-11-20 00:22:47,046 epoch 2 - iter 225/251 - loss 4.52286505 - samples/sec: 351.74 - lr: 0.100000\n",
      "2020-11-20 00:22:49,533 epoch 2 - iter 250/251 - loss 4.48222555 - samples/sec: 321.88 - lr: 0.100000\n",
      "2020-11-20 00:22:49,552 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:22:49,553 EPOCH 2 done: loss 4.4680 - lr 0.1000000\n",
      "2020-11-20 00:22:53,068 DEV : loss 3.9105308055877686 - score 0.9528\n",
      "2020-11-20 00:22:53,080 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:22:56,334 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:22:58,782 epoch 3 - iter 25/251 - loss 4.18890901 - samples/sec: 327.10 - lr: 0.100000\n",
      "2020-11-20 00:23:01,581 epoch 3 - iter 50/251 - loss 4.13324474 - samples/sec: 286.16 - lr: 0.100000\n",
      "2020-11-20 00:23:04,018 epoch 3 - iter 75/251 - loss 4.28234158 - samples/sec: 328.41 - lr: 0.100000\n",
      "2020-11-20 00:23:06,703 epoch 3 - iter 100/251 - loss 4.31049030 - samples/sec: 298.09 - lr: 0.100000\n",
      "2020-11-20 00:23:09,234 epoch 3 - iter 125/251 - loss 4.27449416 - samples/sec: 316.22 - lr: 0.100000\n",
      "2020-11-20 00:23:11,760 epoch 3 - iter 150/251 - loss 4.19995950 - samples/sec: 316.92 - lr: 0.100000\n",
      "2020-11-20 00:23:14,096 epoch 3 - iter 175/251 - loss 4.17925669 - samples/sec: 343.13 - lr: 0.100000\n",
      "2020-11-20 00:23:16,788 epoch 3 - iter 200/251 - loss 4.16567388 - samples/sec: 297.40 - lr: 0.100000\n",
      "2020-11-20 00:23:19,417 epoch 3 - iter 225/251 - loss 4.16599101 - samples/sec: 304.38 - lr: 0.100000\n",
      "2020-11-20 00:23:21,883 epoch 3 - iter 250/251 - loss 4.18307962 - samples/sec: 324.59 - lr: 0.100000\n",
      "2020-11-20 00:23:21,896 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:23:21,897 EPOCH 3 done: loss 4.1666 - lr 0.1000000\n",
      "2020-11-20 00:23:25,482 DEV : loss 3.4715867042541504 - score 0.953\n",
      "2020-11-20 00:23:25,494 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:23:28,014 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:23:30,725 epoch 4 - iter 25/251 - loss 4.32197091 - samples/sec: 295.24 - lr: 0.100000\n",
      "2020-11-20 00:23:33,153 epoch 4 - iter 50/251 - loss 4.12897313 - samples/sec: 329.58 - lr: 0.100000\n",
      "2020-11-20 00:23:35,631 epoch 4 - iter 75/251 - loss 4.04136321 - samples/sec: 323.43 - lr: 0.100000\n",
      "2020-11-20 00:23:37,973 epoch 4 - iter 100/251 - loss 4.00827402 - samples/sec: 341.67 - lr: 0.100000\n",
      "2020-11-20 00:23:40,819 epoch 4 - iter 125/251 - loss 4.07464209 - samples/sec: 281.24 - lr: 0.100000\n",
      "2020-11-20 00:23:43,550 epoch 4 - iter 150/251 - loss 4.10931309 - samples/sec: 293.01 - lr: 0.100000\n",
      "2020-11-20 00:23:46,095 epoch 4 - iter 175/251 - loss 4.05899458 - samples/sec: 314.38 - lr: 0.100000\n",
      "2020-11-20 00:23:48,807 epoch 4 - iter 200/251 - loss 4.04591294 - samples/sec: 295.23 - lr: 0.100000\n",
      "2020-11-20 00:23:51,296 epoch 4 - iter 225/251 - loss 4.05309694 - samples/sec: 321.51 - lr: 0.100000\n",
      "2020-11-20 00:23:53,694 epoch 4 - iter 250/251 - loss 4.04261606 - samples/sec: 333.79 - lr: 0.100000\n",
      "2020-11-20 00:23:53,735 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:23:53,736 EPOCH 4 done: loss 4.0478 - lr 0.1000000\n",
      "2020-11-20 00:23:57,185 DEV : loss 3.283658504486084 - score 0.9547\n",
      "2020-11-20 00:23:57,198 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:24:00,373 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:24:02,735 epoch 5 - iter 25/251 - loss 3.77821436 - samples/sec: 338.85 - lr: 0.100000\n",
      "2020-11-20 00:24:05,262 epoch 5 - iter 50/251 - loss 3.85799335 - samples/sec: 316.73 - lr: 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 00:24:07,789 epoch 5 - iter 75/251 - loss 3.90840168 - samples/sec: 316.69 - lr: 0.100000\n",
      "2020-11-20 00:24:10,340 epoch 5 - iter 100/251 - loss 3.88833683 - samples/sec: 313.63 - lr: 0.100000\n",
      "2020-11-20 00:24:12,645 epoch 5 - iter 125/251 - loss 3.86549913 - samples/sec: 347.37 - lr: 0.100000\n",
      "2020-11-20 00:24:15,081 epoch 5 - iter 150/251 - loss 3.86519271 - samples/sec: 328.60 - lr: 0.100000\n",
      "2020-11-20 00:24:17,683 epoch 5 - iter 175/251 - loss 3.87444750 - samples/sec: 307.70 - lr: 0.100000\n",
      "2020-11-20 00:24:20,057 epoch 5 - iter 200/251 - loss 3.89834839 - samples/sec: 337.10 - lr: 0.100000\n",
      "2020-11-20 00:24:22,485 epoch 5 - iter 225/251 - loss 3.90364290 - samples/sec: 329.64 - lr: 0.100000\n",
      "2020-11-20 00:24:25,168 epoch 5 - iter 250/251 - loss 3.88807444 - samples/sec: 298.28 - lr: 0.100000\n",
      "2020-11-20 00:24:25,187 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:24:25,187 EPOCH 5 done: loss 3.8734 - lr 0.1000000\n",
      "2020-11-20 00:24:28,642 DEV : loss 3.2400383949279785 - score 0.9542\n",
      "2020-11-20 00:24:28,654 BAD EPOCHS (no improvement): 1\n",
      "2020-11-20 00:24:28,655 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:24:31,187 epoch 6 - iter 25/251 - loss 3.71129334 - samples/sec: 316.24 - lr: 0.100000\n",
      "2020-11-20 00:24:33,767 epoch 6 - iter 50/251 - loss 3.78506068 - samples/sec: 310.26 - lr: 0.100000\n",
      "2020-11-20 00:24:36,172 epoch 6 - iter 75/251 - loss 3.72578363 - samples/sec: 332.63 - lr: 0.100000\n",
      "2020-11-20 00:24:38,643 epoch 6 - iter 100/251 - loss 3.69182654 - samples/sec: 323.90 - lr: 0.100000\n",
      "2020-11-20 00:24:41,400 epoch 6 - iter 125/251 - loss 3.71383709 - samples/sec: 290.18 - lr: 0.100000\n",
      "2020-11-20 00:24:43,929 epoch 6 - iter 150/251 - loss 3.73458267 - samples/sec: 316.51 - lr: 0.100000\n",
      "2020-11-20 00:24:46,581 epoch 6 - iter 175/251 - loss 3.77731808 - samples/sec: 301.76 - lr: 0.100000\n",
      "2020-11-20 00:24:49,124 epoch 6 - iter 200/251 - loss 3.78451095 - samples/sec: 314.92 - lr: 0.100000\n",
      "2020-11-20 00:24:51,766 epoch 6 - iter 225/251 - loss 3.76865965 - samples/sec: 302.86 - lr: 0.100000\n",
      "2020-11-20 00:24:54,097 epoch 6 - iter 250/251 - loss 3.76048866 - samples/sec: 343.43 - lr: 0.100000\n",
      "2020-11-20 00:24:54,129 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:24:54,130 EPOCH 6 done: loss 3.7903 - lr 0.1000000\n",
      "2020-11-20 00:24:58,185 DEV : loss 3.3694121837615967 - score 0.9553\n",
      "2020-11-20 00:24:58,198 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:25:00,784 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:25:03,347 epoch 7 - iter 25/251 - loss 3.78981865 - samples/sec: 312.36 - lr: 0.100000\n",
      "2020-11-20 00:25:05,830 epoch 7 - iter 50/251 - loss 3.78743443 - samples/sec: 322.30 - lr: 0.100000\n",
      "2020-11-20 00:25:08,372 epoch 7 - iter 75/251 - loss 3.69512454 - samples/sec: 314.95 - lr: 0.100000\n",
      "2020-11-20 00:25:11,074 epoch 7 - iter 100/251 - loss 3.71261179 - samples/sec: 296.28 - lr: 0.100000\n",
      "2020-11-20 00:25:13,530 epoch 7 - iter 125/251 - loss 3.70915311 - samples/sec: 325.86 - lr: 0.100000\n",
      "2020-11-20 00:25:16,001 epoch 7 - iter 150/251 - loss 3.69639525 - samples/sec: 324.08 - lr: 0.100000\n",
      "2020-11-20 00:25:18,635 epoch 7 - iter 175/251 - loss 3.70081316 - samples/sec: 303.80 - lr: 0.100000\n",
      "2020-11-20 00:25:21,403 epoch 7 - iter 200/251 - loss 3.72430888 - samples/sec: 289.13 - lr: 0.100000\n",
      "2020-11-20 00:25:24,004 epoch 7 - iter 225/251 - loss 3.70259512 - samples/sec: 307.63 - lr: 0.100000\n",
      "2020-11-20 00:25:26,530 epoch 7 - iter 250/251 - loss 3.69372489 - samples/sec: 316.90 - lr: 0.100000\n",
      "2020-11-20 00:25:26,547 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:25:26,548 EPOCH 7 done: loss 3.6799 - lr 0.1000000\n",
      "2020-11-20 00:25:30,342 DEV : loss 2.9217662811279297 - score 0.9577\n",
      "2020-11-20 00:25:30,354 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:25:33,552 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:25:36,014 epoch 8 - iter 25/251 - loss 3.86606586 - samples/sec: 325.10 - lr: 0.100000\n",
      "2020-11-20 00:25:38,835 epoch 8 - iter 50/251 - loss 3.65901531 - samples/sec: 283.71 - lr: 0.100000\n",
      "2020-11-20 00:25:41,803 epoch 8 - iter 75/251 - loss 3.64574049 - samples/sec: 269.61 - lr: 0.100000\n",
      "2020-11-20 00:25:44,221 epoch 8 - iter 100/251 - loss 3.64393917 - samples/sec: 331.03 - lr: 0.100000\n",
      "2020-11-20 00:25:46,634 epoch 8 - iter 125/251 - loss 3.65706974 - samples/sec: 331.60 - lr: 0.100000\n",
      "2020-11-20 00:25:49,016 epoch 8 - iter 150/251 - loss 3.64238139 - samples/sec: 336.04 - lr: 0.100000\n",
      "2020-11-20 00:25:51,528 epoch 8 - iter 175/251 - loss 3.60899781 - samples/sec: 318.88 - lr: 0.100000\n",
      "2020-11-20 00:25:53,823 epoch 8 - iter 200/251 - loss 3.57697817 - samples/sec: 348.79 - lr: 0.100000\n",
      "2020-11-20 00:25:56,416 epoch 8 - iter 225/251 - loss 3.58033529 - samples/sec: 308.61 - lr: 0.100000\n",
      "2020-11-20 00:25:59,136 epoch 8 - iter 250/251 - loss 3.56623843 - samples/sec: 294.25 - lr: 0.100000\n",
      "2020-11-20 00:25:59,166 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:25:59,167 EPOCH 8 done: loss 3.5558 - lr 0.1000000\n",
      "2020-11-20 00:26:02,761 DEV : loss 2.8845086097717285 - score 0.96\n",
      "2020-11-20 00:26:02,774 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:26:05,284 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:26:07,579 epoch 9 - iter 25/251 - loss 3.27833788 - samples/sec: 348.87 - lr: 0.100000\n",
      "2020-11-20 00:26:10,211 epoch 9 - iter 50/251 - loss 3.32876953 - samples/sec: 304.10 - lr: 0.100000\n",
      "2020-11-20 00:26:12,865 epoch 9 - iter 75/251 - loss 3.43247926 - samples/sec: 301.67 - lr: 0.100000\n",
      "2020-11-20 00:26:15,350 epoch 9 - iter 100/251 - loss 3.52401940 - samples/sec: 322.05 - lr: 0.100000\n",
      "2020-11-20 00:26:17,926 epoch 9 - iter 125/251 - loss 3.55807248 - samples/sec: 310.85 - lr: 0.100000\n",
      "2020-11-20 00:26:20,471 epoch 9 - iter 150/251 - loss 3.48171530 - samples/sec: 314.41 - lr: 0.100000\n",
      "2020-11-20 00:26:23,064 epoch 9 - iter 175/251 - loss 3.50462406 - samples/sec: 308.61 - lr: 0.100000\n",
      "2020-11-20 00:26:25,608 epoch 9 - iter 200/251 - loss 3.46572596 - samples/sec: 314.76 - lr: 0.100000\n",
      "2020-11-20 00:26:28,200 epoch 9 - iter 225/251 - loss 3.48972657 - samples/sec: 308.69 - lr: 0.100000\n",
      "2020-11-20 00:26:30,842 epoch 9 - iter 250/251 - loss 3.48964148 - samples/sec: 302.88 - lr: 0.100000\n",
      "2020-11-20 00:26:30,859 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:26:30,860 EPOCH 9 done: loss 3.4820 - lr 0.1000000\n",
      "2020-11-20 00:26:34,401 DEV : loss 2.8093247413635254 - score 0.9594\n",
      "2020-11-20 00:26:34,413 BAD EPOCHS (no improvement): 1\n",
      "2020-11-20 00:26:34,414 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:26:36,919 epoch 10 - iter 25/251 - loss 3.50580778 - samples/sec: 319.48 - lr: 0.100000\n",
      "2020-11-20 00:26:39,568 epoch 10 - iter 50/251 - loss 3.44233538 - samples/sec: 302.15 - lr: 0.100000\n",
      "2020-11-20 00:26:42,010 epoch 10 - iter 75/251 - loss 3.39302898 - samples/sec: 327.76 - lr: 0.100000\n",
      "2020-11-20 00:26:44,574 epoch 10 - iter 100/251 - loss 3.38769508 - samples/sec: 312.09 - lr: 0.100000\n",
      "2020-11-20 00:26:47,313 epoch 10 - iter 125/251 - loss 3.39634285 - samples/sec: 292.40 - lr: 0.100000\n",
      "2020-11-20 00:26:49,798 epoch 10 - iter 150/251 - loss 3.44161186 - samples/sec: 322.17 - lr: 0.100000\n",
      "2020-11-20 00:26:52,290 epoch 10 - iter 175/251 - loss 3.39831263 - samples/sec: 321.33 - lr: 0.100000\n",
      "2020-11-20 00:26:54,948 epoch 10 - iter 200/251 - loss 3.43908817 - samples/sec: 301.14 - lr: 0.100000\n",
      "2020-11-20 00:26:57,741 epoch 10 - iter 225/251 - loss 3.43914257 - samples/sec: 286.54 - lr: 0.100000\n",
      "2020-11-20 00:27:00,118 epoch 10 - iter 250/251 - loss 3.41793852 - samples/sec: 336.89 - lr: 0.100000\n",
      "2020-11-20 00:27:00,138 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 00:27:00,139 EPOCH 10 done: loss 3.4308 - lr 0.1000000\n",
      "2020-11-20 00:27:03,698 DEV : loss 2.6863417625427246 - score 0.9633\n",
      "2020-11-20 00:27:03,711 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-20 00:27:09,485 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-20 00:27:09,486 Testing using best model ...\n",
      "2020-11-20 00:27:09,487 loading file resources\\taggers\\example-pos\\best-model.pt\n",
      "2020-11-20 00:27:13,092 \t0.9592\n",
      "2020-11-20 00:27:13,092 \n",
      "Results:\n",
      "- F-score (micro): 0.9592\n",
      "- F-score (macro): 0.7061\n",
      "- Accuracy (incl. no class): 0.9592\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9704    0.9890    0.9796     32082\n",
      "           B     0.7885    0.6591    0.7180      2235\n",
      "           I     0.6989    0.3009    0.4207       432\n",
      "\n",
      "    accuracy                         0.9592     34749\n",
      "   macro avg     0.8193    0.6497    0.7061     34749\n",
      "weighted avg     0.9554    0.9592    0.9558     34749\n",
      "\n",
      "2020-11-20 00:27:13,093 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9592,\n",
       " 'dev_score_history': [0.9511,\n",
       "  0.9528,\n",
       "  0.953,\n",
       "  0.9547,\n",
       "  0.9542,\n",
       "  0.9553,\n",
       "  0.9577,\n",
       "  0.96,\n",
       "  0.9594,\n",
       "  0.9633],\n",
       " 'train_loss_history': [5.906162380697243,\n",
       "  4.467967755291092,\n",
       "  4.166589154665214,\n",
       "  4.047764904470558,\n",
       "  3.8733545214056493,\n",
       "  3.7903423698774845,\n",
       "  3.679923255605052,\n",
       "  3.5557636374021433,\n",
       "  3.481985113060332,\n",
       "  3.430843458707589],\n",
       " 'dev_loss_history': [4.2321343421936035,\n",
       "  3.9105308055877686,\n",
       "  3.4715867042541504,\n",
       "  3.283658504486084,\n",
       "  3.2400383949279785,\n",
       "  3.3694121837615967,\n",
       "  2.9217662811279297,\n",
       "  2.8845086097717285,\n",
       "  2.8093247413635254,\n",
       "  2.6863417625427246]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "# 1. get the corpus\n",
    "print(corpus)\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'iob'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-pos',\n",
    "              embeddings_storage_mode='gpu',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the two identification systems on a text of your targeted domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rule-based identification system created from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles/txt/doc_01.txt || Text length:  17603\n"
     ]
    }
   ],
   "source": [
    "def docRB():\n",
    "    f = open('resources/doc-RB1.txt', 'w', encoding=\"utf8\")\n",
    "    doc = nlp(get_doc(\"Articles/txt/file_01.txt\"))\n",
    "    annotate_doc(doc, f)\n",
    "    f.close()\n",
    "    \n",
    "docRB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/tagtest/doc-RB1.txt || Text length:  25470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HAL <I> d : hal-01771253 https://hal.inria.fr/hal-01771253 Submitted on 19 Apr 2018 HAL is a multi - disciplinary open <B> access archive for the deposit and dissemination of scientific research documents , whether they are published or not.  The documents may come from teaching and research institutions in France or abroad , or from public or private research centers.  L’archive ouverte pluridisciplinaire HAL , est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche , publiés ou non , émanant des établissements d’enseignement et de recherche français ou étrangers , des laboratoires publics ou privés.  Distributed under a Creative Commons Attribution| 4.0 International License An Analysis of DOTA2 Using Game Refinement Measure Long Zuo , Shuo Xiong , Hiroyuki Iida To cite this version : Long Zuo , Shuo Xiong , Hiroyuki Iida.  An Analysis of DOTA2 Using Game Refinement Measure.  16th International Conference on Entertainment Computing ( ICEC ) , Sep 2017 , Tsukuba City , Japan.  pp.270 - 276 , ff10.1007/978 - 3 - 319 - 66715 - 7_29ff.  ffhal-01771253ff An Analysis of DOTA2 ? using Game Refinement Measure Long Zuo ( \\x0c ) 1 , Shuo Xiong ( \\x0c ) 2 , and Hiroyuki Iida ( \\x0c ) 3 School of Information Science , Japan Advanced Institute of Science and Technology 1 - 1 Asahidai , Nomi , Ishikawa , Japan 923 - 1211 { zuolong , xiongshuo , iida}@jaist.ac.jp1,2,3 Abstract.  DotA is one of the most attractive and influential MOBA games <B> , which has been popular in many countries for over 10 years.  It was designed for fun at first , however , after the emerge of DOTA2 it soon became the highest prize <B> e - sports game <B>.  This paper analyzes the evolutionary changes of the DOTA2.  The game <B> refinement measure is employed for the assessment of game <B> sophistication of DOTA2 series.  The analyzing results show that the rules <B> of DOTA2 have regularly been changed in its history to maintain an appropriate range <B> of game <B> refinement.  We clearly see two directions during the evolutionary changes which are the skillfulness and popularity.  Thus , the analysis makes it possible to overview these evolutionary changes of DOTA2 and find some drawbacks to be improved.  Keywords : MOBA game <B>.  Game <B> refinement theory.  Evolutionary changes.  DOTA2 1 Introduction The rules <B> of sports and mind sports have been elaborated in its long history to be more sophisticated and fascinating.  It is interesting to know their characteristics and evolutionary changes using a measurement of game <B> sophistication , which have recently been reported <B>.  For example , in the domain of sports , soccer and basketball [ 1 ] , volleyball [ 2 ] , table tennis and badminton [ 3 ] , baseball [ 4 ] and boxing [ 5 ] have their own unique histories of the rules <B> change to be more sophisticated.  In the domain of mind sports , chess [ 6 ] , Mah Jong [ 7 ] and shogi [ 8 ] have also a similar way.  These reports <B> indicate that the rules <B> of sports and mind sports have been changed to be more sophisticated , but at the comfortable level <B>.  This may relate to the flow theory [ 9 ].  DOTA2 is a free <B> - to - play <B> MOBA video <B>   game <I> developed and published by Valve Corporation.  The game <B> was released for Microsoft Windows , OS X , and Linux in July 2013 , following a Windows - only public beta testing phase that began in 2011.  DOTA2 is one of the most actively ? DOTA2 R is a registered trademark.  All intellectual property rights in and to the game <B> are owned by Valve Corporation.  played <B> games <B> on Steam , with maximum peaks of over a million concurrent players <B>.  The game <B> follows the same paradigm of a similar game <B> , which was inspired from the original DotA map.  Both games <B> follow the same idea of leveling <B> of a character <B> , gaining items <B> and hunting down non <B> - <I> player <I> controlled <B> monsters and player <B> - controlled <B> heroes <B> with the ultimate goal <B> of destroying the opponent <B> ’s base <I>.  A match <B> ends when one side breaches the opponent <B> ’s stronghold and destroys the Ancient therein.  In this study we focus <B> mainly on the domain of e - sports and have chosen DOTA2 as a benchmark.  One of the big issues in our society is the online <B> gaming <I> addiction on youth since many teenagers spend more than 15 hours a day online <B> games <I> [ 10 ].  However , little is known about the degree of game <B> sophistication of e - sports.  Therefore , we investigate the evolutionary changes of DOTA2 with the following two research questions : – What is the degree of game <B> sophistication of DOTA2 series ? – What is the most remarkable change in the history of DOTA2 series ? The structure of this paper is as follows.  Section 2 presents our assessment methodology with a focus <B> on game <B> refinement theory.  In Section 3 the game <B> refinement theory is applied to DOTA2 for its assessment and the results are discussed.  Finally , concluding remarks are given in Section 4.  2 Assessment Methodology DOTA2 is a game <B> with complex game <B> information , so we need to consider the essential game <B> progress for this game <B>.  During the in <B> - <I> game <I> period <I> , there are totally 3 game <B> information progresses.  One is the gold <B> progress , another two are the experience <B> progress and killing <B> progress.  Thus , we need to figure out an appropriate game <B> progress model of DOTA2 to apply game <B> refinement theory.  Even though DOTA2 is not a score limited game <B> , we can still clearly find a conspicuous scoring board of killing <B> at the top of the interface <B>.  This is the only scoring information that two teams <B> both know during the in <B> - <I> game <I> period <I> and they do not know the exact gold <B> and experience <B> the opponent <B> achieved.  The “ game <B> progress ” is twofold [ 11 ].  One is game <B> speed <I> or scoring rate , while another one is game <B> information progress with a focus <B> on the game <B> outcome.  In sports such as soccer and basketball , the scoring rate is calculated by two factors : ( 1 ) the goal <B> , i.e. , total score and ( 2 ) time <B> or steps to achieve the goal <B>.  Thus , the game <B>   speed <I> is given by the average number of successful shoots divided by the average <B>   number <I> of <I> shoot <I> attempts.  On the other hand , “ game <B> information progress ” presents how certain is the result of the game <B> in a certain time <B> or steps.  Let K and T be the average number of successful killings and the average number of attempt per game <B> , respectively.  If one knows the game <B> information progress , for example after the game <B> , the game <B> progress x(t ) will be given as a linear function of time <B> t with 0 ≤ t ≤ T and 0 ≤ x(t ) ≤ K , as shown in Eq.  ( 1 ).  x(t ) = K T t ( 1 ) However , the game <B> information progress given by Eq.  ( 1 ) is usually unknown during the in <B> - <I> game <I> period <I>.  Hence , the game <B> information progress is reasonably assumed to be exponential or so.  This is because the game <B> outcome is uncertain until the very end <B> of game <I> in many games <B>.  Hence , a realistic model of game <B> information progress is given by Eq.  ( 2 ).  x(t ) = K ( t T ) n ( 2 ) Here n stands for a constant parameter which is given based <B> on the perspective of an observer in the game <B> under consideration.  Thus , the acceleration of game <B> information progress is obtained by deriving Eq.  ( 2 ) twice.  Solving it at the end of the game <B> ( t = T ) , the equation becomes x 00(T ) = Kn(n − 1 ) T n t n−2 = K T2 n(n − 1 ) It is assumed in the current model that the game <B> information progress in any type of games <B> is happening in our minds.  We do not know yet about the physics in our minds , but it is likely that the acceleration of information progress is related to the force in mind.  Hence , it is reasonably expected that the larger the value K T 2 is is , the more the game <B> becomes exciting due to the uncertainty of game <B> outcome.  Thus , we apply its root square √ K T , as a game <B> refinement measure ( say GR ).  We show , in Table 1 , several sophisticated games <B> including chess and Go from boardgames , basketball and soccer from sports and DotA from MOBA games <B> [ 12 ].  We see that sophisticated games <B> have a similar GR value which we recognize a zone <B> value between 0.07 and 0.08.  This indicates the same or similar degree of game <B> sophistication where players <B> may feel the same level <B> of engagement or excitement regardless of different type of games <B>.  Table 1.  Measures of game <B> refinement for various type of games <B> Game <B> G T GR Chess [ 6 ] 35 80 0.074 Go [ 6 ] 250 208 0.076 Basketball [ 3 ] 36.38 82.01 0.073 Soccer [ 3 ] 2.64 22 0.073 Badminton [ 3 ] 46.34 79.34 0.086 Table Tennis [ 3 ] 54.86 96.47 0.077 DotA 6.80 [ 12 ] 68.6 106.2 0.078 3 Analysis and Discussion This section presents the analyzing results of DOTA2 series using the game <B> refinement measure and discusses its rule <B> changes with a focus <B> on prize <B> in a championship.  3.1 Analyzing Results To obtain the latest GR of DOTA2 series , we collect the data from the historical TI championships.  For this purpose , we download all the replay of the final to calculate its GR values.  We show , in Table 2 , GR value of each TI championship , together with prize <B> money compared [ 13 ].  Table 2 and Figure 1 Table 2.  Measures of game <B> refinement for DOTA2 series and prize <B> at TI championship Year Championship K T GR Prize ( US dollars ) 2011 TI1 51.3 93.0 0.077 1,600,000 2012 TI2 32.5 76.3 0.075 1,600,000 2013 TI3 36.6 81.8 0.074 2,874,380 2014 TI4 30.0 77.3 0.071 10,925,709 2015 TI5 39.8 89.4 0.074 18,429,613 2016 TI6 54.0 94.3 0.078 20,746,930 shows that from 2011 to 2014 GR value decreases.  The rules <B> of DOTA2 have been changed for that period to be more competitive <B> as the prize <B> became higher.  However , such rule <B> changes ( decreasing of GR value ) made DOTA2 boring for the viewers.1 On the other hand , the designer of DOTA2 has attempted many rule <B> changes with expectation that DOTA2 would have more uncertainty while adding new items <B> and incorporating the unexpected factors which mean that a lower rating team <B> would win <I> against a higher rating team <B> with higher probability than before.  Thus , after 2014 until now , GR values are increasing <B>.  3.2 Rule Changes in <B> 2011 - <I> 2013 <I> : Towards More Skillful <B> The TI championship series is the most significant and profitable annual event for DOTA2 since 2011 [ 14 ].  The game <B> designer has attempted to modify the rules <B> as described in Table 2.  In 2011 , Smoke was introduced for DOTA2 Ver.  6.70.  The Smoke of Deceit is an item <B> purchasable at the Main Shop , under Consumables <B>.  It turns <B> the user <B> and nearby ally <B> heroes <I> invisible <B> , letting them slip by wards <B> and creeps <B> undetected.  Upon activation , the user <B> and all nearby allied <B> player <B> - controlled <B> units <B> gain invisibility and bonus movement <B> speed for a brief time <B>.  Thus , many new tactics <B> were explored after the emerge of Smoke items <B>.  Then the team <B> behavior became conservative after the only three Smoke items <B> were included during the in <B> - <I> game <I> period <I>.  In 2012 the nerfed numerous heroes <B> in Ver.  6.74 has established the foundation for the TI championship to enhance the game <B> rigorism since DOTA2 has become a game <B> to be played <B> not only for fun but also for prize <B> seriously.  The appearance of the new captain mode in 2013 of Ver.  6.79 has contributed to maintain the fairness at the initial with the expectation that the rule <B> of ban and pick <B> system greatly would influence the 1 Actually many people complained about the conservative game <B> progress.  Fig.  1.  GR values and prize <B> pool of DOTA2 in <B> 2011 - <I> 2016 <I> game <B> result.  For both teams <B> , it is no longer easy <B> to choose an unbalanced hero <B> and relatively hard <B> to successfully kill <B> the enemy <B> as before.  For the period 2011- 2013 , the average number of killing , denoted as K in the game <B> progress model , has decreased year by year.  This implies that GR value has become lower.  As a result DOTA2 has become more skillful <B> and competitive <B>.  Note that DOTA2 mainly focused <B> on hero <B> development and less gank <B> or battle <B>.  3.3 Rule Changes in <B> 2014 - <I> 2016 <I> : Towards More Popular A highly <B> skill - <I> based <I> game <B> would not become popular since skill <B> itself is unfriendly to the beginners <B>.  In 2014 the new rune <B> system in Ver.  6.82 came out and added bounty <B> rune <I>.  Runes <B> are special boosters that spawn on the game <B> map <I>.  Picking <B> up a non - bounty <B> rune <I> grants the player <B> a powerful <B> effect <B> for a short time <B>.  Runes spawn at two points <B> in the river <B>.  The emerge of bounty <B> rune <I> makes the supporter or carry <B> get money easier <B> and the player <B> can purchase the items <B> earlier <B> than before.  This also accelerates the game <B> progress.  In 2015 the game <B> designer reworked the gold <B> and experience <B> mechanism <B> in Ver.  6.84.  The new mechanism <B> encouraged two teams <B> to take part in more battle <B> activities as they can get more gold <B> and experience <B> than before.  The new rules <B> focus <B> more on gank <B> and push issue instead of hero <B> development.  Another interesting mechanism <B> of scan appeared in 2016 of Ver.  6.87 and we can comprehend this mechanism <B> as a strategic skill <B> for both teams <B>.  Players <B> can use the Scan ability <B> on top of the minimap UI to detect any enemy <B> heroes <I> in an area <B>.  This mechanism <B> greatly made the game <B> more exciting and added an extra level <B> of uncertainty as the players <B> do not know the exact number of enemies <B>.  To summarize all these new mechanisms <B> accelerated the game <B> progress and enhanced the uncertainty during the in <B> - <I> game <I>   period <I>.  The new mechanism <B> offers more uncertainty for both teams <B> to win <I> or make mistake <B> in the game <B>.  Then , the game <B> has become more uncertain until the very end of the game <B>.  Thus , we see that GR value has increased <B> after 2014 and it is supposed that DOTA2 will become more and more popular in the future.  We see that the balance between skillfulness and popularity is so important for the survival of a game <B>.  3.4 High Prize As we have mentioned above , DOTA2 has over one million concurrent players <B> while being the most profitable sports in the world <B>.  It seems that DotA was first designed only for fun , however , with the contributions of sponsors and game <B> designer , DOTA2 has become a main trend of e - sports.  The dynamic changes of each version and high prize <B> made DOTA2 the most successful and profitable e - sports even in its short history.  Now DOTA2 has lack of popularity as this game <B> is still unfriendly to the novice <B> players <I> and has a relatively complex game <B> information to learn , as there are totally over 110 heroes <B> and 150 items <B>.  However , compared with other sports , we see that DOTA2 is now at the peak , as shown in Table 3 [ 15 ].  Table 3.  Tournament <B> prize <B> in sports , mind sports and e - sports compared Event Sports Prize ( US dollars ) 1st Prize Australia Open Tennis 35,530,000 1,040,000 NBA Basketball 14,000,000 4,100,000 FIFA Club World Cup Soccer 28,000,000 5,490,000 Ing Cup Go 650,000 400,000 S6 League of Legends 5,070,000 2,130,000 TI 6 DOTA2 20,746,930 9,140,000 4 Concluding Remarks In this study we evaluated the DOTA2 series using the game <B> refinement measurement.  The results indicate that DOTA2 has a similar zone <B> value with sophisticated sports and boardgames.  In addition , DOTA2 championship of every year during 2011 - 2016 was analyzed.  The results show that the game <B> refinement value has stayed within 0.071 - 0.077 , which is slightly lower than DotA. The prize <B> of the championship has strongly influenced the development of DOTA2.  Higher prize <B> enforced the players <B> to be more conservative and the game <B> refinement value became lower which implies that DOTA2 became more skillful <B>.  However , such a direction of game <B> evolution <B> was not accepted in DOTA2 community <B> due to the lack of entertainment.  Later , the direction of DOTA2 evolution <B> was shifted to be more popular while taking stochastic elements into consideration.  Thus we see that a good balance between skillfulness and popularity is essential to survive.  References 1.  Sutiono , A. P. , Purwarianti , A. , Iida , H. ( 2014 , July ).  A mathematical model of game <B> refinement.  In International Conference on Intelligent Technologies for Interactive Entertainment ( pp.  148 - 151 ).  Springer International Publishing.  2.  Takeuchi , J. , Ramadan , R. , Iida , H. ( 2014 ).  Game <B> refinement theory and its application to Volleyball.  Research Report 2014-GI-31 ( 3 ) , Information Processing Society of Japan , 1 - 6.  3.  Nossal , N. , Iida , H. ( 2014 , October ).  Game <B> refinement theory and its application to score limit games <B>.  In Games Media Entertainment ( GEM ) , 2014 IEEE ( pp.  1 - 3 ).  IEEE.  4.  Yuranana , K. , Panumate , C. , Iida , H. , Tanaka , K. ( 2016 ).  Measuring Sophistication of Sports Games : The First Result from Baseball.  5.  Panumate , C. , Iida , H. An Approach to Quantifying Boxing ’s Entertainment.  6.  Cincotti , A. , Iida , H. , Yoshimura , J. ( 2007 ).  Refinement and complexity in the evolution <B> of chess.  In Proceedings of the 10th International Conference on COmputer Science and Informatics ( pp.  650 - 654 ).  7.  Iida , H. , Takahara , K. , Nagashima , J. , Kajihara , Y. , Hashimoto , T. ( 2004 , September ).  An application of game <B> - refinement theory to Mah Jong.  In International Conference on Entertainment Computing ( pp.  333 - 338 ).  Springer Berlin Heidelberg.  8.  Iida , H. , Takeshita , N. , Yoshimura , J. ( 2003 ).  A metric for entertainment of boardgames : its implication for evolution <B> of chess variants.  In Entertainment Computing ( pp.  65 - 72 ).  Springer US.  9.  Csikszentmihalyi , M. ( 1990 ).  Flow.  The Psychology of Optimal Experience.  New York ( HarperPerennial ) 1990.  10.  Perrin , A. , Duggan , M. ( 2015 ).  Americans ’ Internet Access : 2000–2015 : As Internet Use Nears Saturation for Some Groups , a Look at Patterns of Adoption.  [ Pew Internet project data memo ].  11.  M´ajek , P. , Iida , H. ( 2004 ).  Uncertainty of game <B> outcome.  In 3rd International Conference on Global Research and Education in Intelligent Systems ( pp.  171 - 180 ).  12.  Xiong , S. , Zuo , L. , Iida , H. ( 2014 ).  Quantifying engagement of electronic sports game <B>.  Advances in Social and Behavioral Sciences , 5 , 37 - 42.  13.  e - Sports Earnings.  Top Games of 2016.  ( 2017 ) , [ Online ].  http://www.esportsearnings.com/history/2016/games 14.  GAMEPEDIA.  DOTA2Wiki.  ( 2017 ) , [ Online ].  http://DOTA2.gamepedia.com/DOTA2Wiki 15.  List of prizes <B> , medals and awards.  ( 2017 , June 14 ).  [ Online ].  https://en.wikipedia.org/wiki/Listofprizes,medalsandawards '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest2 = get_doc(\"resources/doc-RB1.txt\")\n",
    "doctest2 = doctest2.replace(\"\\n\", \" \")\n",
    "doctest2 = doctest2.replace(\" O \", \" \")\n",
    "doctest2 = doctest2.replace(\" B \", \" <B> \")\n",
    "doctest2 = doctest2.replace(\" I \", \" <I> \")\n",
    "doctest2 = doctest2.replace(\" .\", \".\")\n",
    "doctest2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence tagger trained model from flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-20 00:27:13,580 loading file resources/taggers/example-pos/final-model.pt\n",
      "Articles/txt/doc_01.txt || Text length:  17603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HAL Id : hal-01771253 https :// hal.inria.fr / hal-01771253 Submitted on 19 Apr 2018 HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents , whether they are published or not . The documents may come from teaching and research institutions in France or abroad , or from public or private research centers . L’archive ouverte pluridisciplinaire HAL , est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche , publiés ou non , émanant des établissements d’enseignement et de recherche français ou étrangers , des laboratoires publics ou privés . Distributed under a Creative Commons Attribution | 4.0 International License An Analysis of DOTA2 Using Game <B> Refinement Measure Long Zuo , Shuo Xiong , Hiroyuki Iida To cite this version : Long Zuo , Shuo Xiong , Hiroyuki Iida . An Analysis of DOTA2 Using Game <B> Refinement Measure . 16th International Conference on Entertainment Computing ( ICEC ) , Sep 2017 , Tsukuba City , Japan . pp.270-276 , ff10.1007 / 978-3-319-66715-7 _ 29ff. ffhal-01771253ff An Analysis of DOTA2 ? using Game <B> Refinement Measure Long Zuo ( ) 1 , Shuo Xiong ( ) 2 , and Hiroyuki Iida ( ) 3 School of Information Science , Japan Advanced Institute of Science and Technology 1-1 <B> Asahidai , Nomi , Ishikawa , Japan 923-1211 { zuolong , xiongshuo , iida }@ jaist.ac.jp1,2,3 Abstract . DotA is one of the most attractive and influential MOBA games <B> , which has been popular in many countries for over 10 years . It was designed for fun <B> at first , however , after the emerge of DOTA2 it soon became the highest prize e-sports game <B> . This paper analyzes the evolutionary changes of the DOTA2 . The game <B> refinement measure is employed for the assessment of game <B> sophistication of DOTA2 series . The analyzing results show that the rules of DOTA2 have regularly been changed in its history to maintain an appropriate range of game <B> refinement . We clearly see two directions during the evolutionary changes which are the skillfulness and popularity . Thus , the analysis makes it possible to overview these evolutionary changes of DOTA2 and find some drawbacks to be improved . Keywords : MOBA game <B> . Game <B> refinement theory . Evolutionary changes . DOTA2 1 Introduction The rules of sports and mind sports have been elaborated in its long history to be more sophisticated and fascinating . It is interesting to know their characteristics and evolutionary changes using a measurement of game <B> sophistication , which have recently been reported <B> . For example , in the domain of sports , soccer and basketball [ 1 ] , volleyball [ 2 ] , table tennis and badminton [ 3 ] , baseball [ 4 ] and boxing [ 5 ] have their own unique histories of the rules change to be more sophisticated . In the domain of mind sports , chess [ 6 ] , Mah Jong [ 7 ] and shogi [ 8 ] have also a similar way . These reports <B> indicate that the rules of sports and mind sports have been changed to be more sophisticated , but at the comfortable level <B> . This may relate to the flow theory [ 9 ] . DOTA2 is a free-to-play MOBA video game <B> developed and published by Valve Corporation . The game <B> was released for Microsoft Windows , OS X , and Linux in July 2013 , following a Windows-only public beta testing phase that began in 2011 . DOTA2 is one of the most actively ? DOTA2 R is a registered trademark . All intellectual property rights in and to the game <B> are owned by Valve Corporation . played <B> games <B> on Steam , with maximum peaks of over a million concurrent players <B> . The game <B> follows the same paradigm of a similar game <B> , which was inspired from the original DotA map . Both games <B> follow the same idea of leveling of a character <B> , gaining items <B> and hunting <B> down non-player controlled <B> monsters <B> and player-controlled heroes <B> with the ultimate goal <B> of destroying the opponent <B> ’s base <B> . A match <B> ends <B> when one side breaches the opponent <B> ’s stronghold <B> and destroys the Ancient therein . In this study we focus mainly on the domain of e-sports and have chosen DOTA2 as a benchmark . One of the big <B> issues in our society is the online gaming addiction on youth since many teenagers spend more than 15 hours a day online <B> games <I> [ 10 ] . However , little is known about the degree of game <B> sophistication of e-sports . Therefore , we investigate the evolutionary changes of DOTA2 with the following two research questions : – What is the degree of game <B> sophistication of DOTA2 series ? – What is the most remarkable change in the history of DOTA2 series ? The structure of this paper is as follows . Section 2 presents our assessment methodology with a focus <B> on game <B> refinement theory . In Section 3 the game <B> refinement theory is applied to DOTA2 for its assessment and the results are discussed . Finally , concluding remarks are given in Section 4 . 2 Assessment Methodology DOTA2 is a game <B> with complex game <B> information , so we need to consider the essential game <B> progress for this game <B> . During the in-game period , there are totally 3 game <B> information progresses . One is the gold <B> progress , another two are the experience <B> progress and killing <B> progress . Thus , we need to figure out an appropriate game <B> progress model of DOTA2 to apply game <B> refinement theory . Even though DOTA2 is not a score limited game <B> , we can still clearly find a conspicuous scoring <B> board of killing <B> at the top of the interface . This is the only scoring <B> information that two teams <B> both know during the in-game period <B> and they do not know the exact gold <B> and experience the opponent <B> achieved . The “ game <B> progress ” is twofold [ 11 ] . One is game <B> speed or scoring <B> rate <I> , while another one is game <B> information progress with a focus <B> on the game <B> outcome . In sports such as soccer <B> and basketball <B> , the scoring <B> rate <I> is calculated by two factors : ( 1 ) the goal <B> , i.e. , total score and ( 2 ) time <B> or steps to achieve the goal <B> . Thus , the game <B> speed is given by the average number of successful shoots divided by the average number of shoot <B> attempts . On the other hand , “ game <B> information progress ” presents how certain is the result of the game <B> in a certain time <B> or steps . Let K and T be the average number of successful killings <B> and the average number of attempt per game <B> , respectively . If one knows the game <B> information progress , for example after the game <B> , the game <B> progress x ( t ) will be given as a linear function of time <B> t with 0 ≤ t ≤ T and 0 ≤ x ( t ) ≤ K , as shown in Eq . ( 1 ) . x ( t ) = K T t ( 1 ) However , the game <B> information progress given by Eq . ( 1 ) is usually unknown during the in-game period . Hence , the game <B> information progress is reasonably assumed to be exponential or so . This is because the game <B> outcome is uncertain until the very end of game <B> in many games <B> . Hence , a realistic model of game <B> information progress is given by Eq . ( 2 ) . x ( t ) = K ( t T ) n ( 2 ) Here n stands for a constant parameter which is given based <B> on the perspective of an observer in the game <B> under consideration . Thus , the acceleration of game <B> information progress is obtained by deriving Eq . ( 2 ) twice . Solving it at the end of the game <B> ( t = T ) , the equation becomes x 00 ( T ) = Kn ( n − 1 ) T n t n − 2 = K T2 n ( n − 1 ) It is assumed in the current model that the game <B> information progress in any type of games <B> is happening in our minds . We do not know yet about the physics in our minds , but it is likely that the acceleration of information progress is related to the force <B> in mind . Hence , it is reasonably expected that the larger the value K T 2 is is , the more the game <B> becomes exciting <B> due to the uncertainty of game <B> outcome . Thus , we apply its root square √ K T , as a game <B> refinement measure ( say GR ) . We show , in Table 1 , several sophisticated games <B> including chess and Go from boardgames , basketball <B> and soccer from sports and DotA from MOBA games <B> [ 12 ] . We see that sophisticated games <B> have a similar GR value which we recognize a zone value between 0.07 and 0.08 . This indicates the same or similar degree of game <B> sophistication where players <B> may feel the same level <B> of engagement or excitement regardless of different type of games <B> . Table 1 . Measures of game <B> refinement for various type of games <B> Game <B> G T GR Chess [ 6 ] 35 80 0.074 Go [ 6 ] 250 208 0.076 Basketball <B> [ 3 ] 36.38 82.01 0.073 Soccer [ 3 ] 2.64 22 0.073 Badminton [ 3 ] 46.34 79.34 0.086 Table Tennis [ 3 ] 54.86 96.47 0.077 DotA 6.80 [ 12 ] 68.6 106.2 0.078 3 Analysis and Discussion This section presents the analyzing results of DOTA2 series using the game <B> refinement measure and discusses its rule changes with a focus <B> on prize in a championship <B> . 3.1 Analyzing Results To obtain the latest GR of DOTA2 series , we collect the data from the historical TI championships <B> . For this purpose , we download all the replay of the final <B> to calculate its GR values . We show , in Table 2 , GR value of each TI championship <B> , together with prize money compared [ 13 ] . Table 2 and Figure 1 Table 2 . Measures of game <B> refinement for DOTA2 series and prize at TI championship Year Championship <B> K T GR Prize ( US dollars ) 2011 TI1 51.3 93.0 0.077 1,600,000 2012 TI2 32.5 76.3 0.075 1,600,000 2013 TI3 36.6 81.8 0.074 2,874,380 2014 TI4 30.0 77.3 0.071 10,925,709 2015 TI5 39.8 89.4 0.074 18,429,613 2016 TI6 54.0 94.3 0.078 20,746,930 shows that from 2011 to 2014 GR value decreases . The rules of DOTA2 have been changed for that period <B> to be more competitive <B> as the prize became higher . However , such rule changes ( decreasing of GR value ) made DOTA2 boring for the viewers.1 On the other hand , the designer of DOTA2 has attempted many rule changes with expectation that DOTA2 would have more uncertainty while adding new items <B> and incorporating the unexpected factors which mean that a lower rating team <B> would win <B> against a higher rating team <B> with higher probability than before . Thus , after 2014 until now , GR values are increasing . 3.2 Rule Changes in 2011-2013 : Towards More Skillful The TI championship <B> series is the most significant and profitable annual event <B> for DOTA2 since 2011 [ 14 ] . The game <B> designer has attempted to modify the rules <B> as described in Table 2 . In 2011 , Smoke was introduced for DOTA2 Ver . 6.70 . The Smoke <B> of Deceit is an item <B> purchasable at the Main Shop , under Consumables . It turns the user <B> and nearby ally <B> heroes <B> invisible <B> , letting them slip <B> by wards and creeps undetected . Upon activation , the user <B> and all nearby allied <B> player-controlled units <B> gain invisibility and bonus movement <B> speed for a brief time <B> . Thus , many new tactics <B> were explored after the emerge of Smoke <B> items . Then the team <B> behavior became conservative after the only three Smoke <B> items were included during the in-game period . In 2012 the nerfed numerous heroes <B> in Ver . 6.74 has established the foundation for the TI championship <B> to enhance the game <B> rigorism since DOTA2 has become a game <B> to be played <B> not only for fun <B> but also for prize seriously . The appearance of the new captain mode in 2013 of Ver . 6.79 has contributed to maintain the fairness at the initial with the expectation that the rule of ban and pick <B> system greatly would influence the 1 Actually many people complained about the conservative game <B> progress . Fig. 1. GR values and prize pool of DOTA2 in 2011-2016 game <B> result . For both teams <B> , it is no longer easy to choose an unbalanced hero <B> and relatively hard to successfully kill <B> the enemy <B> as before . For the period 2011 - 2013 , the average number of killing <B> , denoted as K in the game <B> progress model , has decreased year by year . This implies that GR value has become lower . As a result DOTA2 has become more skillful and competitive <B> . Note that DOTA2 mainly focused on hero <B> development and less gank or battle <B> . 3.3 Rule Changes in 2014-2016 : Towards More Popular A highly skill-based game <B> would not become popular since skill <B> itself is unfriendly to the beginners . In 2014 the new rune system in Ver . 6.82 came out and added bounty rune . Runes are special boosters that spawn on the game <B> map . Picking up a non-bounty rune grants the player <B> a powerful <B> effect for a short time <B> . Runes spawn at two points <B> in the river . The emerge of bounty rune makes the supporter <B> or carry get money easier and the player <B> can purchase the items <B> earlier than before . This also accelerates the game <B> progress . In 2015 the game <B> designer reworked the gold <B> and experience mechanism <B> in Ver . 6.84 . The new mechanism <B> encouraged two teams <B> to take part in more battle <B> activities as they can get more gold <B> and experience <B> than before . The new rules focus more on gank and push issue instead of hero <B> development . Another interesting mechanism <B> of scan appeared in 2016 of Ver . 6.87 and we can comprehend this mechanism <B> as a strategic skill <B> for both teams <B> . Players <B> can use the Scan ability <B> on top of the minimap UI to detect any enemy <B> heroes <B> in an area <B> . This mechanism <B> greatly made the game <B> more exciting <B> and added an extra level <B> of uncertainty as the players <B> do not know the exact number of enemies <B> . To summarize all these new mechanisms accelerated the game <B> progress and enhanced the uncertainty during the in-game period . The new mechanism <B> offers more uncertainty for both teams <B> to win <B> or make mistake in the game <B> . Then , the game <B> has become more uncertain until the very end of the game <B> . Thus , we see that GR value has increased <B> after 2014 and it is supposed that DOTA2 will become more and more popular in the future . We see that the balance between skillfulness and popularity is so important for the survival of a game <B> . 3.4 High Prize As we have mentioned above , DOTA2 has over one million concurrent players <B> while being the most profitable sports in the world <B> . It seems that DotA was first designed only for fun , however , with the contributions of sponsors and game <B> designer , DOTA2 has become a main trend of e-sports . The dynamic changes of each version and high prize made DOTA2 the most successful and profitable e-sports even in its short history . Now DOTA2 has lack of popularity as this game <B> is still unfriendly to the novice players <B> and has a relatively complex game <B> information to learn , as there are totally over 110 heroes <B> and 150 items <B> . However , compared with other sports , we see that DOTA2 is now at the peak , as shown in Table 3 [ 15 ] . Table 3 . Tournament <B> prize in sports , mind sports and e-sports compared Event Sports Prize ( US dollars ) 1st Prize Australia Open Tennis <B> 35,530,000 1,040,000 NBA Basketball <B> 14,000,000 4,100,000 FIFA Club World Cup <B> Soccer 28,000,000 5,490,000 Ing Cup <B> Go 650,000 400,000 S6 League of Legends 5,070,000 2,130,000 TI 6 DOTA2 20,746,930 9,140,000 4 Concluding Remarks In this study we evaluated the DOTA2 series using the game <B> refinement measurement . The results indicate that DOTA2 has a similar zone value with sophisticated sports and boardgames . In addition , DOTA2 championship <B> of every year during 2011-2016 was analyzed . The results show that the game <B> refinement value has stayed within 0.071-0.077 , which is slightly lower than DotA . The prize of the championship <B> has strongly influenced the development of DOTA2 . Higher prize enforced the players <B> to be more conservative and the game <B> refinement value became lower which implies that DOTA2 became more skillful . However , such a direction of game <B> evolution was not accepted in DOTA2 community due to the lack of entertainment . Later , the direction of DOTA2 evolution was shifted to be more popular while taking stochastic elements into consideration . Thus we see that a good balance between skillfulness and popularity is essential to survive . References 1. Sutiono , A. P. , Purwarianti , A. , Iida , H. ( 2014 , July ) . A mathematical model of game <B> refinement . In International Conference on Intelligent Technologies for Interactive Entertainment ( pp. 148-151 ) . Springer International Publishing . 2. Takeuchi , J. , Ramadan , R. , Iida , H. ( 2014 ) . Game <B> refinement theory and its application to Volleyball <B> . Research Report 2014-GI-31 ( 3 ) , Information Processing Society of Japan , 1-6 . 3. Nossal , N. , Iida , H. ( 2014 , October ) . Game <B> refinement theory and its application to score limit games <B> . In Games <B> Media Entertainment ( GEM ) , 2014 IEEE ( pp. 1-3 ) . IEEE . 4. Yuranana , K. , Panumate , C. , Iida , H. , Tanaka , K. ( 2016 ) . Measuring Sophistication of Sports Games <B> : The First Result from Baseball . 5. Panumate , C. , Iida , H. An Approach to Quantifying Boxing ’s Entertainment . 6. Cincotti , A. , Iida , H. , Yoshimura , J. ( 2007 ) . Refinement and complexity in the evolution of chess . In Proceedings of the 10th International Conference on COmputer Science and Informatics ( pp. 650-654 ) . 7. Iida , H. , Takahara , K. , Nagashima , J. , Kajihara , Y. , Hashimoto , T. ( 2004 , September ) . An application of game-refinement theory to Mah Jong . In International Conference on Entertainment Computing ( pp. 333-338 ) . Springer Berlin Heidelberg . 8. Iida , H. , Takeshita , N. , Yoshimura , J. ( 2003 ) . A metric for entertainment of boardgames : its implication for evolution of chess variants . In Entertainment Computing ( pp. 65-72 ) . Springer US . 9. Csikszentmihalyi , M. ( 1990 ) . Flow . The Psychology of Optimal Experience . New York ( HarperPerennial ) 1990 . 10. Perrin , A. , Duggan , M. ( 2015 ) . Americans’ Internet Access : 2000 – 2015 : As Internet Use Nears Saturation for Some Groups <B> , a Look at Patterns of Adoption . [ Pew Internet project data memo ] . 11. M´ajek , P. , Iida , H. ( 2004 ) . Uncertainty of game <B> outcome . In 3rd International Conference on Global Research and Education in Intelligent Systems ( pp. 171-180 ) . 12. Xiong , S. , Zuo , L. , Iida , H. ( 2014 ) . Quantifying engagement of electronic sports game <B> . Advances in Social and Behavioral Sciences , 5 , 37-42 . 13. e-Sports Earnings . Top Games <B> of 2016 . ( 2017 ) , [ Online ] . http :// www.esportsearnings.com / history / 2016 / games <B> 14. GAMEPEDIA . DOTA2Wiki . ( 2017 ) , [ Online ] . http :// DOTA2.gamepedia.com / DOTA2Wiki 15. List of prizes , medals and awards . ( 2017 , June 14 ) . [ Online ] . https :// en.wikipedia.org / wiki / Listofprizes,medalsandawards'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# load the model you trained\n",
    "model = SequenceTagger.load('resources/taggers/example-pos/final-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "doctest = get_doc(\"Articles/txt/file_01.txt\")\n",
    "sentence = Sentence(doctest)\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "doctest1 = sentence.to_tagged_string()\n",
    "doctest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
