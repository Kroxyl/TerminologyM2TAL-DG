HAL O
I O
d O
: O
hal-01771253 O
https://hal.inria.fr/hal-01771253 O
Submitted O
on O
19 O
Apr O
2018 O
HAL O
is O
a O
multi O
- O
disciplinary O
open B
access O
archive O
for O
the O
deposit O
and O
dissemination O
of O
scientific O
research O
documents O
, O
whether O
they O
are O
published O
or O
not O
. O

The O
documents O
may O
come O
from O
teaching O
and O
research O
institutions O
in O
France O
or O
abroad O
, O
or O
from O
public O
or O
private O
research O
centers O
. O

L’archive O
ouverte O
pluridisciplinaire O
HAL O
, O
est O
destinée O
au O
dépôt O
et O
à O
la O
diffusion O
de O
documents O
scientifiques O
de O
niveau O
recherche O
, O
publiés O
ou O
non O
, O
émanant O
des O
établissements O
d’enseignement O
et O
de O
recherche O
français O
ou O
étrangers O
, O
des O
laboratoires O
publics O
ou O
privés O
. O

Distributed O
under O
a O
Creative O
Commons O
Attribution| O
4.0 O
International O
License O
An O
Analysis O
of O
DOTA2 O
Using O
Game O
Refinement O
Measure O
Long O
Zuo O
, O
Shuo O
Xiong O
, O
Hiroyuki O
Iida O
To O
cite O
this O
version O
: O
Long O
Zuo O
, O
Shuo O
Xiong O
, O
Hiroyuki O
Iida O
. O

An O
Analysis O
of O
DOTA2 O
Using O
Game O
Refinement O
Measure O
. O

16th O
International O
Conference O
on O
Entertainment O
Computing O
( O
ICEC O
) O
, O
Sep O
2017 O
, O
Tsukuba O
City O
, O
Japan O
. O

pp.270 O
- O
276 O
, O
ff10.1007/978 O
- O
3 O
- O
319 O
- O
66715 O
- O
7_29ff O
. O

ffhal-01771253ff O
An O
Analysis O
of O
DOTA2 O
? O
using O
Game O
Refinement O
Measure O
Long O
Zuo O
( O
 O
) O
1 O
, O
Shuo O
Xiong O
( O
 O
) O
2 O
, O
and O
Hiroyuki O
Iida O
( O
 O
) O
3 O
School O
of O
Information O
Science O
, O
Japan O
Advanced O
Institute O
of O
Science O
and O
Technology O
1 O
- O
1 O
Asahidai O
, O
Nomi O
, O
Ishikawa O
, O
Japan O
923 O
- O
1211 O
{ O
zuolong O
, O
xiongshuo O
, O
iida}@jaist.ac.jp1,2,3 O
Abstract O
. O

DotA O
is O
one O
of O
the O
most O
attractive O
and O
influential O
MOBA O
games B
, O
which O
has O
been O
popular O
in O
many O
countries O
for O
over O
10 O
years O
. O

It O
was O
designed O
for O
fun O
at O
first O
, O
however O
, O
after O
the O
emerge O
of O
DOTA2 O
it O
soon O
became O
the O
highest O
prize B
e O
- O
sports O
game B
. O

This O
paper O
analyzes O
the O
evolutionary O
changes O
of O
the O
DOTA2 O
. O

The O
game B
refinement O
measure O
is O
employed O
for O
the O
assessment O
of O
game B
sophistication O
of O
DOTA2 O
series O
. O

The O
analyzing O
results O
show O
that O
the O
rules B
of O
DOTA2 O
have O
regularly O
been O
changed O
in O
its O
history O
to O
maintain O
an O
appropriate O
range B
of O
game B
refinement O
. O

We O
clearly O
see O
two O
directions O
during O
the O
evolutionary O
changes O
which O
are O
the O
skillfulness O
and O
popularity O
. O

Thus O
, O
the O
analysis O
makes O
it O
possible O
to O
overview O
these O
evolutionary O
changes O
of O
DOTA2 O
and O
find O
some O
drawbacks O
to O
be O
improved O
. O

Keywords O
: O
MOBA O
game B
. O

Game B
refinement O
theory O
. O

Evolutionary O
changes O
. O

DOTA2 O
1 O
Introduction O
The O
rules B
of O
sports O
and O
mind O
sports O
have O
been O
elaborated O
in O
its O
long O
history O
to O
be O
more O
sophisticated O
and O
fascinating O
. O

It O
is O
interesting O
to O
know O
their O
characteristics O
and O
evolutionary O
changes O
using O
a O
measurement O
of O
game B
sophistication O
, O
which O
have O
recently O
been O
reported B
. O

For O
example O
, O
in O
the O
domain O
of O
sports O
, O
soccer O
and O
basketball O
[ O
1 O
] O
, O
volleyball O
[ O
2 O
] O
, O
table O
tennis O
and O
badminton O
[ O
3 O
] O
, O
baseball O
[ O
4 O
] O
and O
boxing O
[ O
5 O
] O
have O
their O
own O
unique O
histories O
of O
the O
rules B
change O
to O
be O
more O
sophisticated O
. O

In O
the O
domain O
of O
mind O
sports O
, O
chess O
[ O
6 O
] O
, O
Mah O
Jong O
[ O
7 O
] O
and O
shogi O
[ O
8 O
] O
have O
also O
a O
similar O
way O
. O

These O
reports B
indicate O
that O
the O
rules B
of O
sports O
and O
mind O
sports O
have O
been O
changed O
to O
be O
more O
sophisticated O
, O
but O
at O
the O
comfortable O
level B
. O

This O
may O
relate O
to O
the O
flow O
theory O
[ O
9 O
] O
. O

DOTA2 O
is O
a O
free B
- O
to O
- O
play B
MOBA O
video B

 O
game I
developed O
and O
published O
by O
Valve O
Corporation O
. O

The O
game B
was O
released O
for O
Microsoft O
Windows O
, O
OS O
X O
, O
and O
Linux O
in O
July O
2013 O
, O
following O
a O
Windows O
- O
only O
public O
beta O
testing O
phase O
that O
began O
in O
2011 O
. O

DOTA2 O
is O
one O
of O
the O
most O
actively O
? O
DOTA2 O
R O
is O
a O
registered O
trademark O
. O

All O
intellectual O
property O
rights O
in O
and O
to O
the O
game B
are O
owned O
by O
Valve O
Corporation O
. O

played B
games B
on O
Steam O
, O
with O
maximum O
peaks O
of O
over O
a O
million O
concurrent O
players B
. O

The O
game B
follows O
the O
same O
paradigm O
of O
a O
similar O
game B
, O
which O
was O
inspired O
from O
the O
original O
DotA O
map O
. O

Both O
games B
follow O
the O
same O
idea O
of O
leveling O
of O
a O
character B
, O
gaining O
items B
and O
hunting O
down O
non B
- I
player I
controlled B
monsters O
and O
player B
- O
controlled B
heroes B
with O
the O
ultimate O
goal B
of O
destroying O
the O
opponent B
’s O
base I
. O

A O
match B
ends O
when O
one O
side O
breaches O
the O
opponent B
’s O
stronghold O
and O
destroys O
the O
Ancient O
therein O
. O

In O
this O
study O
we O
focus B
mainly O
on O
the O
domain O
of O
e O
- O
sports O
and O
have O
chosen O
DOTA2 O
as O
a O
benchmark O
. O

One O
of O
the O
big O
issues O
in O
our O
society O
is O
the O
online O
gaming O
addiction O
on O
youth O
since O
many O
teenagers O
spend O
more O
than O
15 O
hours O
a O
day O
online B
games I
[ O
10 O
] O
. O

However O
, O
little O
is O
known O
about O
the O
degree O
of O
game B
sophistication O
of O
e O
- O
sports O
. O

Therefore O
, O
we O
investigate O
the O
evolutionary O
changes O
of O
DOTA2 O
with O
the O
following O
two O
research O
questions O
: O
– O
What O
is O
the O
degree O
of O
game B
sophistication O
of O
DOTA2 O
series O
? O
– O
What O
is O
the O
most O
remarkable O
change O
in O
the O
history O
of O
DOTA2 O
series O
? O
The O
structure O
of O
this O
paper O
is O
as O
follows O
. O

Section O
2 O
presents O
our O
assessment O
methodology O
with O
a O
focus B
on O
game B
refinement O
theory O
. O

In O
Section O
3 O
the O
game B
refinement O
theory O
is O
applied O
to O
DOTA2 O
for O
its O
assessment O
and O
the O
results O
are O
discussed O
. O

Finally O
, O
concluding O
remarks O
are O
given O
in O
Section O
4 O
. O

2 O
Assessment O
Methodology O
DOTA2 O
is O
a O
game B
with O
complex O
game B
information O
, O
so O
we O
need O
to O
consider O
the O
essential O
game B
progress O
for O
this O
game B
. O

During O
the O
in B
- I
game I
period I
, O
there O
are O
totally O
3 O
game B
information O
progresses O
. O

One O
is O
the O
gold B
progress O
, O
another O
two O
are O
the O
experience B
progress O
and O
killing B
progress O
. O

Thus O
, O
we O
need O
to O
figure O
out O
an O
appropriate O
game B
progress O
model O
of O
DOTA2 O
to O
apply O
game B
refinement O
theory O
. O

Even O
though O
DOTA2 O
is O
not O
a O
score O
limited O
game B
, O
we O
can O
still O
clearly O
find O
a O
conspicuous O
scoring O
board O
of O
killing B
at O
the O
top O
of O
the O
interface B
. O

This O
is O
the O
only O
scoring O
information O
that O
two O
teams B
both O
know O
during O
the O
in B
- I
game I
period I
and O
they O
do O
not O
know O
the O
exact O
gold B
and O
experience B
the O
opponent B
achieved O
. O

The O
“ O
game B
progress O
” O
is O
twofold O
[ O
11 O
] O
. O

One O
is O
game B
speed I
or O
scoring O
rate O
, O
while O
another O
one O
is O
game B
information O
progress O
with O
a O
focus B
on O
the O
game B
outcome O
. O

In O
sports O
such O
as O
soccer O
and O
basketball O
, O
the O
scoring O
rate O
is O
calculated O
by O
two O
factors O
: O
( O
1 O
) O
the O
goal B
, O
i.e. O
, O
total O
score O
and O
( O
2 O
) O
time B
or O
steps O
to O
achieve O
the O
goal B
. O

Thus O
, O
the O
game B

 O
speed I
is O
given O
by O
the O
average O
number O
of O
successful O
shoots O
divided O
by O
the O
average B

 O
number I
of I
shoot I
attempts O
. O

On O
the O
other O
hand O
, O
“ O
game B
information O
progress O
” O
presents O
how O
certain O
is O
the O
result O
of O
the O
game B
in O
a O
certain O
time B
or O
steps O
. O

Let O
K O
and O
T O
be O
the O
average O
number O
of O
successful O
killings O
and O
the O
average O
number O
of O
attempt O
per O
game B
, O
respectively O
. O

If O
one O
knows O
the O
game B
information O
progress O
, O
for O
example O
after O
the O
game B
, O
the O
game B
progress O
x(t O
) O
will O
be O
given O
as O
a O
linear O
function O
of O
time B
t O
with O
0 O
≤ O
t O
≤ O
T O
and O
0 O
≤ O
x(t O
) O
≤ O
K O
, O
as O
shown O
in O
Eq O
. O

( O
1 O
) O
. O

x(t O
) O
= O
K O
T O
t O
( O
1 O
) O
However O
, O
the O
game B
information O
progress O
given O
by O
Eq O
. O

( O
1 O
) O
is O
usually O
unknown O
during O
the O
in B
- I
game I
period I
. O

Hence O
, O
the O
game B
information O
progress O
is O
reasonably O
assumed O
to O
be O
exponential O
or O
so O
. O

This O
is O
because O
the O
game B
outcome O
is O
uncertain O
until O
the O
very O
end B
of O
game I
in O
many O
games B
. O

Hence O
, O
a O
realistic O
model O
of O
game B
information O
progress O
is O
given O
by O
Eq O
. O

( O
2 O
) O
. O

x(t O
) O
= O
K O
( O
t O
T O
) O
n O
( O
2 O
) O
Here O
n O
stands O
for O
a O
constant O
parameter O
which O
is O
given O
based B
on O
the O
perspective O
of O
an O
observer O
in O
the O
game B
under O
consideration O
. O

Thus O
, O
the O
acceleration O
of O
game B
information O
progress O
is O
obtained O
by O
deriving O
Eq O
. O

( O
2 O
) O
twice O
. O

Solving O
it O
at O
the O
end O
of O
the O
game B
( O
t O
= O
T O
) O
, O
the O
equation O
becomes O
x O
00(T O
) O
= O
Kn(n O
− O
1 O
) O
T O
n O
t O
n−2 O
= O
K O
T2 O
n(n O
− O
1 O
) O
It O
is O
assumed O
in O
the O
current O
model O
that O
the O
game B
information O
progress O
in O
any O
type O
of O
games B
is O
happening O
in O
our O
minds O
. O

We O
do O
not O
know O
yet O
about O
the O
physics O
in O
our O
minds O
, O
but O
it O
is O
likely O
that O
the O
acceleration O
of O
information O
progress O
is O
related O
to O
the O
force O
in O
mind O
. O

Hence O
, O
it O
is O
reasonably O
expected O
that O
the O
larger O
the O
value O
K O
T O
2 O
is O
is O
, O
the O
more O
the O
game B
becomes O
exciting O
due O
to O
the O
uncertainty O
of O
game B
outcome O
. O

Thus O
, O
we O
apply O
its O
root O
square O
√ O
K O
T O
, O
as O
a O
game B
refinement O
measure O
( O
say O
GR O
) O
. O

We O
show O
, O
in O
Table O
1 O
, O
several O
sophisticated O
games B
including O
chess O
and O
Go O
from O
boardgames O
, O
basketball O
and O
soccer O
from O
sports O
and O
DotA O
from O
MOBA O
games B
[ O
12 O
] O
. O

We O
see O
that O
sophisticated O
games B
have O
a O
similar O
GR O
value O
which O
we O
recognize O
a O
zone B
value O
between O
0.07 O
and O
0.08 O
. O

This O
indicates O
the O
same O
or O
similar O
degree O
of O
game B
sophistication O
where O
players B
may O
feel O
the O
same O
level B
of O
engagement O
or O
excitement O
regardless O
of O
different O
type O
of O
games B
. O

Table O
1 O
. O

Measures O
of O
game B
refinement O
for O
various O
type O
of O
games B
Game O
G O
T O
GR O
Chess O
[ O
6 O
] O
35 O
80 O
0.074 O
Go O
[ O
6 O
] O
250 O
208 O
0.076 O
Basketball O
[ O
3 O
] O
36.38 O
82.01 O
0.073 O
Soccer O
[ O
3 O
] O
2.64 O
22 O
0.073 O
Badminton O
[ O
3 O
] O
46.34 O
79.34 O
0.086 O
Table O
Tennis O
[ O
3 O
] O
54.86 O
96.47 O
0.077 O
DotA O
6.80 O
[ O
12 O
] O
68.6 O
106.2 O
0.078 O
3 O
Analysis O
and O
Discussion O
This O
section O
presents O
the O
analyzing O
results O
of O
DOTA2 O
series O
using O
the O
game B
refinement O
measure O
and O
discusses O
its O
rule B
changes O
with O
a O
focus B
on O
prize B
in O
a O
championship O
. O

3.1 O
Analyzing O
Results O
To O
obtain O
the O
latest O
GR O
of O
DOTA2 O
series O
, O
we O
collect O
the O
data O
from O
the O
historical O
TI O
championships O
. O

For O
this O
purpose O
, O
we O
download O
all O
the O
replay O
of O
the O
final O
to O
calculate O
its O
GR O
values O
. O

We O
show O
, O
in O
Table O
2 O
, O
GR O
value O
of O
each O
TI O
championship O
, O
together O
with O
prize B
money O
compared O
[ O
13 O
] O
. O

Table O
2 O
and O
Figure O
1 O
Table O
2 O
. O

Measures O
of O
game B
refinement O
for O
DOTA2 O
series O
and O
prize B
at O
TI O
championship O
Year O
Championship O
K O
T O
GR O
Prize O
( O
US O
dollars O
) O
2011 O
TI1 O
51.3 O
93.0 O
0.077 O
1,600,000 O
2012 O
TI2 O
32.5 O
76.3 O
0.075 O
1,600,000 O
2013 O
TI3 O
36.6 O
81.8 O
0.074 O
2,874,380 O
2014 O
TI4 O
30.0 O
77.3 O
0.071 O
10,925,709 O
2015 O
TI5 O
39.8 O
89.4 O
0.074 O
18,429,613 O
2016 O
TI6 O
54.0 O
94.3 O
0.078 O
20,746,930 O
shows O
that O
from O
2011 O
to O
2014 O
GR O
value O
decreases O
. O

The O
rules B
of O
DOTA2 O
have O
been O
changed O
for O
that O
period O
to O
be O
more O
competitive B
as O
the O
prize B
became O
higher O
. O

However O
, O
such O
rule B
changes O
( O
decreasing O
of O
GR O
value O
) O
made O
DOTA2 O
boring O
for O
the O
viewers.1 O
On O
the O
other O
hand O
, O
the O
designer O
of O
DOTA2 O
has O
attempted O
many O
rule B
changes O
with O
expectation O
that O
DOTA2 O
would O
have O
more O
uncertainty O
while O
adding O
new O
items B
and O
incorporating O
the O
unexpected O
factors O
which O
mean O
that O
a O
lower O
rating O
team B
would O
win I
against O
a O
higher O
rating O
team B
with O
higher O
probability O
than O
before O
. O

Thus O
, O
after O
2014 O
until O
now O
, O
GR O
values O
are O
increasing B
. O

3.2 O
Rule O
Changes O
in B
2011 O
- I
2013 I
: O
Towards O
More O
Skillful B
The O
TI O
championship O
series O
is O
the O
most O
significant O
and O
profitable O
annual O
event O
for O
DOTA2 O
since O
2011 O
[ O
14 O
] O
. O

The O
game B
designer O
has O
attempted O
to O
modify O
the O
rules B
as O
described O
in O
Table O
2 O
. O

In O
2011 O
, O
Smoke O
was O
introduced O
for O
DOTA2 O
Ver O
. O

6.70 O
. O

The O
Smoke O
of O
Deceit O
is O
an O
item B
purchasable O
at O
the O
Main O
Shop O
, O
under O
Consumables O
. O

It O
turns B
the O
user B
and O
nearby O
ally B
heroes I
invisible B
, O
letting O
them O
slip O
by O
wards B
and O
creeps B
undetected O
. O

Upon O
activation O
, O
the O
user B
and O
all O
nearby O
allied B
player B
- O
controlled B
units B
gain O
invisibility O
and O
bonus O
movement B
speed O
for O
a O
brief O
time B
. O

Thus O
, O
many O
new O
tactics B
were O
explored O
after O
the O
emerge O
of O
Smoke B
items B
. O

Then O
the O
team B
behavior O
became O
conservative O
after O
the O
only O
three O
Smoke O
items B
were O
included O
during O
the O
in B
- I
game I
period I
. O

In O
2012 O
the O
nerfed O
numerous O
heroes B
in O
Ver O
. O

6.74 O
has O
established O
the O
foundation O
for O
the O
TI O
championship O
to O
enhance O
the O
game B
rigorism O
since O
DOTA2 O
has O
become O
a O
game B
to O
be O
played B
not O
only O
for O
fun O
but O
also O
for O
prize B
seriously O
. O

The O
appearance O
of O
the O
new O
captain O
mode O
in O
2013 O
of O
Ver O
. O

6.79 O
has O
contributed O
to O
maintain O
the O
fairness O
at O
the O
initial O
with O
the O
expectation O
that O
the O
rule B
of O
ban O
and O
pick B
system O
greatly O
would O
influence O
the O
1 O
Actually O
many O
people O
complained O
about O
the O
conservative O
game B
progress O
. O

Fig O
. O

1 O
. O

GR O
values O
and O
prize B
pool O
of O
DOTA2 O
in B
2011 O
- I
2016 I
game B
result O
. O

For O
both O
teams B
, O
it O
is O
no O
longer O
easy B
to O
choose O
an O
unbalanced O
hero B
and O
relatively O
hard B
to O
successfully O
kill B
the O
enemy B
as O
before O
. O

For O
the O
period O
2011- O
2013 O
, O
the O
average O
number O
of O
killing O
, O
denoted O
as O
K O
in O
the O
game B
progress O
model O
, O
has O
decreased O
year O
by O
year O
. O

This O
implies O
that O
GR O
value O
has O
become O
lower O
. O

As O
a O
result O
DOTA2 O
has O
become O
more O
skillful B
and O
competitive B
. O

Note O
that O
DOTA2 O
mainly O
focused B
on O
hero B
development O
and O
less O
gank B
or O
battle B
. O

3.3 O
Rule O
Changes O
in B
2014 O
- I
2016 I
: O
Towards O
More O
Popular O
A O
highly B
skill O
- I
based I
game B
would O
not O
become O
popular O
since O
skill B
itself O
is O
unfriendly O
to O
the O
beginners B
. O

In O
2014 O
the O
new O
rune B
system O
in O
Ver O
. O

6.82 O
came O
out O
and O
added O
bounty B
rune I
. O

Runes B
are O
special O
boosters O
that O
spawn O
on O
the O
game B
map I
. O

Picking B
up O
a O
non O
- O
bounty B
rune I
grants O
the O
player B
a O
powerful B
effect B
for O
a O
short O
time B
. O

Runes B
spawn O
at O
two O
points B
in O
the O
river B
. O

The O
emerge O
of O
bounty B
rune I
makes O
the O
supporter O
or O
carry B
get O
money O
easier B
and O
the O
player B
can O
purchase O
the O
items B
earlier O
than O
before O
. O

This O
also O
accelerates O
the O
game B
progress O
. O

In O
2015 O
the O
game B
designer O
reworked O
the O
gold B
and O
experience B
mechanism B
in O
Ver O
. O

6.84 O
. O

The O
new O
mechanism B
encouraged O
two O
teams B
to O
take O
part O
in O
more O
battle B
activities O
as O
they O
can O
get O
more O
gold B
and O
experience B
than O
before O
. O

The O
new O
rules B
focus B
more O
on O
gank B
and O
push O
issue O
instead O
of O
hero B
development O
. O

Another O
interesting O
mechanism B
of O
scan O
appeared O
in O
2016 O
of O
Ver O
. O

6.87 O
and O
we O
can O
comprehend O
this O
mechanism B
as O
a O
strategic O
skill B
for O
both O
teams B
. O

Players B
can O
use O
the O
Scan O
ability B
on O
top O
of O
the O
minimap O
UI O
to O
detect O
any O
enemy B
heroes I
in O
an O
area B
. O

This O
mechanism B
greatly O
made O
the O
game B
more O
exciting O
and O
added O
an O
extra O
level B
of O
uncertainty O
as O
the O
players B
do O
not O
know O
the O
exact O
number O
of O
enemies B
. O

To O
summarize O
all O
these O
new O
mechanisms B
accelerated O
the O
game B
progress O
and O
enhanced O
the O
uncertainty O
during O
the O
in B
- I
game I

 O
period I
. O

The O
new O
mechanism B
offers O
more O
uncertainty O
for O
both O
teams B
to O
win I
or O
make O
mistake B
in O
the O
game B
. O

Then O
, O
the O
game B
has O
become O
more O
uncertain O
until O
the O
very O
end O
of O
the O
game B
. O

Thus O
, O
we O
see O
that O
GR O
value O
has O
increased B
after O
2014 O
and O
it O
is O
supposed O
that O
DOTA2 O
will O
become O
more O
and O
more O
popular O
in O
the O
future O
. O

We O
see O
that O
the O
balance O
between O
skillfulness O
and O
popularity O
is O
so O
important O
for O
the O
survival O
of O
a O
game B
. O

3.4 O
High O
Prize O
As O
we O
have O
mentioned O
above O
, O
DOTA2 O
has O
over O
one O
million O
concurrent O
players B
while O
being O
the O
most O
profitable O
sports O
in O
the O
world B
. O

It O
seems O
that O
DotA O
was O
first O
designed O
only O
for O
fun O
, O
however O
, O
with O
the O
contributions O
of O
sponsors O
and O
game B
designer O
, O
DOTA2 O
has O
become O
a O
main O
trend O
of O
e O
- O
sports O
. O

The O
dynamic O
changes O
of O
each O
version O
and O
high O
prize B
made O
DOTA2 O
the O
most O
successful O
and O
profitable O
e O
- O
sports O
even O
in O
its O
short O
history O
. O

Now O
DOTA2 O
has O
lack O
of O
popularity O
as O
this O
game B
is O
still O
unfriendly O
to O
the O
novice B
players I
and O
has O
a O
relatively O
complex O
game B
information O
to O
learn O
, O
as O
there O
are O
totally O
over O
110 O
heroes B
and O
150 O
items B
. O

However O
, O
compared O
with O
other O
sports O
, O
we O
see O
that O
DOTA2 O
is O
now O
at O
the O
peak O
, O
as O
shown O
in O
Table O
3 O
[ O
15 O
] O
. O

Table O
3 O
. O

Tournament B
prize B
in O
sports O
, O
mind O
sports O
and O
e O
- O
sports O
compared O
Event O
Sports O
Prize O
( O
US O
dollars O
) O
1st O
Prize O
Australia O
Open O
Tennis O
35,530,000 O
1,040,000 O
NBA O
Basketball O
14,000,000 O
4,100,000 O
FIFA O
Club O
World O
Cup O
Soccer O
28,000,000 O
5,490,000 O
Ing O
Cup O
Go O
650,000 O
400,000 O
S6 O
League O
of O
Legends O
5,070,000 O
2,130,000 O
TI O
6 O
DOTA2 O
20,746,930 O
9,140,000 O
4 O
Concluding O
Remarks O
In O
this O
study O
we O
evaluated O
the O
DOTA2 O
series O
using O
the O
game B
refinement O
measurement O
. O

The O
results O
indicate O
that O
DOTA2 O
has O
a O
similar O
zone B
value O
with O
sophisticated O
sports O
and O
boardgames O
. O

In O
addition O
, O
DOTA2 O
championship O
of O
every O
year O
during O
2011 O
- O
2016 O
was O
analyzed O
. O

The O
results O
show O
that O
the O
game B
refinement O
value O
has O
stayed O
within O
0.071 O
- O
0.077 O
, O
which O
is O
slightly O
lower O
than O
DotA. O
The O
prize B
of O
the O
championship O
has O
strongly O
influenced O
the O
development O
of O
DOTA2 O
. O

Higher O
prize B
enforced O
the O
players B
to O
be O
more O
conservative O
and O
the O
game B
refinement O
value O
became O
lower O
which O
implies O
that O
DOTA2 O
became O
more O
skillful B
. O

However O
, O
such O
a O
direction O
of O
game B
evolution B
was O
not O
accepted O
in O
DOTA2 O
community B
due O
to O
the O
lack O
of O
entertainment O
. O

Later O
, O
the O
direction O
of O
DOTA2 O
evolution B
was O
shifted O
to O
be O
more O
popular O
while O
taking O
stochastic O
elements O
into O
consideration O
. O

Thus O
we O
see O
that O
a O
good O
balance O
between O
skillfulness O
and O
popularity O
is O
essential O
to O
survive O
. O

References O
1 O
. O

Sutiono O
, O
A. O
P. O
, O
Purwarianti O
, O
A. O
, O
Iida O
, O
H. O
( O
2014 O
, O
July O
) O
. O

A O
mathematical O
model O
of O
game B
refinement O
. O

In O
International O
Conference O
on O
Intelligent O
Technologies O
for O
Interactive O
Entertainment O
( O
pp O
. O

148 O
- O
151 O
) O
. O

Springer O
International O
Publishing O
. O

2 O
. O

Takeuchi O
, O
J. O
, O
Ramadan O
, O
R. O
, O
Iida O
, O
H. O
( O
2014 O
) O
. O

Game B
refinement O
theory O
and O
its O
application O
to O
Volleyball O
. O

Research O
Report O
2014-GI-31 O
( O
3 O
) O
, O
Information O
Processing O
Society O
of O
Japan O
, O
1 O
- O
6 O
. O

3 O
. O

Nossal O
, O
N. O
, O
Iida O
, O
H. O
( O
2014 O
, O
October O
) O
. O

Game B
refinement O
theory O
and O
its O
application O
to O
score O
limit O
games B
. O

In O
Games O
Media O
Entertainment O
( O
GEM O
) O
, O
2014 O
IEEE O
( O
pp O
. O

1 O
- O
3 O
) O
. O

IEEE O
. O

4 O
. O

Yuranana O
, O
K. O
, O
Panumate O
, O
C. O
, O
Iida O
, O
H. O
, O
Tanaka O
, O
K. O
( O
2016 O
) O
. O

Measuring O
Sophistication O
of O
Sports O
Games O
: O
The O
First O
Result O
from O
Baseball O
. O

5 O
. O

Panumate O
, O
C. O
, O
Iida O
, O
H. O
An O
Approach O
to O
Quantifying O
Boxing O
’s O
Entertainment O
. O

6 O
. O

Cincotti O
, O
A. O
, O
Iida O
, O
H. O
, O
Yoshimura O
, O
J. O
( O
2007 O
) O
. O

Refinement O
and O
complexity O
in O
the O
evolution B
of O
chess O
. O

In O
Proceedings O
of O
the O
10th O
International O
Conference O
on O
COmputer O
Science O
and O
Informatics O
( O
pp O
. O

650 O
- O
654 O
) O
. O

7 O
. O

Iida O
, O
H. O
, O
Takahara O
, O
K. O
, O
Nagashima O
, O
J. O
, O
Kajihara O
, O
Y. O
, O
Hashimoto O
, O
T. O
( O
2004 O
, O
September O
) O
. O

An O
application O
of O
game B
- O
refinement O
theory O
to O
Mah O
Jong O
. O

In O
International O
Conference O
on O
Entertainment O
Computing O
( O
pp O
. O

333 O
- O
338 O
) O
. O

Springer O
Berlin O
Heidelberg O
. O

8 O
. O

Iida O
, O
H. O
, O
Takeshita O
, O
N. O
, O
Yoshimura O
, O
J. O
( O
2003 O
) O
. O

A O
metric O
for O
entertainment O
of O
boardgames O
: O
its O
implication O
for O
evolution B
of O
chess O
variants O
. O

In O
Entertainment O
Computing O
( O
pp O
. O

65 O
- O
72 O
) O
. O

Springer O
US O
. O

9 O
. O

Csikszentmihalyi O
, O
M. O
( O
1990 O
) O
. O

Flow O
. O

The O
Psychology O
of O
Optimal O
Experience O
. O

New O
York O
( O
HarperPerennial O
) O
1990 O
. O

10 O
. O

Perrin O
, O
A. O
, O
Duggan O
, O
M. O
( O
2015 O
) O
. O

Americans O
’ O
Internet O
Access O
: O
2000–2015 O
: O
As O
Internet O
Use O
Nears O
Saturation O
for O
Some O
Groups O
, O
a O
Look O
at O
Patterns O
of O
Adoption O
. O

[ O
Pew O
Internet O
project O
data O
memo O
] O
. O

11 O
. O

M´ajek O
, O
P. O
, O
Iida O
, O
H. O
( O
2004 O
) O
. O

Uncertainty O
of O
game B
outcome O
. O

In O
3rd O
International O
Conference O
on O
Global O
Research O
and O
Education O
in O
Intelligent O
Systems O
( O
pp O
. O

171 O
- O
180 O
) O
. O

12 O
. O

Xiong O
, O
S. O
, O
Zuo O
, O
L. O
, O
Iida O
, O
H. O
( O
2014 O
) O
. O

Quantifying O
engagement O
of O
electronic O
sports O
game B
. O

Advances O
in O
Social O
and O
Behavioral O
Sciences O
, O
5 O
, O
37 O
- O
42 O
. O

13 O
. O

e O
- O
Sports O
Earnings O
. O

Top O
Games O
of O
2016 O
. O

( O
2017 O
) O
, O
[ O
Online O
] O
. O

http://www.esportsearnings.com/history/2016/games O
14 O
. O

GAMEPEDIA O
. O

DOTA2Wiki O
. O

( O
2017 O
) O
, O
[ O
Online O
] O
. O

http://DOTA2.gamepedia.com O
/ O
DOTA2Wiki O
15 O
. O

List O
of O
prizes B
, O
medals O
and O
awards O
. O

( O
2017 O
, O
June O
14 O
) O
. O

[ O
Online O
] O
. O

https://en.wikipedia.org/wiki/Listofprizes,medalsandawards O
HAL O
I O
d O
: O
hal-01771292 O
https://hal.inria.fr/hal-01771292 O
Submitted O
on O
19 O
Apr O
2018 O
HAL O
is O
a O
multi O
- O
disciplinary O
open B
access O
archive O
for O
the O
deposit O
and O
dissemination O
of O
scientific O
research O
documents O
, O
whether O
they O
are O
published O
or O
not O
. O

The O
documents O
may O
come O
from O
teaching O
and O
research O
institutions O
in O
France O
or O
abroad O
, O
or O
from O
public O
or O
private O
research O
centers O
. O

L’archive O
ouverte O
pluridisciplinaire O
HAL O
, O
est O
destinée O
au O
dépôt O
et O
à O
la O
diffusion O
de O
documents O
scientifiques O
de O
niveau O
recherche O
, O
publiés O
ou O
non O
, O
émanant O
des O
établissements O
d’enseignement O
et O
de O
recherche O
français O
ou O
étrangers O
, O
des O
laboratoires O
publics O
ou O
privés O
. O

Distributed O
under O
a O
Creative O
Commons O
Attribution| O
4.0 O
International O
License O
Possible O
Interpretations O
for O
Game O
Refinement O
Measure O
Shuo O
Xiong O
, O
Long O
Zuo O
, O
Hiroyuki O
Iida O
To O
cite O
this O
version O
: O
Shuo O
Xiong O
, O
Long O
Zuo O
, O
Hiroyuki O
Iida O
. O

Possible O
Interpretations O
for O
Game O
Refinement O
Measure O
. O

16th O
International O
Conference O
on O
Entertainment O
Computing O
( O
ICEC O
) O
, O
Sep O
2017 O
, O
Tsukuba O
City O
, O
Japan O
. O

pp.322 O
- O
334 O
, O
ff10.1007/978 O
- O
3 O
- O
319 O
- O
66715 O
- O
7_35ff O
. O

ffhal-01771292ff O
Possible O
Interpretations O
for O
Game O
Refinement O
Measure O
Shuo O
Xiong O
( O
 O
) O
1 O
Long O
Zuo O
( O
 O
) O
2 O
and O
Hiroyuki O
Iida O
( O
 O
) O
3 O
Japan O
Advanced O
Institute O
of O
Science O
and O
Technology O
1 O
- O
1 O
Asahidai O
, O
Nomi O
, O
Ishikawa O
, O
Japan O
923 O
- O
1211 O
xiongshuo@jaist.ac.jp1 O
zuolong@jaist.ac.jp2 O
iida@jaist.ac.jp3 O
Abstract O
. O

This O
paper O
explores O
possible O
interpretations O
for O
game B
refinement O
measure O
which O
has O
been O
successfully O
used O
to O
quantify O
the O
game B
sophistication O
of O
various O
types O
of O
games B
such O
as O
boardgames O
and O
sports O
. O

It O
presents O
a O
brief O
sketch O
of O
game B
refinement O
theory O
with O
a O
focus B
on O
its O
early B
works O
with O
boardgames O
, O
expansion O
into O
continuous O
movement B
games B
such O
as O
sports O
, O
and O
a O
bridge O
between O
sports O
and O
boardgames O
. O

It O
then O
highlights B
the O
bridging O
idea O
while O
considering O
possible O
interpretations O
for O
game B
refinement O
measure O
, O
and O
the O
meaning O
of O
game B
refinement O
measure O
is O
discussed O
with O
a O
focus B
on O
the O
skill B
and O
chance B
aspects O
in O
game B
playing O
. O

It O
enables O
to O
have O
a O
new O
perspective O
of O
game B
refinement O
theory O
. O

Moreover O
, O
an O
example O
of O
interpretation O
for O
game B
refinement O
measure O
from O
boardgames O
and O
continuous O
movement B
games B
such O
as O
MOBA O
game B
is O
shown O
. O

The O
interpretation O
is O
well O
fitting O
to O
our O
intuition O
as O
game B
players B
and O
spectators O
. O

Keyword O
: O
Game B
refinement O
measure O
. O

Game B
progress O
model O
. O

Boardgame O
. O

Continuous O
movement B
game B
, O
sports O
, O
MOBA O
game B
1 O
Introduction O
Game O
theory O
is O
a O
discipline O
which O
stands O
from O
the O
game B
player B
’s O
point B
of O
view O
with O
a O
focus B
on O
how O
to O
win O
a O
game B
. O

However O
, O
game B
designers O
would O
consider O
another O
important O
aspect O
: O
how O
to O
make O
a O
game B
more O
attractive O
. O

With O
such O
motivation O
, O
a O
new O
game B
theory O
from O
the O
game B
designer O
’s O
point B
of O
view O
, O
called B
game B
refinement O
theory O
[ O
2 O
, O
3 O
] O
was O
proposed O
in O
the O
early B
2000s O
. O

von O
Neumann O
[ O
4 O
] O
was O
a O
pioneer O
who O
formed O
the O
foundation O
for O
the O
modern O
game B
theory O
, O
which O
has O
widely O
been O
applied O
in O
various O
fields B
. O

For O
example O
, O
Shannon O
[ O
5 O
] O
and O
Turing O
[ O
6 O
] O
proposed O
the O
basic O
framework O
for O
computer O
chess O
that O
is O
the O
minimax O
gametree O
search O
, O
being O
inspired O
by O
the O
concept O
of O
minimax O
equilibrium O
[ O
4 O
] O
, O
typical O
framework O
of O
computer O
chess O
called B
game B
- O
tree O
search O
was O
proposed O
by O
Shannon O
[ O
5 O
] O
and O
Turing O
[ O
6 O
] O
, O
respectively O
. O

One O
direction O
with O
game B
theory O
was O
to O
find O
the O
best O
move B
in O
a O
game B
or O
to O
ensure O
the O
possibility O
of O
winning O
the O
game B
based B
on O
the O
understanding O
of O
current O
positions O
. O

Another O
direction O
with O
game B
refinement O
theory O
was O
to O
assess O
the O
attractiveness O
or O
sophistication O
of O
a O
game B
. O

In O
particular O
, O
game B
refinement O
theory O
gives O
a O
measure O
to O
quantify O
the O
sophistication O
of O
a O
game B
. O

This O
enables O
to O
obtain O
the O
deep O
insight O
into O
the O
current O
game B
and O
improve O
the O
quality O
of O
the O
game B
[ O
7 O
, O
8 O
] O
. O

The O
measure O
of O
game B
refinement O
can O
also O
be O
used O
to O
obtain O
the O
deep O
insight O
into O
the O
history O
of O
games B
. O

For O
example O
, O
it O
is O
observed O
that O
the O
evolution B
of O
chess O
has O
two O
different O
directions O
: O
one O
is O
to O
increase B
the O
search O
- O
space O
complexity O
and O
another O
one O
is O
to O
shift O
to O
the O
comfortable O
degree O
of O
game B
refinement O
measure O
[ O
9 O
] O
. O

Hence O
, O
it O
gives O
a O
reasonable O
look O
on O
the O
evolution B
of O
specific O
game B
variants O
. O

In O
another O
way O
, O
game B
refinement O
theory O
provides O
us O
with O
another O
viewpoint O
of O
games B
from O
the O
entertainment O
aspect O
while O
game B
theory O
helps O
us O
understand O
about O
the O
game B
’s O
mechanism B
itself O
. O

From O
that O
viewpoint O
, O
we O
can O
extend O
the O
idea O
of O
game B
refinement O
into O
other O
domains O
in O
human O
life O
such O
as O
sports O
games B
, O
video B

 O
games I
, O
education O
or O
business O
. O

The O
possibility O
of O
extension O
comes O
from O
the O
core O
idea O
of O
game B
refinement O
theory O
that O
is O
quantifying O
the O
engagement O
. O

In O
many O
activities O
of O
human O
, O
the O
engagement O
is O
usually O
used O
as O
one O
of O
the O
important O
standards O
to O
evaluate O
the O
effectiveness O
of O
those O
activities O
. O

Game O
refinement O
theory O
has O
been O
widely O
applied O
to O
many O
different O
types B

 O
of I
games I
with O
the O
promising O
results O
. O

However O
, O
the O
theory O
has O
just O
one O
decade O
history O
, O
which O
may O
not O
be O
established O
yet O
. O

This O
paper O
explores O
possible O
interpretations O
for O
game B
refinement O
measure O
. O

It O
highlights B
the O
bridging O
idea O
between O
boardgames O
and O
continuous O
movement B
games B
like O
sports O
. O

Thus O
the O
meaning O
of O
game B
refinement O
measure O
is O
discussed O
with O
a O
focus B
on O
the O
skill B
and O
chance B
aspects O
in O
game B
playing O
. O

It O
will O
enable O
to O
have O
a O
new O
perspective O
of O
game B
refinement O
theory O
. O

Moreover O
, O
an O
example O
of O
interpretation O
for O
game B
refinement O
measure O
from O
boardgames O
and O
continuous O
movement B
games B
such O
as O
MOBA O
game B
is O
shown O
. O

2 O
An O
Overview O
of O
Game O
Refinement O
Theory O
In O
this O
section O
an O
overview O
of O
game B
refinement O
theory O
is O
presented O
. O

The O
model O
of O
game B
refinement O
was O
first O
investigated O
in O
the O
domain O
of O
boardgames O
such O
as O
chess O
, O
later O
expanded O
into O
continuous O
movement B
games B
such O
as O
sports O
games O
and O
video B
games I
while O
considering O
the O
gap O
between O
boardgames O
and O
continuous O
movement B
games B
. O

2.1 O
Original O
Model O
We O
review O
the O
early B
work O
of O
game B
refinement O
theory O
from O
[ O
2 O
] O
. O

The O
decision O
space O
is O
the O
minimal O
search O
space O
without O
forecasting O
. O

It O
provides O
the O
common O
measures O
for O
almost O
all O
boardgames O
. O

The O
dynamics O
of O
decision O
options O
in O
the O
decision O
space O
has O
been O
investigated O
and O
it O
is O
observed O
that O
this O
dynamics O
is O
a O
key O
factor O
for O
game B
entertainment O
. O

Thus O
a O
measure O
of O
the O
refinement O
in O
games O
was O
proposed O
[ O
3 O
] O
. O

Later O
, O
the O
following O
works O
are O
sketched O
from O
[ O
7 O
, O
10 O
] O
that O
expands O
the O
model O
of O
game B
refinement O
which O
was O
cultivated O
in O
the O
domain O
of O
boardgames O
into O
continuous O
movement B
games B
such O
as O
sports O
games B
and O
video B
games I
. O

The O
game B
progress O
is O
twofold O
. O

One O
is O
game B
speed I
or O
scoring O
rate O
, O
while O
another O
one O
is O
game B
information O
progress O
with O
a O
focus B
on O
the O
game B
outcome O
. O

Game B
information O
progress O
presents O
the O
degree O
of O
certainty O
of O
a O
game B
’s O
result O
in O
time B
or O
in O
steps O
. O

Having O
full B
information O
of O
the O
game B
progress O
, O
i.e. O
after O
its O
conclusion O
, O
game B
progress O
x(t O
) O
will O
be O
given O
as O
a O
linear O
function O
of O
time B
t O
with O
0 O
≤ O
t O
≤ O
tk O
and O
0 O
≤ O
x(t O
) O
≤ O
x(tk O
) O
, O
as O
shown O
in O
Eq O
. O

( O
1 O
) O
. O

x(t O
) O
= O
x(tk O
) O
tk O
t O
( O
1 O
) O
However O
, O
the O
game B
information O
progress O
given O
by O
Eq O
. O

( O
1 O
) O
is O
unknown O
during O
the O
in B
- I
game I
period I
. O

The O
presence O
of O
uncertainty O
during O
the O
game B
, O
often O
until O
the O
final O
moments O
of O
a O
game B
, O
reasonably O
renders O
game B
progress O
as O
exponential O
. O

Hence O
, O
a O
realistic O
model O
of O
game B
information O
progress O
is O
given O
by O
Eq O
. O

( O
2 O
) O
. O

x(t O
) O
= O
x(tk O
) O
( O
t O
tk O
) O
n O
( O
2 O
) O
Here O
n O
stands O
for O
a O
constant O
parameter O
which O
is O
given O
based B
on O
the O
perspective O
of O
an O
observer O
of O
the O
game B
considered O
. O

Only O
a O
very O
boring O
game B
would O
progress O
in O
a O
linear O
function O
, O
however O
, O
and O
most O
of O
course O
do O
not O
. O

Therefore O
, O
it O
is O
reasonable O
to O
assume O
a O
parameter O
n O
, O
based B
on O
the O
perception O
of O
game B
progress O
prior O
to O
completion O
. O

If O
the O
information O
of O
the O
game B
is O
completely O
known O
( O
i.e. O
, O
after O
the O
end O
of O
the O
game B
) O
and O
the O
value O
of O
n O
is O
1 O
, O
the O
game B
progress O
curve O
appears O
as O
a O
straight B
line O
. O

In O
most O
games B
, O
especially O
in O
competitive B
ones O
, O
much O
of O
the O
information O
is O
incomplete O
, O
the O
value O
of O
n O
can O
not O
be O
assumed O
, O
and O
therefore O
game B
progress O
is O
a O
steep O
curve O
until O
its O
completion O
, O
along O
with O
x(tk O
) O
, O
tk O
, O
x(t O
) O
and O
t O
, O
just O
prior O
to O
game B
’s O
end O
. O

Then O
acceleration O
of O
game B
information O
progress O
is O
obtained O
by O
deriving O
Eq O
. O

( O
2 O
) O
twice O
. O

Solving O
it O
at O
t O
= O
tk O
, O
we O
have O
Eq O
. O

( O
3 O
) O
. O

x O
00(tk O
) O
= O
x(tk O
) O
( O
tk O
) O
n O
( O
tk O
) O
n−2 O
n(n O
− O
1 O
) O
= O
x(tk O
) O
( O
tk O
) O
2 O
n(n O
− O
1 O
) O
( O
3 O
) O
It O
is O
assumed O
in O
the O
current O
model O
that O
game B
information O
progress O
in O
any O
type B

 O
of I
game I
is O
encoded O
and O
transported O
in O
our O
brains B
. O

We O
do O
not O
yet O
know O
about O
the O
physics O
of O
information O
in O
the O
brain B
, O
but O
it O
is O
likely O
that O
the O
acceleration O
of O
information O
progress O
is O
subject O
to O
the O
forces O
and O
laws O
of O
physics O
. O

Too O
little O
game B
information O
acceleration O
may O
be O
easy B
for O
human O
observers O
and O
players B
to O
compute O
, O
and O
becomes O
boring O
. O

In O
contrast O
, O
too O
much O
game B
information O
acceleration O
surpasses O
the O
entertaining O
range B
and O
will O
be O
frustration O
, O
and O
at O
some O
points B
beyond O
that O
could O
become O
overwhelming O
and O
incomprehensible O
. O

Therefore O
, O
we O
expect O
that O
the O
larger O
the O
value O
x(tk O
) O
( O
tk O
) O
2 O
is O
, O
the O
more O
the O
game B
becomes O
exciting O
, O
due O
in O
part O
to O
the O
uncertainty O
of O
game B
outcome O
. O

Thus O
, O
we O
use O
its O
root O
square O
, O
√ O
x(tk O
) O
tk O
, O
as O
a O
game B
refinement O
measure O
for O
the O
game B
under O
consideration O
. O

We O
call B
it O
R O
value O
for O
short O
as O
shown O
in O
Eq O
. O

( O
4 O
) O
. O

R O
= O
p O
x(tk O
) O
tk O
p O
n(n O
− O
1 O
) O
= O
C O
p O
x(tk O
) O
tk O
( O
4 O
) O
2.2 O
A O
Bridge O
between O
Sports O
and O
Boardgames O
Here O
we O
consider O
the O
gap O
of O
game B
refinement O
model O
between O
boardgames O
and O
sports O
games B
. O

We O
review O
the O
observation O
from O
[ O
10 O
] O
. O

One O
round O
in O
boardgames O
can O
be O
illustrated O
as O
decision O
tree O
. O

At O
each O
depth O
of O
the O
game B
tree O
, O
one O
will O
choose O
a O
move B
and O
the O
game B
will O
progress O
. O

Figure O
1 O
illustrates O
one O
level B
of O
game B
tree O
. O

The O
distance O
d O
, O
which O
has O
been O
shown O
in O
Figure O
1 O
, O
can O
be O
found O
by O
using O
simple O
Pythagoras O
theorem O
, O
thus O
resulting O
in O
d O
= O
√ O
∆l2 O
+ O
1 O
. O

Fig O
. O

1 O
. O

Illustration O
of O
one O
level B
of O
game B
tree O
[ O
10 O
] O
Assuming O
that O
the O
approximate O
value O
of O
horizontal O
difference O
between O
nodes O
is O
B O
2 O
, O
then O
we O
can O
make O
a O
substitution O
and O
get O
d O
= O
q O
( O
B O
2 O
) O
2 O
+ O
1 O
. O

Here O
B O
stands O
for O
the O
average O
branching O
factor O
of O
a O
game B
tree O
. O

The O
game B
progress O
for O
one O
game B
is O
the O
total O
level B
of O
game B
tree O
times I
d. O
For O
the O
meantime O
, O
we O
do O
not O
consider O
∆t2 O
because O
the O
value O
( O
∆t2 O
= O
1 O
) O
is O
assumed O
to O
be O
much O
smaller O
compared O
to O
B. O
The O
game B
length O
will O
be O
normalized O
by O
the O
average O
game B
length O
D O
, O
then O
the O
game B
progress O
x(t O
) O
is O
given O
by O
x(t O
) O
= O
t O
D O
· O
d O
= O
t O
D O
q O
( O
B O
2 O
) O
2 O
= O
Bt O
2D O
. O

Then O
, O
in O
general O
we O
have O
Eq O
. O

( O
5 O
) O
. O

x(t O
) O
= O
c O
B O
D O
t O
( O
5 O
) O
where O
c O
is O
a O
different O
constant O
which O
depends O
on O
the O
game B
considered O
. O

However O
, O
we O
manage O
to O
explain O
how O
to O
obtain O
the O
game B
information O
progress O
value O
itself O
. O

The O
game B
progress O
in O
the O
domain O
of O
boardgames O
forms O
a O
linear O
graph O
with O
the O
maximum O
value O
x(t O
) O
of O
B. O
Assuming O
1 O
c O
= O
1 O
, O
then O
we O
have O
a O
realistic O
game B
progress O
model O
for O
boardgames O
, O
which O
is O
given O
by O
x(t O
) O
= O
B O
( O
t O
D O
) O
n O
. O

( O
6 O
) O
We O
show O
, O
in O
Table O
1 O
, O
measures O
of O
game B
refinement O
for O
various O
games B
[ O
11 O
, O
12 O
, O
13 O
] O
. O

From O
the O
results O
, O
we O
conjecture O
the O
relation O
between O
the O
measure O
of O
game B
refinement O
and O
game B
sophistication O
, O
as O
stated O
in O
Remark O
1 O
. O

1 O
In O
this O
study O
we O
concern O
about O
this O
assumption O
. O

Remark O
1 O
. O

Sophisticated O
games B
have O
a O
common O
factor O
( O
i.e. O
, O
same O
degree O
of O
informatical O
acceleration O
value O
, O
say O
0.07 O
- O
0.08 O
) O
to O
feel O
engaged O
or O
excited O
regardless O
of O
different O
type O
of O
games B
. O

Table O
1 O
. O

Measures O
of O
game B
refinement O
for O
various O
types O
of O
games B
Game O
x(tk O
) O
tk O
R O
Chess O
35 O
80 O
0.074 O
Shogi O
80 O
115 O
0.078 O
Go O
250 O
208 O
0.076 O
Basketball O
36.38 O
82.01 O
0.073 O
Soccer O
2.64 O
22 O
0.073 O
Badminton O
46.336 O
79.344 O
0.086 O
Table O
tennis O
54.863 O
96.465 O
0.077 O
DotA O
ver O
6.80 O
68.6 O
106.2 O
0.078 O
StarCraft O
II O
Terran O
1.64 O
16 O
0.081 O
The O
king O
of O
the O
fighters B
98 O
14.6 O
36.7 O
0.104 O
3 O
Game O
Refinement O
Measure O
Revisited O
It O
seems O
that O
the O
bridge O
between O
boardgame O
and O
continuous O
movement B
game B
was O
successfully O
built O
. O

However O
, O
we O
claim O
that O
it O
is O
not O
yet O
completed O
. O

For O
this O
purpose O
we O
detail O
the O
problem O
while O
considering O
the O
meaning O
of O
parameter O
c O
in O
Eq O
. O

( O
5 O
) O
. O

3.1 O
Possible O
Interpretations O
for O
Game O
Refinement O
Measure O
For O
the O
sports O
games B
such O
as O
soccer O
, O
all O
the O
attempted O
shots O
or O
successful O
shots O
( O
goals B
) O
are O
parts O
of O
the O
strategy B
to O
win O
the O
match B
, O
so O
they O
are O
an O
integral O
part O
of O
the O
game B
. O

In O
the O
domain O
of O
video B
games I
such O
as O
StarCraft O
II O
, O
the O
branching O
factor O
was O
calculated O
only O
by O
reasonable O
strategies B
to O
be O
considered O
as O
part O
of O
the O
winning O
[ O
12 O
] O
. O

This O
suggests O
that O
parameter O
c O
in O
Eq O
. O

( O
5 O
) O
is O
a O
key O
factor O
when O
considering O
the O
gap O
between O
boardgames O
and O
continuous O
movement B
games B
. O

It O
also O
indicates O
that O
the O
parameter O
c O
can O
be O
replaced O
with O
p O
n(n O
− O
1 O
) O
in O
Eq O
. O

( O
4 O
) O
. O

From O
Eq O
. O

( O
5 O
) O
we O
obtain O
the O
measure O
of O
game B
refinement O
for O
boardgame O
as O
shown O
in O
Eq O
. O

( O
7 O
) O
. O

R O
= O
√ O
cB O
D O
( O
1 O
B O
≤ O
c O
≤ O
1 O
) O
( O
7 O
) O
Where O
we O
have O
cB O
= O
B O
when O
c O
= O
1 O
, O
and O
cB O
= O
1 O
when O
c O
= O
1 O
B O
. O

Hence O
, O
the O
assumption O
c O
= O
1 O
means O
that O
we O
focus B
on O
a O
specific O
level B
of I
players I
or O
a O
certain O
property O
of O
the O
game B
under O
consideration O
. O

When O
we O
focus B
on O
a O
certain O
level B
of I

 O
players I
like O
masters B
in O
boardgames O
, O
the O
crucial O
factor O
is O
the O
game B
property O
. O

If O
a O
Fig O
. O

2 O
. O

A O
model O
of O
candidate O
move B
selection O
based B
on O
skill I
and I
chance I
[ O
14 O
] O
game B
is O
skillful B
, O
the O
parameter O
c O
will O
decrease O
, O
whereas O
if O
the O
game B
is O
stochastic O
, O
c O
will O
increase B
. O

This O
is O
because O
it O
is O
usually O
hard B
in O
such O
a O
stochastic O
game B
to O
distinguish O
only O
fewer O
good O
candidates O
among O
all O
possible O
moves B
. O

On O
the O
other O
hands O
, O
it O
would O
be O
possible O
in O
boardgames O
like O
chess O
for O
masters B
to O
identify O
a O
few O
plausible O
moves B
. O

Note O
that O
continuous O
movement B
games B
such O
as O
sports O
games B
are O
basically O
stochastic O
when O
compared O
with O
boardgames O
. O

Remark O
2 O
. O

The O
parameter O
c O
= O
1 O
in O
Eq O
. O

( O
5 O
) O
means O
that O
the O
game B
under O
consideration O
is O
assumed O
to O
be O
insufficiently O
deterministic O
to O
identify O
plausible O
candidates O
. O

We O
show O
from O
[ O
14 O
] O
, O
in O
Figure O
2 O
, O
a O
model O
of O
move B
candidate O
selection O
based B
on O
skill B
and O
chance B
. O

This O
illustration O
shows O
that O
skillful B
players B
would O
consider O
a O
set O
( O
say O
b O
) O
of O
fewer O
plausible O
candidates O
among O
all O
possible O
moves B
( O
say O
B O
) O
to O
find O
a O
move B
to O
play B
. O

For O
example O
, O
in O
chess O
where O
B O
= O
35 O
and O
D O
= O
80 O
, O
when O
assuming O
c O
= O
1 O
, O
then O
R O
= O
0.074 O
. O

On O
the O
other O
hands O
, O
as O
suggested O
in O
[ O
15 O
] O
[ O
14 O
] O
, O
masters B
in O
sophisticated O
boardgames O
would O
consider O
a O
very O
few O
moves B
on O
average O
in O
their O
look O
- O
ahead O
thinking O
framework O
. O

An O
estimation O
of O
the O
number O
of O
plausible O
candidates O
as O
a O
function O
of O
the O
strength B
of I
players I
( O
say O
s O
) O
may O
be O
given O
by O
Eq O
. O

( O
8) O
[ O
3 O
] O
. O

b O
= O
B O
1 O
s O
( O
1 O
≤ O
s O
∈ O
N O
) O
( O
8) O
Let O
us O
consider O
the O
sports O
case O
with O
consideration O
on O
such O
a O
parameter O
. O

Like O
the O
boardgame O
case O
, O
we O
may O
have O
a O
parameter O
( O
say O
Cs O
) O
, O
as O
shown O
in O
Eq O
. O

( O
9 O
) O
. O

R O
= O
Cs O
√ O
G O
T O
( O
9 O
) O
Here O
we O
suspect O
that O
Cs O
may O
depend O
on O
the O
skill B
of O
teams B
. O

For O
the O
analysis O
, O
many O
data O
from O
soccer O
leagues O
with O
different O
ranking O
were O
collected O
. O

We O
show O
, O
in O
Table O
2 O
, O
measures O
of O
game B
refinement O
for O
each O
soccer O
league O
together O
with O
the O
average O
number O
of O
goals B
( O
G O
) O
and O
shots O
attempts O
( O
T O
) O
per O
game B
. O

Two O
typical O
groups B
are O
compared O
. O

The O
first O
group B
is O
clearly O
stronger O
than O
the O
second O
groups B
. O

England O
Premier O
League O
( O
EPL O
) O
, O
Primera O
division O
de O
Liga O
( O
LIGA O
) O
and O
Serie O
A O
in O
the O
first O
group B
are O
the O
higher O
ranking O
leagues O
, O
whereas O
Chinese O
Football O
Association O
Super O
League O
( O
CSL O
) O
in O
the O
second O
group B
is O
the O
lower O
ranking O
league O
[ O
16 O
] O
. O

We O
notice O
from O
the O
results O
in O
Table O
2 O
that O
when O
two O
similar O
- O
level B
teams B
play B
each O
other O
in O
their O
leagues O
R O
value O
is O
quite O
similar O
. O

Thus O
, O
we O
assume O
Cs O
= O
1 O
in O
this O
study O
. O

Table O
2 O
. O

Measures O
of O
game B
refinement O
for O
each O
league O
football O
match B
G O
T O
R O
EPL O
( O
2016 O
) O
2.84 O
25.6 O
0.066 O
LIGA(2016 O
) O
2.75 O
23.6 O
0.070 O
Serie O
A O
( O
2016 O
) O
2.77 O
25.4 O
0.065 O
CSL O
( O
2014 O
) O
2.75 O
24.6 O
0.067 O
CSL O
( O
2015 O
) O
2.80 O
24.6 O
0.068 O
CSL O
( O
2016 O
) O
2.67 O
24.6 O
0.066 O
We O
show O
, O
in O
Figure O
3 O
, O
the O
relationship O
between O
the O
parameter O
c O
and O
chanceand O
- O
skill B
aspect O
. O

Note O
that O
we O
assume O
the O
estimation O
of O
the O
plausible O
candidates O
as O
described O
in O
Eq O
. O

( O
8) O
. O

From O
Figure O
3 O
we O
conjecture O
that O
the O
parameter O
c O
relates O
Fig O
. O

3 O
. O

The O
parameter O
c O
and O
chance B
- O
and O
- O
skill B
aspect O
of O
games B
to O
the O
strength B
of I
players I
or O
the O
difficulty O
of O
a O
game B
, O
as O
stated O
in O
Remark O
3 O
. O

Remark O
3 O
. O

The O
value O
of O
parameter O
c O
should O
be O
lower O
in O
the O
case O
where O
the O
game B
under O
consideration O
is O
simple O
to O
identify O
fewer O
plausible O
candidates O
or O
the O
case O
where O
players B
are O
very O
skillful B
like O
grandmasters O
. O

Using O
the O
estimation O
of O
plausible O
candidates O
as O
shown O
in O
Eq O
. O

( O
8) O
, O
we O
obtain O
game B
refinement O
measures O
as O
described O
in O
Table O
3 O
. O

We O
here O
summarize O
the O
meaning O
of O
game B
refinement O
measure O
. O

– O
In O
a O
game B
where O
its O
game B
refinement O
measure O
is O
higher O
than O
the O
zone B
value O
( O
0.07 O
- O
0.08 O
) O
, O
people O
may O
feel O
more O
entertaining O
. O

This O
is O
because O
the O
game B
is O
too O
stochastic O
or O
players B
are O
too O
weak B
to O
identify O
fewer O
plausible O
candidates O
. O

Table O
3 O
. O

Measures O
of O
game B
refinement O
for O
boardgames O
with O
different O
parameters O
R O
( O
c O
= O
1 O
) O
R O
( O
cB O
= O
b O
, O
s O
= O
2 O
) O
Chess O
0.074 O
0.030 O
Shogi O
0.078 O
0.026 O
Go O
0.076 O
0.019 O
– O
The O
game B
with O
a O
zone B
value O
of O
game B
refinement O
measure O
has O
a O
good O
balance O
between O
chance B
and O
skill B
, O
in O
which O
people O
may O
feel O
comfortable O
and O
then O
the O
game B
is O
sophisticated O
or O
fascinating O
. O

– O
In O
a O
game B
where O
its O
game B
refinement O
measure O
is O
smaller O
than O
the O
zone B
value O
, O
people O
may O
feel O
less O
entertaining O
. O

This O
is O
because O
the O
game B
is O
too O
simple O
or O
players B
are O
too O
strong O
to O
experience B
harmonic O
uncertainty O
during O
the O
game B
playing O
. O

In O
this O
situation O
the O
game B
tends O
to O
be O
competitive B
[ O
17 O
] O
. O

3.2 O
Relative O
Game O
Refinement O
Measure O
The O
game B
refinement O
theory O
is O
basically O
used O
to O
evaluate O
the O
property O
( O
sophistication O
) O
of O
games B
with O
a O
focus B
on O
the O
game B
outcome O
uncertainty O
. O

Let O
us O
consider O
the O
individual O
match B
analysis O
using O
game B
refinement O
measure O
[ O
18 O
] O
[ O
19 O
] O
. O

Since O
each O
match B
has O
an O
independent O
game B
process O
, O
game B
refinement O
measure O
can O
be O
applied O
. O

We O
demonstrate O
an O
analysis O
of O
two O
extreme O
conditions O
and O
special O
cases O
. O

The O
first O
example O
is O
The O
2014 O
World O
Cup O
semi O
- O
final O
[ O
20 O
] O
: O
Germany O
vs. O
Brazil O
, O
where O
the O
number O
of O
goals B
G O
= O
8 O
and O
the O
number O
of O
shot O
attempts O
T O
= O
31 O
. O

When O
focusing B
on O
this O
match B
, O
R O
value O
is O
given O
by O
Eq O
. O

( O
10 O
) O
. O

R O
= O
√ O
G O
T O
= O
√ O
8 O
31 O
= O
0.091 O
( O
10 O
) O
In O
fact O
, O
this O
match B
was O
not O
a O
well O
balanced O
. O

Brazil O
had O
1 O
goal B
, O
whereas O
Germany O
had O
7 O
goals B
. O

Individually O
, O
game B
refinement O
measure O
for O
Brazil O
( O
say O
RB O
) O
and O
Brazil O
( O
say O
RG O
) O
is O
given O
in O
Eq O
. O

( O
11 O
) O
and O
Eq O
. O

( O
12 O
) O
, O
respectively O
. O

RB O
= O
√ O
1 O
31 O
= O
0.032 O
( O
11 O
) O
RG O
= O
√ O
7 O
31 O
= O
0.085 O
( O
12 O
) O
Apparently O
, O
the O
R O
value O
of O
Germany O
is O
higher O
than O
Brazil O
, O
which O
means O
that O
Germany O
had O
better O
playing O
skill B
. O

Even O
more O
we O
need O
to O
know O
the O
psychological O
meaning O
of O
game B
refinement O
value O
for O
each O
team B
’s O
perception O
. O

Then O
the O
relative O
game B
refinement O
measure O
for O
Brazil O
( O
say O
Rr O
) O
is O
given O
by O
Eq O
. O

( O
13 O
) O
. O

Rr O
= O
R O
× O
RB O
RG O
= O
0.091 O
× O
0.032 O
0.085 O
= O
0.034 O
( O
13 O
) O
Similarly O
, O
the O
relative O
game B
refinement O
measure O
for O
Germany O
is O
given O
by O
Eq O
. O

( O
14 O
) O
. O

Rr O
= O
R O
× O
RG O
RB O
= O
0.091 O
× O
0.085 O
0.032 O
= O
0.242 O
( O
14 O
) O
From O
Eq O
. O

( O
13 O
) O
and O
Eq O
. O

( O
14 O
) O
we O
see O
that O
from O
Germany O
’s O
perspective O
, O
people O
can O
enjoy O
the O
game B
for O
fun O
. O

Meanwhile O
from O
Brazil O
’s O
perspective O
, O
people O
may O
feel O
very O
tough O
and O
they O
must O
seriously O
face O
the O
game B
progress O
. O

Larger O
R O
value O
means O
higher O
fun O
, O
whereas O
smaller O
R O
value O
means O
more O
serious O
or O
competitiveness O
. O

Illustration O
in O
Figure O
4 O
shows O
the O
relation O
between O
R O
value O
and O
balance O
between O
skill B
and O
chance B
in O
boardgames O
as O
well O
as O
continuous O
movement B
games B
. O

Fig O
. O

4 O
. O

An O
illustration O
of O
the O
meaning O
of O
game B
refinement O
measure O
3.3 O
Analysis O
of O
MOBA O
Games O
Multi O
- O
player B
On O
- O
line O
Battle O
Arena O
( O
MOBA O
) O
[ O
21 O
] O
is O
the O
most O
popular O
game B
type O
, O
in O
which O
a O
player B
controls B
a O
single O
character B
at O
one O
of O
two O
teams B
. O

MOBA O
game B
is O
a O
typical O
continuous O
movement B
game B
. O

The O
objective B
is O
to O
destroy O
the O
opponent B
team I
’s O
main O
structure O
with O
the O
assistance O
of O
periodically O
spawned O
computer O
controlled B
units B
. O

Player B
characters B
typically O
have O
various O
abilities B
and O
advantages B
that O
improve O
over O
the O
course O
of O
a O
game B
and O
that O
contribute O
to O
a O
team B
’s O
overall O
strategy B
. O

Mainly O
in O
the O
world B
market O
, O
it O
was O
followed O
by O
three O
spiritual O
successors O
: O
“ O
League O
of O
Legends O
” O
( O
LOL O
) O
, O
“ O
Defense O
of O
the O
Ancients O
” O
( O
DotA O
) O
and O
“ O
Heroes O
of O
the O
Storm O
” O
( O
HotS O
) O
[ O
22 O
] O
. O

The O
game B
progress O
model O
of O
MOBA O
is O
given O
by O
the O
average O
number O
of O
successful O
killing O
heroes B
and O
destroying O
fortress O
( O
say O
K O
) O
over O
the O
average O
number O
of O
attempts O
per O
game B
( O
say O
A O
) O
[ O
13 O
] O
. O

Hence O
, O
the O
game B
refinement O
measure O
of O
MOBA O
is O
given O
by O
Eq O
. O

( O
15 O
) O
. O

R O
= O
√ O
K O
A O
( O
15 O
) O
The O
measures O
of O
game B
refinement O
for O
various O
MOBA O
games B
are O
shown O
in O
Table O
4 O
. O

Because O
of O
the O
game B
battle B
system O
and O
macro O
mechanism B
, O
in O
DotA O
and O
LOL O
one O
tower B
equals O
to O
1 O
kill B
, O
and O
in O
HotS O
one O
castle B
equals O
to O
4 O
kills B
[ O
22 O
] O
. O

For O
killing B
tendency O
A O
, O
any O
tower B
or O
castle B
as O
1 O
attempt O
is O
calculated O
. O

It O
is O
found O
that O
R O
- O
value O
of O
sophisticated O
games B
is O
located O
somewhere O
between O
0.07 O
to O
0.08 O
[ O
2 O
] O
[ O
10 O
] O
. O

Distinctly O
, O
we O
notice O
that O
the O
game B
refinement O
value O
in O
LOL O
battle B
is O
so O
high O
. O

It O
means O
that O
LOL O
will O
be O
too O
excited O
with O
high O
entertainment O
and O
low O
competitiveness O
. O

Table O
4 O
. O

Measures O
of O
game B
refinement O
for O
three O
MOBA O
games B

 O
map I
or O
version O
K O
A O
R O
HotS O
Blackheart O
’s O
bay O
70.90 O
80.10 O
0.105 O
Sky O
temple O
77.68 O
79.90 O
0.110 O
Dragon O
Shire O
63.90 O
88.80 O
0.090 O
Tomb O
of O
the O
SQ O
75.00 O
98.00 O
0.088 O
Infernal O
shrines O
63.08 O
93.00 O
0.085 O
Cursed O
hollow O
69.55 O
100.70 O
0.083 O
Battlefield O
of O
eternity O
99.30 O
168.8 O
0.082 O
Garden O
of O
terror O
68.83 O
88.90 O
0.093 O
Haunted O
mines O
55.68 O
78.10 O
0.096 O
DotA O
Version O
6.48 O
69.2 O
110.8 O
0.075 O
Version O
6.51 O
68.4 O
110.2 O
0.074 O
Version O
6.59 O
69.8 O
110.0 O
0.076 O
Version O
6.61 O
70.0 O
111.6 O
0.075 O
Version O
6.64 O
68.4 O
110.4 O
0.075 O
Version O
6.69 O
67.8 O
108.4 O
0.076 O
Version O
6.74 O
62.4 O
102.6 O
0.077 O
Version O
6.77 O
62.8 O
102.8 O
0.077 O
Version O
6.80 O
68.6 O
106.2 O
0.078 O
LOL O
Version O
6.6 O
37.65 O
44.26 O
0.138 O
Below O
we O
summarize O
the O
entertaining O
and O
competitiveness O
aspect O
of O
MOBA O
games B
based B
on O
the O
game B
refinement O
values O
. O

DotA O
: O
DotA O
is O
a O
very O
stable O
game B
, O
also O
it O
is O
a O
typical O
“ O
G O
- O
T O
Model O
” O
( O
continuous O
movement B
games B
) O
, O
for O
each O
version O
R O
- O
values O
are O
all O
seated O
between O
0.07 O
to O
0.08 O
. O

Therefore O
, O
DotA O
is O
a O
well O
designed O
game B
with O
a O
good O
balance O
between O
entertainment O
and O
competitiveness O
, O
which O
is O
suited O
for O
competitions B
. O

For O
the O
activity O
population O
, O
DotA2 O
has O
7.9 O
million O
per O
month O
all O
over O
the O
world B
[ O
23 O
] O
. O

The O
measure O
of O
game B
refinement O
indicates O
that O
DotA O
is O
the O
most O
successful O
and O
well O
balanced O
MOBA O
game B
in O
the O
world B
. O

LOL B
: O
Generally O
, O
R O
- O
value O
in O
LOL O
is O
too O
high O
, O
whereas O
DotA O
is O
almost O
in O
the O
window O
value O
. O

It O
means O
that O
DotA O
fits O
for O
setting O
as O
e O
- O
sports O
competition B
, O
but O
LOL O
is O
suited O
to O
enjoy O
for O
entertainment O
. O

DotA O
has O
powerful B
skill I
and O
more O
visual O
impact B
for O
each O
hero B
, O
which O
cares O
more O
about O
management O
and O
running O
. O

Players B
need O
to O
make O
a O
stable O
and O
safe B
environment O
to O
carry B
and O
develop O
. O

Gank O
usually O
happens O
during O
the O
whole O
game B
. O

Generally O
, O
a O
DotA O
game B
may O
spend O
about O
50 O
minutes O
but O
LOL O
usually O
takes O
around O
30 O
minutes O
. O

LOL O
provides O
players B
with O
a O
new O
style O
of O
MOBA O
game B
that O
spends O
less O
time B
for O
each O
game B
and O
forms O
a O
fast O
rhythm O
. O

For O
the O
activity O
population O
, O
LOL O
has O
67 O
million O
per O
month O
all O
over O
the O
world B
[ O
23 O
] O
. O

The O
rhythm O
of O
LOL O
is O
faster O
and O
its O
game B
refinement O
is O
higher O
than O
others O
. O

This O
implies O
that O
LOL O
is O
able O
to O
attract O
more O
children O
, O
female O
or O
beginners B
who O
prefer O
to O
play B
it O
because O
of O
the O
higher O
entertainment O
property O
[ O
1 O
] O
. O

HotS B
: O
For O
HotS O
, O
the O
most O
important O
point B
is O
large O
- O
scale O
team B
combat I
and O
the O
game B
rhythm O
is O
much O
higher O
than O
DotA O
or O
LOL O
. O

As O
a O
new O
game B
, O
HotS O
still O
has O
some O
insufficient O
aspects O
. O

According O
to O
Table O
4 O
, O
the O
most O
interesting O
and O
exciting O
map O
is O
‘ O
Sky O
temple O
’ O
. O

‘ O
Battlefield B
of O
eternity O
’ O
and O
‘ O
Cursed O
hollow O
’ O
have O
the O
highest B
level I
competitiveness O
. O

However O
, O
the O
game B
refinement O
measures O
of O
HotS O
are O
higher O
than O
0.08 O
, O
which O
means O
that O
compared O
with O
DotA O
, O
HotS O
is O
not O
so O
suitable O
for O
e O
- O
sports O
competition B
. O

Also O
some O
serious O
mechanism B
issue O
existed O
in O
HotS O
, O
DotA O
focuses B
on O
the O
ana O
- O
phase O
period O
during O
the O
game B
, O
but O
the O
core O
mechanism B
in O
HotS O
is O
wild O
monster O
. O

For O
this O
reason O
, O
the O
game B
depth O
of O
HotS O
is O
less O
than O
DotA O
and O
gets O
a O
larger O
R O
- O
value O
. O

Therefore O
, O
HotS O
cares O
more O
about O
teamwork B
than O
personal O
operation O
and O
game B
awareness O
, O
then O
we O
can O
only O
find O
valid O
data O
about O
the O
population O
of O
HotS O
in O
US O
server B
is O
0.13 O
million O
, O
the O
expected O
number O
all O
over O
the O
world B
will O
not O
be O
larger O
than O
DotA2 O
. O

Nevertheless O
, O
the O
fun O
of O
HotS O
is O
not O
derived O
only O
from O
the O
battle B
. O

The O
various O
heroes B
and O
their O
talents O
can O
provide O
a O
lot O
of O
enjoyment O
for O
Blizzard O
fans O
. O

In O
addition O
, O
they O
can O
design O
maps O
which O
become O
more O
interesting O
and O
well O
balanced O
. O

Also O
the O
design O
group B
of O
HotS O
needs O
to O
revise O
the O
game B
mechanism B
. O

All O
property O
of O
these O
three O
MOBA O
games B
can O
be O
shown O
as O
Figure O
5 O
. O

Fig O
. O

5 O
. O

Entertainment O
and O
competition B
property O
of O
three O
MOBA O
games B
4 O
Concluding O
Remarks O
The O
notion O
of O
game B
progress O
and O
game B
information O
progress O
model O
for O
continuous O
movement B
games B
was O
introduced O
in O
the O
development O
of O
game B
refinement O
measure O
. O

It O
seemed O
to O
be O
a O
successful O
bridge O
between O
continuous O
movement B
games B
like O
sports O
and O
boardgames O
. O

However O
, O
this O
paper O
claimed O
with O
a O
focus B
on O
the O
parameter O
c O
in O
the O
game B
progress O
model O
for O
boardgames O
. O

The O
parameter O
c O
relates O
to O
the O
game B
balance O
. O

The O
condition O
c O
= O
1 O
corresponds O
to O
the O
case O
where O
the O
game B
is O
more O
chance B
- O
based B
one O
. O

If O
the O
parameter O
c O
becomes O
lower O
, O
the O
game B
will O
be O
more O
skill B
- O
based B
one O
. O

Moreover O
, O
a O
new O
perspective O
of O
game B
refinement O
measure O
was O
obtained O
. O

Higher O
( O
lower O
) O
R O
value O
means O
more O
entertaining O
( O
competitive B
) O
, O
whereas O
0.07 O
- O
0.08 O
should O
be O
a O
comfortable O
zone B
due O
to O
its O
good O
balance O
between O
skill B
and O
chance B
in O
game B
playing O
. O

The O
analysis B

 O
of I
popular I
MOBA I
games I
using O
game B
refinement O
measure O
supports B
the O
observation O
. O

The O
concept O
of O
relative O
game B
refinement O
measure O
was O
proposed O
to O
focus B
on O
individual O
team B
performance I
in O
two O
team B
sports O
such O
as O
soccer O
. O

The O
game B
refinement O
measure O
has O
been O
used O
to O
quantify O
the O
game B
sophistication O
for O
the O
game B
under O
consideration O
. O

However O
, O
we O
considered O
the O
possibility O
of O
quantifying O
the O
game B
sophistication O
from O
the O
viewpoint O
of O
individual O
team B
. O

Acknowledgements O
This O
research O
is O
funded O
by O
a O
grant O
from O
the O
Japan O
Society O
for O
the O
Promotion O
of O
Science O
( O
JSPS O
) O
, O
within O
the O
framework O
of O
the O
Grant O
- O
in O
- O
Aid O
for O
Challenging O
Exploratory O
Research O
( O
grant O
number O
26540189 O
) O
and O
Grant O
- O
inAid O
for O
JSPS O
Fellow O
. O

References O
1 O
. O

Gaudiosi O
, O
J. O
( O
2012 O
) O
. O

Riot B
games I
’ O
league B
of I
legends I
officially O
becomes O
most O
played B
PC O
game B
in O
the O
world B
. O

Forbes O
. O

2 O
. O

Iida O
, O
H. O
, O
Takahara O
, O
K. O
, O
Nagashima O
, O
J. O
, O
Kajihara O
, O
Y. O
and O
Hashimoto O
, O
T. O
( O
2004 O
) O
. O

An O
application O
of O
game B
- O
refinement O
theory O
to O
Mah O
Jong O
. O

In O
International O
Conference O
on O
Entertainment O
Computing O
, O
pp.333–338 O
. O

Springer O
Berlin O
Heidelberg O
. O

3 O
. O

Iida O
, O
H. O
, O
Takeshita O
, O
N. O
and O
Yoshimura O
, O
J. O
( O
2003 O
) O
. O

A O
metric O
for O
entertainment O
of O
boardgames O
: O
its O
implication O
for O
evolution B
of O
chess O
variants O
. O

In O
Entertainment O
Computing O
, O
pp.65–72 O
. O

Springer O
US O
. O

4 O
. O

Neumann O
, O
J. O
( O
1928 O
) O
. O

Zur O
theorie O
der O
gesellschaftsspiele O
. O

Mathematische O
Annalen O
, O
100(1):295—320 O
. O

5 O
. O

Shannon O
, O
C. O
E. O
( O
1988 O
) O
. O

Programming O
a O
computer O
for O
playing B
chess O
. O

In O
Computer O
chess O
compendium O
, O
pp.2–13 O
. O

Springer O
New O
York O
. O

6 O
. O

Turing O
, O
A. O
( O
1953 O
) O
. O

Chess O
. O

part O
of O
the O
collection O
Digital O
Computers O
Applied O
to O
Games O
. O

In O
Bertram O
Vivian O
Bowden O
( O
editor O
) O
, O
Faster O
Than O
Thought O
, O
a O
symposium O
on O
digital O
computing O
machines O
. O

7 O
. O

Panumate O
, O
C. O
, O
Xiong O
, O
S. O
and O
Iida O
, O
H. O
( O
2015 O
) O
“ O
An O
Approach O
to O
Quantifying O
Pokemon O
’s O
Entertainment O
Impact O
with O
Focus O
on O
Battle O
” O
, O
3rd O
International O
Conference O
on O
Applied O
Computing O
and O
Information O
Technology/2nd O
International O
Conference O
on O
Computational O
Science O
and O
Intelligence O
, O
Okayama O
, O
pp O
. O

60–66 O
. O

8 O
. O

Punyawee O
, O
A. O
, O
Panumate O
, O
C. O
and O
Iida O
, O
H. O
( O
2016 O
) O
. O

Finding O
Comfortable O
Settings O
of O
Snake O
Game O
Using O
Game O
Refinement O
Measurement O
. O

In O
International O
Conference O
on O
Computer O
Science O
and O
its O
Applications O
, O
pp.66–73 O
. O

Springer O
Singapore O
. O

9 O
. O

Cincotti O
, O
A. O
, O
Iida O
, O
H. O
, O
and O
Yoshimura O
, O
J. O
( O
2007 O
) O
. O

Refinement O
and O
complexity O
in O
the O
evolution B
of O
chess O
. O

In O
Proceedings O
of O
the O
10th O
International O
Conference O
on O
Computer O
Science O
and O
Informatics O
, O
pp.650–654 O
. O

10 O
. O

Sutiono O
, O
A. O
P. O
, O
Purwarianti O
, O
A. O
and O
Iida O
, O
H. O
( O
2014 O
) O
. O

A O
mathematical O
model O
of O
game B
refinement O
. O

In O
International O
Conference O
on O
Intelligent O
Technologies O
for O
Interactive O
Entertainment O
, O
pp.148–151 O
. O

Springer O
International O
Publishing O
. O

11 O
. O

Nossal O
, O
N. O
Expansion O
of O
game B
refinement O
theory O
into O
continuous O
movement B
games B
with O
consideration O
on O
functional O
brain B
measurement O
. O

Ph.D. O
Thesis O
, O
Japan O
Advanced O
Institution O
of O
Science O
and O
Technology O
, O
2015 O
. O

12 O
. O

Xiong O
, O
S. O
and O
Iida O
, O
H. O
( O
2014 O
) O
. O

Attractiveness O
of O
real O
time B
strategy B
games B
. O

In O
2nd O
International O
Conference O
on O
Systems O
and O
Informatics O
( O
ICSAI O
) O
, O
pp.271–276 O
. O

IEEE O
. O

13 O
. O

Xiong O
, O
S. O
, O
Zuo O
, O
L. O
, O
Chiewvanichakorn O
, O
R. O
and O
Iida O
, O
H. O
( O
2014 O
) O
. O

Quantifying O
engagement O
of O
various O
games B
. O

In O
The O
19th O
Game O
Programming O
Workshop O
, O
pp.101–106 O
. O

Information O
Processing O
Society O
of O
Japan O
. O

14 O
. O

Iida O
, O
H. O
( O
2008 O
) O
. O

Fairness O
, O
judges O
and O
thrill O
in O
games B
. O

IPSJ O
- O
SIG O
- O
GI O
Technical O
Report O
, O
28 O
, O
pp.61–68 O
. O

15 O
. O

De O
Groot O
, O
A. O
D. O
( O
1965 O
) O
. O

Thought O
and O
choice O
in O
chess O
. O

Mouton O
Publishers O
. O

16 O
. O

Ranking B
data O
of O
soccer O
league,[online O
] O
. O

http://goal.sports.163.com O
, O
URL O
accessed O
, O
2017 O
17 O
. O

Xiong O
, O
S. O
, O
Tiwary O
, O
P. O
P. O
and O
Iida O
, O
H. O
( O
2016 O
) O
. O

Solving O
the O
Sophistication O
- O
Population O
Paradox O
of O
Game O
Refinement O
Theory O
. O

In O
International O
Conference O
on O
Entertainment O
Computing O
, O
pp.266–271 O
. O

Springer O
International O
Publishing O
. O

18 O
. O

Prasertsakul O
, O
P. O
, O
Iida O
, O
H. O
and O
Kondo O
, O
T. O
( O
2016 O
) O
. O

Boring O
game B
identification O
: O
case O
study O
using O
popular O
sports O
games B
. O

In O
SICE O
Annual O
Conference O
, O
Society O
of O
Instrument O
and O
Control O
Engineers O
. O

19 O
. O

Panumate O
, O
C. O
and O
Iida O
, O
H. O
( O
2016 O
) O
. O

Quantifying O
Enjoyment O
of O
Individual O
Match O
in O
Games O
. O

In O
The O
Annual O
Conference O
on O
Engineering O
and O
Applied O
Science O
, O
Higher O
Education O
Forum O
. O

20 O
. O

Groll O
, O
A. O
, O
Schauberger O
, O
G. O
and O
Tutz O
, O
G. O
( O
2014 O
) O
. O

Brazil O
or O
Germany O
- O
who O
will O
win O
the O
trophy O
? O
Prediction O
of O
the O
FIFA O
World O
Cup O
2014 O
based B
on O
team B
- O
specific O
regularized O
Poisson O
regression O
. O

21 O
. O

Johnson O
, O
D. O
, O
Nacke O
, O
L. O
E. O
and O
Wyeth O
, O
P. O
( O
2015 O
) O
. O

All O
about O
that O
base B
: O
differing O
player B
experiences B
in O
video B
game I
genres O
and O
the O
unique O
case O
of O
moba B
games I
. O

In O
Proceedings O
of O
the O
33rd O
Annual O
ACM O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
, O
pp.2265–2274 O
. O

22 O
. O

Xiong O
, O
S. O
, O
Zahi O
, O
H. O
, O
Zuo O
, O
L. O
, O
Wu O
, O
M. O
and O
Iida O
, O
H. O
( O
2015 O
) O
. O

Analysis O
of O
the O
” O
Heroes O
of O
the O
Storm O
” O
. O

International O
Journal O
on O
Advances O
in O
Computer O
Science O
, O
4(6):79–82 O
. O

23 O
. O

Minotti O
, O
M. O
( O
2015 O
) O
. O

Comparing O
MOBAs B
: O
League O
of O
Legends O
vs. O
DOTA2 O
vs. O
Smite O
vs. O
Heroes O
of O
the O
Storm O
, O
[ O
online O
] O
. O

http://venturebeat.com/2015/07/15/comparingmobas-league-of-legends-vs-dota-2-vs-smite-vs-heroes-of-the-storm/ O
STFU O
NOOB O
! O
Predicting O
Crowdsourced O
Decisions O
on O
Toxic O
Behavior O
in O
Online O
Games O
[ O
Please O
cite O
the O
WWW’14 O
version O
of O
this O
paper O
] O
Jeremy O
Blackburn O
University O
of O
South O
Florida O
Tampa O
, O
FL O
, O
USA O
jhblackb@mail.usf.edu O
Haewoon O
Kwak O
Telefonica O
Research O
Barcelona O
, O
Spain O
kwak@tid.es O
ABSTRACT O
One O
problem O
facing O
players B
of O
competitive B
games I
is O
negative O
, O
or O
toxic O
, O
behavior O
. O

League O
of O
Legends O
, O
the O
largest O
eSport O
game B
, O
uses O
a O
crowdsourcing O
platform O
called B
the O
Tribunal O
to O
judge O
whether O
a O
reported B
toxic B
player I
should O
be O
punished O
or O
not O
. O

The O
Tribunal O
is O
a O
two O
stage B
system O
requiring O
reports B
from O
those O
players B
that O
directly O
observe O
toxic O
behavior O
, O
and O
human O
experts O
that O
review O
aggregated O
reports B
. O

While O
this O
system O
has O
successfully O
dealt O
with O
the O
vague O
nature O
of O
toxic O
behavior O
by O
majority O
rules B
based B
on O
many O
votes O
, O
it O
naturally O
requires O
tremendous O
cost B
, O
time B
, O
and O
human O
efforts O
. O

In O
this O
paper O
, O
we O
propose O
a O
supervised O
learning O
approach O
for O
predicting O
crowdsourced O
decisions O
on O
toxic O
behavior O
with O
largescale O
labeled O
data O
collections O
; O
over O
10 O
million O
user B
reports I
involved O
in O
1.46 O
million O
toxic B
players O
and I
corresponding I
crowdsourced I
decisions O
. O

Our O
result O
shows O
good O
performance O
in O
detecting O
overwhelmingly O
majority O
cases O
and O
predicting O
crowdsourced O
decisions O
on O
them O
. O

We O
demonstrate O
good O
portability O
of O
our O
classifier O
across O
regions O
. O

Finally O
, O
we O
estimate O
the O
practical O
implications O
of O
our O
approach O
, O
potential O
cost B
savings O
and O
victim B
protection B
. O

Categories O
and O
Subject O
Descriptors O
K.4.2 O
[ O
Computers O
and O
Society O
] O
: O
Social O
Issues O
— O
Abuse O
and O
crime O
involving O
computers O
; O
J.4 O
[ O
Computer O
Applications O
] O
: O
Social O
and O
Behavioral O
Sciences O
— O
Sociology O
, O
Psychology O
Keywords O
League O
of O
Legends O
; O
online B
video O
games I
; O
toxic O
behavior O
; O
crowdsourcing O
; O
machine O
learning O
0 O
. O

EXECUTIVE O
SUMMARY O
“ O
STFU O
NOOB O
! O
” O
If O
you O
’ve O
played B
an O
online B
video O
game I
, O
there O
’s O
a O
good O
chance B
you O
’ve O
heard O
this O
before O
. O

Toxic O
behavior O
is O
a O
part O
of O
life O
in O
the O
world B
of O
modern O
mulitiplayer O
games B
, O
but O
does O
it O
have O
to O
be O
? O
Do O
gamers B
need O
to O
be O
subjected O
to O
beratement O
for O
every O
little O
mistake B
? O
If O
another O
player B
does O
n’t O
get O
his O
way O
, O
do O
his O
teammates B
have O
to O
sit O
and O
watch O
them O
actively O
try O
to O
sabotage O
the O
rest O
of O
the O
game B
? O
If O
someone O
consistently O
exhibits O
this O
type O
of O
behavior O
, O
does O
the O
entire O
community B
just O
have O
to O
suffer O
? O
As O
gaming O
has O
grown O
from O
simple O
two O
player B
games B
like O
PONG O
to O
a O
multi O
- O
billion O
dollar O
, O
so O
too O
has O
toxic O
behavior O
become O
more O
severe O
. O

Toxic O
behavior O
poses O
a O
large O
threat O
to O
the O
gaming O
industry O
. O

Estimates O
put O
griefing B
as O
the O
cause O
of O
about O
25 O
% O
of O
calls B
to O
customer O
support B
lines O
. O

Not O
only O
is O
this O
a O
huge O
cost B
to O
game B
operators O
, O
but O
, O
it O
demonstrates O
the O
kind O
of O
damage B
that O
toxic B
players I
can O
cause O
. O

The O
stress O
caused O
by O
harassment B
and O
other O
forms O
of O
toxic O
behavior O
can O
cause O
players B
to O
become O
fatigued O
to O
the O
point B
that O
they O
quit O
the O
game B
. O

With O
the O
advent O
of O
the O
free B
- O
to O
- O
play B
business O
model O
, O
where O
the O
game B
is O
given O
away O
for O
free B
and O
the O
developers O
are O
supported B
via O
micro B
- I
transactions I
, O
a O
game B
’s O
sustainability B
is O
directly O
related O
to O
the O
strength B
of O
its O
community B
. O

A O
large O
, O
healthy O
community B
attracts O
new O
players B
and O
keeps O
existing O
players B
engaged O
and O
willing O
to O
spend O
money O
. O

Left O
unchecked O
, O
toxic O
behavior O
threatens O
to O
tear O
a O
game B
’s O
community B
apart O
. O

There O
have O
been O
many O
attempts O
to O
deal O
with O
toxic O
behavior O
. O

The O
earliest O
required O
direct O
human O
intervention O
, O
usually O
from O
a O
game B
master B
/ O
administrator O
. O

While O
accurate O
and O
decisive O
, O
this O
type O
of O
system O
does O
n’t O
scale O
when O
there O
are O
10s O
of O
millions O
of O
active B
players I
. O

More O
recently O
, O
crowdsourced O
systems O
, O
which O
make O
use O
of O
the O
input B
of O
many O
humans O
, O
have O
arisen O
. O

For O
example O
, O
the O
Overwatch O
system O
uses O
expert O
input B
to O
detect O
cheaters B
in B
Counter O
- I
Strike I
: O
Global O
Offensive O
. O

Another O
example O
is O
the O
League O
of O
Legends O
Tribunal O
, O
which O
allows O
people O
to O
vote O
on O
whether O
an O
accused O
player B
is O
toxic O
. O

While O
Overwatch O
deals O
with O
a O
somewhat O
black O
- O
and O
- O
white O
decision O
, O
the O
Tribunal O
deals O
with O
a O
more O
nebulously O
defined O
set O
of O
behaviors O
. O

Maybe O
the O
player B
that O
appears O
to O
be O
deliberately O
rushing B
in O
and O
dying O
just O
needs O
to O
“ O
git O
gud O
. O

” O
Maybe O
the O
player B
lashing O
out O
in O
chat B
just O
had O
a O
bad O
day O
and O
would O
normally O
never O
act O
like O
that O
. O

The O
Tribunal O
collects O
multiple O
instances O
of O
potentially O
toxic O
behavior O
, O
collected O
from O
reports B
by O
players B
that O
actually O
experienced B
the O
behavior O
, O
and O
presents O
them O
to O
a O
panel O
of O
reviewers O
recruited O
from O
the O
community B
who O
cast O
votes O
for O
guilt O
or O
innocence O
. O

The O
Tribunal O
has O
been O
deemed O
a O
success O
with O
claims O
that O
it O
results O
in O
reduced O
recidivism O
. O

But O
, O
it O
still O
requires O
significant O
human O
effort O
, O
and O
it O
is O
slow B
by O
nature O
: O
a O
number O
of O
reports B
must O
be O
received O
before O
cases O
are O
even O
presented O
to O
reviewers O
. O

In O
this O
paper O
, O
we O
analyze O
reports B
from O
several O
million O
League O
of O
Legends O
matches B
and O
their O
corresponding O
crowdsourced O
decisions O
from O
the O
Tribunal O
. O

We O
find O
that O
Tribunal O
reviewers O
base B
their O
decisions O
on O
in B
- I
game I
performance O
of O
both O
the O
accused O
and O
the O
other O
players B
in O
the O
game B
, O
chat B
logs O
, O
and O
the O
information O
provided O
by O
ingame B
reports B
. O

We O
then O
use O
machine O
learning O
techniques B
to O
build O
a O
model O
for O
detecting O
toxic O
behavior O
that O
successfully O
discriminates O
between O
guilty O
and O
innocent O
behavior O
about O
80 O
% O
of O
the O
time B
, O
and O
between O
overwhelming O
agreement O
on O
innocence O
about O
88 O
% O
of O
the O
time B
. O

Our O
findings O
have O
broad O
impact B
. O

There O
is O
an O
obvious B
contribution O
to O
the O
gaming O
industry O
, O
but O
, O
we O
also O
provide O
a O
deeper O
understanding O
of O
toxic O
behavior O
. O

We O
show O
that O
, O
at O
least O
in O
certain O
scenarios O
, O
computers O
can O
be O
used O
to O
help O
find O
bad O
behavior O
. O

Our O
experimentally O
derived O
model O
hints O
at O
the O
possibility O
for O
automatic O
detection O
of O
, O
for O
example O
, O
cyber O
bullying O
. O

Ultimately O
, O
we O
demonstrate O
that O
machine O
learning O
, O
with O
a O
little O
help O
from O
the O
crowd O
, O
can O
tell O
if O
you O
are O
being O
toxic O
or O
just O
need O
to O
git O
gud O
. O

arXiv:1404.5905v1 O
[ O
cs O
. O

SI O
] O
23 O
Apr O
2014 O
1 O
. O

INTRODUCTION O
Bad O
behavior O
on O
the O
Internet O
, O
or O
toxic O
disinhibition O
[ O
36 O
] O
, O
is O
a O
growing O
concern O
. O

The O
anonymity O
afforded O
by O
, O
and O
ubiquity O
of O
, O
online O
interactions O
has O
coincided O
with O
an O
alarming O
amount O
of O
bad O
behavior O
. O

Computer O
- O
mediated O
- O
communication B
( O
CMC O
) O
, O
without O
presence O
of O
face O
- O
to O
- O
face O
communication B
, O
naturally O
leads B
to O
hostility O
and O
aggressiveness O
[ O
16 O
] O
. O

In B
multi O
- I
player I
gaming O
, O
one O
of O
the O
most O
popular O
online O
activities O
, O
bad O
behavior O
is O
already O
pervasive O
. O

Over O
the O
past O
two O
decades O
, O
multi O
- O
player B
games B
have O
evolved O
past O
simple B

 O
games I
like O
PONG O
, O
growing O
so O
popular O
as O
to O
spawn O
an O
“ O
eSports O
” O
industry O
with O
professional O
players B
competing O
for O
millions O
of O
dollars O
in O
prize B
money O
. O

Competition B
has O
been O
considered O
a O
good B
game I
design O
element O
for O
enjoyment O
, O
but O
at O
the O
same O
time B
, O
it O
leads B
to O
intraand O
inter O
- O
group B
conflicts O
and O
naturally O
leads B
to O
aggressive O
, O
bad O
behavior O
. O

Usually O
, O
bad O
behavior O
in B
multi O
- I
player I
games B
is O
called B
toxic O
because O
numerous O
players B
can O
be O
exposed O
to O
such O
behavior O
via O
games B
’ O
reliance O
on O
player B
interactions O
and O
the O
damage B
it O
does O
to O
the O
community B
. O

The O
impact B
of O
toxic B
players I
is O
problematic O
to O
the O
gaming O
industry O
. O

For O
instance O
, O
a O
quarter O
of O
customer O
support B
calls B
to O
online B

 O
game I
companies O
are O
complaints O
about O
toxic B
players I
[ O
8 O
] O
. O

However O
, O
sometimes O
the O
boundary O
of O
toxic O
playing O
is O
unclear O
[ O
7 O
, O
10 O
, O
23 O
, O
27 O
] O
because O
the O
expected O
behavior O
, O
customs O
, O
rules B
, O
or O
ethics O
are O
different O
across O
games B
[ O
41 O
] O
. O

Across O
individuals O
, O
the O
perception O
of O
this O
grief O
inducing O
behavior O
is O
unique O
. O

Subjective O
perception O
of O
toxic O
playing O
makes O
toxic B
players I
themselves O
sometimes O
fail O
to O
recognize O
their O
behavior O
as O
toxic O
[ O
23 O
] O
. O

This O
inherently O
vague O
nature O
of O
toxic O
behavior O
opens B
research O
challenges O
to O
define O
, O
detect O
, O
and O
prevent O
toxic O
behavior O
in O
scalable O
manner O
. O

A O
prevalent O
solution O
for O
dealing O
with O
toxicity B
is O
to O
allow O
reporting O
of O
badly O
behaving O
players B
, O
taking O
action O
once O
a O
certain O
threshold O
of O
reports B
is O
met O
. O

Unfortunately O
, O
such O
systems O
have O
flaws O
, O
both O
in O
practice O
and O
, O
perhaps O
more O
importantly O
, O
their O
perceived O
effectiveness O
by O
the O
player B
base B
. O

For O
example O
, O
Dota O
2 O
, O
a O
popular O
multiplayer O
developed O
by O
Valve O
, O
has O
a O
report B
system I
for O
abusive O
communication B
that O
automatically O
“ O
mutes B
” O
players B
for O
a O
given O
period O
of O
time B
. O

While O
the O
Dota O
2 O
developers O
claim O
this O
has O
resulted O
in O
a O
significant O
drop B
in O
toxic O
communication B
, O
players B
report B
a O
different O
experience B
: O
not O
only O
does O
abusive O
communication B
still O
occur O
, O
but O
the O
report B
system I
is O
ironically O
used O
to O
grief O
innocent O
players123 O
. O

Riot O
Games O
, O
which O
operates O
one O
of O
the O
most O
popular O
competitive B

 O
games I
, O
called B
League O
of O
Legends O
( O
LoL O
) O
, O
introduced O
the O
Tribunal O
in O
May O
2011 O
. O

As O
its O
name O
indicates O
, O
it O
uses O
the O
wisdom O
of O
crowds O
to O
judge O
guilt O
and O
innocence O
of O
players B
accused O
of O
toxic O
behavior O
; O
accused O
toxic B
players I
are O
put O
on O
trial O
with O
human O
jurors O
instead O
of O
automatically O
being O
punished O
. O

While O
this O
system O
has O
successfully O
dealt O
with O
the O
vague O
nature O
of O
toxic O
behavior O
by O
majority O
rule B
based B
on O
many O
votes O
, O
it O
naturally O
requires O
tremendous O
cost B
, O
time B
, O
and O
human O
efforts O
. O

Thus O
, O
our O
challenge O
lies O
in O
determining O
whether O
or O
not O
we O
can O
assist B
the O
Tribunal O
by O
machine O
learning O
. O

Although O
toxic O
behavior O
has O
been O
known O
as O
hard B
to O
define O
, O
we O
have O
a O
huge O
volume O
of O
labeled O
data O
. O

In O
this O
paper O
, O
we O
explore O
toxic O
behavior O
using O
supervised O
learning O
techniques B
. O

We O
collect O
over O
10 O
million O
user B
reports I
involved O
in O
1.46 O
million O
toxic B
players O
and I
corresponding I
crowdsourced I
decisions O
. O

We O
extract O
534 O
features O
from O
in B
- I
game I
performance O
, O
user B

 O
reports I
, O
and O
chats B
. O

We O
use O
a O
Random O
Forest O
classifier O
for O
crowdsourced O
decision O
prediction O
. O

Our O
results O
show O
good O
performance O
in O
detecting O
overwhelmingly O
majority O
cases O
and O
predicting O
crowd1 O
http://tinyurl.com/stfunub1 O
2 O
http://tinyurl.com/stfunub2 O
3 O
http://tinyurl.com/stfunub3 O
sourced O
decisions O
on O
them O
. O

In O
addition O
, O
we O
reveal O
important O
features O
in O
predicting O
crowdsourced O
decisions O
. O

We O
demonstrate O
good O
portability O
of O
our O
classifier O
across O
regions O
. O

Finally O
, O
we O
estimate O
the O
practical O
implications O
of O
our O
approach O
, O
potential O
cost B
savings O
, O
and O
victim B
protection B
. O

We O
make O
several O
contributions O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
provide O
the O
first O
demonstration O
that O
toxic O
behavior O
, O
notoriously O
difficult O
to O
objectively O
define O
, O
can O
be O
detected O
by O
a O
supervised O
classifier O
trained B
with O
large O
- O
scale O
crowdsourced O
data O
. O

Next O
, O
we O
show O
that O
we O
can O
successfully O
discriminate O
behavior O
that O
is O
overwhelmingly O
deemed O
innocent O
by O
human O
reviewers O
. O

We O
also O
identify O
several O
factors O
which O
are O
used O
for O
the O
decision O
making O
in O
the O
Tribunal O
. O

We O
show O
that O
psychological O
measures O
of O
linguistic O
“ O
goodness O
” O
successfully O
quantify O
the O
tone O
of O
chats B
in O
online O
gaming O
. O

Finally O
, O
we O
apply O
our O
models O
to O
different O
regions O
of O
the O
world B
, O
which O
have O
different O
characteristics O
in O
how O
toxic O
behavior O
is O
exhibited O
and O
responded O
to O
, O
finding O
common O
elements O
across O
cultures O
. O

In O
Section O
2 O
we O
give O
background O
on O
LoL O
and O
its O
crowdsourced O
solution O
for O
dealing O
with O
toxic O
behavior O
. O

In O
Section O
3 O
we O
review O
previous O
literature O
. O

In O
Section O
4 O
we O
provide O
details O
on O
our O
dataset O
. O

In O
Section O
5 O
we O
outline O
our O
research O
goals B
. O

In O
Section O
6 O
we O
explain O
features O
extracted O
from O
in B
- I
game I
performance O
, O
user B
reports I
, O
and O
linguistic O
signatures O
based B
on O
chat B
logs O
. O

In O
Section O
7 O
we O
build O
several O
models O
using O
these O
features O
. O

In O
Section O
8 O
we O
propose O
a O
supervised O
learner O
and O
present O
test O
results O
. O

In O
Section O
9 O
, O
we O
discuss O
practical O
implications O
and O
future O
research O
directions O
, O
and O
we O
conclude O
in O
Section O
10 O
. O

2 O
. O

BACKGROUND O
To O
help O
readers O
who O
are O
unfamiliar O
with O
LoL B
, O
we O
briefly O
introduce O
its O
basic O
gameplay B
mechanisms B
, O
toxic O
playing B
in O
LoL O
, O
and O
the O
Tribunal O
system O
. O

2.1 O
League O
of O
Legends O
LoL B
is O
a O
match B
- O
based B
team B
competition I
game I
. O

Teams B
are O
most O
often O
composed O
of O
five O
players B
who O
are O
randomly O
matched B
together O
, O
and O
friends O
can O
also O
form O
pre O
- O
made O
teams B
. O

LoL O
features O
a O
symmetric O
map O
with O
three O
paths B
, O
or O
“ O
lanes B
” O
. O

The O
lanes B
are O
colloquially O
known O
as O
“ O
top O
” O
, O
“ O
mid O
” O
, O
and O
“ O
bot O
” O
and O
have O
a O
“ O
jungle B
” O
area B
between O
them O
. O

The O
goal B
of O
LoL B
is O
to O
penetrate O
the O
enemy B
team I
’s O
central O
base B
, O
or O
“ O
Nexus O
” O
, O
and O
destroy O
it O
. O

To O
destroy O
the O
Nexus O
, O
players B
must O
first O
destroy O
towers B
in O
at O
least O
one O
lane B
. O

Typically O
, O
when O
a O
match B
starts B
players B
choose O
to O
go O
to O
one O
particular O
lane B
, O
often O
taking O
on O
different O
roles B
depending O
on O
the O
character B
they O
chose O
to O
play B
. O

Players B
’ O
in B
- I
game I
characters B
are O
called B
champions B
. O

Riot O
has O
released O
115 O
champions B
as O
of O
September O
2013 O
. O

A O
weekly O
rotation O
of O
10 O
champions B
is O
offered O
to O
all O
players B
, O
but O
they O
can O
also O
be O
permanently O
purchased O
via O
points B
earned O
through O
play B
or O
real O
world B
money O
. O

Each O
champion B
has O
different O
strengths B
and O
thus O
naturally O
lend O
themselves O
to O
the O
different O
styles O
of O
play B
expected O
from O
a O
particular O
lane B
. O

2.2 O
Reporting O
Toxic O
Players O
After O
a O
match B
, O
players B
can O
report B
toxic B
players I
in O
one O
of O
10 O
predefined O
categories O
: O
assisting B
enemy B
team I
, O
intentional O
feeding O
, O
offensive O
language O
, O
verbal O
abuse O
, O
negative O
attitude O
, O
inappropriate O
[ O
handle O
] O
name O
, O
spamming B
, O
unskilled B
player I
, O
refusing O
to O
communicate O
with O
team B
, O
and O
leaving O
the O
game B
/ O
AFK O
[ O
away O
from O
keyboard O
] O
. O

Intentional O
feeding O
indicates O
that O
a O
player B
allowed O
themselves I
to I
be O
killed B
by O
the O
opposing B
team I
on O
purpose O
, O
and O
leaving O
the O
game B
is O
doing O
nothing O
but O
staying O
at O
the O
base B
for O
the O
entire O
game B
. O

To O
understand O
the O
motivation O
behind O
intentional O
feeding O
and O
assisting B
the O
enemy B
team I
, O
we O
present O
a O
common O
scenario O
when O
such O
toxic O
play B
happens O
. O

LoL B
matches I
do O
not O
have O
a O
time B
limit O
, O
instead O
continuing O
until O
one O
team B
’s O
Nexus O
is O
destroyed O
. O

If O
a O
team B
is O
at O
a O
disadvantage B
, O
real O
or O
perceived O
, O
at O
some O
point B
during O
the O
match B
, O
they O
may O
give O
up O
by O
voting O
to O
surrender B
. O

However O
, O
surrendering B
is O
allowed O
only O
after O
20 O
minutes O
have O
passed O
, O
and O
the O
vote O
requires O
at O
least O
four O
affirmatives O
to O
pass O
. O

If O
a O
surrender B
vote O
fails O
, O
players B
who O
voted O
for O
the O
surrender B
may O
lose O
interest O
and O
regard O
continuing O
the O
game B
as O
wasting O
time B
. O

Some O
of O
these O
players B
exhibit O
extreme O
behavior O
in O
response O
to O
the O
failed O
vote O
, O
such O
as O
leaving O
the O
game B
/ O
AFK O
, O
intentional O
feeding O
, O
or O
assisting B
the O
enemy B
. O

Leaving O
the O
game B
is O
particularly O
painful O
since O
being O
down O
even O
one O
player B
greatly O
reduces O
the O
chance B
of O
a O
“ O
come O
from O
behind O
” O
victory B
. O

In O
other O
online B
games I
, O
leaving O
is O
not O
typically O
categorized O
as O
a O
toxic O
because O
it O
usually O
does O
not O
harm O
other O
players B
in O
such O
a O
catastrophic O
manner O
. O

2.3 O
LoL O
Tribunal O
To O
take O
the O
subjective O
perception O
of O
toxic O
behavior O
into O
account O
, O
Riot O
developed O
a O
crowdsourcing O
system O
to O
judge O
whether O
reported B
players B
should O
be O
punished O
, O
called B
the O
Tribunal O
. O

The O
verdict O
is O
determined O
by O
majority O
votes O
. O

When O
a O
player B
is O
reported B
hundreds O
of O
times B
over O
dozens O
of O
matches B
, O
the O
reported B
player B
is O
brought O
to O
the O
Tribunal4 O
. O

Riot B
randomly O
selects O
at O
most O
five O
reported B
matches5 O
and O
aggregates O
them O
as O
a O
single O
case O
. O

In O
other O
words O
, O
one O
case O
includes O
up O
to O
5 O
matches B
. O

Cases O
include O
detailed O
information O
for O
each O
match B
, O
such O
as O
the O
result O
of O
the O
match B
, O
reason O
and O
comments O
from O
reports B
, O
the O
entire O
chat B
log O
during O
the O
match B
, O
and O
the O
scoreboard B
, O
that O
reviewers O
use O
to O
decide O
whether O
that O
toxic B
player I
should O
be O
pardoned O
or O
punished O
. O

To O
protect O
privacy O
, O
players B
’ O
handles O
are O
removed O
in O
the O
Tribunal O
, O
and O
no O
information O
about O
social O
connections B
are O
available O
. O

We O
note O
that O
our O
dataset O
does O
not O
include O
reports B
of O
unskilled B
player I
, O
refusing O
to O
communicate O
with O
team B
, O
and O
leaving O
the O
game B
in O
our O
datasets O
, O
even O
though O
players B
are O
able O
to O
choose O
from O
the O
full B
set O
of O
10 O
predefined O
categories6 O
. O

The O
Tribunal O
also O
institutes O
mechanisms B
to O
maintain O
the O
quality O
of O
reviewers O
’ O
tasks O
. O

One O
is O
a O
limit O
on O
the O
number O
of O
cases O
that O
reviewers O
can O
see O
a O
day O
. O

Next O
is O
a O
minimum O
duration O
for O
decision O
making O
, O
limiting O
mechanical O
clicks O
without O
careful O
consideration O
. O

Another O
is O
a O
skip O
feature O
for O
difficult O
cases O
. O

After O
some O
time B
has O
passed O
, O
reviewers O
can O
see O
the O
final O
crowdsourced O
decisions O
for O
the O
cases O
they O
reviewed O
as O
well O
as O
the O
level B
of O
agreement O
( O
majority O
, O
strong O
majority O
, O
or O
overwhelming O
majority O
) O
. O

To O
encourage O
user B
participation O
, O
Riot O
adopts O
reviewers O
’ O
accuracy O
score O
and O
ranking B
as O
gamification O
element O
. O

3 O
. O

RELATED O
WORK O
3.1 O
Toxic O
Playing B
in O
Online O
Games O
Toxic O
behavior O
in O
online B
games I
is O
a O
form O
of O
cyberbullying O
, O
defined O
as O
repetitive O
intentional O
behavior O
to O
harm O
others O
through O
electronic O
channels O
[ O
34 O
] O
. O

Computer O
- O
mediated O
- O
communication B
through O
electronic O
channels O
without O
face O
- O
to O
- O
face O
communication B
lacks O
social O
psychological O
influences O
and O
can O
lead B
to O
more O
hostile O
interaction O
[ O
16 O
] O
. O

In O
psychology O
and O
education O
, O
offline B
bullying O
has O
been O
actively O
researched O
[ O
29 O
] O
, O
and O
offers O
a O
theoretical O
background O
to O
understand O
cyberbullying O
. O

4 O
http://tinyurl.com/stfunub4 O
5 O
http://tinyurl.com/stfunub5 O
6To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
intentional O
on O
the O
part O
of O
Riot O
. O

Griefing O
is O
a O
term O
describing O
cyberbullying O
in O
online O
gaming O
, O
and O
those O
who O
enjoy O
the O
act O
of O
disrupting O
other O
players B
are O
called B
grief O
players B
( O
“ O
griefers B
” O
) O
[ O
10 O
, O
25 O
] O
. O

Griefers B
make O
other O
players B
annoyed O
and O
feel O
fatigued O
. O

Sometimes O
victims B
even O
leave O
the O
game B
[ O
27 O
] O
, O
exhibiting O
toxic O
behavior O
themselves O
to O
escape O
beratement O
. O

Although O
griefing O
is O
intuitively O
understood O
, O
its O
boundary O
is O
unclear O
[ O
7 O
, O
10 O
, O
23 O
, O
27 O
] O
because O
customs O
, O
rules B
, O
or O
ethics O
can O
be O
different O
across O
games B
[ O
41 O
] O
. O

In O
addition O
, O
the O
perception O
of O
grief O
behavior O
is O
unique O
across O
individuals O
. O

As O
a O
consequence O
, O
even O
griefers B
themselves O
may O
not O
recognize O
what O
they O
have O
done O
as O
griefing B
[ O
23 O
] O
. O

This O
inherent O
vagueness O
makes O
it O
hard B
to O
understand O
grief O
behavior O
. O

A O
few O
studies O
have O
characterized O
grief B
playing I
. O

Foo O
and O
Koivisto O
divide O
grief O
behavior O
into O
four O
categories O
: O
harassment B
, O
power B
imposition O
, O
scamming O
, O
and O
greed B
play B
[ O
10 O
] O
. O

They O
focus B
on O
the O
intention O
of O
behavior O
and O
distinguish O
griefing O
from O
greed B
playing O
because O
motivation O
behind O
greed B
playing O
is O
usually O
for O
the O
win O
instead O
of O
disrupting O
other O
players B
. O

Barnett O
discovers O
that O
griefing O
is O
one O
of O
factors O
provoking O
anger O
in O
World O
of O
Warcraft O
by O
survey O
of O
33 O
participants O
[ O
1 O
] O
. O

Chen O
et O
al O
. O

correlate O
personality O
and O
grief B
playing I
. O

They O
reveal O
that O
players B
who O
enjoy O
the O
anonymous O
experience B
tend O
to O
like O
the O
motivation O
of O
grief B
playing I
, O
such O
as O
“ O
I O
like O
the O
experience B
of O
wanting O
to O
feel O
powerful B
while O
playing B
online B
games I
” O
[ O
6 O
] O
. O

3.2 O
Challenges O
in O
Crowdsourcing O
While O
crowdsourcing O
, O
coined O
by O
Jeff O
Howe O
[ O
17 O
] O
, O
has O
had O
huge O
impacts B
on O
a O
variety O
of O
areas B
, O
such O
as O
answering O
queries O
[ O
12 O
] O
, O
assessing O
visual O
design O
[ O
15 O
] O
, O
translating O
texts O
[ O
43 O
] O
, O
conducting O
user B
studies O
[ O
20 O
] O
, O
and O
collecting O
disaster O
responses O
[ O
14 O
] O
, O
inherent O
noise O
in O
outcomes O
by O
laborers O
has O
received O
much O
attention O
[ O
18 O
, O
30 O
] O
. O

This O
issue O
is O
not O
limited O
to O
amateur O
labor O
groups B
. O

Voorhees O
demonstrate O
that O
even O
experts O
show O
low O
levels B
of O
agreement O
on O
subjective O
problems O
, O
such O
as O
judging O
the O
relevance O
of O
documents O
to O
a O
given O
topic O
[ O
40 O
] O
. O

Majority O
votes O
or O
overlapping O
labels O
are O
popular O
methods O
to O
improve O
the O
overall O
quality O
of O
error O
- O
prone O
labelers O
. O

Nowak O
and O
Rüger O
demonstrate O
that O
majority O
voting O
on O
crowdsourcing O
is O
able O
to O
filter O
noisy O
judgments B
and O
to O
improve O
the O
quality O
of O
experts O
’ O
annotations O
[ O
28 O
] O
. O

Sheng O
et O
al O
. O

investigate O
how O
overlapping O
labels O
for O
low O
agreement O
tasks O
improve O
quality O
, O
especially O
when O
labelers O
are O
imperfect O
[ O
33 O
] O
. O

They O
find O
that O
a O
selective O
repeated O
- O
labeling O
strategy B
can O
improve O
data O
quality O
substantially O
. O

Some O
studies O
benefit O
from O
maximizing O
the O
utility B
of O
more O
expert O
workers O
. O

Snow O
et O
al O
. O

propose O
giving O
different O
weights O
to O
each O
labeler O
based B
on O
how O
their O
accuracies O
improve O
the O
label O
quality O
[ O
35 O
] O
. O

Crowdsourcing O
has O
been O
widely O
used O
for O
producing O
ground O
- O
truth O
labels O
for O
data O
- O
driven O
machine O
learning O
, O
such O
as O
labeling O
sentiment O
in O
social O
media O
[ O
5 O
] O
. O

To O
obtain O
more O
accurate O
classifiers O
from O
crowdsourced O
labels O
with O
noise O
, O
Brew O
et O
al O
. O

report B
that O
a O
classifier O
with O
training O
sets O
only O
from O
labelers O
showing O
high O
agreement O
achieves O
better O
accuracy O
than O
with O
less O
agreement O
[ O
5 O
] O
. O

Also O
, O
Kumar O
and O
Lease O
show O
that O
incorporating O
the O
accuracies O
of O
each O
labeler O
has O
a O
large O
impact B
on O
the O
overall O
accuracy O
of O
a O
supervised O
learner O
[ O
21 O
] O
. O

Tang O
and O
Lease O
propose O
semi O
- O
supervised O
learning O
with O
many O
unlabeled O
tasks O
but O
a O
small O
set O
of O
expert O
- O
labeled O
tasks O
[ O
37 O
] O
. O

Our O
approach O
, O
which O
regards O
supervised O
learning O
and O
crowdsourcing O
as O
complementary O
mechanisms B
to O
balance O
loads O
, O
is O
inspired O
by O
two O
recent O
systems O
: O
CrowdFlow O
and O
CrowdSynth O
. O

Quinn O
et O
al O
. O

develop O
CrowdFlow O
[ O
32 O
] O
, O
an O
integrated O
framework O
of O
human O
efforts O
and O
machine O
learning O
classifiers O
, O
and O
discuss O
the O
efficiency O
of O
the O
crowd O
- O
assisted B
machine O
learning O
[ O
31 O
] O
. O

As O
a O
sample O
application O
, O
they O
first O
translate O
texts O
via O
Google O
Translate O
, O
and O
then O
let O
experts O
identify O
problematic O
parts O
and O
offer O
alternatives O
. O

They O
do O
not O
automatically O
identify O
difficult O
parts O
of O
texts O
but O
require O
man- O
ual O
labor O
. O

Kamar O
et O
al O
. O

present O
a O
prototype O
of O
CrowdSynth O
[ O
19 O
] O
to O
guide O
workers O
with O
predicting O
the O
labels O
in O
a O
well O
- O
known O
citizen O
science O
application O
called B
Galaxy O
Zoo O
. O

Their O
models O
are O
derived O
from O
relatively O
rich O
data O
, O
such O
as O
detailed O
records O
for O
labels O
, O
workers O
, O
and O
combinations O
of O
them O
. O

4 O
. O

DATA O
COLLECTION O
We O
developed O
distributed O
crawlers O
to O
collect O
over O
1.46 O
million O
cases O
in O
the O
Tribunal O
, O
made O
up O
of O
10 O
million O
user B
reports I
. O

We O
carefully O
designed O
our O
crawlers O
not O
to O
degrade O
the O
performance O
of O
Riot O
’s O
web O
servers B
. O

In O
addition O
to O
the O
parsing O
time B
of O
the O
crawled O
contents O
, O
we O
explicitly O
let O
our O
crawlers O
be O
idle O
for O
seconds O
before O
the O
next O
request O
. O

Riot O
divides O
the O
world B
into O
several O
regions O
, O
each O
served O
by O
dedicated O
servers B
due O
to O
quality O
of O
service O
concerns O
. O

We O
collected O
data O
from O
North O
America O
( O
NA O
) O
, O
Western O
Europe O
( O
EUW O
) O
, O
and O
South O
Korea O
( O
KR O
) O
servers B
by O
considering O
the O
representativeness O
of O
cultural O
uniqueness O
and O
the O
familiarity O
of O
authors O
. O

We O
reasonably O
assume O
most O
of O
the O
players B
connect O
to O
the O
servers B
in O
the O
region O
closest B
to O
them O
for O
the O
best O
user B
experience B
. O

As O
a O
result O
, O
in O
April O
2013 O
, O
we O
crawled O
all O
available O
cases O
from O
the O
Tribunal O
of O
the O
each O
of O
the O
three O
regions O
. O

We O
summarize O
our O
data O
collection O
in O
Table O
1 O
. O

We O
reiterate O
that O
a O
case O
includes O
multiple O
matches B
, O
which O
in O
turn B
contain O
one O
or O
more O
reports B
. O

EUW O
NA O
KR O
Total O
Cases O
649,419 O
590,311 O
220,614 O
1,460,344 O
Matches O
2,841,906 O
2,107,522 O
1,066,618 O
6,016,046 O
Reports O
5,559,968 O
3,441,557 O
1,898,433 O
10,899,958 O
Table O
1 O
: O
Data O
collection O
summary O
5 O
. O

RESEARCH O
QUESTIONS O
With O
our O
large O
- O
scale O
labeled O
data O
via O
crowdsourcing O
, O
we O
raise O
two O
research O
questions O
. O

RQ1 O
: O
Can O
we O
predict O
the O
crowdsourced O
decisions O
? O
Since O
the O
perception O
of O
toxic O
behavior O
is O
subjective O
and O
different O
across O
individuals O
, O
the O
Tribunal O
deals O
with O
toxic O
behavior O
by O
a O
majority O
rule B
based B
on O
crowdsourcing O
. O

It O
has O
worked O
quite O
well O
, O
but O
requires O
a O
long O
time B
to O
obtain O
enough O
votes O
. O

Our O
research O
question O
is O
to O
validate O
whether O
a O
certain O
part O
of O
the O
Tribunal O
can O
be O
assisted B
by O
machine O
learning O
. O

A O
few O
considerations O
are O
carefully O
addressed O
here O
. O

Defining O
machine O
learning O
tasks O
We O
can O
define O
various O
machine O
learning O
tasks O
on O
crowdsourced O
decisions O
in O
the O
Tribunal O
. O

Classifying O
6 O
different O
combinations O
of O
decision O
and O
level B
of O
agreements O
, O
dividing O
cases O
into O
punished O
or O
pardoned O
, O
extracting O
high O
agreement O
cases O
, O
and O
recognizing O
less O
agreement O
cases O
are O
all O
possible O
tasks O
that O
machine O
learning O
could O
help O
. O

It O
is O
not O
only O
about O
the O
accuracy O
of O
the O
classifier O
but O
also O
about O
the O
application O
. O

Refining O
training O
set O
A O
Tribunal O
decision O
is O
either O
punish O
or O
pardon O
with O
a O
level B
of O
agreement O
: O
majority O
, O
strong O
majority O
, O
and O
overwhelming O
majority O
. O

Previous O
literature O
demonstrate O
that O
crowdsourced O
responses O
with O
high O
agreement O
are O
better O
for O
training O
a O
classifier O
than O
less O
agreement O
[ O
5 O
] O
. O

Unfortunately O
, O
it O
is O
unknown O
how O
Riot O
divides O
these O
levels B
of O
agreement O
. O

We O
thus O
create O
different O
training O
sets O
and O
compare O
the O
accuracy O
of O
trained B
classifiers O
. O

Using O
non O
- O
linguistic O
features O
only O
LoL O
has O
a O
global O
userbase O
in O
a O
few O
tens O
of O
countries O
across O
the O
world B
. O

This O
implies O
that O
chat B
logs O
in O
the O
Tribunal O
are O
not O
always O
written O
in O
English O
. O

For O
example O
, O
most O
messages B
in O
the O
Korean O
Tribunal O
are O
written O
in O
Korean O
. O

Various O
languages O
can O
potentially O
limit O
the O
portability O
of O
the O
classifier O
if O
it O
largely O
depends O
on O
the O
textual O
information O
left O
in O
chats B
. O

In O
other O
words O
, O
more O
non O
- O
linguistic O
features O
increase B
the O
generality O
of O
the O
classifier O
and O
bring O
higher O
practical O
impacts B
. O

Detecting O
sentiments O
in O
chats B
This O
is O
the O
inverse O
of O
the O
above O
, O
maximizing O
the O
benefit O
of O
linguistic O
features O
. O

Many O
methods O
have O
been O
developed O
for O
detecting O
sentiments O
conveyed O
in O
texts O
. O

Our O
intuition O
is O
that O
negative O
sentiments O
might O
be O
captured B
from O
toxic O
behavior O
or O
other O
players B
’ O
reaction O
when O
toxic O
playing O
occurs O
. O

RQ2 O
: O
What O
do O
the O
important O
features O
imply O
? O
The O
next O
research O
goal B
is O
understanding O
decision B
- I
making I
in O
the O
Tribunal O
from O
the O
important O
features O
observed O
through O
supervised O
learning O
. O

Which O
features O
are O
important O
for O
predicting O
crowdsourced O
decisions O
? O
What O
do O
they O
mean O
? O
Answering O
these O
questions O
leads B
to O
several O
interesting O
challenges O
. O

First O
of O
all O
, O
features O
we O
find O
could O
be O
a O
huge O
advance O
in O
online O
violence O
research O
. O

Toxic O
behavior O
has O
been O
typically O
considered O
hard B
to O
define O
. O

If O
we O
obtain O
a O
good O
quality O
supervised O
- O
learned O
classifier O
, O
it O
indicates O
the O
important O
building O
blocks O
in O
defining O
and O
understanding O
toxic O
behavior O
. O

Then O
, O
we O
draw O
a O
subsequent O
question O
. O

Can O
we O
apply O
our O
classifier O
or O
the O
important O
features O
to O
other O
games B
, O
or O
Internet O
communities B
? O
If O
important O
features O
in O
our O
classifier O
are O
LoL B
- O
independent O
, O
the O
answer O
to O
the O
question O
will O
be O
in O
the O
affirmative O
. O

6 O
. O

FEATURES O
The O
Tribunal O
can O
be O
seen O
as O
a O
2-stage O
crowdsourced O
solution O
. O

Stage B
1 O
is O
the O
per O
- O
match B
reporting O
done O
by O
players B
that O
actually O
experienced B
the O
alleged O
toxic O
behavior O
. O

Stage B
2 O
is O
the O
Tribunal O
caseby O
- O
case O
judgments B
. O

Toxicity B
is O
not O
having O
a O
bad B
game I
( O
possibly O
perceived O
as O
feeding O
or O
assisting B
the O
enemy B
) O
or O
having O
a O
bad O
day O
and O
lashing O
out O
at O
a O
teammate B
( O
harassment B
) O
. O

According O
to O
Riot O
, O
a O
certain O
threshold O
of O
reports B
from O
stage B
1 O
must O
be O
met O
before O
moving B
on O
to O
stage B
2 O
, O
which O
reveals O
repeated O
toxic O
behavior O
. O

The O
reason O
that O
stage B
2 O
is O
necessary O
has O
to O
do O
with O
the O
vagueness O
of O
toxic O
behavior O
. O

Consider O
a O
player B
who O
is O
not O
very O
skilled B
. O

This O
player B
might O
exhibit O
tendencies O
that O
could O
be O
interpreted O
as O
toxic O
, O
for O
example O
intentionally O
feeding O
. O

This O
is O
compounded O
by O
attribution O
theory O
where O
a O
negative O
outcome O
( O
losing O
a O
match B
) O
triggers B
a O
search O
for O
an O
external O
cause O
. O

I.e. O
, O
when O
a O
team B
loses O
, O
as O
is O
the O
the O
case O
in O
the O
majority O
of O
reported B
matches B
[ O
22 O
] O
, O
reporters O
might O
attribute O
the O
loss O
to O
a O
poorly O
performing O
player B
. O

Although O
there O
is O
an O
unskilled B

 O
player I
report B
type O
, O
players B
are O
aware O
that O
no O
punishment O
is O
handed O
out O
for O
this O
category O
and O
thus O
might O
choose O
one O
of O
the O
punishable O
offenses O
instead O
. O

Thus O
, O
the O
second O
stage B
removes O
the O
subjectivity O
associated O
with O
direct O
interaction O
with O
the O
possibly O
toxic B
player I
, O
as O
well O
as O
providing O
a O
more O
complete O
view O
of O
the O
accused O
’s O
behavior O
over O
multiple O
matches B
. O

Players B
that O
have O
invested O
significant O
time B
, O
and O
are O
thus O
familiar O
with O
LoL O
, O
are O
able O
to O
pick B
out O
patterns O
of O
toxic O
behavior O
when O
reported B
matches B
are O
combined O
into O
a O
case O
. O

The O
challenge O
lies O
in O
representing O
the O
parsimonious O
data O
presented O
to O
Tribunal O
reviewers O
in O
a O
form O
digestible O
by O
machine O
learning O
algorithms O
. O

We O
thus O
extract O
summarized O
statistics O
from O
each O
Tribunal O
case O
. O

We O
make O
use O
of O
two O
primary O
sources O
of O
information O
: O
1 O
) O
domain O
specific O
values O
extracted O
from O
the O
results O
of O
reported B
matches B
, O
and O
2 O
) O
the O
information O
provided O
by O
the O
stage B
1 O
Tribunal O
participants O
. O

There O
are O
, O
unfortunately O
, O
several O
points B
of O
variation O
when O
it O
comes O
to O
extracting O
the O
in B
- I
game I
values O
. O

First O
, O
each O
case O
has O
a O
varying O
amount O
of O
matches B
with O
no O
guarantee O
on O
the O
sequence O
in O
which O
the O
matches B
took O
place O
. O

Second O
, O
because O
of O
the O
variety O
of O
game B
play B
in O
LoL O
, O
there O
is O
no O
guarantee O
that O
matches B
are O
directly O
compara- O
ble O
, O
especially O
across O
different O
players B
. O

For O
example O
, O
a O
player B
with O
below O
average B
skill I
is O
likely O
to O
have O
a O
lower O
KDA O
( O
an O
in B
- I
game I
performance O
metric O
explained O
in O
the O
next O
section O
) O
than O
a O
player B
with O
higher O
skill B
. O

Now O
assume O
that O
the O
low O
skill B
player B
is O
not O
toxic O
, O
while O
the O
high B
skill I
player I
is O
toxic O
, O
yet O
both O
are O
reported B
for O
intentional O
feeding O
. O

There O
might O
not O
be O
a O
way O
of O
discriminating O
between O
the O
two O
using O
just O
KDA O
. O

Although O
we O
average O
the O
per O
- O
match B
statistics O
across O
all O
matches B
with O
a O
given O
report B
type O
for O
each O
case O
, O
we O
could O
also O
use O
the O
worst O
/ O
best O
matches B
. O

This O
is O
somewhat O
problematic O
as O
it O
requires O
a O
definition O
of O
worst O
and O
best O
. O

We O
include O
the O
standard O
deviation O
of O
each O
averaged O
statistic O
as O
a O
feature O
, O
which O
provides O
a O
sense O
of O
inter O
- O
match B
performance O
differences O
. O

We O
then O
augment O
each O
instance O
with O
information O
provided O
by O
the O
Stage O
1 O
Tribunal O
participants O
. O

Namely O
, O
we O
make O
use O
of O
the O
number O
of O
allied O
and O
enemy B
reports B
in O
a O
given O
match B
, O
the O
number O
of O
reports B
where O
the O
reporter O
included O
optional O
human O
readable O
text O
about O
the O
offense O
, O
and O
the O
most O
common O
type O
of O
behavior O
reported B
for O
a O
given O
match B
. O

For O
each O
possible O
report B
type O
, O
we O
compute O
the O
relevant O
statistics O
across O
all O
matches B
in O
the O
case O
with O
said O
most O
common O
report B
type O
. O

6.1 O
In B
- I
game I
Performance O
In B
- I
game I
performance O
is O
the O
category O
of O
features O
that O
most O
requires O
the O
input B
of O
experts O
. O

LoL O
is O
a O
complicated O
game B
and O
the O
meaning O
of O
the O
various O
match B
- O
related O
statistics O
is O
unlikely O
to O
be O
divined O
by O
a O
reviewer O
, O
especially O
with O
respect O
to O
toxic O
behavior O
, O
without O
having O
investing O
a O
significant O
number O
of O
hours O
in O
gameplay B
themselves O
. O

Nevertheless O
, O
they O
are O
the O
richest O
and O
easiest B
features O
to O
represent O
to O
a O
computer O
and O
so O
we O
extract O
a O
set O
of O
relevant O
statistics O
from O
the O
matches B
in O
each O
Tribunal O
case O
. O

We O
first O
begin O
with O
the O
most O
basic O
statistic O
, O
one O
that O
is O
common O
to O
nearly O
all O
competitive B
games I
: O
kills B
, O
deaths B
, O
and O
assists B
. O

Kills O
and O
deaths B
are O
relatively O
self O
- O
explanatory O
: O
simple O
counts O
of O
the O
number O
of O
enemies B
a O
player B
killed B
and O
the O
number O
of O
times B
said O
player B
died O
. O

Likewise O
, O
an O
assist B
is O
awarded O
to O
a O
player B
that O
participated O
in O
eliminating O
an O
enemy B
, O
but O
did O
not O
land O
the O
killing B
blow O
. O

The O
details O
of O
what O
qualifies O
an O
assist B
varies O
per O
game B
, O
but O
LoL O
awards O
an O
assist B
to O
any O
player B
that O
did O
damage B
or O
contributed O
passively O
( O
e.g. O
, O
healed O
a O
teammate B
that O
landed O
the O
killing O
blow O
) O
within O
10 O
seconds O
prior O
to O
the O
death B
of O
an O
enemy B
. O

Kills B
, O
deaths B
, O
and O
assist B
are O
raw O
scores O
, O
but O
are O
often O
normalized O
to O
a O
KDA O
metric O
, O
defined O
as O
: O
KDA O
= O
kills B
+ O
assists B
deaths B
+ O
1 O
. O

Unfortunately O
, O
due O
to O
the O
reliance O
on O
teamwork B
in O
games B
like O
LoL O
, O
a O
single O
toxic B
player I
can O
severely O
impact B
his O
teammates B
abilities B
to O
perform O
at O
a O
high B
level I
. O

For O
example O
, O
an O
intentional O
feeder B
is O
supplying O
the O
enemy B
team I
with O
gold B
and O
experience B
points I
, O
allowing O
them O
to O
acquire O
powerful B
items I
and O
abilities B
much O
faster O
than O
the O
feeder B
’s O
allies B
. O

In O
turn B
, O
this O
can O
result O
in O
a O
low O
KDA O
not O
only O
for O
the O
toxic B
player I
, O
but O
makes O
it O
difficult O
for O
his O
allies B
to O
maintain O
a O
good O
KDA O
as O
well O
. O

For O
this O
reason O
, O
it O
might O
be O
difficult O
to O
distinguish O
toxic B
players I
based B
solely O
on O
KDA O
and O
our O
initial O
analysis O
[ O
22 O
] O
indicated O
reviewers O
were O
not O
basing B
decisions O
only O
on O
KDA O
. O

However O
, O
two O
other O
statistics O
, O
damage B
dealt O
and O
received O
, O
might O
shed O
additional O
light O
on O
toxic B
players I
. O

In O
LoL B
, O
attacks B
do O
a O
certain O
amount B
of I
base O
damage I
to O
other O
players B
, O
removing O
a O
portion O
of O
their O
hit B
points I
( O
“ O
life O
” O
) O
. O

When O
a O
player B
’s O
hit B
points I
reach O
0 O
, O
he O
dies O
, O
a O
kill B
( O
with O
associated O
gold B
and O
experience B
) O
is O
awarded O
to O
his O
killers B
, O
and O
he O
must O
wait O
a O
certain O
period O
of O
time B
to O
“ O
respawn B
” O
and O
return O
to O
the O
fight B
. O

The O
base B
damage I
is O
modified O
depending O
on O
various O
items B
that O
players B
purchase O
during O
the O
match B
. O

Anecdotally O
, O
toxic B
players I
in O
the O
feeding O
and O
assisting B
enemy B
categories O
will O
not O
buy O
items B
that O
aid O
in O
offense O
or O
defense B
. O

Thus O
, O
we O
might O
expect O
a O
feeder B
to O
have O
very O
low B
damage I
dealt O
and O
very O
high B
damage I
received O
relative O
to O
his O
teammates B
who O
have O
made O
purchases O
of O
useful O
items B
; O
even O
though O
they O
might O
not O
have O
the O
power B
to O
actually O
kill B
enemies B
( O
due O
to O
a O
gold B
and O
experience B
advantage B
given O
to O
the O
other O
team B
by O
the O
feeder B
) O
, O
fair O
players B
’ O
efforts O
are O
likely O
to O
show O
in O
terms O
of O
damage B
. O

Seeing O
which O
items B
a O
player B
bought O
could O
give O
more O
details O
, O
but O
it O
is O
overly O
specific O
and O
loses O
generality O
. O

Next O
, O
we O
include O
the O
total O
gold B
and O
gold B
per O
minute O
earned O
by O
the O
offender O
. O

In O
LoL B
players I
earn O
gold B
in O
several O
ways O
: O
1 O
) O
passively O
at O
a O
pre O
- O
determined O
rate O
, O
2 O
) O
destroying O
towers B
, O
3 O
) O
kills B
or O
assists B
, O
and O
4 O
) O
killing O
creeps B
, O
which O
are O
computer O
controlled B
monsters O
. O

Gold B
is O
known O
to O
be O
a O
primary O
determinant O
of O
a O
match B
’s O
outcome O
. O

The O
final O
performance O
related O
feature O
is O
time B
played B
, O
which O
is O
useful O
for O
detecting O
leavers B
or O
players B
going O
AFK O
. O

In O
total O
, O
there O
are O
364 O
features O
generated O
from O
in B
- I
game I
performance O
values O
only O
. O

This O
number O
is O
large O
because O
we O
group B
per O
- O
match B
values O
based B
on O
the O
most O
common O
report B
type O
. O

6.2 O
User O
reports B
The O
first O
stage B
of O
the O
Tribunal O
, O
reports B
submitted O
players B
who O
directly O
observed O
the O
behavior O
, O
provides O
us O
with O
several O
pieces B
of O
information O
. O

First O
, O
we O
know O
the O
most O
common O
category O
of O
behavior O
reported B
per O
match B
. O

Next O
, O
we O
know O
how O
many O
allies B
and O
enemies B
made O
a O
report B
. O

Finally O
, O
reports B
can O
include O
a O
short O
( O
500 O
character B
limit O
) O
comment O
from O
the O
reporter O
. O

Intuitively O
, O
the O
extra O
effort O
required O
to O
add O
a O
comment O
to O
a O
report B
might O
indicate O
the O
intensity O
of O
the O
toxic O
behavior O
. O

Again O
, O
we O
group B
the O
per O
match B
values O
in O
the O
case O
based B
on O
the O
common O
report B
type O
for O
that O
match B
, O
resulting O
in O
a O
total O
of O
28 O
features O
. O

6.3 O
Chats B
As O
seen O
in O
Figure O
1 O
, O
around O
60 O
% O
of O
cases O
have O
more O
than O
about O
25 O
% O
of O
the O
matches B
in O
them O
reported B
for O
offensive O
language O
or O
verbal O
abuse O
. O

This O
indicates O
that O
the O
observed O
toxic O
behavior O
was O
expressed O
( O
at O
least O
partially O
) O
via O
the O
chat B
system O
. O

For O
this O
reason O
, O
we O
intuit O
that O
the O
chat B
logs O
have O
predictive O
power B
. O

Linguistic O
analysis O
is O
an O
area B
of O
intense O
research O
[ O
11 O
, O
13 O
, O
38 O
] O
, O
and O
the O
corpus O
of O
chat B
logs O
in O
our O
data O
set O
would O
provide O
an O
interesting O
case O
for O
cutting O
edge O
techniques B
. O

We O
use O
an O
intuitive O
and O
straightforward O
method O
, O
“ O
happiness O
” O
index O
. O

Happiness O
, O
and O
its O
relation O
to O
language O
, O
is O
a O
widely O
studied O
area B
of O
psychology O
[ O
2 O
] O
. O

For O
example O
, O
in O
the O
Affective O
Norms O
for O
English O
Words O
( O
ANEW O
) O
study O
[ O
4 O
] O
, O
participants O
graded O
a O
set O
of O
1,034 O
words O
on O
a O
valence O
scale O
of O
1 O
to O
9 O
( O
in O
0.5 O
increments O
) O
. O

Valence O
is O
the O
psychological O
term O
for O
the O
attractiveness O
( O
positive O
) O
or O
aversion O
( O
negative O
) O
to O
something O
; O
in O
this O
case O
a O
word O
. O

In O
other O
words O
, O
valence O
quantifies O
the O
“ O
goodness O
” O
or O
“ O
badness O
” O
of O
a O
word O
. O

Valence O
scores O
in O
the O
ANEW O
dataset O
are O
well O
distributed O
, O
as O
can O
be O
seen O
in O
Figure O
2 O
( O
a O
) O
, O
which O
plots O
the O
distribution O
of O
scores O
for O
all O
words O
in O
the O
ANEW O
dataset O
. O

The O
ANEW O
study O
polled O
both O
female O
and O
male O
respondents O
. O

Riot O
reports B
that O
90 O
% O
of O
LoL B
players I
are O
male7 O
, O
and O
we O
thus O
use O
male O
respondent O
scores O
only O
. O

Although O
gender O
swapping O
often O
occurs O
in O
social O
games B
[ O
24 O
] O
, O
according O
to O
Flurry O
’s O
report8 O
, O
action O
and O
strategy B
are O
the O
top O
two O
genres O
that O
females O
usually O
avoid O
. O

Also O
, O
positive O
effects B
of O
gender O
swapping O
, O
enjoying O
a O
kind O
of O
“ O
second O
” O
life O
, O
do O
not O
apply O
to O
LoL. O
7 O
http://tinyurl.com/stfunub6 O
8 O
http://tinyurl.com/stfunub7 O
Figure O
2 O
: O
CDF O
of O
valence O
scores O
. O

0 O
50000 O
100000 O
150000 O
200000 O
1 O
2 O
3 O
4 O
5 O
Number O
of O
matches B
reported B
per O
category O
Number O
of O
cases O
category O
Assisting O
Enemy O
Team O
Inappropriate O
Name O
Intentionally O
Feeding O
Negative O
Attitude O
Offensive O
Language O
Spamming B
Verbal O
Abuse O
Figure O
1 O
: O
The O
number O
of O
matches B
reported B
in O
a O
case O
for O
each O
category O
of O
toxic O
behavior O
per O
Tribunal O
case O
. O

As O
demonstrated O
by O
Dodds O
and O
Danforth O
[ O
9 O
] O
, O
valence O
scores O
for O
individual O
words O
can O
be O
used O
to O
estimate O
the O
valence O
for O
a O
larger O
corpus O
of O
text O
. O

The O
valence O
of O
a O
piece B
of O
text O
is O
defined O
as O
: O
vtext O
= O
Pn O
i=1 O
P O
vifi O
n O
i=1 O
fi O
, O
where O
vi O
is O
the O
valence O
score O
of O
the O
ith O
word O
from O
the O
ANEW O
study O
, O
and O
fi O
is O
the O
number O
of O
times B
said O
word O
appears O
in O
a O
given O
piece B
of O
text O
. O

While O
we O
acknowledge O
that O
chat B
logs O
are O
likely O
to O
contain O
typos O
and O
abbreviations O
, O
vtext O
has O
been O
shown O
to O
be O
robust O
across O
genres O
of O
text O
, O
including O
tweets O
[ O
9 O
, O
26 O
] O
, O
another O
medium O
where O
we O
might O
expect O
“ O
Internet O
- O
style O
” O
speech O
patterns O
. O

For O
cases O
where O
no O
ANEW O
words O
were O
present O
, O
we O
define O
vtext O
= O
0 O
. O

Figure O
2 O
( O
b O
) O
plots O
the O
distribution O
of O
valence O
scores O
of O
all O
messages B
sent O
in O
a O
case O
for O
both O
pardoned O
and O
punished O
cases O
where O
vtext O
≥ O
1 O
. O

A O
two O
- O
sample O
Kolmogorov O
- O
Smirnov O
test O
confirms O
the O
distributions O
are O
different O
. O

When O
compared O
to O
Figure O
2 O
( O
a O
) O
, O
we O
see O
that O
“ O
verbal O
” O
communication B
in O
LoL O
is O
generally O
neutral O
: O
most O
valence O
scores O
fall B
between O
5 O
and O
6 O
. O

Further O
, O
cases O
that O
resulted O
in O
a O
punishment O
tend O
to O
have O
a O
lower O
valence O
score O
when O
compared O
to O
pardoned O
cases O
. O

This O
indicates O
that O
the O
chat B
logs O
are O
likely O
to O
be O
valuable O
in O
detecting O
toxic O
behavior O
. O

When O
looking O
at O
the O
scores O
of O
messages B
sent O
only O
by O
potential O
toxic B
players I
, O
offenders O
, O
in O
Figure O
2 O
( O
c O
) O
, O
it O
becomes O
clear B
that O
toxicity B
is O
present O
in O
a O
quantifiable O
way O
within O
the O
chat B
logs O
. O

The O
distributions O
for O
both O
punished O
and O
pardoned O
offenders O
is O
lower O
than O
the O
valence O
of O
the O
entire O
chat B
logs O
. O

The O
mean O
for O
punished O
and O
pardoned O
users B
are O
5.725 O
and O
5.779 O
, O
respectively O
. O

Pardoned O
users B
are O
indeed O
more O
likely O
to O
have O
higher O
valence O
scores O
. O

Interestingly O
, O
the O
difference O
is O
mainly O
present O
in O
terms O
of O
“ O
above O
average O
” O
( O
≥ O
5 O
) O
valence O
scores O
for O
pardoned O
users B
as O
opposed O
to O
a O
tendency O
towards O
below O
average O
scores O
for O
punished O
players B
. O
We I
also I
discover O
that O
the O
difference O
between O
punished O
and O
pardoned O
offender O
becomes O
bigger O
if O
more O
reviewers O
are O
agreed O
. O

Figure O
2 O
( O
d O
) O
shows O
the O
valence O
score O
of O
toxic B
players I
when O
overwhelming O
majority O
is O
agreed O
. O

The O
mean O
for O
only O
those O
who O
are O
punished O
or O
pardoned O
in O
this O
case O
are O
5.699 O
and O
5.751 O
, O
respectively O
. O

In O
addition O
to O
the O
scores O
described O
above O
, O
we O
also O
include O
the O
valence O
scores O
for O
bystanders O
and O
victims B
for O
each O
report B
category O
. O

Here O
, O
we O
treat O
verbal O
abuse O
, O
offensive O
language O
, O
and O
negative O
attitude O
differently O
from O
the O
other O
categories O
. O

For O
these O
cases O
we O
have O
previously O
observed O
that O
reports B
by O
enemies B
are O
much O
more O
common O
. O

This O
can O
be O
attributed O
to O
bystander O
theory O
, O
which O
says O
that O
bystanders O
, O
i.e. O
, O
those O
not O
directly O
harmed O
by O
bad O
behavior O
, O
are O
much O
less O
likely O
to O
take O
action O
against O
it O
. O

In O
the O
case O
of O
, O
e.g. O
, O
intentional O
feeding O
, O
not O
only O
are O
enemy B
teams I
not O
directly O
harmed O
by O
the O
behavior O
, O
they O
actually O
receive O
a O
benefit O
. O

While O
the O
quality O
of O
the O
competition B
may O
decrease O
, O
the O
odds O
of O
a O
win O
are O
much O
more O
in O
their O
favor O
. O

When O
it O
comes O
to O
chat B
based B
offenses O
, O
however O
, O
a O
toxic B
player I
can O
lash O
out O
at O
everyone O
in O
the O
match B
. O

He O
can O
insult O
the O
enemy B

 O
team I
when O
they O
are O
performing O
well O
, O
and O
trash B
talk I
when O
they O
are O
performing O
poorly O
. O

For O
example O
, O
a O
common O
insult O
in O
LoL O
is O
to O
call B
someone O
a O
“ O
noob B
, O
” O
slang O
for O
“ O
newbie B
, O
” O
implying O
a O
lack O
of O
ability B
. O

It O
is O
a O
catch O
- O
all O
negative O
term O
used O
as O
a O
response O
to O
criticism O
, O
to O
call B
out O
poor B
play B
, O
as O
a O
form O
of O
trash B
talk I
, O
and O
just O
plain O
meanness O
. O

The O
word O
“ O
noob B
” O
appears O
3,551,328 O
times B
in O
the O
chat B
logs O
for O
the O
NA O
region O
; O
an O
average O
of O
6 O
times B
per O
case O
and O
1.7 O
times B
per O
match B
, O
with O
players B
under O
review O
saying O
“ O
noob B
” O
at O
more O
than O
double O
the O
rate O
per O
message B
sent O
as O
non O
- O
offenders O
. O

Because O
these O
communication B
based B
categories O
can O
affect O
both O
enemies B
of O
the O
offender O
, O
allies B
, O
or O
neither O
, O
we O
consider O
their O
victims B
to O
be O
the O
offender O
’s O
allies B
if O
only O
allies B
reported B
him O
, O
enemies B
if O
only O
enemies B
reported B
, O
and O
all O
players B
if O
both O
enemies B
and O
allies B
reported B
him O
. O

With O
the O
above O
in O
mind O
, O
we O
extract O
60 O
features O
per O
case O
from O
the O
chat B
logs O
. O

7 O
. O

MODELS O
As O
we O
introduced O
above O
, O
we O
extract O
features O
from O
different O
categories O
. O

We O
then O
build O
separate O
models O
for O
each O
category O
, O
and O
a O
full B
model O
to O
contain O
all O
the O
features O
: O
an O
in B
- I
game I
performance O
model O
, O
a O
user B
report I
model O
, O
a O
chat B
model O
, O
and O
a O
full B
model O
. O

The O
intuition O
behind O
these O
basic O
models O
is O
comparing O
different O
sources O
. O

First O
, O
in O
the O
in B
- I
game I
performance O
model O
, O
we O
can O
divide O
features O
by O
offender O
performance O
and O
other O
players B
’ O
performance O
. O

Offender O
performance O
can O
reflect O
intentional O
feeding O
. O

Players B
with O
noticeably O
bad O
performance O
might O
die O
over O
and O
over O
intentionally O
. O

We O
note O
that O
non O
- O
toxic O
unintentional O
feeding O
does O
occur O
, O
but O
only O
intentional O
feeding O
is O
toxic O
behavior O
. O

Since O
there O
is O
an O
unskilled B

 O
player I
Stage O
1 O
report B
category O
that O
is O
unused O
in O
the O
Tribunal O
, O
we O
ensure O
that O
our O
classifier O
is O
trained B
only O
on O
reviewers O
’ O
inference O
of O
intention O
, O
and O
not O
a O
judgment B
of O
player B
skill I
. O

Other O
players B
’ O
performance O
relates O
to O
the O
team B
competition B
aspect O
of O
LoL. O
Attribution O
theory O
says O
that O
individuals O
will O
look O
for O
external O
causes O
of O
failure O
[ O
42 O
] O
. O

The O
most O
obvious B
cause O
would O
be O
poor B
performance O
by O
an O
ally B
, O
and O
is O
likely O
to O
manifest O
as O
verbal O
abuse O
( O
harassment B
) O
. O

In O
other O
words O
, O
a O
toxic B
player I
might O
lash O
out O
at O
the O
worst O
performing O
ally B
due O
to O
the O
perception O
that O
a O
loss O
was O
the O
fault O
of O
said O
ally B
. O

We O
hypothesize O
that O
the O
intensity O
of O
the O
verbal O
abuse O
, O
and O
thus O
the O
likelihood O
of O
punishment O
in O
the O
Tribunal O
, O
increases B
as O
the O
offender O
’s O
performance O
diverges O
from O
the O
worst O
performing O
player B
on O
his O
team B
. O

A O
less O
intuitive O
reasoning O
in O
favor O
of O
this O
model O
is O
that O
a O
poor B
performance O
by O
a O
player B
does O
have O
an O
impact B
on O
the O
rest O
of O
his O
team B
. O

Our O
previous O
analysis O
indicates O
that O
KDA O
alone O
is O
insufficient O
in O
predicting O
the O
judgment B
of O
the O
tribunal O
: O
there O
was O
no O
correlation O
with O
KDA O
and O
corresponding O
Tribunal O
decision O
[ O
22 O
] O
. O

I.e. O
, O
players B

 O
with I
the I
best I
KDA O
were O
about O
as O
likely O
to O
be O
punished O
as O
those O
with O
the O
worst O
. O

The O
user B
report I
model O
depends O
on O
how O
players B
in O
a O
match B
perceive O
toxic O
playing O
. O

Although O
user B
perception O
on O
toxic O
behavior O
is O
different O
, O
more O
reports B
in O
a O
single O
match B
means O
more O
people O
recognize O
it O
, O
and O
implies O
more O
severe O
toxic O
behavior O
. O

In O
our O
initial O
analysis O
[ O
22 O
] O
, O
we O
find O
that O
the O
number O
of O
reports B
in O
a O
single O
match B
is O
highly O
correlated O
with O
the O
likelihood O
of O
being O
punished O
. O

The O
chat B
model O
deals O
with O
behavior O
that O
is O
expressed O
only O
via O
communication B
; O
verbal O
abuse O
and O
offensive O
language O
. O

Additionally O
, O
Tribunal O
reviewers O
can O
see O
the O
final O
in B
- I
game I
performance O
but O
not O
how O
the O
match B
played B
out O
over O
time B
. O

Chats B
are O
thus O
the O
only O
source O
to O
give O
reviewers O
context O
about O
what O
happened O
and O
what O
other O
players B
thought O
about O
it O
. O

8 O
. O

RESULTS O
8.1 O
Decision O
confidence O
and O
classifier O
training O
We O
first O
answer O
whether O
or O
not O
reviewer O
agreement O
is O
relevant O
when O
training B
a O
classifier O
. O

To O
do O
this O
, O
we O
grow O
forests B
from O
only O
cases O
of O
a O
given O
agreement O
. O

We O
then O
evaluate O
each O
classifier O
with O
a O
separate O
test O
set O
for O
each O
agreement O
. O

The O
intuition O
is O
that O
the O
cases O
with O
the O
highest B
level I
of O
agreement O
from O
the O
reviewers O
display O
the O
most O
egregious O
toxic O
behavior O
, O
providing O
a O
baseline O
for O
the O
remainder O
of O
cases O
. O

Figure O
3 O
plots O
the O
ROC O
curves O
for O
testing O
sets O
of O
each O
agreement O
type O
with O
classifiers O
trained B
from O
varying O
agreement O
cases O
. O

We O
observe O
that O
training B
the O
classifier O
with O
overwhelming O
majority O
decisions O
results O
in O
the O
highest O
AUC O
across O
all O
cases O
. O

Our O
ability B
to O
distinguish O
between O
guilty O
and O
not O
guilty O
increases B
with O
the O
level B
of O
agreement O
that O
we O
train B
the O
classifier O
with O
. O

This O
is O
consistent O
with O
previous O
research O
[ O
5 O
] O
. O

While O
the O
classifier O
trained B
with O
overwhelming O
majority O
is O
the O
most O
discriminating O
across O
the O
board O
, O
Figure O
4 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curve O
for O
predicting O
Tribunal O
decisions O
with O
models O
using O
in B
- I
game I
performance O
( O
P O
) O
, O
user B
report I
( O
R O
) O
, O
chats B
( O
C O
) O
, O
and O
all O
available O
features O
( O
F O
) O
. O

training O
with O
strong O
majority O
cases O
has O
similar O
performance O
, O
while O
performance O
drops B
off O
considerably O
when O
training B
with O
the O
majority O
decision O
cases O
. O

This O
experiment O
has O
several O
implications O
. O

First O
, O
it O
might O
be O
beneficial O
to O
look O
at O
the O
extremes O
of O
behavior O
, O
clear B
cut O
pardons O
and O
punishes O
, O
to O
better O
predict O
borderline O
cases O
. O

Second O
, O
it O
might O
be O
fruitful O
to O
predict O
decisions O
based B
on O
confidence O
. O

I.e. O
, O
finding O
the O
most O
obviously O
guilty O
or O
innocent O
individuals O
, O
leaving O
the O
borderline O
cases O
to O
human O
reviewers O
. O

Third O
, O
it O
reveals O
the O
difficulty O
in O
discriminating O
between O
all O
but O
the O
most O
egregious O
offenses O
. O

8.2 O
What O
are O
Tribunal O
decisions O
based B
on O
? O
We O
now O
address O
what O
information O
Tribunal O
reviewers O
might O
be O
basing B
their O
decision O
on O
. O

We O
present O
a O
few O
learning O
results O
to O
show O
the O
performance O
of O
our O
Random O
Forest O
classifier O
. O

We O
begin O
with O
the O
performance O
of O
predicting O
decisions O
, O
pardon O
or O
punish O
without O
considering O
the O
agreement O
level B
. O

Figure O
4 O
presents O
ROC O
curve O
for O
predicting O
decisions O
, O
punish O
or O
pardon O
, O
by O
different O
models O
. O

AUCs O
are O
0.7187 O
, O
0.7195 O
, O
0.7157 O
, O
and O
0.7991 O
for O
the O
performance O
, O
report B
, O
chat B
, O
and O
full B
models O
, O
respectively O
. O

We O
observe O
that O
each O
model O
shows O
comparable O
performance O
. O

Table O
2 O
shows O
the O
5 O
most O
important O
variables O
for O
predicting O
decisions O
. O

Due O
to O
lack O
of O
space O
, O
we O
omit O
the O
important O
variables O
for O
each O
category O
of O
toxic O
behavior O
, O
but O
it O
is O
similar O
across O
the O
categories O
. O

In O
the O
performance O
model O
, O
we O
find O
that O
enemy B
performance O
is O
a O
good O
predictor O
for O
decisions O
because O
offender O
or O
ally B
performance O
is O
relative O
in O
team B
competition I
games I
. O

Also O
, O
offender O
performance O
itself O
is O
important O
for O
decision O
making O
in O
the O
Tribunal O
. O

Interestingly O
, O
the O
number O
of O
deaths B
is O
more O
important O
than O
KDA O
. O

This O
might O
relate O
to O
partner O
blaming O
. O

A O
toxic B
player I
might O
blame O
teammates B
, O
e.g. O
, O
saying O
“ O
noob B
” O
, O
when O
he O
dies O
. O

The O
implication O
is O
the O
toxic B
player I
died O
because O
allies B
did O
not O
help O
him O
. O

In O
this O
situation O
, O
the O
number O
of O
deaths B
could O
be O
a O
good O
measure O
to O
reveal O
that O
the O
toxic B
player I
performed O
poorly O
rather O
than O
his O
allies B
not O
helping O
out O
. O

In O
the O
user B
report I
model O
, O
top O
variables O
are O
related O
to O
how O
many O
reports B
are O
submitted O
. O

The O
more O
reports B
, O
the O
more O
likely O
the O
Tribunal O
will O
decide O
to O
punish O
. O

This O
strongly O
agrees O
with O
our O
previous O
analysis O
[ O
22 O
] O
. O

Additionally O
, O
we O
find O
that O
reviewers O
care O
about O
short O
comments O
in O
user B
reports I
. O

This O
implies O
that O
a O
user B
interface I
en- O
( O
a O
) O
Majority O
( O
b O
) O
Strong O
Majority O
( O
c O
) O
Overwhelming O
Majority O
Figure O
3 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curves O
for O
cases O
with O
a O
majority O
/ O
strong O
majority O
/ O
overwhelming O
majority O
decision O
using O
classifier O
trained B
from O
majority O
cases O
( O
“ O
M O
” O
) O
, O
strong O
majority O
cases O
( O
“ O
SM O
” O
) O
, O
and O
overwhelming O
majority O
cases O
( O
“ O
OM O
” O
) O
Rank O
Feature O
Performance O
only O
1 O
verbal.abuse.enemies.kda O
2 O
verbal.abuse.enemies.gpm O
3 O
verbal.abuse.offender.deaths O
4 O
verbal.abuse.enemies.kda.avg.per.player O
5 O
verbal.abuse.offender.kda O
Report O
only O
1 O
verbal.abuse.allied.report.count O
2 O
verbal.abuse.allied.report.comment.count O
3 O
intentionally.feeding.allied.report.count O
4 O
intentionally.feeding.allied.report.comment.count O
5 O
offensive.language.allied.report.count O
Chat O
only O
1 O
case.offender.valence O
2 O
verbal.abuse.offender.chat.msgs O
3 O
offensive.language.offender.chat.msgs O
4 O
verbal.abuse.offender.valence O
5 O
verbal.abuse.total.chat.msgs O
Full O
1 O
case.offender.valence O
2 O
verbal.abuse.allied.report.count O
3 O
verbal.abuse.offender.chat.msgs O
4 O
offensive.language.offender.chat.msgs O
5 O
verbal.abuse.allied.report.comment.count O
Table O
2 O
: O
The O
top O
5 O
ranked B
features O
from O
an O
information O
gain O
evaluator O
for O
Tribunal O
decisions O
. O

couraging O
comments O
might O
be O
helpful O
for O
crowdsourced O
decisionmaking O
. O

In O
the O
chat B
model O
, O
we O
find O
that O
the O
most O
important O
variable O
is O
the O
valence O
score O
of O
offender O
no O
matter O
what O
the O
reported B
toxic O
category O
is O
. O

This O
agrees O
with O
our O
intuition O
that O
reviewers O
can O
see O
the O
context O
from O
chats B
and O
infer O
what O
happened O
. O

They O
gain O
insights O
from O
chat B
not O
only O
for O
verbal O
abuse O
or O
offensive O
language O
, O
but O
also O
other O
categories O
of O
toxic O
behavior O
. O

Also O
, O
this O
demonstrates O
that O
the O
ANEW O
dataset O
works O
well O
even O
with O
chats B
in O
online B
games I
. O

The O
second O
and O
third O
most O
important O
variable O
are O
the O
number O
of O
messages B
sent O
by O
the O
offender O
when O
the O
reported B
reason O
is O
verbal O
abuse O
and O
offensive O
language O
. O

This O
implies O
that O
the O
likelihood O
of O
toxic O
behavior O
goes O
up O
if O
the O
offender O
talks O
more O
. O

This O
can O
be O
easily O
recognized O
in B
real O
- I
time I
and O
thus O
pragmatically O
used O
for O
warning O
toxic B
players I
through O
visual O
cues O
. O

The O
fourth O
important O
variable O
is O
the O
valence O
score O
when O
the O
behavior O
is O
reported B
as O
verbal O
abuse O
. O

This O
is O
straightforward O
to O
understand O
. O

We O
find O
that O
number O
of O
total O
messages B
sent O
when O
the O
report B
reason O
is O
verbal O
abuse O
is O
the O
fifth O
important O
variable O
. O

This O
is O
the O
only O
feature O
in O
the O
top O
five O
that O
is O
not O
only O
related O
to O
the O
offender O
, O
but O
also O
others O
. O

If O
more O
players B
are O
involved O
in O
a O
quarrel O
, O
it O
is O
a O
strong O
sign O
of O
verbal O
abuse O
having O
occurred O
. O

The O
top O
5 O
features O
in O
the O
full B
model O
are O
entirely O
from O
the O
chat B
and O
report B
models O
. O

The O
total O
valence O
of O
the O
case O
is O
the O
number O
one O
feature O
, O
which O
highlights B
how O
much O
toxic O
behavior O
is O
visible O
/ O
expressed O
via O
in B
- I
game I
communication B
. O

The O
second O
most O
important O
feature O
in O
the O
full B
model O
comes O
from O
the O
report B
only O
model O
, O
highlighting B
how O
our O
approach O
dovetails O
with O
the O
first O
crowdsourced O
stage B
of O
the O
Tribunal O
. O

The O
hints O
provided O
by O
those O
that O
directly O
experience B
toxic O
behavior O
are O
useful O
not O
only O
to O
human O
reviewers O
, O
but O
, O
for O
an O
algorithmic O
solution O
as O
well O
. O

Next O
, O
we O
note O
that O
the O
6th O
and O
7th O
most O
important O
features O
in O
the O
full B
model O
are O
from O
the O
performance O
model O
. O

Thus O
, O
while O
in B
- I
game I
performance O
numbers O
are O
a O
predictor O
of O
toxic O
behavior O
, O
context O
is O
key O
. O

We O
also O
look O
into O
the O
top O
5 O
important O
variables O
in O
predicting O
overwhelming O
majority O
pardon O
and O
punish O
, O
respectively O
. O

Due O
to O
lack O
of O
space O
, O
we O
omit O
the O
details O
but O
we O
highlight B
two O
findings O
by O
comparing O
them O
. O

One O
is O
that O
there O
are O
great O
discrepancies O
of O
important O
variables O
between O
the O
model O
for O
predicting O
overwhelming O
majority O
pardon O
and O
punish O
. O

It O
implies O
that O
reviewers O
might O
make O
a O
decision O
for O
punish O
and O
pardon O
according O
to O
different O
mechanisms B
. O

The O
other O
is O
that O
, O
similar O
to O
predicting O
decisions O
, O
there O
are O
some O
commonalities O
in O
important O
variables O
across O
the O
category O
of O
toxic O
behavior O
for O
predicting O
overwhelming O
majority O
pardon O
and O
punish O
. O

For O
example O
, O
in O
predicting O
overwhelming O
majority O
pardon O
, O
the O
most O
important O
variable O
in O
the O
report B
only O
model O
is O
the O
number O
of O
reports B
by O
allies B
across O
the O
category O
. O

Similarly O
, O
in O
predicting O
overwhelming O
majority O
punish O
, O
the O
most O
important O
variable O
in O
the O
chat B
only O
model O
is O
the O
number O
of O
messages B
sent O
by O
the O
offender O
across O
the O
categories O
. O

Of O
course O
, O
there O
are O
some O
specifics O
for O
each O
category O
. O

For O
predicting O
overwhelming O
majority O
punish O
, O
in O
the O
report B
only O
model O
, O
the O
number O
of O
reports B
by O
enemies B
is O
more O
important O
than O
the O
number O
by O
allies B
in O
intentional O
feeding O
, O
but O
in O
verbal O
abuse O
, O
allies B
’ O
reports B
are O
more O
important O
than O
enemies B
’ O
. O

For O
future O
work O
, O
we O
intend O
to O
combine O
this O
result O
with O
qualitative O
user B
interviews O
and O
plan O
to O
reveal O
details O
of O
the O
mechanism B
of O
reviewers O
’ O
decisions O
. O

Figures O
5 O
and O
6 O
show O
ROC O
curves O
for O
predicting O
overwhelming O
pardon O
and O
overwhelming O
punish O
, O
respectively O
. O

Their O
AUC O
are O
Figure O
5 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curve O
for O
predicting O
overwhelming O
pardon O
decisions O
with O
models O
using O
in B
- I
game I
performance O
( O
P O
) O
, O
user B
report I
( O
R O
) O
, O
chats B
( O
C O
) O
, O
and O
using O
all O
available O
features O
( O
F O
) O
Figure O
6 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curve O
for O
predicting O
overwhelming O
punish O
decisions O
with O
models O
using O
in B
- I
game I
performance O
( O
P O
) O
, O
user B
report I
( O
R O
) O
, O
chats B
( O
C O
) O
, O
and O
using O
all O
available O
features O
( O
F O
) O
0.8049 O
, O
0.8055 O
, O
0.8269 O
, O
and O
0.8811 O
for O
overwhelming O
pardon O
decisions O
and O
0.6509 O
, O
0.6886 O
, O
0.6190 O
, O
and O
0.7461 O
for O
overwhelming O
punish O
decisions O
. O

There O
are O
some O
interesting O
differences O
between O
the O
curves O
for O
the O
agreement O
levels B
. O

First O
, O
detecting O
overwhelming O
pardon O
is O
easier B
to O
find O
than O
overwhelming O
majority O
punish O
and O
shows O
quite O
good O
performance O
. O

It O
is O
mainly O
because O
overwhelming O
majority O
punish O
is O
very O
close B
to O
strong O
majority O
punish O
, O
as O
we O
mentioned O
in O
Figure O
3 O
. O

This O
proves O
the O
feasibility O
of O
automatically O
assigning O
tasks O
to O
crowds O
and O
machines O
according O
to O
their O
difficulties O
. O

Quinn O
et O
al O
. O

demonstrate O
that O
crowd O
- O
assisted B
machine O
learning O
can O
achieve O
high O
overall O
accuracy O
when O
assigning O
easy B
tasks O
to O
machine O
and O
fuzzy O
tasks O
to O
human O
[ O
32 O
] O
. O

Although O
they O
divide O
cases O
into O
two O
classes B
by O
human O
experts O
, O
our O
result O
demonstrates O
that O
we O
can O
do O
it O
automatically O
. O

Figure O
7 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curve O
for O
EUW O
decisions O
with O
classifier O
trained B
on O
NA O
. O

In O
context O
of O
LoL B
, O
properly O
dealing O
with O
overwhelming O
pardon O
case O
is O
more O
important O
than O
overwhelming O
punish O
. O

Wrongly O
punished O
players B
would O
leave O
LoL B
, O
while O
wrongly O
pardoned O
players B
sometimes O
would O
be O
back O
to O
the O
Tribunal O
. O

If O
they O
do O
not O
come O
to O
the O
Tribunal O
again O
, O
it O
means O
that O
they O
are O
reformed O
and O
fine O
for O
the O
overall O
LoL B
ecosystem O
. O

In O
addition O
, O
we O
achieve O
high O
accuracy O
to O
predict O
decisions O
of O
punish O
or O
pardon O
on O
clear B
- O
cut O
cases O
, O
overwhelming O
majority O
punish O
and O
pardon O
cases O
, O
as O
in O
Figure O
3 O
. O

Thus O
, O
it O
is O
feasible O
that O
our O
classifier O
can O
automatically O
extract O
clear B
- O
cut O
cases O
and O
make O
accurate O
decisions O
on O
them O
. O

It O
is O
great O
way O
to O
assist B
crowdsourcing O
platform O
by O
machine O
learning O
. O

Second O
, O
the O
order O
of O
models O
by O
performance O
is O
different O
in O
two O
cases O
. O

In O
detecting O
overwhelming O
majority O
pardon O
, O
we O
observe O
that O
a O
chat B
model O
shows O
the O
best O
performance O
, O
while O
a O
user B
report I
model O
is O
quite O
comparable O
for O
the O
most O
part O
. O

By O
contrast O
, O
in O
detecting O
overwhelming O
majority O
punish O
, O
a O
user B
report I
model O
shows O
quite O
good O
performance O
. O

This O
is O
an O
interesting O
finding O
. O

Reviewers O
need O
to O
understand O
context O
from O
chats B
to O
prove O
not O
guilty O
, O
but O
they O
also O
see O
why O
and O
how O
many O
times B
a O
toxic B
player I
is O
charged O
. O

This O
is O
consistent O
with O
our O
initial O
analysis O
, O
revealing O
the O
number O
of O
user B
reports I
is O
highly O
correlated O
with O
the O
likelihood O
of O
being O
punished O
[ O
22 O
] O
. O

8.3 O
Classifier O
portability O
Finally O
, O
we O
explore O
whether O
or O
not O
our O
classifier O
is O
portable O
. O

Based B
on O
previous O
analysis O
[ O
22 O
] O
, O
we O
saw O
that O
there O
were O
statistically O
significant O
differences O
in O
Tribunal O
cases O
across O
the O
various O
regions O
that O
LoL O
operates O
. O

One O
underlying O
reason O
behind O
this O
is O
likely O
due O
to O
cultural O
differences O
realizing O
themselves O
both O
in O
the O
tendencies O
of O
toxic B
players I
as O
well O
as O
the O
reviewers O
. O

Because O
of O
these O
differences O
, O
we O
expect O
models O
trained B
on O
the O
North O
American O
dataset O
to O
not O
perform O
as O
well O
on O
the O
other O
regions O
. O

However O
, O
we O
do O
expect O
the O
models O
to O
remain O
superior O
to O
a O
coin O
flip O
in O
terms O
of O
discriminatory O
power B
. O

In O
other O
words O
, O
while O
we O
believe O
the O
models O
we O
specify O
are O
meaningful O
regardless O
of O
the O
region O
, O
the O
thresholds O
learned O
are O
probably O
sub O
- O
optimal O
. O

Before O
presenting O
results O
, O
we O
stress O
an O
additional O
caveat O
related O
to O
linguistic O
features O
. O

The O
ANEW O
dataset O
is O
based B
on O
English O
words O
and O
was O
built O
from O
American O
respondents O
. O

This O
makes O
the O
linguistic O
features O
( O
highly O
relevant O
in O
the O
NA O
dataset O
) O
useless O
when O
applied O
to O
the O
KR O
dataset O
since O
English O
language O
words O
are O
almost O
Figure O
8 O
: O
[ O
Best O
viewed O
in O
color O
. O

] O
ROC O
curve O
for O
EUW O
Overwhelming O
Majority O
Pardons O
with O
classifier O
trained B
on O
NA O
. O

non O
- O
existent O
. O

The O
EUW O
dataset O
includes O
players B
with O
a O
variety O
of O
native O
tongues O
, O
and O
anecdotally O
French O
, O
German O
, O
and O
Spanish O
are O
all O
spoken O
in B
- I
game I
. O

However O
, O
there O
is O
no O
language O
requirement O
to O
become O
a O
reviewer O
; O
you O
only O
have O
to O
have O
an O
account O
within O
the O
region O
you O
are O
reviewing O
. O

Also O
, O
English O
is O
a O
common O
tongue O
for O
gamers B
world B
wide O
. O

In O
fact O
, O
we O
see O
that O
less O
than O
1 O
% O
of O
EUW O
cases O
have O
an O
undefined O
vtext O
. O

Figures O
7 O
and O
8 O
show O
ROC O
curves O
of O
predicting O
EUW O
decisions O
and O
detecting O
EUW O
overwhelming O
majority O
pardon O
cases O
by O
using O
classifier O
trained B
on O
NA O
. O

The O
performance O
of O
predicting O
EUW O
decision O
does O
not O
reach O
that O
of O
NA O
decision O
, O
but O
detecting O
EUW O
overwhelming O
majority O
pardon O
is O
as O
good O
as O
NA O
. O

As O
with O
our O
hypothesis O
, O
this O
shows O
the O
potential O
of O
classifier O
portability O
, O
but O
at O
the O
same O
time B
, O
the O
existence O
of O
regional O
differences O
[ O
22 O
] O
. O

9 O
. O

DISCUSSION O
9.1 O
Estimating O
the O
Real B
- I
world I
Impacts B
We O
present O
the O
potential O
gain O
of O
time B
, O
cost B
, O
and O
accuracy O
if O
our O
classifier O
assists B
the O
Tribunal O
. O

One O
challenge O
is O
estimating O
the O
actual O
cost B
, O
time B
, O
and O
accuracy O
of O
reviewing O
cases O
in O
the O
Tribunal O
because O
Riot O
does O
not O
release O
detail O
statistics O
thereof O
, O
except O
a O
few O
infographics O
. O

We O
collect O
and O
complement O
partially O
available O
information O
to O
estimate O
required O
sources O
for O
the O
Tribunal O
. O

First O
, O
we O
estimate O
the O
actual O
cost B
for O
crowdsourcing O
decisions O
. O

Initially O
, O
Riot O
gave O
5 O
Influence O
Points B
( O
IP O
) O
as O
rewards B
to O
each O
vote O
only O
when O
a O
majority O
vote O
is O
reached O
, O
but O
have O
since O
removed O
this O
payment O
system O
. O

In O
LoL O
, O
IP O
is O
used O
for O
buying O
champions B
or O
skins O
. O

To O
measure O
how O
big O
5 O
IP O
is O
, O
we O
need O
to O
convert O
it O
to O
real O
money O
. O

Some O
champions B
whose O
price O
are O
450 O
IP O
can O
also O
be O
bought O
for O
260 O
Riot O
Points O
( O
RP O
) O
, O
that O
can O
be O
purchased O
by O
real O
money O
. O

Players B
pay O
$ O
10 O
for O
1380 O
RP O
. O

Through O
a O
simple O
calculation O
, O
we O
reach O
$ O
0.02 O
for O
each O
correct O
vote O
. O

This O
is O
comparable O
fare O
with O
other O
crowdsourcing O
platforms O
[ O
32 O
] O
. O

Second O
, O
we O
estimate O
the O
time B
required O
for O
each O
case O
. O

According O
to O
the O
talk O
by O
Jeffrey O
“ O
Lyte O
” O
Lin O
at O
Game O
Developers O
Conference O
( O
GDC O
) O
20139 O
, O
reviewers O
have O
cast O
105 O
million O
votes O
and O
reformed O
280,000 O
toxic B
players I
. O

Other O
announcements O
by O
Steve O
“ O
Pendragon O
” O
9 O
http://tinyurl.com/stfunub8 O
Mescon10 O
reveal O
50 O
% O
of O
players B
warned O
by O
Tribunal O
are O
reformed O
. O

We O
thus O
assume O
that O
105 O
million O
votes O
make O
verdicts O
for O
560,000 O
toxic B
players O
and I
half I
of I
them O
are O
reformed O
. O

We O
conservatively O
assume O
this O
is O
the O
lower O
bound O
of O
players B
who O
came O
to O
the O
Tribunal O
. O

This O
means O
that O
187.5 O
votes O
are O
required O
for O
majority O
votes O
on O
a O
single O
case O
. O

From O
the O
same O
source O
, O
Riot O
reveals O
more O
than O
47 O
million O
votes O
were O
cast O
in O
the O
first O
year O
of O
the O
Tribunal O
, O
implying O
that O
1.49 O
votes O
per O
second O
are O
cast O
. O

From O
both O
, O
we O
can O
infer O
125.85 O
seconds O
are O
required O
to O
reach O
a O
verdict O
for O
one O
case O
in O
the O
Tribunal O
. O

We O
reasonably O
assume O
that O
this O
is O
the O
acceptable O
speed O
where O
Riot O
’s O
in O
- O
house O
experts O
manually O
review O
some O
intriguing O
cases O
. O

Finally O
, O
we O
estimate O
the O
accuracy O
of O
the O
Tribunal O
. O

Lin O
said O
, O
“ O
approximately O
80 O
% O
agreement O
between O
the O
[ O
Tribunal O
] O
community B
and O
Riot O
’s O
in O
- O
house O
team B
” O
, O
in O
the O
same O
GDC O
talk O
. O

He O
added O
that O
the O
inhouse O
team B
is O
less O
lenient O
than O
the O
Tribunal O
. O

Surprisingly O
, O
the O
overall O
accuracy O
of O
the O
Tribunal O
is O
comparable O
with O
our O
classifier O
with O
respect O
to O
Riot O
’s O
in O
- O
house O
decisions O
. O

That O
is O
, O
in O
contrast O
to O
CrowdFlow O
[ O
32 O
] O
, O
our O
supervised O
learner O
has O
the O
potential O
to O
replace O
the O
crowdsourcing O
part O
of O
the O
Tribunal O
with O
no O
real O
sacrifice O
in O
accuracy O
. O

We O
now O
estimate O
the O
gain O
of O
Riot O
from O
the O
view O
of O
cost B
and O
victim B
players B
. O

Cost B
savings O
We O
already O
compute O
that O
the O
cost B
of O
each O
correct O
vote O
is O
$ O
0.02 O
. O

Conservatively O
, O
we O
estimate O
50 O
% O
of O
all O
votes O
fall B
into O
majority O
for O
each O
case O
. O

Since O
the O
Tribunal O
got O
47 O
million O
votes O
the O
first O
year O
, O
its O
cost B
is O
47 O
M O
( O
votes O
) O
x O
50 O
( O
% O
) O
x O
0.02 O
( O
$ O
) O
= O
470,000 O
( O
$ O
) O
. O

As O
of O
March O
2013 O
, O
the O
number O
of O
votes O
reached O
105 O
millions O
. O

Its O
potential O
cost B
surpasses O
1 O
million O
dollars O
. O

With O
the O
success O
of O
new O
regions O
and O
a O
growing O
userbase O
, O
this O
cost B
will O
become O
huge O
. O

Protecting O
victims B
As O
of O
October O
2012 O
, O
Riot O
announced O
that O
12 O
million O
players B
play B
LoL B
everyday11 O
and O
they O
play B
more O
than O
1 O
billion O
hours O
every O
month O
. O

Thus O
, O
we O
estimate O
that O
a O
player B
enjoys O
LoL O
83 O
minutes O
everyday O
which O
equates O
to O
2.21 O
matches B
where O
one O
match B
usually O
spans O
30 O
to O
45 O
minutes O
. O

In O
the O
first O
year O
, O
the O
Tribunal O
detected O
47 O
M O
/ O
187.5 O
= O
250,667 O
toxic B
players I
. O

On O
average O
, O
686.75 O
toxic B
players I
are O
warned O
by O
the O
Tribunal O
everyday O
. O

From O
this O
number O
we O
can O
compute O
number O
of O
potential O
victims B
who O
are O
protected O
by O
the O
Tribunal O
everyday O
. O

The O
number O
of O
matches B
toxic B

 O
players I
participate O
everyday O
is O
1,517.74 O
, O
and O
13,659.61 O
innocent O
players B
are O
exposed O
to O
toxic B
players I
. O

If O
our O
classifier O
and O
the O
Tribunal O
works O
together O
in O
a O
50 O
- O
50 O
manner O
, O
we O
can O
protect O
13,659 O
more O
players B
everyday O
and O
more O
than O
400 O
thousand O
per O
month O
. O

9.2 O
Limitations O
and O
Consequences O
Although O
we O
have O
shown O
our O
approach O
is O
relatively O
robust O
, O
even O
across O
different O
regions O
of O
the O
world B
, O
there O
are O
some O
limitations O
. O

First O
, O
although O
outside O
the O
scope O
of O
this O
paper O
, O
LoL O
features O
an O
ever O
changing O
“ O
meta B
- O
game B
” O
which O
dictates O
commonly O
used O
strategies B
and O
tactics B
. O

Although O
the O
features O
we O
use O
in O
this O
paper O
are O
not O
directly O
tied O
to O
the O
meta B
, O
for O
example O
which O
particular O
items B
or O
champions B
are O
selected O
, O
they O
are O
indirectly O
related O
. O

E.g. O
, O
the O
amount B
of I

 O
damage I
dealt O
and O
received O
or O
gold B
earned O
might O
be O
influenced O
by O
the O
current O
meta B
. O

Although O
the O
datasets O
in O
this O
paper O
span O
multiple O
metas B
and O
we O
still O
see O
good O
results O
, O
accuracy O
might O
be O
improved O
by O
training B
classifiers O
on O
data O
only O
from O
the O
meta B
of O
cases O
under O
examination O
. O

We O
also O
note O
that O
changes O
in O
meta B
only O
affect O
the O
performance O
related O
features O
/ O
models O
. O

Toxic B
players I
could O
, O
in O
theory O
, O
adapt O
to O
avoid O
detection O
. O

For O
example O
, O
toxic B
players I
might O
say O
“ O
Wow O
, O
you O
are O
a O
great O
player B
! O
” O
sarcastically O
instead O
of O
calling B
someone O
a O
noob B
. O

Or O
, O
perhaps O
an O
inten10http://tinyurl.com/stfunub9 O
11http://tinyurl.com/stfunub10 O
tional O
feeder B
would O
go O
out O
of O
his O
way O
to O
damage B
the O
enemy B
team I
, O
but O
not O
enough O
to O
actually O
kill B
them O
. O

This O
raises O
some O
interesting O
points B
. O

First O
, O
toxic O
behavior O
only O
has O
an O
impact B
if O
it O
actually O
affects O
people O
in O
a O
negative O
way O
. O

It O
would O
take O
a O
major O
shift O
in O
the O
community B
’s O
understanding O
of O
language O
to O
infer O
that O
a O
seemingly O
positive O
statement O
was O
meant O
in O
a O
negative O
way O
. O

Second O
, O
in O
the O
case O
of O
toxic B
players I
adapting O
their O
play B
style I
, O
we O
argue O
that O
is O
a O
hidden B
benefit O
of O
our O
approach O
. O

Detecting O
toxic O
behavior O
has O
significant O
value O
, O
but O
preventing O
it O
wholesale O
or O
reducing O
its O
impact B
is O
a O
much O
better O
solution O
. O

Although O
a O
feeder B
could O
attempt O
to O
hide B
his O
intentions O
by O
damaging B
the O
enemy B
team I
, O
he O
is O
consequently O
reducing O
the O
negative O
impact B
of O
his O
feeding O
by O
still O
providing O
some O
utility B
to O
his O
team B
. O

9.3 O
Future O
Research O
Directions O
One O
interesting O
research O
direction O
is O
real B
- I
time I
detection O
of O
toxic O
playing B
and O
adaptive O
interface B
design O
. O

Currently O
, O
the O
Tribunal O
deals O
with O
toxic B
players I
after O
victims B
are O
exposed O
to O
online O
violence O
. O

We O
quantitatively O
show O
that O
many O
people O
are O
exposed O
to O
toxic O
behavior O
even O
though O
a O
relatively O
well O
designed O
crowdsourcing O
framework O
to O
deal O
with O
toxic B
players I
is O
operating O
. O

Our O
suggestion O
is O
to O
protect O
innocent O
players B
before O
they I
are I
exposed O
to O
toxic O
playing B
through O
adaptive O
an O
user B
interface I
. O

Simply O
, O
we O
can O
warn O
toxic B
players I
when O
they O
exhibit O
toxicity B
. O

Since O
toxic B
players I
sometimes O
do O
not O
recognize O
what O
they O
did O
as O
toxic O
playing B
[ O
23 O
] O
, O
some O
visual O
cues O
representing O
toxicity B
will O
be O
helpful O
to O
reform O
them O
. O

As O
another O
psychological O
experiment O
, O
we O
are O
interested O
in O
whether O
they O
can O
be O
treated O
by O
giving O
penalties O
within O
a O
game B
, O
such O
as O
degrading B
champion I
status I
in O
real O
time B
, O
when O
toxic O
playing O
is O
detected O
. O

Since O
the O
competitive B
nature O
of O
LoL O
is O
the O
origin O
of O
some O
toxic O
playing B
, O
in B
- I
game I
penalties O
might O
have O
an O
impact B
. O

The O
other O
reason O
that O
this O
might O
work O
is O
due O
to O
the O
immediacy O
of O
punishment O
. O

It O
is O
well O
known O
that O
instant O
punishment O
( O
reinforcement O
) O
is O
more O
effective O
than O
delayed O
one O
[ O
39 O
] O
. O

Degrading B
champion I
status I
or O
marking B
them O
visibly O
[ O
3 O
] O
as O
toxic B
players I
could O
serve O
as O
an O
intermediate O
, O
direct O
punishment O
, O
while O
warning O
by O
the O
Tribunal O
is O
a O
delayed O
punishment O
. O

How O
toxic B
players I
react O
to O
this O
intermediate O
punishment O
will O
be O
an O
interesting O
avenue O
of O
future O
research O
. O

10 O
. O

SUMMARY O
In O
this O
paper O
we O
explored O
the O
use O
of O
crowdsourced O
decisions O
on O
toxic O
behavior O
made O
by O
millions O
of O
experts O
. O

Using O
the O
same O
sparse O
information O
available O
to O
the O
reviewers O
, O
we O
trained B
classifiers O
to O
detect O
the O
presence O
, O
and O
severity O
of O
toxicity B
. O

We O
built O
several O
models O
oriented O
around O
in B
- I
game I
performance O
, O
reports B
by O
victims B
of O
toxic O
behavior O
, O
and O
linguistic O
features O
of O
chat B
messages B
. O

We O
found O
that O
training O
with O
high O
agreement O
decisions O
resulted O
in O
more O
accuracy O
on O
low O
agreement O
decisions O
and O
that O
our O
classifier O
was O
adept O
in O
detecting O
clear B
cut O
innocence O
. O

Finally O
, O
we O
showed O
that O
our O
classifier O
is O
relatively O
robust O
across O
cultural O
regions O
; O
our O
classifier O
built O
from O
a O
North O
American O
dataset O
performed O
adequately O
on O
a O
European O
dataset O
. O

Ultimately O
, O
our O
work O
can O
be O
used O
as O
a O
foundation O
for O
the O
further O
study O
of O
toxic O
behavior O
. O

The O
features O
we O
found O
to O
be O
important O
can O
serve O
as O
a O
spring O
board O
for O
more O
in O
depth O
learning O
. O

Further O
, O
a O
preliminary O
cost B
analysis O
indicates O
that O
our O
classifier O
has O
the O
potential O
to O
, O
at O
minimum O
, O
alleviate O
the O
some O
of O
the O
burden O
placed O
on O
human O
reviewers O
. O

11 O
. O

REFERENCES O
[ O
1 O
] O
J. O
Barnett O
, O
M. O
Coulson O
, O
and O
N. O
Foreman O
. O

Examining O
player B
anger O
in O
World O
of O
Warcraft O
. O

In O
Online O
worlds B
: O
Convergence O
of O
the O
real O
and O
the O
virtual O
, O
pages O
147–160 O
. O

2010 O
. O

[ O
2 O
] O
F. O
S. O
Bellezza O
, O
A. O
G. O
Greenwald O
, O
and O
M. O
R. O
Banaji O
. O

Words O
high O
and O
low O
in O
pleasantness O
as O
rated O
by O
male O
and O
female O
college O
students O
. O

Behavior O
Research O
Methods O
, O
Instruments O
, O
& O
Computers O
, O
18(3):299–303 O
, O
1986 O
. O

[ O
3 O
] O
J. O
Blackburn O
, O
R. O
Simha O
, O
N. O
Kourtellis O
, O
X. O
Zuo O
, O
M. O
Ripeanu O
, O
J. O
Skvoretz O
, O
and O
A. O
Iamnitchi O
. O

Branded O
with O
a O
scarlet O
" O
C O
" O
: O
Cheaters B
in O
a O
gaming O
social O
network O
. O

In O
WWW O
’ O
12 O
, O
pages O
81–90 O
, O
2012 O
. O

[ O
4 O
] O
M. O
M. O
Bradley O
and O
P. O
J. O
Lang O
. O

Affective O
norms O
for O
English O
words O
( O
ANEW O
) O
: O
Instruction O
manual O
and O
affective O
ratings O
. O

Technical O
report B
, O
Technical O
Report O
C-1 O
, O
The O
Center O
for O
Research O
in O
Psychophysiology O
, O
University O
of O
Florida O
, O
1999 O
. O

[ O
5 O
] O
A. O
Brew O
, O
D. O
Greene O
, O
and O
P. O
Cunningham O
. O

Using O
crowdsourcing O
and O
active O
learning O
to O
track O
sentiment O
in O
online O
media O
. O

In O
ECAI O
, O
pages O
145–150 O
, O
2010 O
. O

[ O
6 O
] O
V. O
H.-H. O
Chen O
, O
H. O
B.-L. O
Duh O
, O
and O
C. O
W. O
Ng O
. O

Players B
who O
play B
to O
make O
others O
cry O
: O
The O
influence O
of O
anonymity O
and O
immersion O
. O

In O
ACE O
’ O
09 O
, O
2009 O
. O

[ O
7 O
] O
T. O
Chesney O
, O
I. O
Coyne O
, O
B. O
Logan O
, O
and O
N. O
Madden O
. O

Griefing B
in O
virtual O
worlds B
: O
Causes O
, O
casualties O
and O
coping O
strategies B
. O

Information O
Systems O
Journal O
, O
19(6):525–548 O
, O
2009 O
. O

[ O
8 O
] O
M. O
Davies O
. O

Gamers B
do O
n’t O
want O
any O
more O
grief O
, O
2011 O
. O

http://tinyurl.com/stfunub11 O
. O

[ O
9 O
] O
P. O
S. O
Dodds O
and O
C. O
M. O
Danforth O
. O

Measuring O
the O
happiness O
of O
large O
- O
scale O
written O
expression O
: O
Songs O
, O
blogs O
, O
and O
presidents O
. O

Journal O
of O
Happiness O
Studies O
, O
11(4):441–456 O
, O
2010 O
. O

[ O
10 O
] O
C. O
Y. O
Foo O
and O
E. O
M. O
I. O
Koivisto O
. O

Defining O
grief O
play B
in O
MMORPGs O
: O
Player B
and O
developer O
perceptions O
. O

In O
ACE O
’ O
04 O
, O
2004 O
. O

[ O
11 O
] O
M. O
R. O
Frank O
, O
L. O
Mitchell O
, O
P. O
S. O
Dodds O
, O
and O
C. O
M. O
Danforth O
. O

Happiness O
and O
the O
patterns O
of O
life O
: O
A O
study O
of O
geolocated O
tweets O
. O

Scientific O
Reports O
, O
3(2625 O
) O
, O
2013 O
. O

[ O
12 O
] O
M. O
J. O
Franklin O
, O
D. O
Kossmann O
, O
T. O
Kraska O
, O
S. O
Ramesh O
, O
and O
R. O
Xin O
. O

CrowdDB O
: O
Answering O
queries O
with O
crowdsourcing O
. O

In O
SIGMOD O
’ O
11 O
, O
pages O
61–72 O
, O
2011 O
. O

[ O
13 O
] O
S. O
A. O
Golder O
and O
M. O
W. O
Macy O
. O

Diurnal O
and O
seasonal O
mood O
vary O
with O
work O
, O
sleep O
, O
and O
daylength O
across O
diverse O
cultures O
. O

Science O
, O
333(6051):1878–1881 O
, O
2011 O
. O

[ O
14 O
] O
M. O
F. O
Goodchild O
and O
J. O
A. O
Glennon O
. O

Crowdsourcing O
geographic O
information O
for O
disaster O
response O
: O
a O
research O
frontier O
. O

International O
Journal O
of O
Digital O
Earth O
, O
3(3):231–241 O
, O
2010 O
. O

[ O
15 O
] O
J. O
Heer O
and O
M. O
Bostock O
. O

Crowdsourcing O
graphical O
perception O
: O
Using O
Mechanical O
Turk O
to O
assess O
visualization O
design O
. O

In O
CHI O
’ O
10 O
, O
pages O
203–212 O
, O
2010 O
. O

[ O
16 O
] O
S. O
S. O
Ho O
and O
D. O
M. O
McLeod O
. O

Social O
- O
psychological O
influences O
on O
opinion O
expression O
in B
face O
- I
to I
- O
face O
and O
computer O
- O
mediated O
communication B
. O

Communication O
Research O
, O
35(2):190–207 O
, O
2008 O
. O

[ O
17 O
] O
J. O
Howe O
. O

The O
rise O
of O
crowdsourcing O
. O

Wired O
magazine O
, O
14(6):1–4 O
, O
2006 O
. O

[ O
18 O
] O
P. O
G. O
Ipeirotis O
, O
F. O
Provost O
, O
and O
J. O
Wang O
. O

Quality O
management O
on O
Amazon O
Mechanical O
Turk O
. O

In O
SIGKDD O
’ O
10 O
Workshop O
on O
Human O
Computation O
, O
pages O
64–67 O
, O
2010 O
. O

[ O
19 O
] O
E. O
Kamar O
, O
S. O
Hacker O
, O
and O
E. O
Horvitz O
. O

Combining O
human O
and O
machine O
intelligence O
in B
large O
- I
scale I
crowdsourcing O
. O

In O
AAMAS O
’ O
12 O
, O
pages O
467–474 O
, O
2012 O
. O

[ O
20 O
] O
A. O
Kittur O
, O
E. O
H. O
Chi O
, O
and O
B. O
Suh O
. O

Crowdsourcing O
user B
studies O
with O
Mechanical O
Turk O
. O

In O
CHI O
’ O
08 O
, O
pages O
453–456 O
, O
2008 O
. O

[ O
21 O
] O
A. O
Kumar O
and O
M. O
Lease O
. O

Modeling O
annotator O
accuracies O
for O
supervised O
learning O
. O

In O
WSDM O
’ O
11 O
Workshop O
on O
Crowdsourcing O
for O
Search O
and O
Data O
Mining O
, O
pages O
19–22 O
, O
2011 O
. O

[ O
22 O
] O
H. O
Kwak O
and O
S. O
Han O
. O

“ O
So O
many O
bad O
guys O
, O
so O
little O
time B
” O
: O
Understanding O
toxic B
behavior O
and I
reaction I
in I
team B
competition I

 O
games I
. O

Submitted O
. O

[ O
23 O
] O
H. O
Lin O
and O
C.-T. O
Sun O
. O

The O
" O
white O
- O
eyed O
" O
player B
culture O
: O
Grief O
play B
and O
construction O
of O
deviance O
in O
MMORPGs O
. O

In O
DiGRA O
’ O
05 O
, O
2005 O
. O

[ O
24 O
] O
J.-K. O
Lou O
, O
K. O
Park O
, O
M. O
Cha O
, O
J. O
Park O
, O
C.-L. O
Lei O
, O
and O
K.-T. O
Chen O
. O

Gender O
swapping O
and O
user B
behaviors O
in O
online B
social O
games I
. O

In O
WWW O
’ O
13 O
, O
pages O
827–836 O
, O
2013 O
. O

[ O
25 O
] O
K. O
Y. O
A. O
McKenna O
and O
J. O
A. O
Bargh O
. O

Plan O
9 O
from O
cyberspace O
: O
The O
implications O
of O
the O
Internet O
for O
personality O
and O
social O
psychology O
. O

Personality O
and O
Social O
Psychology O
Review O
, O
4(1):57–75 O
, O
2000 O
. O

[ O
26 O
] O
L. O
Mitchell O
, O
M. O
R. O
Frank O
, O
K. O
D. O
Harris O
, O
P. O
S. O
Dodds O
, O
and O
C. O
M. O
Danforth O
. O

The O
geography O
of O
happiness O
: O
Connecting O
Twitter O
sentiment O
and O
expression O
, O
demographics O
, O
and O
objective B
characteristics O
of O
place O
. O

PloS O
one O
, O
8(5):e64417 O
, O
2013 O
. O

[ O
27 O
] O
J. O
Mulligan O
, O
B. O
Patrovsky O
, O
and O
R. O
Koster O
. O

Developing O
online B
games I
: O
An O
insider O
’s O
guide O
. O

Pearson O
Education O
, O
2003 O
. O

[ O
28 O
] O
S. O
Nowak O
and O
S. O
Rüger O
. O

How O
reliable O
are O
annotations O
via O
crowdsourcing O
: O
A O
study O
about O
inter O
- O
annotator O
agreement O
for O
multi O
- O
label O
image O
annotation O
. O

In O
Proceedings O
of O
the O
international O
conference O
on O
Multimedia O
information O
retrieval O
, O
MIR O
’ O
10 O
, O
pages O
557–566 O
, O
2010 O
. O

[ O
29 O
] O
D. O
Olweus O
. O

Bullying O
at O
school O
: O
Long O
term O
outcomes O
for O
the O
victims B
and O
an O
effective O
school O
- O
based B
intervention O
program O
. O

Aggressive O
Behavior O
: O
Current O
Perspectives O
, O
pages O
97–130 O
, O
1996 O
. O

[ O
30 O
] O
G. O
Paolacci O
, O
J. O
Chandler O
, O
and O
P. O
Ipeirotis O
. O

Running O
experiments O
on O
Amazon O
Mechanical O
Turk O
. O

Judgment B
and O
Decision O
Making O
, O
5(5):411–419 O
, O
2010 O
. O

[ O
31 O
] O
A. O
J. O
Quinn O
and O
B. O
B. O
Bederson O
. O

Human O
- O
machine O
hybrid B
computation O
. O

In O
CHI O
’ O
11 O
Workshop O
On O
Crowdsourcing O
And O
Human O
Computation O
, O
2011 O
. O

[ O
32 O
] O
A. O
J. O
Quinn O
, O
B. O
B. O
Bederson O
, O
T. O
Yeh O
, O
and O
J. O
Lin O
. O

CrowdFlow O
: O
Integrating O
machine O
learning O
with O
Mechanical O
Turk O
for O
speed O
- O
cost B
- O
quality O
flexibility O
. O

Better O
Performance O
Over O
Iterations O
, O
2010 O
. O

[ O
33 O
] O
V. O
S. O
Sheng O
, O
F. O
Provost O
, O
and O
P. O
G. O
Ipeirotis O
. O

Get O
another O
label O
? O
Improving O
data O
quality O
and O
data O
mining O
using O
multiple O
, O
noisy O
labelers O
. O

In O
KDD O
’ O
08 O
, O
pages O
614–622 O
, O
2008 O
. O

[ O
34 O
] O
P. O
K. O
Smith O
, O
J. O
Mahdavi O
, O
M. O
Carvalho O
, O
S. O
Fisher O
, O
S. O
Russell O
, O
and O
N. O
Tippett O
. O

Cyberbullying O
: O
Its O
nature O
and O
impact B
in O
secondary O
school O
pupils O
. O

Journal O
of O
Child O
Psychology O
and O
Psychiatry O
, O
49(4):376–385 O
, O
2008 O
. O

[ O
35 O
] O
R. O
Snow O
, O
B. O
O’Connor O
, O
D. O
Jurafsky O
, O
and O
A. O
Y. O
Ng O
. O

Cheap O
and O
fast O
— O
but O
is O
it O
good O
? O
: O
Evaluating O
non O
- O
expert O
annotations O
for O
natural O
language O
tasks O
. O

In O
Proceedings O
of O
the O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
, O
pages O
254–263 O
. O

Association O
for O
Computational O
Linguistics O
, O
2008 O
. O

[ O
36 O
] O
J. O
Suler O
. O

The O
online O
disinhibition O
effect B
. O

Cyberpsychology O
& O
behavior O
, O
7(3):321–326 O
, O
2004 O
. O

[ O
37 O
] O
W. O
Tang O
and O
M. O
Lease O
. O

Semi O
- O
supervised O
consensus O
labeling O
for O
crowdsourcing O
. O

In O
SIGIR O
’ O
11 O
Workshop O
on O
Crowdsourcing O
for O
Information O
Retrieval O
, O
2011 O
. O

[ O
38 O
] O
A. O
Tumasjan O
, O
T. O
O. O
Sprenger O
, O
P. O
G. O
Sandner O
, O
and O
I. O
M. O
Welpe O
. O

Predicting O
elections O
with O
Twitter O
: O
What O
140 O
characters B
reveal O
about O
political O
sentiment O
. O

In O
ICWSM O
’ O
10 O
, O
pages O
178–185 O
, O
2010 O
. O

[ O
39 O
] O
R. O
Van O
Houten O
. O

Punishment O
: O
From O
the O
animal O
laboratory O
to O
the O
applied O
setting O
. O

The O
Effects B
of O
Punishment O
on O
Human O
Behavior O
, O
pages O
13–44 O
, O
1983 O
. O

[ O
40 O
] O
E. O
M. O
Voorhees O
. O

Variations O
in O
relevance O
judgments B
and O
the O
measurement O
of O
retrieval O
effectiveness O
. O

Information O
processing O
& O
management O
, O
36(5):697–716 O
, O
2000 O
. O

[ O
41 O
] O
D. O
E. O
Warner O
and O
M. O
Ratier O
. O

Social O
context O
in B
massively O
- I
multiplayer I
online B
games I
( O
MMOGs O
) O
: O
Ethical O
questions O
in O
shared O
space O
. O

International O
Review O
of O
Information O
Ethics O
, O
4(7 O
) O
, O
2005 O
. O

[ O
42 O
] O
B. O
Weiner O
. O

A O
cognitive O
( O
attribution)-emotion O
- O
action O
model O
of O
motivated O
behavior O
: O
An O
analysis O
of O
judgments B
of O
help O
- O
giving O
. O

Journal O
of O
Personality O
and O
Social O
Psychology O
, O
39(2):186 O
, O
1980 O
. O

[ O
43 O
] O
O. O
Zaidan O
and O
C. O
Callison O
- O
Burch O
. O

Crowdsourcing O
translation O
: O
Professional O
quality O
from O
non O
- O
professionals O
. O

In O
ACL O
’ O
11 O
, O
pages O
1220–1229 O
, O
2011 O
. O

Linguistic O
Analysis O
of O
Toxic O
Behavior O
in O
an O
Online O
Video O
Game O
Haewoon O
Kwak O
† O
and O
Jeremy O
Blackburn O
‡ O
†Qatar O
Computing O
Research O
Institute O
, O
Doha O
, O
Qatar O
hkwak@qf.org.qa O
‡Telefonica O
Research O
, O
Barcelona O
, O
Spain O
jeremyb@tid.es O
Abstract O
. O

In O
this O
paper O
we O
explore O
the O
linguistic O
components O
of O
toxic O
behavior O
by O
using O
crowdsourced O
data O
from O
over O
590 O
thousand O
cases O
of O
accused O
toxic B
players I
in O
a O
popular O
match B
- O
based B
competition B
game B
, O
League O
of O
Legends O
. O

We O
perform O
a O
series O
of O
linguistic O
analyses O
to O
gain O
a O
deeper O
understanding O
of O
the O
role B
communication B
plays B
in O
the O
expression O
of O
toxic O
behavior O
. O

We O
characterize O
linguistic O
behavior O
of O
toxic B
players O
and I
compare I
it I
with O
that O
of O
typical B
players I
in O
an O
online B
competition O
game I
. O

We O
also O
find O
empirical O
support B
describing O
how O
a O
player B
transitions O
from O
typical O
to O
toxic O
behavior O
. O

Our O
findings O
can O
be O
helpful O
to O
automatically O
detect O
and O
warn O
players B
who O
may O
become O
toxic O
and O
thus O
insulate O
potential O
victims B
from O
toxic O
playing O
in O
advance O
. O

Keywords O
: O
Toxic O
behavior O
· O
verbal O
violence O
· O
Tribunal O
· O
League O
of O
Legends O
· O
cyberbullying O
· O
online B
games I
1 O
Introduction O
Multiplayer O
games B
provide O
players B
with O
the O
thrill O
of O
true O
competition B
. O

Players B
prove O
themselves O
superior O
to O
other O
humans O
that O
exhibit O
dynamic O
behavior O
far O
beyond O
that O
of O
any O
computer O
controlled B
opponent B
. O

Additionally O
, O
some O
multiplayer O
games B
provide O
another O
wrinkle O
: O
teamwork B
. O

Now O
, O
not O
only O
is O
it O
a O
test O
of O
skill B
between O
two O
individuals O
, O
but O
cooperation B
, O
strategy B
, O
and O
communication B
between O
teammates B
can O
ensure O
victory B
. O

Unfortunately O
, O
the O
presence O
of O
teammates B
and O
their O
influence O
on O
victory B
and O
defeat B
can O
result O
in O
toxic O
behavior O
. O

Toxic O
behavior O
, O
also O
known O
as O
cyberbullying O
[ O
1 O
] O
, O
griefing B
[ O
4 O
] O
, O
or O
online O
disinhibition O
[ O
7 O
] O
, O
is O
bad O
behavior O
that O
violates O
social O
norms O
, O
inflicts O
misery O
, O
continues O
to O
cause O
harm O
after O
it O
occurs O
, O
and O
affects O
an O
entire O
community B
. O

The O
anonymity O
afforded O
by O
, O
and O
ubiquity O
of O
, O
computer O
- O
mediated O
- O
communication B
( O
CMC O
) O
naturally O
leads B
to O
hostility O
and O
aggressiveness O
[ O
3,8 O
] O
. O

A O
major O
obstacle B
in O
understanding O
toxic O
behavior O
is O
its O
subjective O
perception O
. O

Unlike O
unethical O
behavior O
like O
cheating B
, O
toxic O
behavior O
is O
nebulously O
defined O
; O
toxic B
players I
themselves O
sometimes O
fail O
to O
recognize O
their O
behavior O
as O
toxic O
[ O
6 O
] O
. O

Nevertheless O
, O
because O
of O
the O
very O
real O
impact B
toxic O
behavior O
has O
on O
our O
daily O
lives O
, O
even O
outside O
of O
games B
, O
a O
deeper O
understanding O
is O
necessary O
. O

arXiv:1410.5185v1 O
[ O
cs O
. O

SI O
] O
20 O
Oct O
2014 O
2 O
Haewoon O
Kwak O
and O
Jeremy O
Blackburn O
To O
further O
our O
understanding O
, O
in O
this O
paper O
we O
explore O
the O
linguistic O
components O
of O
toxic O
behavior O
. O

Using O
crowdsourced O
data O
from O
over O
590 O
thousand O
“ O
judicial O
trials O
” O
of O
accused O
toxic B
players I
representing O
over O
2.1 O
million O
matches B
of O
a O
popular O
match B
- O
based B
competition B
game B
, O
League O
of O
Legends1 O
, O
we O
perform O
a O
series O
of O
linguistic O
analyses O
to O
gain O
a O
deeper O
understanding O
of O
the O
role B
communication B
plays B
in O
the O
expression O
of O
toxic O
behavior O
. O

In O
our O
previous O
work O
[ O
2 O
] O
, O
we O
found O
that O
offensive O
language O
is O
the O
most O
reported B
reason O
across O
all O
the O
three O
regions O
. O

Also O
, O
in O
North O
America O
, O
verbal O
abuse O
is O
the O
second O
most O
reported B
reason O
. O

In O
other O
words O
, O
linguistic O
components O
are O
a O
prime O
method O
of O
expressing O
toxicity B
. O

From O
our O
analyses O
we O
draw O
several O
findings O
. O

First O
, O
the O
volume O
of O
communication B
is O
not O
uniform O
throughout O
the O
length O
of O
the O
match B
, O
instead O
showing O
a O
bi O
- O
modal O
shape O
with O
peaks O
at O
the O
beginning O
and O
end O
of O
a O
match B
. O

By O
comparing O
the O
distribution O
of O
frequency O
of O
communications B
between O
normal B
players I
and O
toxic B
players I
, O
we O
find O
subtle O
differences O
. O

Typical B
players I
chat B
relatively O
more O
at O
the O
beginning O
of O
a O
match B
, O
which O
is O
mainly O
for O
ice O
breaking O
, O
morale O
boosting O
, O
and O
sharing O
early B
strategic O
information O
. O

In O
contrast O
, O
toxic B
players I
chat B
less O
at O
the O
beginning O
but O
constantly O
more O
than O
typical B
players I
after O
some O
time B
point B
, O
i.e. O
phase B
transition I
. O

Next O
, O
we O
find O
discriminative O
uni- O
and O
bi O
- O
grams O
used O
by O
typical B

 O
and I
toxic I
players I
, O
as O
signatures O
of O
them O
, O
examine O
the O
differences O
, O
and O
show O
that O
certain O
bi O
- O
grams O
can O
be O
classified O
based B
on O
when O
they O
appear O
in O
a O
match B
. O

Temporal O
patterns O
of O
the O
linguistic O
signature O
of O
toxic B
players I
illustrate O
what O
kind O
of O
toxic O
playing O
happens O
as O
the O
match B
progresses O
. O

Deeper O
analysis O
of O
temporal O
analysis O
of O
words O
used O
by O
toxic B
and I
typical I
players I
reveals O
a O
more O
interesting O
picture O
. O

We O
focus B
on O
how O
a O
player B
transitions O
to O
toxic O
by O
comparing O
the O
temporal O
usage O
of O
popular O
uni O
- O
grams O
between O
typical B
players O
and I
toxic I
players I
. O

Our O
contribution O
is O
two O
- O
fold O
. O

First O
, O
we O
characterize O
linguistic O
behavior O
of O
toxic B
players O
and I
compare I
it I
with O
that O
of O
typical B
players I
in O
online O
competition B
games B
. O

Second O
, O
we O
find O
empirical O
support B
to O
describe O
how O
a O
player B
turns B
to O
be O
toxic O
. O

Our O
findings O
would O
be O
helpful O
to O
automatically O
detect O
and O
warn O
players B
who O
may O
turn B
to O
be O
toxic O
and O
thus O
save O
potential O
victims B
of O
toxic O
playing B
in O
advance O
. O

2 O
Dataset O
The O
League O
of O
Legends O
( O
LoL O
) O
is O
the O
most O
popular O
Multiplayer O
Online O
Battle O
Arena O
out O
today O
, O
and O
suffers O
from O
a O
high O
degree O
of O
toxic O
behavior O
. O

The O
LoL B
Tribunal O
is O
a O
crowdsourced O
system O
for O
determining O
the O
guilt O
of O
players B
accused O
of O
tocix O
behavior O
. O

We O
collected O
590,311 O
Tribunal O
cases O
from O
the O
North O
America O
region O
representing O
a O
total O
of O
2,107,522 O
individual O
matches B
. O

Each O
Tribunal O
case O
represents O
a O
single O
player B
and O
includes O
up O
to O
5 O
matches B
in O
which O
he O
was O
accused O
of O
toxic O
behavior O
. O

In O
LoL B
players I
can O
communicate O
via O
chat B
, O
which O
is O
ostensibly O
used O
to O
share O
strategic O
plans O
and O
other O
important O
information O
during O
the O
game B
. O

However O
, O
chat B
is O
also O
a O
prime O
vector O
for O
exhibiting O
toxic O
behavior O
. O

Thus O
, O
although O
1 O
http://leagueoflegends.com O
Linguistic O
Analysis O
of O
Toxic O
Behavior O
in O
an O
Online O
Video O
Game O
3 O
0.00 O
0.01 O
0.02 O
0.03 O
0 O
25 O
50 O
75 O
100 O
Normalized O
Time O
PDF O
offender O
typical O
Fig O
. O

1 O
. O

Change O
of O
chat B
volume I
during O
a O
match B
. O

a O
variety O
of O
information O
is O
presented O
to O
Tribunal O
reviewers O
[ O
2 O
] O
, O
in O
this O
paper O
we O
focus B
exclusively O
on O
the O
in B
- I
game I
chat B
logs O
. O

We O
extract O
24,039,184 O
messages B
from O
toxic B
players O
and I
33,252,018 I
messages I
from O
typical B
players I
. O

Because O
the O
teammates B
of O
toxic B
players I
are O
directly O
impacted B
by O
toxic B
playing O
and I
readily I
express I
aggressive O
reactions O
to O
a O
toxic B
player I
, O
we O
define O
typical B
players I
as O
the O
set O
of O
players B
on O
the O
opposite B
team I
when O
none O
of O
them O
report B
the O
toxic B
player I
. O

Before O
continuing O
, O
we O
report B
some O
basic O
statistics O
about O
the O
size O
of O
vocabulary O
and O
the O
length O
of O
messages B
. O

We O
found O
1,042,940 O
unique O
tokens O
in O
toxic B
player I
messages B
and O
1,176,356 O
unique O
tokens O
in O
typical B
player I
messages B
. O

While O
typical B

 O
players I
send O
38 O
% O
more O
messages B
than O
toxic B
players I
, O
the O
messages B
are O
composed O
of O
only O
13 O
% O
more O
unique O
tokens O
. O

Interestingly O
, O
we O
find O
that O
toxic B
players I
send O
longer O
messages B
than O
typical B
players I
; O
the O
average O
number O
of O
words O
per O
message B
is O
3.139 O
and O
2.732 O
for O
toxic B
and I
typical I
players I
, O
respectively O
. O

3 O
Chat O
Volume O
over O
a O
Match B
We O
begin O
our O
analysis O
by O
exploring O
chat B
volume I
over O
time B
. O

A O
LoL B
match I
can O
be O
broken O
up O
into O
logical O
stages B
. O

First O
is O
the O
early B
game I
( O
also O
known O
as O
the O
“ O
laning B
phase O
” O
) O
, O
where O
characters B
are O
low O
level B
and O
weak B
. O

In O
the O
early B
game I
, O
players B
expend O
great O
effort O
towards O
“ O
farming B
” O
computer O
controlled B
minions B
to O
gain O
experience B
and O
gold B
, O
with O
aggressive B
plays I
against O
the O
other O
team B
usually O
coming O
as O
the O
result O
of O
an O
over O
extension O
or O
other O
mistake B
. O

As O
players B
earn O
gold B
and O
experience B
, O
they O
level B
up O
and O
become O
stronger O
, O
and O
the O
match B
transitions O
to O
the O
mid B
game I
. O

During O
the O
mid B
game I
, O
players B
become O
more O
aggressive O
and O
tend O
to O
group B
up O
with O
teammates B
to O
make O
plays B
on O
their O
opponents B
. O

Finally O
, O
once O
players B
are O
reaching O
their O
maximum O
power B
levels B
, O
the O
match B
transitions O
into O
the O
end B
game I
, O
where O
teams B
will O
group B
together O
and O
make O
hard B
pushes O
towards O
taking O
objectives B
and O
winning O
the O
match B
. O

4 O
Haewoon O
Kwak O
and O
Jeremy O
Blackburn O
While O
these O
phases O
are O
not O
dictated O
by O
the O
programming O
of O
LoL B
, O
and O
thus O
there O
is O
no O
hard B
cut O
off O
point B
for O
when O
the O
transitions O
between O
phases O
occur O
, O
we O
suspect O
that O
each O
phase O
has O
an O
associated O
pattern O
of O
communication B
. O

Thus O
, O
in O
Figure O
1 O
we O
plot O
the O
density O
of O
chat B
messages B
written O
by O
toxic B
and I
typical I

 O
players I
as O
a O
function O
of O
the O
normalized O
time B
during O
a O
match B
. O

The O
plot O
confirms O
our O
suspicions O
: O
communication B
is O
not O
uniform O
throughout O
the O
match B
. O

Instead O
, O
we O
see O
three O
distinct O
levels B
of O
communication B
, O
likely O
corresponding O
to O
the O
three O
phases O
of O
a O
match B
, O
with O
relative O
peaks O
at O
the O
beginning O
and O
end O
of O
the O
match B
. O

This O
finding O
can O
be O
explained O
with O
a O
deeper O
understanding O
of O
how O
a O
LoL B

 O
match I
progresses O
. O

As O
mentioned O
above O
, O
in O
the O
early B
game I
players B
are O
relatively O
weak B
and O
must O
focus B
on O
farming B
for O
resources B
. O

Early B
game I
farming B
occurs O
via O
players B
choosing O
one O
of O
three O
lanes B
to O
spend O
their O
time B
in O
. O

The O
lanes B
are O
quite O
far O
from O
each O
other O
on O
the O
map O
( O
10 O
+ O
seconds O
or O
so O
to O
travel O
between O
them O
) O
and O
thus O
players B
on O
the O
same O
team B
tend O
to O
be O
relatively O
isolated O
from O
each O
other O
. O

To O
take O
advantage B
of O
this O
isolation O
, O
and O
to O
get O
an O
early B
lead B
, O
players B
might O
roam O
from O
the O
lane B
they O
chose O
to O
play B
in O
to O
another O
lane B
. O

In O
turn B
, O
this O
provides O
their O
teammate B
in O
the O
other O
lane B
with O
a O
numbers O
advantage B
over O
opposing O
player B
in O
the O
lane B
. O

Colloquially O
, O
this O
roaming O
to O
provide O
a O
temporary O
numbers O
advantage B
is O
known O
as O
a O
“ O
gank B
. O

” O
To O
deal O
with O
ganks B
in O
the O
early B
game I
, O
players B
tend O
to O
communicate O
via O
chat B
when O
the O
opposing O
player B
in O
their I
lane I
has O
gone O
missing O
. O

As O
the O
match B
transitions O
to O
mid B
game I
, O
teammates B
start B
grouping B
up O
. O

Since O
they O
are O
no O
longer O
so O
isolated O
the O
fear O
of O
ganks B
dissipates O
, O
and O
the O
need O
to O
communicate O
missing O
players B
diminishes O
. O

Additionally O
, O
since O
teammates B
are O
grouped B
together O
, O
they O
are O
seeing O
the O
same O
portion O
of O
the O
map O
, O
and O
there O
is O
not O
really O
that O
much O
additional O
information O
they O
can O
convey O
to O
each O
other O
. O

Finally O
, O
as O
late B
game I
comes O
around O
, O
teams B
must O
focus B
and O
work O
together O
to O
complete O
objectives B
and O
win O
the O
match B
. O

In O
practice O
, O
this O
might O
involve O
coming O
to O
agreement O
on O
a O
final O
“ O
push O
” O
for O
an O
objective B
, O
or O
agreeing O
on O
which O
lane B
the O
team B
should O
travel O
down O
. O

Also O
, O
there O
are O
some O
customs O
in B
e O
- I
sports I
, O
saying O
‘ O
gg O
( O
good B

 O
game I
) O
’ O
at O
the O
end O
of O
the O
game B
. O

The O
sharp O
spikes O
contain O
those O
messages B
as O
well O
. O

While O
this O
might O
explain O
some O
of O
the O
spike O
seen O
at O
the O
end O
of O
Figure O
1 O
another O
, O
simpler O
explanation O
is O
that O
players B
are O
simply O
communicating O
their O
( O
dis)pleasure O
in O
winning O
or O
losing O
the O
match B
. O

A O
more O
interesting O
finding O
is O
the O
subtle O
difference O
in O
the O
distributions O
of O
typical B
and I
toxic I
players I
. O

At O
the O
early B
stage B
we O
see O
more O
active O
communication B
by O
normal B
players I
. O

We O
suppose O
that O
it O
includes O
all O
the O
messages B
for O
ice O
breaking O
or O
cheering O
( O
e.g. O
gl O
( O
good O
luck O
) O
or O
hf O
( O
have O
fun O
) O
) O
. O

However O
, O
at O
some O
point B
after O
the O
short O
period O
, O
toxic B
players I
begin O
to O
chat B
more O
than O
typical B
players O
and I
keep I

 I
such O
pattern O
until O
by O
the O
last O
stage B
. O

At O
the O
last O
stage B
of O
the O
match B
, O
typical B

 O
players I
again O
chat B
more O
socially O
, O
for O
example O
, O
sending O
smile O
emoticons O
, O
which O
are O
:D O
or O
:) O
, O
and O
also O
saying O
gg O
, O
as O
we O
mentioned O
. O

The O
transition O
point B
, O
where O
the O
distribution O
of O
toxic B
players I
cross O
over O
that O
of O
typical B
players I
, O
is O
a O
basis O
of O
our O
further O
analysis O
in O
Section O
5 O
. O

Linguistic O
Analysis O
of O
Toxic O
Behavior O
in O
an O
Online O
Video O
Game O
5 O
Fig O
. O

2 O
. O

Top O
10 O
discriminative O
uni- O
and O
bi O
- O
grams O
4 O
Discriminative O
Words O
of O
Toxic O
and O
Typical O
Players O
The O
linguistic O
approach O
to O
the O
chat B
log O
characterizes O
toxic B
players I
with O
context O
. O

We O
conduct O
n O
- O
gram O
analysis O
because O
it O
is O
intuitive O
and O
straightforward O
. O

We O
filter O
the O
stopwords O
and O
then O
count O
the O
frequency O
of O
uni- O
and O
bi O
- O
grams O
from O
the O
chat B
log O
involving O
toxic O
reports B
of O
either O
verbal O
abuse O
or O
offensive O
language O
. O

In O
order O
to O
find O
discriminative O
n O
- O
grams O
of O
toxic B
players I
we O
need O
a O
reference O
for O
comparison O
. O

We O
conduct O
the O
same O
n O
- O
gram O
analysis O
from O
enemy B
’s O
chat B
log O
when O
verbal O
abuse O
or O
offensive O
language O
is O
not O
reported B
from O
the O
enemies B
. O

We O
consider O
it O
as O
a O
normal O
conversation O
among O
players B
and O
call B
those O
enemies B
typical B

 O
players I
. O

We O
create O
the O
top O
1,000 O
uni- O
and O
bi O
- O
grams O
for O
toxic B
and I
typical I
players I
, O
respectively O
. O

We O
find O
867 O
uni- O
and O
748 O
bi O
- O
grams O
in O
common O
. O

Then O
we O
obtain O
133 O
non O
- O
overlapped O
uni- O
and O
252 O
bi O
- O
grams O
for O
toxic B
and I
typical I
players I
; O
they O
appear O
only O
in O
either O
toxic O
or O
typical B
players I
. O

We O
define O
them O
as O
discriminative O
uni- O
and O
bi O
- O
gram O
for O
toxic B
and I
typical I
players I
, O
respectively O
. O

Figure O
2 O
shows O
top O
10 O
discriminative O
uni- O
and O
bi O
- O
grams O
of O
toxic B
and I
typical I

 O
players I
. O

Top O
10 O
discriminative O
uni- O
and O
bi O
- O
grams O
of O
toxic B
players I
are O
filled O
with O
bad O
words O
. O

That O
is O
, O
Riot O
Games O
does O
not O
offer O
even O
the O
basic O
level B
of O
bad O
word O
filtering O
, O
and O
such O
bad O
words O
can O
be O
used O
as O
the O
signatures O
of O
toxic B
players I
who O
used O
verbal O
abuse O
or O
offensive O
language O
. O

We O
find O
that O
several O
discriminative O
bi O
- O
grams O
of O
typical B
players I
are O
about O
strategies B
, O
while O
most O
of O
toxic B
players I
’ O
bigrams O
are O
bad O
words O
. O

We O
note O
that O
some O
variations O
of O
‘ O
fucking O
’ O
are O
discriminative O
uni O
- O
grams O
but O
‘ O
fucking O
’ O
itself O
is O
not O
. O

It O
means O
that O
‘ O
fucking O
’ O
is O
often O
used O
not O
only O
by O
toxic B
players I
but O
also O
by O
typical B
players I
as O
well O
. O

This O
shows O
the O
difficulties O
of O
filtering O
bad O
words O
by O
a O
simple O
dictionary O
- O
based B
approach O
. O

As O
the O
next O
step O
of O
the O
linguistic O
approach O
, O
we O
are O
interested O
in O
when O
verbal O
abuse O
occurs O
from O
a O
temporal O
perspective O
during O
a O
match B
. O

We O
divide O
252 O
discriminative O
bi O
- O
grams O
of O
toxic B
players I
into O
three O
classes B
, O
early- O
, O
mid- O
, O
and O
latebi O
- O
grams O
, O
based B
on O
when O
their O
highest O
frequencies O
occur O
. O

Figure O
3 O
presents O
an O
example O
of O
three O
temporal O
classes B
of O
bi O
- O
grams O
. O

Interestingly O
, O
209 O
( O
82.94 O
% O
) O
out O
of O
252 O
bi O
- O
grams O
are O
late O
- O
bi O
- O
gram O
. O

The O
early B
- O
bi O
- O
gram O
“ O
ill O
6 O
Haewoon O
Kwak O
and O
Jeremy O
Blackburn O
0.00 O
0.25 O
0.50 O
0.75 O
1.00 O
0.00 O
0.25 O
0.50 O
0.75 O
1.00 O
Normalized O
time B
Scaled O
density O
ill O
feed O
fucking O
bot O
report B
noob I
Fig O
. O

3 O
. O

Example O
of O
early- O
, O
mid- O
, O
and O
late O
- O
bi O
- O
gram O
feed O
” O
is O
a O
domain O
specific O
example O
of O
toxic O
behavior O
. O

In O
LoL B
, O
one O
of O
the O
ways O
players B
earn O
gold B
and O
experience B
during O
a O
match B
is O
by O
killing B
players B
on O
the O
opposite B
team I
. O

Intentional O
feeding O
is O
when O
a O
player B
deliberately O
allows O
the O
other O
team B
to O
kill B
them O
, O
thus O
“ O
feeding O
” O
the O
enemies B
with O
gold B
and O
experience B
, O
in O
turn B
allowing O
them O
to O
become O
quite O
powerful B
. O

The O
mid O
- O
bi O
- O
gram O
“ O
fucking O
bot O
” O
is O
the O
toxic B
player I
expressing O
his O
displeasure O
for O
the O
performance O
of O
the O
bottom B
lane I
. O

The O
bottom B
lane I
is O
usually O
manned O
by O
characters B
that O
have O
a O
primarily O
late B
- O
game I
presence I
, O
and O
thus O
being O
behind O
during O
the O
mid B
- O
game I
has I
a O
significant O
impact B
on O
the O
remainder O
of O
the O
match B
. O

Most O
verbal O
abuse O
of O
toxic B
players I
occurs O
in O
the O
late O
stage B
of O
the O
game B
. O

For O
example O
, O
“ O
report B
noob I
” O
is O
the O
toxic B
player I
requesting O
that O
the O
rest O
of O
his O
team B
report B
a O
player B
( O
the O
“ O
noob B
” O
) O
that O
he O
singled O
out O
for O
his O
ire O
. O

We O
believe O
the O
most O
likely O
explanation O
for O
this O
is O
that O
verbal O
abuse O
is O
most O
likely O
a O
response O
to O
losing O
a O
game B
, O
which O
is O
often O
not O
apparent O
until O
the O
late B
- O
game I
. I
For O
example O
, O
consider O
a O
scenario O
where O
one O
player B
on O
the O
team B
has O
a O
bad B
game I
, O
perhaps O
making O
poor B
decisions O
resulting O
in O
the O
enemy B
team I
becoming O
quite O
strong O
. O

In O
the O
early- O
, O
and O
even O
mid B
- O
game I
phases I
, O
a O
toxic B
player I
might O
still O
be O
able O
to O
hold O
his O
own O
, O
however O
, O
when O
the O
enemy B
team I
groups B
up O
and O
makes O
coordinated O
pushes O
in O
the O
late B
- O
game I
, I
their O
relative O
strength B
will O
often O
result O
in O
quick O
and O
decisive O
victories B
in O
teamfights B
. O

If O
toxic O
playing O
can O
be O
detected O
in B
real O
- I
time I
, O
we O
could O
protect O
potential O
victims B
from O
verbal O
violence O
, O
for O
example O
via O
alerts B
or O
simply O
not O
delivering O
such O
messages B
. O

Temporal O
dynamics O
of O
bi O
- O
grams O
might O
help O
to O
create O
a O
mental O
model O
of O
toxic B

 O
players I
. O

For O
instance O
, O
10 O
bi O
- O
grams O
containing O
‘ O
bot O
’ O
are O
divided O
into O
1 O
early B
- O
bigram O
, O
5 O
mid O
- O
bi O
- O
grams O
, O
and O
4 O
late O
- O
bi O
- O
grams O
. O

Through O
manual O
inspection O
, O
we O
confirm O
that O
the O
early B
- O
bi O
- O
gram O
( O
‘ O
go O
bot O
’ O
) O
is O
strategic O
and O
non O
- O
aggressive O
, O
the O
mid O
- O
bigrams O
are O
cursing O
, O
and O
the O
late O
- O
bi O
- O
grams O
are O
blaming O
the O
result O
of O
the O
match B
on O
the O
bot O
player(s O
) O
. O

This O
provides O
us O
with O
a O
rough O
idea O
of O
how O
toxic B
players I
might O
behave O
and O
think O
over O
time B
: O
initially O
they O
have O
a O
similar O
mindset O
as O
typical O
play- O
Linguistic O
Analysis O
of O
Toxic O
Behavior O
in O
an O
Online O
Video O
Game O
7 O
Fig O
. O

4 O
. O

Time B
difference O
of O
last O
used O
time B
of O
uni O
- O
gram O
ers O
, O
but O
, O
as O
the O
game B
plays B
out O
contrary O
to O
their O
desires O
, O
they O
grow O
increasingly O
aggressive O
, O
eventually O
lashing O
out O
with O
purely O
abusive B
language I
. O

We O
leave O
more O
sophisticated O
modeling O
of O
toxic B
players I
’ O
thought O
process O
as O
future O
work O
. O

5 O
Phase O
Transition O
of O
Toxic O
Players O
In O
the O
previous O
section O
we O
recognize O
which O
words O
are O
exclusively O
used O
by O
toxic B

 O
and I
normal I
players I
. O

However O
, O
some O
words O
are O
used O
by O
both O
toxic B
players O
and I

 I
normal I
players B
. O

For O
these O
, O
the O
emerging O
patterns O
in O
a O
temporal O
sense O
could O
be O
quite O
different O
. O

If O
we O
assume O
that O
toxic B
players I
exhibit O
toxic O
behavior O
in O
reaction O
to O
certain O
events O
happening O
during O
the O
match B
, O
then O
the O
linguistic O
behavior O
of O
such O
toxic B
players I
should O
be O
the O
same O
as O
typical B
players I
before O
those O
events O
happen O
. O

To O
validate O
the O
above O
hypothesis O
, O
we O
conduct O
the O
following O
experiment O
which O
is O
focused B
on O
finding O
some O
words O
that O
are O
not O
used O
after O
some O
time B
point B
by O
toxic B
players I
, O
while O
they O
are O
continuously O
used O
by O
normal B
players I
. O

We O
extract O
the O
top O
30 O
uni O
- O
gram O
at O
every O
normalized O
time B
unit B
( O
ranging B
from O
0 O
to O
100 O
) O
for O
toxic B
players O
and I
normal I
players I
, O
respectively O
. O

Since O
top O
30 O
uni O
- O
grams O
are O
quite O
stable O
during O
the O
match B
, O
we O
obtain O
unique O
80 O
uni O
- O
gram O
for O
toxic B
players O
and I
91 I

 I
uni O
- O
grams O
for O
normal B
players I
. O

We O
first O
observe O
that O
toxic B
players I
have O
slightly O
smaller O
vocabularies O
than O
that O
by O
normal B
players I
. O

For O
each O
of O
these O
uni O
- O
grams O
, O
we O
compute O
the O
normalized O
time B
of O
last O
use O
by O
toxic B
players O
and I
normal I
players I
, O
respectively O
. O

Finally O
, O
we O
compute O
the O
difference O
of O
the O
last O
used O
time B
between O
toxic B
and I
normal I
players I
for O
the O
common O
uni O
- O
grams O
. O

Figure O
4 O
lists O
the O
uni O
- O
grams O
with O
a O
time B
difference O
greater O
than O
30 O
. O

I.e. O
, O
words O
in O
the O
list O
are O
used O
later O
into O
the O
match B
by O
normal B
players I
. O

Some O
interesting O
patterns O
are O
present O
in O
the O
results O
. O

First O
, O
emoticons O
, O
particularly O
smile O
emoticons O
, O
are O
almost O
never O
used O
by O
toxic B

 O
players I
. O

Second O
, O
apologies O
( O
e.g. O
, O
‘ O
sorry O
’ O
) O
are O
also O
exclusively O
used O
by O
normal B
players I
. O

Third O
, O
some O
words O
for O
strategic O
team B
maneuvers O
( O
e.g. O
, O
‘ O
come O
’ O
, O
‘ O
ult O
’ O
, O
‘ O
blue B
’ O
, O
‘ O
ward B
’ O
) O
are O
used O
by O
toxic B
players I
, O
but O
this O
ceases O
at O
some O
point B
during O
the O
match B
. O

Fourth O
, O
some O
words O
primarily O
used O
for O
communicating O
movements B
with O
partners O
in O
the O
same O
lane B
( O
e.g. O
‘ O
back O
’ O
, O
‘ O
b O
’ O
, O
‘ O
brb O
’ O
( O
be O
right O
back O
) O
, O
‘ O
omw O
’ O
( O
on O
my O
way O
) O
, O
‘ O
k O
’ O
8 O
Haewoon O
Kwak O
and O
Jeremy O
Blackburn O
( O
okay O
) O
) O
are O
also O
used O
by O
toxic B
players I
, O
but O
again O
, O
after O
some O
point B
toxic B
players I
stop O
this O
form O
of O
communication B
. O

Fifth O
, O
toxic B
players I
stop O
praising O
( O
e.g. O
, O
‘ O
gj O
’ O
( O
good O
job O
) O
) O
their O
teammates B
after O
some O
point B
in O
time B
. O

All O
these O
findings O
reveal O
how O
toxicity B
is O
born O
during O
a O
match B
. O

It O
seems O
to O
be O
a O
kind O
of O
phase B
transition I
. O

They O
behave O
the O
same O
as O
normal B
players I
during O
the O
early B
stage B
of O
the O
match B
, O
but O
at O
some O
point B
they O
change O
their O
behavior O
. O

After O
some O
point B
, O
they O
utter O
neither O
apologies O
nor O
praise O
to O
express O
their O
feelings O
, O
and O
also O
stop O
strategic O
communication B
with O
team B
members I
. O

By O
combining O
this O
finding O
with O
discriminative O
words O
of O
toxic B
players I
, O
we O
see O
the O
possibility O
for O
detecting O
a O
certain O
point B
that O
a O
player B
transitions O
to O
be O
toxic O
without O
using O
detailed O
in B
- I
game I
action O
logs O
, O
but O
just O
chat B
logs O
. O

Thus O
, O
linguistic O
analysis O
of O
toxic B
players I
shows O
not O
just O
how O
different O
they O
are O
and O
when O
they O
become O
different O
as O
well O
. O

6 O
Conclusion O
and O
Future O
work O
In O
this O
work O
we O
have O
examined O
crowdsourced O
data O
from O
590 O
thousand O
cases O
of O
accused O
toxic B
players I
in O
a O
popular O
match B
- O
based B
competition B
game B
, O
League O
of O
Legends O
. O

We O
have O
performed O
a O
series O
of O
linguistic O
analyses O
to O
gain O
a O
deeper O
understanding O
of O
the O
role B
communication B
plays B
in O
the O
expression O
of O
toxic O
behavior O
. O

We O
have O
several O
interesting O
findings O
: O
a O
bi O
- O
modal O
distribution O
of O
chats B
during O
a O
match B
, O
a O
difference O
between O
temporal O
chat B
patterns O
between O
toxic B
and I
typical I

 O
players I
, O
a O
list O
of O
discriminative O
uni- O
and O
bi O
- O
grams O
used O
by O
typical B
and I
toxic I
players I
as O
signatures O
of O
them O
, O
temporal O
patterns O
of O
the O
linguistic O
signature O
of O
toxic B

 O
players I
, O
and O
a O
possible O
footprint O
of O
transitions O
from O
typical O
behavior O
to O
toxic O
behavior O
. O

Our O
findings O
would O
be O
helpful O
to O
automatically O
detect O
and O
warn O
players B
who O
may O
turn B
to O
be O
toxic O
and O
thus O
save O
potential O
victims B
of O
toxic O
playing B
in O
advance O
. O

Finally O
, O
we O
suggest O
several O
directions O
for O
future O
work O
. O

First O
, O
is O
focusing B
on O
interaction O
between O
typical B
and I
toxic I
players I
. O

In O
this O
work O
the O
unit B
of O
our O
analysis O
is O
a O
message B
, O
but O
we O
do O
not O
delve O
into O
the O
flow O
of O
messages B
. O

Interaction O
analysis O
could O
reveal O
more O
clear B
narratives O
of O
how O
a O
player B
transitions O
to O
toxic O
behavior O
. O

Next O
, O
is O
building O
a O
pre O
- O
warning O
system O
to O
detect O
toxic O
playing B
earlier O
. O

The O
main O
challenge O
here O
is O
to O
build O
a O
dictionary O
of O
words O
that O
are O
signs O
of O
toxic O
playing O
. O

As O
we O
have O
seen O
a O
list O
of O
discriminative O
uni- O
and O
bi O
- O
grams O
of O
toxic B
and I
typical I

 O
players I
, O
some O
bad O
words O
are O
also O
used O
by O
typical B
players I
as O
well O
. O

This O
behavior O
is O
prevalent O
in O
“ O
trash B
talk I
” O
culture O
, O
and O
an O
important O
factor O
in O
immersing O
players B
in O
a O
competitive B
game I
[ O
5 O
] O
. O

Thus O
, O
any O
pre O
- O
warning O
system O
must O
be O
effective O
in O
detecting O
toxic O
playing O
while O
being O
flexible O
enough O
to O
allow O
for O
trash B
talk I
to O
avoid O
breaking O
the O
immersive O
gaming O
experience B
. O

We O
believe O
that O
the O
signature O
of O
toxic B
and I
typical I
players I
we O
found O
is O
a O
first O
step O
for O
building O
the O
dictionary O
for O
a O
pre O
- O
warning O
system O
. O

Linguistic O
Analysis O
of O
Toxic O
Behavior O
in O
an O
Online O
Video O
Game O
9 O
References O
1 O
. O

J. O
Barli´nska O
, O
A. O
Szuster O
, O
and O
M. O
Winiewski O
. O

Cyberbullying O
among O
adolescent O
bystanders O
: O
role B
of O
the O
communication B
medium O
, O
form O
of O
violence O
, O
and O
empathy O
. O

Journal O
of O
Community O
& O
Applied O
Social O
Psychology O
, O
23(1):37–51 O
, O
2013 O
. O

2 O
. O

J. O
Blackburn O
and O
H. O
Kwak O
. O

Stfu O
noob B
! O
: O
Predicting O
crowdsourced O
decisions O
on O
toxic O
behavior O
in O
online B
games I
. O

In O
Proceedings O
of O
the O
23rd O
International O
Conference O
on O
World O
Wide O
Web O
, O
WWW O
’ O
14 O
, O
pages O
877–888 O
, O
2014 O
. O

3 O
. O

V. O
H.-H. O
Chen O
, O
H. O
B.-L. O
Duh O
, O
and O
C. O
W. O
Ng O
. O

Players B
who O
play B
to O
make O
others O
cry O
: O
The O
influence O
of O
anonymity O
and O
immersion O
. O

In O
Proceedings O
of O
the O
International O
Conference O
on O
Advances O
in O
Computer O
Enterntainment O
Technology O
, O
ACE O
’ O
09 O
, O
pages O
341–344 O
, O
2009 O
. O

4 O
. O

T. O
Chesney O
, O
I. O
Coyne O
, O
B. O
Logan O
, O
and O
N. O
Madden O
. O

Griefing B
in O
virtual O
worlds B
: O
causes O
, O
casualties O
and O
coping O
strategies B
. O

Information O
Systems O
Journal O
, O
19(6):525–548 O
, O
2009 O
. O

5 O
. O

O. O
B. O
Conmy O
. O

Trash O
Talk O
in O
a O
Competitive B
Setting I
: O
Impact B
on O
Self O
- O
efficacy O
, O
Affect O
, O
and O
Performance O
. O

ProQuest O
, O
2008 O
. O

6 O
. O

H. O
Lin O
and O
C.-T. O
Sun O
. O

The O
” O
white O
- O
eyed O
” O
player B
culture O
: O
Grief O
play B
and O
construction O
of O
deviance O
in O
mmorpgs O
. O

In O
Proceedings O
of O
DiGRA O
2005 O
Conference O
, O
2005 O
. O

7 O
. O

J. O
Suler O
. O

The O
online O
disinhibition O
effect B
. O

Cyberpsychology O
& O
behavior O
, O
7(3):321–326 O
, O
2004 O
. O

8 O
. O

P. O
Thompson O
. O

What O
s O
fueling O
the O
flames B
in O
cyberspace O
? O
a O
social O
influence O
model O
. O

Communication B
and O
cyberspace O
: O
Social O
interaction O
in O
an O
electronic O
environment O
, O
pages O
293–311 O
, O
1996 O
. O

Player O
Behavior O
and O
Optimal O
Team O
Composition O
in O
Online O
Multiplayer O
Games O
Hao O
Yi O
Ong1 O
, O
Sunil O
Deolalikar2 O
and O
Mark O
V. O
Peng3 O
Abstract O
— O
We O
consider O
clustering O
player B
behavior I
and O
learning O
the O
optimal O
team B
composition I
for O
multiplayer O
online B
games I
. O

The O
goal B
is O
to O
determine O
a O
set O
of O
descriptive O
play B
style I
groupings O
and O
learn O
a O
predictor O
for O
win O
/ O
loss O
outcomes O
. O

The O
predictor O
takes O
in O
as O
input B
the O
play B
styles I
of O
the O
participants O
in O
each O
team B
; O
i.e. O
, O
the O
various O
team B
compositions I
in O
a O
game B
. O

Our O
framework O
uses O
unsupervised O
learning O
to O
find O
behavior O
clusters O
, O
which O
are O
, O
in O
turn B
, O
used O
with O
classification O
algorithms O
to O
learn O
the O
outcome O
predictor O
. O

For O
our O
numerical O
experiments O
, O
we O
consider O
League O
of O
Legends O
, O
a O
popular O
team B
- O
based I
role I
- I
playing I
game I
developed I
by O
Riot O
Games O
. O

We O
observe O
the O
learned O
clusters O
to O
not O
only O
corroborate O
well O
with O
game B
knowledge I
, O
but O
also O
provide O
insights O
surprising O
to O
expert B
players I
. O

We O
also O
demonstrate O
that O
game B
outcomes O
can O
be O
predicted O
with O
fairly O
high O
accuracy O
given O
team B

 O
composition I
- O
based B
features O
. O

Index O
Terms O
— O
team B
performance I
, O
team B
composition I
, O
player B

 O
behavior I
, O
video B
games I
, O
multiplayer O
games B
, O
game B
prediction O
I. O
INTRODUCTION O
Online O
virtual O
worlds B
are O
an O
increasingly O
significant O
venue O
for O
human O
interaction O
. O

By O
far O
the O
most O
active O
virtual O
worlds B
belong O
to O
a O
genre O
of O
video B
games I
called B
massively O
multiplayer O
online O
role B
- O
playing B
games B
( O
MMORPGs O
) O
, O
where O
players B
interact O
with I
each I
other I
in O
a O
virtual O
world B
[ O
1 O
] O
. O

In O
an O
MMORPG O
, O
players B
assume O
the O
role B
of O
in B
- I
game I
characters B
and O
take O
control B
over O
most O
of O
their O
characters B
’ O
actions O
, O
often O
working O
in O
teams B
to O
accomplish O
a O
common O
objective B
, O
such O
as O
defeating B
opposing B
teams I
. O

Due O
to O
the O
shared O
, O
persistent O
nature O
of O
these O
virtual O
worlds B
, O
user B
behaviors O
and O
experiences B
are O
shaped O
by O
various O
social O
factors O
. O

Besides O
profit O
- O
making O
, O
an O
understanding O
of O
these O
social O
dynamics O
would O
provide O
insight O
to O
human O
interactions O
in O
the O
real O
world B
and O
the O
potential O
of O
virtual O
worlds B
for O
education O
, O
training O
, O
and O
scientific O
research O
[ O
2 O
] O
, O
[ O
3 O
] O
. O

Numerous O
prior O
studies O
in O
social O
sciences O
and O
management O
have O
investigated O
how O
team B
compositions I
can O
affect O
team B
performance I
[ O
4 O
] O
, O
[ O
5 O
] O
. O

However O
, O
little O
is O
understood O
about O
player B
behavior I
and O
team B
performance I
and O
factors O
contributing O
to O
it O
in O
competitive B
MMORPGs O
. O

To O
address O
this O
, O
we O
develop O
a O
machine O
learning O
framework O
that O
uses O
game B
histories O
to O
learn O
player B

 O
behavior I
clusters O
and O
predict O
the O
outcome O
of O
games B
given O
prior O
knowledge O
about O
the O
game B
and O
its O
players B
. O

The O
contributions O
of O
this O
paper O
are O
twofold O
. O

First O
, O
we O
present O
several O
approaches O
that O
group B
player B
behaviors I
in O
online B
games I
. O

Second O
, O
we O
develop O
predictors O
that O
determine O
how O
likely O
it O
is O
that O
a O
team B
of O
players B
can O
emerge O
victorious O
1Mechanical O
Engineering O
Department O
, O
Stanford O
University O
2Aeronautics O
and O
Astronautics O
Department O
, O
Stanford O
University O
3Computer O
Science O
Department O
, O
Stanford O
University O
Email O
: O
{ O
haoyi,sunild93,mvpeng}@stanford.edu O
given O
the O
said O
team B
’s O
composition I
of O
players B
, O
all O
of O
whom O
may O
have O
different O
play B
styles I
. O

Specifically O
, O
we O
consider O
kmeans O
and O
DP O
- O
means O
— O
an O
expectation O
maximization O
algorithm O
[ O
6]—for O
clustering O
play B
styles I
and O
logistic O
regression O
( O
LR O
) O
, O
Gaussian O
discriminant O
analysis O
( O
GDA O
) O
, O
and O
support B
vector O
machines O
( O
SVMs O
) O
for O
determining O
win O
/ O
loss O
outcomes O
. O

The O
rest O
of O
the O
paper O
is O
structured O
as O
follows O
. O

Section O
II O
describes O
the O
target B
game I
of O
our O
numerical O
experiments O
and O
our O
data O
collection O
method O
. O

Sections O
III O
and O
IV O
demonstrate O
several O
methods O
and O
their O
effectiveness O
for O
learning O
play B
style I
clusters O
and O
outcome O
predictors O
. O

Some O
concluding O
remarks O
are O
drawn O
and O
future O
works O
mentioned O
in O
Section O
V. O
II O
. O

TARGET O
GAME O
DESCRIPTION O
We O
begin O
with O
a O
description O
of O
the O
MMORPG O
used O
for O
our O
numerical O
experiments O
and O
the O
data O
acquisition O
method O
. O

A. O
League O
of O
Legends O
For O
this O
project O
we O
consider O
a O
popular O
MMORPG O
— O
the O
League O
of O
Legends O
( O
LoL O
) O
. O

LoL O
is O
a O
multiplayer O
online O
battle B
arena O
video B
game I
developed O
and O
published O
by O
Riot O
Games O
with O
27 O
million O
daily O
players B
[ O
7 O
] O
. O

Furthermore O
, O
LoL B
is O
a O
representative O
MMORPG O
of O
its O
genre O
, O
with O
many O
similar O
counterparts O
such O
as O
World O
of O
Warcraft O
’s O
Dota O
2 O
[ O
8]—giving O
us O
a O
measure O
of O
generalizability O
to O
other O
games B
in O
its O
genre O
. O

In O
this O
MMORPG O
, O
a O
standard O
game B
consists O
of O
two O
opposing B

 O
teams I
of O
five O
players B
. O

Each O
player B
assumes O
the O
role B
of O
one O
of O
over O
120 O
different O
characters B
battling B
each O
other O
to O
destroy O
the O
opposing B
team I
’s O
“ O
towers”—structures O
that O
fall B
after O
suffering O
enough O
attacks B
from O
characters B
. O

A O
game B
is O
won O
when O
all O
of O
either O
team B
’s O
towers B
are O
destroyed O
. O

B. O
Data O
set O
acquisition O
The O
developer O
of O
LoL O
has O
made O
the O
game B
’s O
player B
statistics I
and O
match B
histories O
freely O
available O
through O
a O
web O
- O
based B
application O
programming O
interface B
( O
API O
) O
[ O
9 O
] O
. O

We O
randomly O
gathered O
over O
100,000 O
instances O
of O
player B
statistics I
and O
over O
10,000 O
instances O
of O
match B
histories O
from O
the O
2013 O
- O
2014 O
season B
. O

We O
then O
parsed O
and O
cleaned O
the O
raw O
game B
data O
to O
construct O
our O
training O
and O
testing O
sets O
, O
depending O
on O
the O
features O
we O
chose O
. O

Player B
statistics I
include O
performance O
indicators O
such O
as O
average B
damage I
dealt O
and O
number O
of O
wins O
. O

Match B
histories O
contain O
information O
such O
as O
participant O
ID O
numbers O
and O
character B
choices O
. O

III O
. O

BEHAVIORAL O
CLUSTERING O
The O
target B
game I
’s O
developers O
have O
grouped B
the O
120 O
different O
in B
- I
game I
characters B
into O
six O
classes B
, O
such O
as O
assassin B
1 O
arXiv:1503.02230v1 O
[ O
cs O
. O

SI O
] O
8 O
Mar O
2015 O
or O
support B
, O
which O
indicates O
the O
character B
’s O
gameplay B
style I
. O

While O
these O
classes B
reflect O
the O
developers O
’ O
design O
intent O
for O
the O
characters B
, O
they O
do O
not O
necessarily O
reveal O
the O
behavior O
of O
actual O
players B
in O
games B
. O

Using O
statistics O
from O
various O
players B
, O
we O
present O
our O
feature O
selection O
method O
and O
the O
gameplay B

 O
styles I
learned O
by O
applying O
various O
clustering O
algorithms O
to O
our O
data O
set O
. O

We O
validate O
our O
results O
and O
the O
insights O
derived O
from O
it O
with O
expert O
analysis O
from O
ranked B
players B
. O

A. O
Feature O
selection O
For O
our O
clustering O
algorithms O
, O
the O
features O
were O
21 O
normalized O
player B
statistics I
, O
such O
as O
average B
damage I
dealt O
and O
money O
earned O
. O

The O
statistics O
were O
normalized O
over O
their O
range B
of O
values O
, O
preventing O
clusters O
from O
being O
formed O
due O
to O
order O
of O
magnitude O
differences O
between O
statistics O
. O

For O
instance O
, O
damage B
dealt O
values O
are O
often O
7 O
orders O
of O
magnitude O
greater O
than O
kill B
streaks I
, O
which O
means O
small O
variations O
in O
damage B
dealt O
are O
erroneously O
considered O
as O
much O
more O
important O
than O
kill B
streaks I
if O
taken O
directly O
as O
feature O
values O
. O

B. O
Clustering O
models O
1 O
) O
k O
- O
means O
: O
Given O
a O
set O
of O
observations O
, O
k O
- O
means O
clustering O
aims O
to O
partition O
them O
into O
k O
sets O
S O
= O
{ O
S1, O
... O
,Sk O
} O
so O
as O
to O
minimize O
the O
within O
- O
cluster O
sum O
of O
squares O
; O
i.e. O
, O
find O
the O
minimizer O
S O
? O
of O
the O
distortion O
function O
: O
k O
∑ O
i=1 O
∑ O
x∈Si O
kx− O
µik O
2 O
2 O
, O
( O
1 O
) O
where O
x O
is O
an O
observation O
and O
µi O
is O
the O
i O
th O
cluster O
centroid O
. O

In O
general O
, O
this O
problem O
is O
computationally O
difficult O
( O
NPhard O
) O
. O

For O
our O
clustering O
, O
we O
employ O
Lloyd O
’s O
algorithm O
, O
which O
is O
a O
heuristic O
that O
consists O
of O
randomly O
choosing O
observations O
as O
cluster O
centroids O
and O
iteratively O
assigning O
observations O
to O
their O
closest B
centroids O
and O
updating B
the O
centroids O
with O
the O
mean O
of O
their O
respective O
clusters O
[ O
10 O
] O
. O

To O
select O
the O
number O
of O
clusters O
k O
, O
we O
run O
10-fold O
cross O
validation O
over O
k O
to O
find O
a O
local O
optimizer O
. O

The O
scoring O
function O
for O
the O
cross O
validation O
is O
simply O
the O
average O
distortion O
given O
by O
( O
1 O
) O
over O
the O
held O
- O
out O
sets O
. O

2 O
) O
DP O
- O
means O
: O
DP O
- O
means O
is O
a O
nonparametric O
expectationmaximization O
( O
EM O
) O
algorithm O
derived O
using O
a O
Dirichlet O
process O
( O
DP O
) O
mixture O
of O
Gaussians O
model O
, O
which O
In O
other O
words O
, O
the O
user B
does O
not O
choose O
the O
number O
of O
clusters O
beforehand O
. O

The O
technique B
being O
the O
topic O
of O
a O
series O
of O
papers O
, O
we O
will O
only O
provide O
a O
brief O
description O
of O
the O
algorithm O
. O

The O
reader O
is O
referred O
to O
[ O
11 O
] O
, O
[ O
6 O
] O
for O
a O
thorough O
review O
of O
DP O
- O
means O
. O

Recall O
that O
the O
standard O
mixture O
of O
Gaussians O
assumes O
that O
one O
chooses O
a O
cluster O
with O
probability O
πc O
and O
then O
generates O
an O
observation O
from O
the O
k O
Gaussians O
corresponding O
to O
that O
chosen O
cluster O
. O

In O
contrast O
, O
the O
DP O
mixture O
of O
Gaussians O
is O
a O
Bayesian O
extension O
to O
this O
model O
that O
arises O
by O
first O
placing O
a O
Dirichlet O
prior O
Dir(k O
, O
π0 O
) O
on O
the O
k O
mixing O
Gaussian O
coefficients O
( O
i.e. O
, O
the O
probability O
of O
choosing O
a O
cluster O
) O
for O
some O
initial O
set O
of O
coefficients O
π0 O
( O
e.g. O
, O
uniform O
prior O
) O
. O

As O
observations O
are O
made O
, O
the O
prior O
is O
updated B
and O
the O
mixture O
coefficients O
change O
to O
reflect O
these O
new O
knowledge O
. O

The O
derivation O
of O
DP O
- O
means O
is O
inspired O
by O
the O
connection B
between O
k O
- O
means O
EM O
with O
a O
finite O
mixture O
of O
Gaussians O
model O
. O

Namely O
, O
the O
k O
- O
means O
algorithm O
may O
be O
viewed O
as O
a O
limit O
of O
the O
EM O
algorithm O
if O
all O
of O
the O
covariance O
matrices O
corresponding O
to O
the O
clusters O
in O
a O
Gaussian O
mixture O
model O
are O
equal O
to O
σI. O
As O
σ O
→ O
0 O
, O
the O
negative O
log O
- O
likelihood O
of O
the O
mixture O
of O
Gaussians O
model O
approaches O
the O
k O
- O
means O
clustering O
objective B
( O
1 O
) O
. O

Correspondingly O
, O
the O
EM O
steps O
approach O
the O
k O
- O
means O
steps O
in O
Lloyd O
’s O
algorithm O
. O

In O
the O
case O
of O
DP O
- O
means O
, O
[ O
6 O
] O
shows O
how O
to O
perform O
a O
similar O
limiting O
argument O
. O

Specifically O
, O
suppose O
that O
the O
generative O
model O
for O
the O
EM O
algorithm O
was O
a O
DP O
mixture O
of O
Gaussians O
model O
with O
covariances O
equal O
to O
σI. O
Letting O
σ O
→ O
0 O
for O
the O
DP O
mixture O
model O
yields O
the O
objective B
function O
k O
∑ O
i=1 O
∑ O
x∈Si O
kx− O
µik O
2 O
2 O
+ O
( O
k O
−1)λ O
2 O
, O
( O
2 O
) O
where O
S O
= O
{ O
S1, O
... O
,Sk O
} O
is O
the O
set O
of O
clusters O
, O
x O
is O
an O
observation O
, O
and O
µi O
is O
the O
i O
th O
cluster O
centroid O
. O

Note O
that O
, O
unlike O
in B
k O
- I
means I
, O
k O
is O
now O
a O
variable O
to O
be O
optimized O
over O
. O

This O
leads B
to O
an O
algorithm O
with O
clustering O
assignments O
similar O
to O
the O
classical O
k O
- O
means O
algorithm O
and O
the O
same O
monotonic O
local O
convergence O
guarantees O
. O

( O
See O
Algorithm O
1 O
. O

) O
The O
difference O
is O
that O
a O
new O
cluster O
is O
formed O
whenever O
an O
observation O
is O
sufficiently O
far O
away O
from O
all O
existing O
cluster O
centroids O
, O
with O
some O
user B
- O
defined O
threshold O
distance O
λ O
. O

Intuitively O
, O
λ O
is O
a O
penalty O
on O
the O
number O
of O
clusters O
, O
on O
top O
of O
the O
original O
k O
- O
means O
distortion O
function O
. O

Algorithm O
1 O
: O
DP O
- O
means O
input B
: O
X O
: O
input B
data O
, O
λ O
: O
threshold O
distance O
output O
: O
Clustering O
S1, O
... O
,Sk O
, O
number O
of O
clusters O
k O
k O
← O
1 O
S1 O
← O
random O
observation O
 O
x O
rand O
∈ O
X O

	
 O
µ1 O
← O
x O
rand O
repeat O
X O
perm O
← O
random O
ordered O
permutation O
of O
X O
// O
cluster O
assignments O
for O
x O
∈ O
X O
perm O
in O
order O
do O
c O
← O
argmini∈{1, O
... O
,k O
} O
kx− O
µik O
2 O
2 O
if O
kx− O
µck O
2 O
2 O
> O
λ O
2 O
then O
k O
← O
k O
+1 O
µk O
← O
x O
else O
Sc O
← O
Sc O
∪{x O
} O
// O
centroid O
updates B
for O
i O
= O
1 O
, O
... O
, O
k O
do O
µi O
← O
1 O
|Si O
| O
∑x∈Si O
x O
until O
S1, O
... O
,Sk O
converge O
We O
ran O
DP O
- O
means O
with O
10-fold O
cross O
validation O
over O
a O
range B
of O
λ O
values O
, O
setting O
our O
scoring O
function O
as O
the O
average O
of O
the O
objective B
values O
from O
( O
2 O
) O
over O
the O
held O
- O
out O
sets O
. O

2 O
C. O
Numerical O
results O
Due O
to O
the O
random O
initializations O
, O
we O
ran O
20 O
trials O
for O
each O
clustering O
algorithm O
in O
order O
to O
obtain O
the O
best O
locally O
optimal O
centroids O
. O

These O
optima O
correspond O
to O
12 O
and O
8 O
clusters O
for O
k O
- O
means O
and O
DP O
- O
means O
, O
respectively O
. O

All O
code O
were O
implemented O
in O
MATLAB O
and O
computations O
executed O
on O
a O
2.7 O
GHz O
Intel O
Core O
i7 O
with O
8 O
GB O
RAM O
. O

Figure O
1 O
shows O
an O
example O
of O
the O
log O
of O
distortion O
values O
attained O
over O
the O
range B
of O
k O
values O
for O
the O
k O
- O
means O
algorithm O
ran O
with O
10- O
fold O
cross O
validation O
. O

Table O
I O
summarizes O
the O
results O
for O
the O
clustering O
algorithms O
. O

The O
recorded O
computation O
times B
were O
averaged O
over O
the O
20 O
trials O
, O
and O
do O
not O
include O
preprocessing O
and O
transforming O
data O
into O
features O
, O
etc O
. O

k O
4 O
6 O
8 O
10 O
12 O
14 O
16 O
18 O
20 O
22 O
24 O
log O
distortion O
value O
7.4 O
7.6 O
7.8 O
8 O
8.2 O
8.4 O
8.6 O
Fig O
. O

1 O
. O

Best O
trial O
out O
of O
20 O
: O
The O
log O
distortion O
values O
show O
a O
local O
optimum O
at O
k O
= O
12 O
over O
the O
range B
of O
5 O
to O
24 O
clusters O
( O
magenta O
asterisk O
) O
. O

TABLE O
I O
PLAY B
STYLE I
CLUSTERING O
SUMMARY O
RESULTS O
cross O
val O
. O

param O
. O

range B
clusters O
cpu O
time B
k O
- O
means O
10-fold O
k O
= O
5, O
... O
,24 O
12 O
154.1 O
s O
DP O
- O
means O
10-fold O
λ O
= O
2.5,2.6, O
... O
,4.4 O
8 O
65.4 O
s O
D. O
Cluster O
interpretation O
Surprisingly O
, O
our O
consultations O
with O
expert O
, O
highly B
- I
ranked I
( O
top O
0.2 O
% O
worldwide O
) O
LoL O
players B
corroborated O
the O
correctness O
of O
the O
behavior O
clusters O
learned O
by O
our O
algorithms O
. O

By O
checking O
the O
centroid O
values O
corresponding O
to O
each O
feature O
and O
using O
information O
about O
the O
frequency O
of O
in B
- I
game I
characters B
used O
for O
each O
cluster O
, O
these O
expert B
players I
were O
able O
map O
each O
cluster O
to O
a O
specific O
gameplay B
type O
that O
they O
had O
experienced B
in B
- I
game I
. O

This O
suggests O
that O
our O
clustering O
were O
intuitively O
correct O
. O

The O
mappings O
determined O
for O
the O
12-clusters O
k O
- O
means O
result O
are O
as O
follows O
. O

• O
Ranged B
physical B
attacker I
Clusters O
1 O
, O
7 O
, O
and O
9 O
– O
Players B
who O
maintain O
distance O
from O
fights B
while O
dealing B
high O
damage I
with O
long O
- O
range B
attacks B
– O
Players B
in O
each O
cluster O
differ O
in O
risk O
attitudes O
, O
such O
as O
whether O
they O
attack B
deeper O
in O
enemy B
territory O
• O
Ambusher O
Clusters O
3 O
, O
8 O
, O
11 O
, O
and O
12 O
– O
Players B
who O
move B
stealthily O
around O
the O
battlefield B
and O
engage O
in O
quick O
, O
close B
- O
ranged O
combat B
– O
Some O
players B
prefer O
a O
team B
oriented O
style O
, O
whereas O
others O
prefer O
a O
more O
“ O
lone O
wolf O
” O
approach O
– O
Includes O
“ O
hybrid B
” O
roles B
with O
other O
behavior O
clusters O
• O
Team B
support B
Cluster O
5 O
– O
Players B
who O
typically O
assist B
ranged B
physical B
attackers I
( O
healing B
, O
cooperative O
attacks B
, O
etc O
. O

) O
• O
Magic B
attacker I
Clusters O
6 O
and O
10 O
– O
Players B
who O
rely O
on O
magic O
- O
based B
attacks B
; O
as O
opposed O
to O
physical B
damage I
in O
the O
above O
clusters O
– O
Differ O
in O
preference O
for O
close- O
or O
ranged B
- O
combat B
• O
Miscellaneous O
Clusters O
2 O
and O
4 O
– O
No O
clear B
style O
preference O
– O
Differs O
in O
skill B
: O
either O
a O
novice B
player I
or O
prefers O
an O
all O
- O
around O
gameplay B
style I
Interestingly O
, O
we O
notice O
from O
expert O
analysis O
that O
there O
appears O
to O
be O
a O
hierarchy O
of O
clusters O
. O

For O
instance O
, O
clusters O
6 O
and O
10 O
fall B
under O
the O
broader O
“ O
magic B
attacker I
” O
category O
. O

This O
suggests O
that O
we O
might O
consider O
other O
clustering O
models O
than O
k O
- O
means O
or O
DP O
- O
means O
, O
as O
these O
methods O
assign O
each O
observation O
to O
only O
one O
cluster O
. O

We O
address O
this O
further O
in O
Section O
V. O
E. O
Cluster O
visualization O
with O
PCA O
Fig O
. O

2 O
shows O
the O
result O
of O
applying O
principal O
component O
analysis O
( O
PCA O
) O
to O
reduce O
our O
feature O
dimension O
and O
visualize O
it O
in O
three O
dimensions O
. O

Notice O
that O
the O
data O
is O
clearly O
clustered O
into O
8 O
distinct O
groups B
, O
suggesting O
that O
in O
higher O
dimensions O
there O
are O
probably O
more O
clusters O
. O

Overlaying O
our O
12-groups O
clustering O
from O
the O
k O
- O
means O
technique B
in O
color O
, O
we O
observe O
that O
they O
are O
consistent O
with O
the O
PCA O
results O
: O
Almost O
all O
points B
in O
any O
k O
- O
means O
cluster O
are O
in O
the O
same O
PCA O
cluster O
. O

Fig O
. O

2 O
. O

Visualizing O
our O
data O
with O
3 O
principal O
components O
reveals O
at O
least O
8 O
distinct O
clusters O
. O

The O
12-clusters O
k O
- O
means O
results O
are O
overlaid O
in O
color O
. O

3 O
IV O
. O

GAME B
OUTCOME O
PREDICTION O
We O
illustrate O
the O
accuracy O
of O
game B
outcome O
predictors O
that O
use O
team B
composition I
features O
based B
on O
our O
gameplay B

 O
style I
clusters O
learned O
in O
the O
previous O
section O
. O

We O
present O
our O
feature O
selection O
, O
the O
classification O
models O
used O
to O
learn O
our O
predictors O
, O
and O
the O
accuracies O
for O
our O
predictors O
. O

A. O
Feature O
selection O
For O
our O
classification O
algorithms O
, O
the O
features O
are O
the O
two O
teams B
’ O
player B
compositions O
. O

Associated O
with O
each O
team B
is O
a O
vector O
of O
counts O
of O
players B
that O
fall B
into O
a O
certain O
play B
style I
category O
, O
which O
were O
, O
say O
, O
derived O
from O
one O
of O
the O
clustering O
algorithms O
. O

The O
feature O
vector O
is O
the O
concatenation O
of O
the O
count O
vectors O
of O
teams B
1 O
and O
2 O
. O

The O
labels O
for O
each O
sample O
are O
the O
win O
/ O
loss O
indicator O
for O
the O
game B
, O
with O
1 O
corresponding O
to O
a O
victory B
and O
0 O
a O
loss O
by O
team B
1 O
to O
team B
2 O
. O

For O
instance O
, O
there O
are O
8 O
clusters O
, O
teams B
1 O
and O
2 O
have O
the O
count O
vectors O
x1 O
∈ O
R O
8 O
and O
x2 O
∈ O
R O
8 O
, O
and O
team B
1 O
beats O
team B
2 O
. O

The O
feature O
vector O
- O
label O
pair O
would O
then O
be O
( O
x O
, O
y O
) O
= O
 O
x1 O
x2 O
 O
, O
1 O
 O
, O
where O
x O
is O
the O
feature O
vector O
and O
y O
is O
the O
sample O
binary O
label O
. O

B. O
Classification O
models O
To O
obtain O
the O
best O
win O
/ O
loss O
outcome O
predictor O
, O
we O
consider O
different O
classification O
models O
. O

To O
determine O
the O
accuracy O
of O
our O
predictors O
learned O
using O
the O
various O
models O
, O
we O
held O
out O
10 O
% O
of O
our O
total O
sample O
set O
( O
over O
130,000 O
in O
total O
) O
for O
training O
, O
and O
used O
the O
held O
- O
out O
samples O
for O
testing O
. O

1 O
) O
Logistic O
regression O
: O
For O
this O
model O
, O
we O
use O
the O
Bernoulli O
family O
of O
distributions O
to O
model O
the O
conditional O
distribution O
of O
winning O
or O
losing O
given O
the O
team B
composition I
features O
. O

That O
is O
, O
adhering O
to O
our O
notation O
introduced O
above O
, O
y O
| O
x;θ O
∼ O
Bernoulli(φ O
) O
, O
where O
θ O
is O
our O
model O
parameter O
and O
φ O
= O
hθ O
( O
x O
) O
= O
1/ O


 O
1+exp(−θ O
T O
x O
) O
 O
is O
our O
hypothesis O
, O
which O
is O
derived O
from O
formulating O
the O
Bernoulli O
distribution O
as O
an O
exponential O
family O
distribution O
. O

To O
learn O
our O
model O
, O
we O
find O
a O
parameter O
θ O
that O
maximizes O
the O
log O
- O
likelihood O
function O
` O
( O
θ O
) O
= O
log O
m O
∏ O
i=1 O
p O
 O
y O
( O
i O
) O
| O
x O
( O
i O
) O
; O
θ O
 O
( O
3 O
) O
= O
m O
∑ O
i=1 O
y O
( O
i O
) O
loghθ O
 O
x O
( O
i O
) O
 O
+ O
 O
1−y O
( O
i O
) O
 O
log O
1−hθ O
 O
x O
( O
i O
) O
 O
, O
( O
4 O
) O
where O
m O
is O
the O
sample O
set O
size O
. O

We O
used O
stochastic O
gradient O
ascent O
to O
efficiently O
find O
the O
optimizer O
θ O
? O
. O

2 O
) O
Gaussian O
discriminant O
analysis O
: O
In O
this O
model O
, O
we O
assume O
that O
the O
input B
features O
x O
are O
continuous O
- O
valued O
random O
variables O
and O
model O
p(x O
| O
y O
) O
using O
a O
multivariate O
normal O
distribution O
. O

In O
other O
words O
, O
we O
use O
a O
generative O
learning O
model O
. O

In O
our O
case O
, O
y O
∼ O
Bernoulli(φ O
) O
( O
5 O
) O
x O
| O
y O
= O
0 O
∼ O
N O
( O
µ0,Σ O
) O
( O
6 O
) O
x O
| O
y O
= O
1 O
∼ O
N O
( O
µ1,Σ O
) O
, O
( O
7 O
) O
where O
µ0 O
, O
µ1 O
, O
and O
Σ O
are O
the O
means O
and O
covariance O
of O
the O
Gaussian O
distributions O
. O

Here O
, O
we O
maximize O
the O
log O
- O
likelihood O
of O
the O
m O
- O
samples O
data O
` O
( O
φ,µ0,µ1,Σ O
) O
= O
log O
m O
∏ O
i=1 O
p O
 O
x O
( O
i O
) O
, O
y O
( O
i O
) O
; O
φ,µ0,µ1,Σ O
 O
. O

( O
8) O
The O
result O
of O
maximizing O
` O
with O
respect O
to O
the O
model O
parameters O
is O
a O
set O
of O
exact O
analytic O
equations O
[ O
10 O
] O
, O
which O
we O
compute O
directly O
. O

The O
derivation O
of O
these O
equations O
are O
simple O
, O
and O
we O
omit O
them O
for O
brevity O
. O

3 O
) O
Support B
vector O
machine O
: O
Assuming O
that O
our O
data O
are O
separable O
with O
a O
large O
“ O
gap O
, O
” O
a O
support B
vector O
machine O
model O
posits O
that O
the O
size O
of O
the O
geometric O
margin O
between O
some O
observation O
point B
and O
the O
decision O
boundary O
is O
proportional O
to O
“ O
confidence O
level B
” O
that O
the O
observation O
is O
classified O
correctly O
. O

The O
result O
of O
this O
model O
is O
an O
optimization O
problem O
that O
seeks O
the O
maximum O
margin O
separating O
hyperplane O
for O
our O
samples O
. O

For O
our O
problem O
, O
we O
use O
` O
1 O
regularization O
since O
we O
are O
uncertain O
about O
whether O
our O
data O
is O
linearly O
separable O
( O
e.g. O
, O
outliers O
, O
erroneous O
data O
) O
. O

The O
resulting O
problem O
is O
solved O
using O
the O
sequential O
minimal O
optimization O
algorithm O
[ O
12 O
] O
. O

C. O
Evaluation O
criteria O
To O
evaluate O
the O
usefulness O
of O
game B
outcome O
predictor O
models O
with O
features O
based B
on O
our O
learned O
behavior O
clusters O
, O
we O
compare O
them O
against O
a O
baseline O
predictor O
with O
features O
based B
on O
the O
game B
developers O
’ O
official O
gameplay B
classes B
. O

As O
introduced O
in O
Section O
III O
, O
the O
game B
developers O
have O
grouped B
the O
in B
- I
game I
characters B
into O
six O
broad O
categories O
, O
such O
as O
assassin B
or O
support B
, O
which O
supposedly O
reflects O
the O
character B
’s O
gameplay B
style I
. O

We O
learn O
a O
logistic O
regression O
model O
with O
features O
constructed O
using O
these O
categories O
and O
use O
the O
10 O
% O
hold O
- O
out O
method O
for O
cross O
validation O
. O

D. O
Results O
and O
discussion O
To O
ensure O
fairness O
of O
results O
, O
we O
ran O
20 O
trials O
for O
each O
model O
to O
determine O
the O
predictor O
accuracies O
, O
which O
are O
based B
on O
different O
randomized O
train B
and O
test O
sets O
. O

As O
with O
our O
clustering O
algorithms O
, O
all O
code O
were O
implemented O
in O
MATLAB O
, O
the O
computations O
were O
executed O
on O
a O
2.7 O
GHz O
Intel O
Core O
i7 O
with O
8 O
GB O
RAM O
, O
and O
the O
computation O
times B
were O
averaged O
over O
the O
20 O
random O
trials O
. O

Again O
, O
these O
times B
do O
not O
include O
preprocessing O
and O
transforming O
data O
into O
features O
, O
etc O
. O

As O
we O
observe O
in O
Table O
II O
, O
the O
best O
predictor O
learned O
using O
our O
behavior O
clusters O
- O
based B
features O
uses O
an O
SVM O
model O
with O
features O
derived O
from O
our O
k O
- O
means O
clustering O
. O

This O
predictor O
did O
significantly O
better O
( O
16 O
% O
better O
) O
than O
the O
baseline O
algorithm O
on O
the O
test O
sets O
, O
which O
had O
55.1 O
% O
and O
54.4 O
% O
accuracies O
on O
the O
training O
and O
testing O
sets O
. O

The O
other O
predictors O
were O
also O
competitive B
— O
all O
were O
only O
less O
accurate O
by O
a O
tiny O
percentage O
. O

Other O
than O
illustrating O
the O
relatively O
high O
accuracy O
of O
our O
team B
composition O
- I
based I
outcome I
prediction I
approach I
, I
this O
result O
also O
implies O
that O
our O
behavior O
clusters O
learned O
had O
more O
descriptive O
power B
than O
the O
official O
game B
developers O
’ O
version O
. O

This O
indirectly O
concurs O
with O
what O
we O
have O
shown O


 O
TABLE O
II O
OUTCOME O
PREDICTION O
SUMMARY O
RESULTS O
k O
- O
means O
DP O
- O
means O
train B
acc O
. O

test O
acc O
. O

cpu O
time B
train B
acc O
. O

test O
acc O
. O

cpu O
time B
LR O
72.3 O
% O
68.8 O
% O
7.4 O
s O
69.7 O
% O
67.1 O
% O
7.1 O
s O
GDA O
74.8 O
% O
70.1 O
% O
7.7 O
s O
70.9 O
% O
68.4 O
% O
7.1 O
s O
SVM O
74.8 O
% O
70.4 O
% O
91.2 O
s O
71.7 O
% O
69.2 O
% O
41.6 O
s O
from O
our O
clustering O
models O
: O
The O
official O
gameplay B
style I
categories O
that O
were O
used O
for O
the O
baseline O
algorithm O
do O
not O
necessarily O
correspond O
to O
the O
behaviors O
of O
actual O
players B
in O
games B
. O

Overall O
, O
our O
results O
validate O
our O
framework O
of O
first O
clustering O
players B
by O
their I
gameplay I
style O
and O
then O
using O
team B

 O
composition I
features O
based B
on O
these O
learned O
styles O
to O
predict O
team B
performance I
. O

And O
since O
our O
target B
game I
is O
a O
representative O
title O
for O
games B
of O
the O
same O
type O
( O
i.e. O
, O
team B
- O
based I
role B
- O
playing B
games B
) O
, O
we O
expect O
this O
framework O
to O
also O
be O
effective O
and O
generalizable O
to O
other O
multiplayer O
games B
. O

V. O
CONCLUSION O
AND O
EXTENSIONS O
In O
this O
brief O
, O
we O
have O
presented O
an O
algorithmic O
framework O
for O
outcome O
prediction O
: O
By O
learning O
in B
- I
game I
player B
behavior I
categories O
through O
clustering O
and O
using O
them O
in O
features O
for O
game B
outcome O
predictors O
based B
on O
classification O
models O
, O
we O
are O
able O
to O
determine O
wins O
and O
losses O
with O
over O
70 O
% O
accuracy O
for O
our O
target B
game I
. O

This O
approach O
could O
be O
used O
to O
evaluate O
how O
team B
compositions I
can O
affect O
performance O
in O
games B
other O
than O
the O
one O
we O
have O
considered O
. O

Future O
work O
will O
include O
adding O
time B
- O
dependent O
player B

 O
statistics I
features O
. O

Unlike O
the O
overall O
game B
statistics O
we O
used O
, O
these O
timed B
statistics O
might O
give O
an O
additional O
layer O
of O
descriptive O
power B
, O
allowing O
the O
model O
to O
differentiate O
between O
clusters O
based B
on O
how O
players B
behave O
early B
and O
later O
in O
the O
game B
. O

This O
might O
lead B
to O
a O
better O
features O
for O
a O
more O
accurate O
team B
composition O
- I
based I
win I
/ I
loss I
predictor I
. O

As O
another O
extension O
, O
we O
could O
also O
consider O
different O
clustering O
models O
, O
such O
as O
one O
that O
captures B
the O
ostensibly O
hierarchical O
clustering O
seen O
in O
the O
expert O
analysis O
of O
the O
k O
- O
means O
results O
. O

For O
instance O
, O
the O
BP O
- O
means O
model O
described O
in O
[ O
11 O
] O
is O
designed O
to O
capture B
such O
hierarchical O
clustering O
relationships O
. O

ACKNOWLEDGMENTS O
We O
thank O
Professor O
Andrew O
Ng O
and O
the O
course O
staff O
for O
motivating O
and O
giving O
feedback O
for O
our O
work O
. O

We O
are O
also O
grateful O
to O
the O
LoL O
expert B
players I
who O
helped O
with O
our O
cluster O
analysis O
. O

REFERENCES O
[ O
1 O
] O
E. O
Tomai O
, O
R. O
Salazar O
, O
and O
R. O
Flores O
, O
“ O
Simulating O
aggregate O
player B

 O
behavior I
with O
learning O
behavior O
trees O
, O
” O
in O
Proceedings O
of O
the O
22nd O
Annual O
Conference O
on O
Behavior O
Representation O
in O
Modeling O
and O
Simulation O
, O
W. O
G. O
Kennedy O
, O
D. O
Reitter O
, O
and O
R. O
S. O
Amant O
, O
Eds O
. O

BRIMS O
Society O
, O
Jul. O
2013 O
. O

[ O
2 O
] O
W. O
S. O
Bainbridge O
, O
“ O
The O
scientific O
research O
potential O
of O
virtual O
worlds B
, O
” O
Science O
, O
vol O
. O

317 O
, O
pp O
. O

472–476 O
, O
2012 O
. O

[ O
3 O
] O
M. O
D. O
Dickey O
, O
“ O
Three O
dimensional O
virtual O
worlds B
and O
distance O
learning O
: O
Two O
case O
studies O
of O
active O
worlds B
as O
a O
medium O
for O
distance O
education O
, O
” O
British O
Journal O
of O
Educational O
Technology O
, O
vol O
. O

36 O
, O
pp O
. O

439–451 O
, O
2005 O
. O

[ O
4 O
] O
H. O
E. O
Spotts O
, O
“ O
Evaluating O
the O
effects B
of O
team B
composition I
and O
performance O
environment O
on O
team B
performance I
, O
” O
Journal O
of O
Behavioral O
and O
Applied O
Management O
, O
2011 O
. O

[ O
5 O
] O
K. O
Hellerstedt O
and O
H. O
E. O
Aldrich O
, O
“ O
The O
impact B
of O
initial O
team B
composition I
and O
performance O
on O
team B
dynamics O
and O
survival O
, O
” O
Academy O
of O
Management O
, O
p. O
6 O
, O
2008 O
. O

[ O
6 O
] O
B. O
Kulis O
and O
M. O
I. O
Jordan O
, O
“ O
Revisiting O
k O
- O
means O
: O
New O
algorithms O
via O
bayesian O
nonparametrics O
, O
” O
in O
Proceedings O
of O
the O
29th O
International O
Conference O
on O
Machine O
Learning O
, O
J. O
Langford O
and O
J. O
Pineau O
, O
Eds O
. O

Omnipress O
, O
Jun. O
2012 O
. O

[ O
7 O
] O
P. O
Tassi O
, O
“ O
Riot B
’s O
‘ O
League O
of O
Legends O
’ O
reveals O
astonishing O
27 O
million O
daily O
players B
, O
67 O
million O
monthly O
, O
” O
Forbes O
, O
2014 O
. O

[ O
8 O
] O
S. O
Ford O
, O
“ O
League O
of O
Legends O
: O
Marc O
Merrill O
Q&A O
, O
” O
Warcry O
Network O
, O
2009 O
. O

[ O
Online O
] O
. O

Available O
: O
http://www.warcry.com/articles/ O
view O
/ O
interviews/5686-League O
- O
of O
- O
Legends O
- O
Marc O
- O
Merrill O
- O
Q O
- O
A O
[ O
9 O
] O
Riot O
Games O
, O
Inc. O
, O
“ O
Riot O
Games O
API O
, O
” O
2014 O
. O

[ O
Online O
] O
. O

Available O
: O
https://developer.riotgames.com/ O
[ O
10 O
] O
A. O
Ng O
, O
“ O
Cs O
229 O
: O
Machine O
learning O
course O
notes O
, O
” O
2014 O
. O

[ O
Online O
] O
. O

Available O
: O
http://cs229.stanford.edu/materials.html O
[ O
11 O
] O
T. O
Broderick O
, O
B. O
Kulis O
, O
and O
M. O
I. O
Jordan O
, O
“ O
MAD O
- O
Bayes O
: O
MAPbased O
asymptotic O
derivations O
from O
Bayes O
, O
” O
in O
Proceedings O
of O
the O
30th O
International O
Conference O
on O
Machine O
Learning O
, O
S. O
Dasgupta O
and O
D. O
McAllester O
, O
Eds O
. O

Omnipress O
, O
Jun. O
2013 O
. O

[ O
12 O
] O
J. O
Platt O
, O
“ O
Using O
analytic O
QP O
and O
sparseness O
to O
speed O
training O
of O
Support B
Vector O
Machines O
, O
” O
in O
Advances O
in O
Neural O
Information O
Processing O
Systems O
11 O
, O
M. O
J. O
Kearns O
, O
S. O
A. O
Solla O
, O
and O
D. O
A. O
Cohn O
, O
Eds O
. O

MIT O
Press O
, O
1998 O
. O

5 O
A O
Python O
Engine O
for O
Teaching O
Artificial O
Intelligence O
in O
Games O
Mark O
O. O
Riedl O
School O
of O
Interactive O
Computing O
Georgia O
Institute O
of O
Technology O
riedl@cc.gatech.edu O
Abstract O
Computer O
games B
play B
an O
important O
role B
in O
our O
society O
and O
motivate O
people O
to O
learn O
computer O
science O
. O

Since O
artificial O
intelligence O
is O
integral O
to O
most O
games B
, O
they O
can O
also O
be O
used O
to O
teach O
artificial O
intelligence O
. O

We O
introduce O
the O
Game O
AI O
Game O
Engine O
( O
GAIGE O
) O
, O
a O
Python O
game B
engine O
specifically O
designed O
to O
teach O
about O
how O
AI O
is O
used O
in O
computer O
games B
. O

A O
progression B
of O
seven O
assignments O
builds O
toward O
a O
complete O
, O
working O
MultiUser O
Battle O
Arena O
( O
MOBA O
) O
game B
. O

We O
describe O
the O
engine O
, O
the O
assignments O
, O
and O
our O
experiences B
using O
it O
in O
a O
class B
on O
Game O
Artificial O
Intelligence O
. O

Introduction O
For O
many O
, O
exposure O
to O
computer O
games B
leads B
to O
an O
interest O
in O
computer O
programming O
and O
computer O
science O
. O

Computer O
games B
are O
also O
a O
gateway O
for O
interest O
in O
artificial O
intelligence O
; O
computer O
games B
are O
often O
the O
first O
exposure O
people O
have O
to O
AI O
. O

Artificial O
intelligence O
in O
computer O
games B
appears O
simple O
, O
when O
compared O
to O
the O
state O
of O
the O
art O
in O
AI O
research O
. O

However O
, O
this O
also O
makes O
computer O
games B
an O
excellent O
way O
to O
introduce O
people O
to O
artificial O
intelligence O
programming O
. O

The O
virtues O
of O
using O
computer O
game B
environments O
to O
teach O
computer O
programming O
have O
been O
well O
- O
explored O
( O
DeNero O
and O
Klein O
2010 O
; O
Wong O
, O
Zink O
, O
and O
Koenig O
2010 O
; O
Taylor O
2011 O
; O
Sosnowski O
et O
al O
. O

2013 O
) O
. O

Computer O
games B
have O
also O
been O
used O
to O
teach O
artificial O
intelligence O
, O
including O
the O
Berkeley O
Pac O
- O
Man O
AI O
course O
materials O
( O
http://ai.berkeley O
. O

edu O
/ O
project_overview.html O
) O
. O

Computer O
game B
artificial O
intelligence O
, O
often O
shorted O
as O
Game O
AI O
, O
is O
often O
considered O
a O
distinct O
sub O
- O
discipline O
of O
artificial O
intelligence O
focused B
on O
the O
short O
- O
term O
illusion O
of O
believable O
agent O
behavior O
. O

In O
computer O
games B
, O
artificial O
intelligence O
in O
some O
form O
is O
responsible O
for O
the O
behavior O
for O
every O
virtual O
entity B
encountered O
by O
the O
human O
player B
. O

In O
some O
games B
, O
some O
form O
of O
artificial O
intelligence O
may O
also O
take O
additional O
roles B
such O
as O
generating O
terrain O
or O
adapting O
the O
game B
to O
increase B
player B
engagement O
. O

Many O
of O
the O
algorithms O
taught O
in O
introductory O
artificial O
intelligence O
are O
prominently O
used O
in O
creating O
engaging O
virtual O
experiences B
: O
graph O
search O
, O
agent O
decision B
- I
making I
, O
planning O
, O
and O
, O
to O
a O
lesser O
extent O
, O
machine O
learning O
and O
data O
analysis O
. O

Game O
AI O
, O
however O
, O
focuses B
on O
pragmatic O
aspects O
of O
artificial O
intelligence O
as O
applied O
to O
the O
highly O
- O
constrained O
computing O
environment O
of O
computer O
games B
. O

Game O
AI O
must O
be O
concerned O
with O
scalability O
. O

Virtual O
agents O
must O
operate O
in O
real B
- I
time I
and O
there O
can O
be O
dozens O
, O
hundreds O
, O
or O
thousands O
of O
agents O
consuming O
a O
highly O
- O
constrained O
number O
of O
computation O
cycles O
. O

Game O
AI O
is O
also O
concerned O
with O
player B
experience B
. O

It O
explores O
the O
question O
of O
what O
behaviors O
can O
be O
conducted O
by O
a O
virtual O
agent O
facilitate O
players B
’ O
momentarily O
suspension O
of O
disbelief O
. O

Often O
the O
constraints O
of O
scalability O
and O
player B
experience B
result O
in O
“ O
simple O
” O
solutions O
such O
as O
finite O
state O
machines O
, O
which O
belie O
the O
complexity O
of O
design O
need O
to O
create O
the O
illusion O
of O
behavior O
that O
would O
otherwise O
be O
considered O
“ O
AI O
Hard B
” O
in B
real O
- I
time I
with O
few O
computational O
resources B
. O

Consequently O
, O
many O
universities O
teach O
Game O
AI O
as O
a O
separate O
course O
from O
an O
introductory O
artificial O
intelligence O
course O
. O

Game B
AI O
classes B
emphasize O
design O
and O
selecting O
the O
right O
AI O
technique B
for O
the O
job O
. O

Because O
AI O
is O
an O
integral O
part O
of O
any O
game B
design O
, O
the O
AI O
algorithms O
can O
not O
be O
easily O
separated O
from O
the O
game B
. O

In O
this O
paper O
, O
we O
introduce O
a O
game B
engine O
specifically O
designed O
for O
teaching O
Game O
AI O
. O

Instead O
of O
asking O
students O
to O
integrate O
AI O
into O
an O
existing O
game B
, O
the O
Game O
AI O
Game O
Engine O
( O
GAIGE O
) O
provides O
all O
the O
core O
functionality O
of O
a O
computer O
game B
except O
for O
the O
AI O
algorithms O
. O

Through O
a O
controlled B
progression B
of O
Python O
programming O
assignments O
, O
students O
build O
a O
fully O
functional O
Multi O
- O
User O
Online O
Battle O
Arena O
( O
MOBA O
) O
game B
piece B
by O
piece B
. O

Each O
piece B
requires O
the O
implementation O
of O
a O
Game O
AI O
algorithm O
. O

The O
progression B
breaks O
the O
task O
of O
creating O
a O
fully O
functioning O
computer O
game B
into O
manageable O
pieces B
that O
can O
be O
individually O
graded O
by O
autograder O
, O
and O
provides O
students O
with O
a O
sense O
of O
accomplishment O
. O

GAIGE O
can O
be O
downloaded O
from O
http://game-ai.gatech.edu O
. O

Game O
AI O
The O
term O
, O
Game O
AI O
, O
has O
come O
to O
refer O
to O
the O
set O
of O
tools O
— O
algorithms O
and O
representations O
— O
developed O
specifically O
to O
aid O
the O
creation O
and O
management O
of O
interactive O
, O
real B
- I
time I
, O
digital O
entertainment O
experiences B
. O

While O
games B
are O
played B
by O
humans O
, O
there O
are O
a O
number O
of O
aspects O
of O
the O
game B
playing O
experience I
that O
must O
be O
automated O
: O
roles B
that O
would O
be O
best O
performed O
by O
humans O
but O
are O
not O
practical O
to O
do O
so O
: O
• O
Opponents B
and O
enemies B
that O
are O
meant O
to O
survive O
for O
only O
a O
short O
time B
before O
losing O
. O

arXiv:1511.07714v1 O
[ O
cs O
. O

CY O
] O
24 O
Nov O
2015 O
• O
Non B
- I
player I
characters B
in O
roles B
that O
are O
not O
“ O
fun O
” O
to O
play B
such O
as O
shopkeepers O
, O
farmers B
, O
victims B
, O
or O
footsoldiers O
. O

• O
Companions O
in B
single O
- I
player I
experiences B
and O
non B
- I
player I
characters B
in O
support B
roles B
. O

• O
Drama O
manager O
to O
adjust O
the O
game B
plot O
in O
response O
to O
real B
- I
time I
player B
behaviors I
( O
Riedl O
and O
Bulitko O
2013 O
) O
. O

• O
Game B
designer O
for O
personalized O
experiences B
at O
scale O
. O

As O
we O
go O
down O
this O
list O
, O
Game O
AI O
is O
charged O
with O
taking O
progressively O
more O
responsibility O
for O
the O
quality O
of O
the O
human O
players B
’ O
experiences B
in O
the O
game B
. O

Game O
AI O
programming O
is O
often O
considered O
part O
of O
the O
game B
design O
process O
because O
it O
both O
constrains O
the O
creative O
process O
of O
designers O
and O
also O
realizes O
the O
creative O
vision O
of O
the O
game B
designers O
. O

Because O
of O
this O
, O
Game O
AI O
is O
often O
considered O
a O
separate O
discipline O
from O
that O
that O
we O
conventionally O
refer O
to O
as O
artificial O
intelligence O
. O

In O
artificial O
intelligence O
, O
rational O
behavior O
and O
optimality O
are O
idealized O
criteria O
for O
success O
; O
researchers O
seek O
to O
improve O
rationality O
and O
accuracy O
via O
increasingly O
sophisticated O
algorithms O
and O
data O
sets O
. O

In O
Game O
AI O
, O
the O
goal B
is O
to O
create O
the O
most O
engaging O
experience B
possible O
for O
human O
players B
. O

Any O
and O
all O
techniques B
or O
algorithms O
that O
result O
in O
the O
temporary O
suspension O
of O
disbelief O
when O
interacting O
with O
virtual O
entities B
are O
valid O
solutions O
. O

Furthermore O
, O
graphical O
rendering O
often O
consumes O
a O
vast O
majority O
of O
the O
computing O
power B
of O
the O
computing O
device O
( O
PCs O
, O
consoles O
, O
mobile B
devices I
, O
etc O
. O

) O
, O
requiring O
any O
AI O
technique B
implemented O
within O
a O
game B
to O
be O
execute O
in O
near O
real B
- I
time I
and O
with O
an O
extreme O
dearth O
of O
computing O
cycles O
. O

Despite O
the O
difference O
in O
goals B
between O
conventionallydefined O
artificial O
intelligence O
and O
Game O
AI O
, O
they O
draw O
from O
the O
same O
basic O
desire O
to O
create O
artificial O
agents O
that O
appear O
, O
at O
least O
for O
a O
short O
time B
, O
to O
perform O
behaviors O
that O
one O
might O
deem O
to O
require O
intelligence O
when O
performed O
by O
a O
human O
. O

A O
vast O
majority O
of O
AI O
problems O
in O
game B
development O
fall B
into O
two O
categories O
: O
pathfinding O
and O
decision B
- I
making I
. O

Pathfinding O
is O
the O
problem O
of O
identifying O
an O
efficient O
path B
from O
one O
part O
of O
a O
virtual O
environment O
to O
another O
whilst O
avoiding O
collisions O
with O
obstacles B
that O
can O
shatter O
the O
illusion O
of O
intelligence O
. O

Graph O
search O
algorithms O
such O
as O
Djikstra O
’s O
Algorithm O
, O
the O
Floyd O
- O
Warshall O
algorithm O
, O
and O
A O
* O
, O
are O
commonly O
implemented O
in O
games B
. O

Decision O
- O
making O
involves O
choosing O
and O
executing O
agent O
behaviors O
that O
enhance O
the O
engagement O
of O
the O
player B
. O

Even O
goals B
that O
are O
adversarial O
in O
nature O
, O
e.g. O
, O
attacking B
the O
player B
’s O
avatar B
, O
may O
not O
be O
as O
straightforward O
as O
optimizing O
the O
amount B
of I
damage I
inflicted O
to O
the O
player B
’s O
avatar B
. O

For O
example O
, O
different O
types B

 O
of I
adversaries I
are O
expected O
to O
manifest O
different O
“ O
personalities O
” O
( O
e.g. O
, O
a O
zombie O
versus O
a O
soldier O
) O
and O
engage O
the O
player B
in O
different O
levels B
of O
challenge O
. O

One O
of O
the O
most O
common B
techniques I
for O
implementing O
decision B
- I
making I
is O
the O
finite O
- O
state O
machine O
, O
which O
provides O
a O
high O
degree O
of O
designer O
control B
over O
the O
real B
- I
time I
behavior O
of O
a O
virtual O
entity B
and O
consumes O
little O
computational O
overhead O
. O

Recall O
that O
they O
goal B
of O
Game O
AI O
is O
the O
illusion O
of O
intelligence O
for O
a O
short O
period O
of O
time B
, O
and O
a O
well O
- O
designed O
finite O
- O
state O
machine O
can O
achieve O
this O
goal B
effectively O
. O

For O
more O
dynamic O
entity B
behavior O
, O
some O
games B
implement O
behavior O
trees O
, O
a O
greedy O
form O
of O
hierarchical O
task O
network O
( O
HTN O
) O
planners O
( O
c.f O
. O

, O
( O
Ghallab O
, O
Nau O
, O
and O
Traverso O
2004 O
) O
) O
that O
create O
plans O
in B
real O
- I
time I
but O
can O
not O
backtrack O
, O
bind O
variables O
, O
nor O
interleave O
methods O
. O

Like O
finite O
- O
state O
machines O
, O
behavior O
trees O
provide O
designers O
with O
the O
ability B
to O
specify O
proper O
behavior O
while O
leaving O
the O
timing B
and O
, O
to O
some O
degree O
, O
the O
sequence O
of O
decisions O
to O
the O
agent O
. O

Game O
AI O
includes O
forms O
of O
intelligence O
other O
than O
agent O
decision B
- I
making I
and O
navigation O
. O

Procedural O
content O
generation O
is O
the O
use O
of O
algorithms O
to O
create O
game B
levels B
, O
maps O
, O
enemies B
, O
quests O
and O
missions B
. O

Player B
modeling O
is O
used O
to O
predict O
player B
behavior I
or O
preferences O
, O
which O
can O
then O
be O
used O
to O
alter O
parameters O
of O
game B
play B
— O
such O
as O
difficulty O
— O
or O
to O
generate O
content O
. O

Data O
mining O
is O
used O
extensively O
to O
analyze O
how O
players B
are O
interacting O
with O
the O
game B
; O
data O
can O
be O
used O
to O
guide O
the O
development O
of O
patches O
, O
new O
content O
, O
or O
make O
business O
decisions O
. O

Related O
Work O
A O
course O
in O
Game O
AI O
may O
be O
taught O
as O
an O
advanced O
artificial O
intelligence O
class B
or O
as O
an O
alternative O
to O
an O
introductory O
artificial O
intelligence O
class B
. O

One O
of O
the O
primary O
challenges O
of O
designing O
a O
Game O
AI O
course O
is O
the O
creation O
of O
meaningful O
project O
work O
that O
allows O
students O
to O
get O
hands O
- O
on O
experience B
making O
design O
decisions O
and O
writing O
AI O
algorithms O
. O

There O
are O
three O
common O
approaches O
to O
coursework O
: O
( O
1 O
) O
using O
one O
or O
more O
commercial O
computer O
games B
; O
( O
2 O
) O
using O
a O
commercial O
game B
engine O
; O
or O
( O
3 O
) O
building O
a O
home O
- O
grown O
game B
and/or O
game B
engine O
. O

Many O
commercially O
available O
computer O
games B
can O
be O
“ O
modded O
, O
” O
allowing O
custom O
code O
to O
be O
incorporated O
into O
the O
game B
. O

However O
, O
these O
games B
prioritize O
graphics O
rendering O
and O
are O
not O
designed O
with O
any O
functionality O
in O
mind O
other O
than O
that O
delivered O
to O
the O
user B
. O

This O
makes O
it O
difficult O
to O
craft O
meaningful O
pedagogical O
experiences B
because O
they O
may O
not O
expose O
the O
right O
functionality O
through O
APIs O
. O

Games B
are O
likely O
to O
already O
have O
implementations O
of O
algorithms O
one O
wishes O
the O
student O
to O
write O
, O
which O
must O
be O
removed O
or O
ignored O
. O

One O
of O
the O
biggest O
challenges O
is O
the O
substantial O
time B
that O
must O
be O
devoted O
to O
learning O
the O
complex O
underlying O
codebase O
. O

In O
the O
event O
that O
any O
one O
game B
does O
n’t O
support B
all O
of O
the O
AI O
techniques B
covered O
in O
a O
class B
, O
switching O
between O
different O
code O
bases O
and O
programming O
languages O
can O
result O
in O
significant O
overhead O
that O
does O
not O
contribute O
directly O
to O
the O
pedagogical O
goals B
of O
the O
class B
. O

Furthermore O
, O
students O
may O
be O
required O
to O
purchase O
the O
game B
, O
and O
end O
user B
licenses O
may O
impose O
on O
the O
rights O
of O
students O
to O
own O
their O
own O
coursework O
. O

An O
alternative O
is O
working O
with O
general O
- O
purpose O
game B
engines O
, O
which O
are O
not O
full B
games I
, O
but O
provide O
core O
functionalities O
found O
across O
many O
games B
. O

One O
of O
the O
most O
popular O
, O
free B
game B
engines O
is O
Unity3D O
( O
http://unity3d.com O
) O
. O

Game O
engines O
operate O
as O
high B
- O
level I
programming I
environments O
with O
specialized O
libraries O
for O
common O
aspects O
of O
games B
. O

Commercial O
game B
engines O
have O
high O
learning O
curves O
because O
of O
their O
general O
purpose O
nature O
— O
most O
of O
the O
available O
complexity O
and O
functionality O
is O
irrelevant O
to O
the O
task O
of O
learning O
to O
program O
AI O
algorithms O
. O

Further O
, O
game B
engines O
are O
not O
themselves O
games B
require O
a O
game B
to O
be O
created O
either O
by O
the O
instructor O
or O
by O
students O
. O

Homegrown O
computer O
game B
environments O
and O
simulations O
have O
been O
developed O
by O
instructors O
. O

The O
Berkeley O
PacMan O
AI O
course O
materials O
have O
been O
adopted O
by O
many O
AI O
courses O
. O

However O
, O
Pac O
- O
Man O
is O
a O
toy O
problem O
in O
the O
sense O
that O
agent O
control B
of O
the O
Pac O
- O
Man O
agent O
is O
an O
artificial O
task O
— O
PacMan O
is O
supposed O
to O
be O
controlled B
by O
the O
human O
. O

The O
SEPIA O
simulation O
environment O
( O
Sosnowski O
et O
al O
. O

2013 O
) O
is O
a O
more O
realistic O
example O
because O
the O
opponent B
in O
a O
Real O
Time O
Strategy O
( O
RTS O
) O
game B
is O
often O
computer O
- O
controlled B
. O

Homegrown O
game B
and O
simulation O
environments O
overcome O
many O
of O
the O
limitations O
of O
commercial O
games B
and O
game B
engines O
. O

However O
, O
students O
are O
still O
often O
asked O
to O
re O
- O
implement O
existing O
AI O
functionality O
, O
which O
may O
be O
unsatisfying O
and O
perceived O
as O
make O
- O
work O
. O

The O
Game O
AI O
Game O
Engine O
( O
GAIGE O
) O
Our O
approach O
follows O
the O
homegrown O
game B
engine O
strategy B
. O

The O
Game O
AI O
Game O
Engine O
( O
GAIGE O
) O
is O
implemented O
in O
Python O
using O
the O
PyGame O
package O
for O
sprite O
rendering O
and O
animation B
. O

As O
a O
game B
engine O
, O
GAIGE O
provides O
general O
functionality O
used O
across O
many O
types O
of O
games B
except O
artificial O
intelligence O
. O

The O
game B
engine O
, O
however O
, O
is O
designed O
to O
support B
Game O
AI O
instruction O
by O
providing O
convenient O
hooks O
for O
artificial O
intelligence O
implementations O
. O

GAIGE O
uses O
simple O
sprite O
- O
based B
graphics O
to O
deemphasize O
the O
focus B
on O
the O
graphics O
of O
the O
game B
and O
keep O
the O
codebase O
simple O
enough O
that O
one O
can O
learn O
how O
it O
works O
quickly O
. O

For O
the O
purposes O
of O
most O
AI O
implementation O
in O
computer O
games B
, O
whether O
graphics O
are O
3D O
or O
2D O
has O
little O
bearing O
. O

Despite O
the O
simplicity O
of O
the O
code O
base B
, O
the O
game B
engine O
is O
modeled O
after O
much O
more O
complex O
engines O
such O
as O
the O
Unreal O
Tournament O
engine O
. O

The O
code O
base B
is O
highly B
object O
- I
oriented I
and O
modular O
, O
which O
allows O
new O
functionality O
to O
be O
integrated O
into O
the O
engine O
by O
sub O
- O
classing O
basic O
components O
. O

For O
example O
, O
new O
game B
types O
can O
be O
created O
by O
extending O
a O
single O
class B
type O
. O

Similarly O
, O
different O
pathfinding O
algorithms O
can O
be O
implemented O
by O
extending O
existing O
class B
types O
. O

GAIGE O
is O
implemented O
in O
Python O
, O
a O
popular O
scripting O
language O
that O
is O
becoming O
a O
popular O
language O
for O
University O
courses O
. O

Because O
GAIGE O
is O
implemented O
as O
a O
scripting O
language O
that O
is O
interpreted O
at O
run O
- O
time B
, O
it O
allows O
us O
to O
easy B
develop O
autograders O
for O
programming O
assignments O
. O

Unlike O
game B
development O
courses O
that O
involve O
a O
lot O
of O
design O
, O
Game O
AI O
is O
an O
algorithms O
class B
and O
algorithms O
either O
produce O
the O
right O
result O
or O
not O
and O
operate O
optimally O
or O
not O
. O

Special O
autograder O
scripts B
have O
been O
written O
that O
unit B
- O
test O
student O
- O
written O
code O
without O
all O
of O
the O
baggage O
of O
the O
rest O
of O
the O
game B
engine O
. O

When O
the O
entire O
game B
engine O
is O
necessary O
for O
evaluation O
, O
the O
game B
engine O
can O
be O
operated O
in O
“ O
headless O
” O
mode O
, O
meaning O
graphics O
are O
suppressed O
and O
the O
engine O
can O
be O
run O
many O
times B
faster O
than O
real B
- I
time I
. O

We O
designed O
and O
implemented O
a O
series O
of O
seven O
programming O
assignments O
. O

Artificial O
intelligence O
algorithms O
appear O
prominently O
and O
numerously O
in O
computer O
games B
. O

Each O
programming O
assignment O
contributes O
to O
the O
development O
of O
a O
fully O
functional O
Multi O
- O
user B
Online O
Battle O
Arena O
( O
MOBA O
) O
game B
. O

Assignments O
build O
off O
each O
other O
and O
create O
a O
sense O
of O
progression B
toward O
an O
ultimate O
purpose O
, O
instead O
of O
a O
series O
of O
disjoint O
exercises O
. O

Figure O
1 O
: O
Screenshot B
of O
GAIGE O
running O
a O
complete O
implementation O
of O
a O
Multi O
- O
User O
Online O
Battle O
Arena O
game B
. O

Multi O
- O
User O
Online O
Battle O
Arenas O
A O
Multi O
- O
user B
Online O
Battle O
Area O
( O
MOBA O
) O
is O
a O
genre O
of O
computer O
game B
in O
which O
players B
on O
opposing B
teams I
attempt O
to O
destroy O
each O
others O
’ O
bases O
. O

MOBAs B
are O
typically O
top O
- O
down O
perspective O
games B
and O
aesthetically O
resemble O
real B
- I
time I
strategy I
( O
RTS O
) O
games B
. O

Unlike O
RTS O
games B
in O
which O
each O
player B
is O
in O
charge O
of O
micro O
- O
managing O
an O
entire O
army O
of O
entities B
, O
MOBA O
players B
control B
a O
single O
entity B
, O
called B
a O
hero B
, O
which O
is O
just O
one O
of O
many O
entities B
in O
an O
army O
. O

Entities B
that O
are O
not O
directly O
controlled B
by O
the O
player B
, O
called B
minions B
, O
are O
controlled B
by O
the O
computer O
. O

Minions B
are O
numerous O
, O
simple O
, O
and O
weak B
, O
whereas O
there O
are O
few O
heroes B
that O
are O
relatively O
powerful B
and O
have O
special B
abilities I
. O

Thus O
, O
MOBAs B
combine O
elements O
of O
real B
- I
time I
strategy O
games I
and O
first O
- O
person O
shooters O
. O

AI O
manifests O
itself O
in O
MOBAs B
in O
a O
number O
of O
places O
including O
tactical O
decision O
making O
and O
pathfinding O
of O
minions B
. O

Students O
are O
asked O
to O
consider O
a O
single O
- O
player B
version O
of O
a O
MOBA O
where O
a O
user B
plays B
against O
a O
fully O
- O
automated O
opponent B
. O

Thus O
, O
AI O
must O
also O
be O
implemented O
for O
the O
opponent B
Hero O
’s O
pathfinding O
and O
more O
complicated O
, O
strategic B

 O
decision I
- I
making I
. O

Supporting B
AI O
Instruction O
with O
Modular O
Design O
To O
facilitate O
Game O
AI O
instruction O
, O
GAIGE O
uses O
a O
modular O
design O
in O
which O
artificial O
intelligence O
is O
abstracted O
out O
of O
the O
agent O
into O
separate O
, O
object B
- O
oriented O
modules O
. O

There O
are O
two O
principal O
classes B
of O
intelligence O
in O
computer O
games B
: O
( O
1 O
) O
path B
finding O
, O
in O
which O
an O
agent O
must O
find O
the O
optimal O
set O
of O
points B
to O
traverse O
to O
reach O
a O
target B
destination I
without O
colliding O
with O
the O
physical O
terrain O
, O
and O
( O
2 O
) O
decision O
making O
, O
Agent O
Navigator O
APSPNavigator O
AStarNavigator O
StateAgent O
Minion O
StateMachine O
State O
GridNavigator O
PathNavigator O
BehaviorTree O
BTNode O
Hero O
BTAgent O
... O
... O
Figure O
2 O
: O
Class B
diagram O
showing O
the O
modular O
design O
of O
artificial O
intelligence O
in O
GAIGE O
. O

in O
which O
an O
agent O
determines O
, O
at O
run O
time B
, O
which O
behaviors O
and O
animations B
to O
trigger B
( O
including O
initiating O
path B
finding O
) O
. O

In O
GAIGE O
, O
agents O
are O
very O
simple O
and O
are O
, O
by O
themselves O
, O
incapable O
of O
performing O
behavior O
more O
sophisticated O
than O
turning B
, O
shooting O
, O
and O
walking O
in O
a O
straight B
line O
. O

Instead O
of O
directly O
modifying O
the O
code O
of O
the O
agent O
, O
intelligence O
is O
implemented O
in O
separate O
objects B
that O
attach O
to O
agent O
objects B
and O
make O
callbacks O
to O
the O
agent O
to O
control B
where O
it O
moves B
or O
what O
it O
does O
. O

The O
modular O
separation O
of O
intelligence O
from O
the O
agent O
is O
shown O
in O
the O
class B
diagram O
in O
Figure O
2 O
. O

All O
agents O
make O
use O
of O
a O
Navigator O
object B
that O
is O
responsible O
for O
monitoring O
an O
agent O
’s O
location B
and O
make O
sure O
that O
the O
agent O
can O
move B
to O
a O
desired O
target B
location B
without O
colliding O
with O
obstacles B
. O

Given O
a O
target B
destination I
, O
Navigators O
can O
implement O
any O
algorithm O
to O
produce O
a O
path B
— O
a O
sequence O
of O
line O
segments O
guaranteed O
to O
avoid O
obstacles B
. O

Since O
agents O
only O
know O
how O
to O
walk O
in O
a O
straight B
line O
, O
a O
Navigator O
instructs O
the O
agent O
at O
the O
appropriate O
times B
on O
where O
to O
move B
next O
. O

Figure O
2 O
shows O
how O
Navigator O
can O
be O
sub O
- O
classed O
to O
implement O
grid O
- O
based B
navigation O
or O
two O
different O
path B
network O
search O
algorithms O
. O

Unless O
the O
behavior O
of O
an O
agent O
is O
hard B
- O
coded O
into O
the O
agent O
itself O
, O
some O
external O
object B
must O
monitor O
the O
agent O
’s O
local O
environment O
and O
make O
real B
- I
time I
decisions O
about O
behaviors O
to O
perform O
, O
animations B
to O
play B
, O
and O
where O
to O
move B
. O

Figure O
2 O
shows O
two O
strategies B
for O
decision B
- I
making I
: O
finitestate O
machines O
and O
behavior O
trees O
. O

The O
agent O
subclasses O
from O
a O
finite O
state O
machine O
or O
a O
behavior O
tree O
executor O
, O
respectively O
, O
and O
chooses O
which O
State O
object B
or O
Behavior O
( O
BTNode O
) O
object B
is O
currently O
“ O
in O
control B
” O
. O

The O
State O
or O
BTNode O
in O
control B
is O
responsible O
for O
calling B
back O
to O
the O
agent O
with O
functions O
( O
e.g. O
, O
move B
, O
shoot O
, O
etc O
. O

) O
or O
determining O
that O
another O
State O
or O
BTNode O
should O
be O
in O
control B
. O

In O
GAIGE O
, O
States O
and O
BTNodes O
have O
special O
code O
that O
executes O
when O
the O
object B
first O
takes O
control B
of O
the O
agent O
, O
when O
the O
object B
cedes O
control B
of O
the O
agent O
, O
and O
at O
every O
frame B
. O

Using O
GAIGE O
: O
Assignments O
In O
this O
section O
, O
we O
describe O
the O
sequence O
of O
assignments O
that O
explore O
the O
different O
ways O
in O
which O
AI O
is O
essential O
for O
games B
and O
build O
a O
complete O
, O
working O
MOBA O
game B
. O

Assignment O
1 O
: O
Grid O
Navigation O
Some O
computer O
games B
, O
such O
as O
real O
time B
strategy B
games B
, O
use O
grid O
- O
based B
location B
of O
agents O
. O

In O
this O
assignment O
, O
students O
must O
set O
up O
the O
data O
structures O
for O
a O
navigation O
grid O
— O
a O
table O
of O
Booleans O
indicating O
traversable O
space O
in O
the O
map O
. O

This O
table O
is O
used O
by O
greedy O
navigator O
provided O
to O
the O
students O
. O

The O
point B
of O
the O
assignment O
is O
to O
familiarize O
the O
student O
with O
the O
inner O
working O
of O
the O
game B
engine O
and O
how O
Navigator O
objects B
control B
agents O
. O

Assignment O
2 O
: O
Navigation O
Meshes O
We O
turn B
away O
from O
grid O
- O
based B
navigation O
to O
the O
much O
more O
commonly O
used O
pathnode O
networks O
, O
in O
which O
invisible B
waypoints O
are O
positioned O
at O
key O
points B
in O
the O
virtual O
world B
. O

A O
sparse O
set O
of O
waypoints O
and O
arcs O
indicate O
navigable O
areas B
in O
the O
map O
, O
although O
an O
agent O
can O
deviate O
from O
the O
pathnode O
network O
when O
engaged O
in O
combat B
, O
to O
pick B
up O
items B
, O
or O
to O
cut O
a O
corner O
. O

In O
many O
games B
, O
pathnode O
networks O
are O
hard B
- O
coded O
into O
virtual O
worlds B
by O
designers O
. O

However O
, O
for O
procedurally O
generated O
worlds B
, O
or O
worlds B
created O
by O
end O
- O
user B
content O
designers O
, O
the O
pathnode O
network O
must O
be O
generated O
automatically O
. O

The O
optimal O
placement O
of O
pathnodes O
in O
a O
virtual O
environment O
is O
non O
- O
trivial O
. O

One O
technique B
is O
to O
first O
generate O
a O
navigation O
mesh O
, O
a O
set O
of O
convex O
polygonal O
hulls O
that O
overlay O
navigable O
space O
. O

An O
agent O
can O
move B
in O
a O
straight B
line O
between O
any O
two O
points B
within O
a O
convex O
hull O
without O
risk O
of O
collision O
with O
static O
terrain O
obstacles B
. O

Waypoints O
can O
be O
placed O
along O
common O
edges O
between O
convex O
hulls O
, O
guaranteeing O
a O
navigable O
pathnode O
network O
. O

In O
this O
assignment O
, O
students O
write O
the O
code O
to O
create O
a O
navigation O
mesh O
and O
the O
automatic O
construction O
of O
a O
pathnode O
network O
. O

The O
next O
two O
assignments O
build O
off O
this O
one O
, O
implementing O
graph O
search O
on O
networks O
that O
were O
first O
generated O
by O
the O
student O
’s O
code O
. O

This O
is O
an O
unique O
opportunity O
to O
practice O
implementing O
graph O
search O
on O
networks O
that O
are O
not O
pristine O
, O
toy O
graphs O
provided O
by O
the O
instructor O
. O

We O
feel O
this O
to O
be O
a O
more O
realistic O
scenario O
. O

Should O
students O
’ O
solutions O
to O
this O
assignment O
be O
flawed O
, O
they O
are O
provided O
with O
an O
instructor O
solution O
so O
they O
can O
move B
forward O
. O

Assignment O
3 O
: O
All O
- O
Pairs O
Shortest O
- O
Path B
Navigation O
While O
A O
* O
is O
the O
most O
commonly O
used O
graph O
search O
algorithm O
, O
all O
- O
pairs O
shortest O
- O
path B
( O
APSP O
) O
algorithms O
such O
as O
the O
Floyd O
- O
Warshall O
algorithm O
can O
be O
used O
in O
worlds B
with O
static O
obstacles B
because O
the O
shortest O
path B
between O
any O
two O
waypoints O
will O
never O
change O
. O

In O
this O
assignment O
, O
students O
pre O
- O
process O
a O
pathnode O
network O
( O
generated O
from O
a O
navigation O
mesh O
) O
with O
an O
APSP O
algorithm O
, O
producing O
a O
successor O
- O
node O
table O
. O

Students O
then O
implement O
linear O
time B
routines O
to O
reconstruct O
the O
shortest O
path B
between O
any O
two O
waypoints O
. O

An O
agent O
is O
provided O
that O
uses O
the O
students O
’ O
navigation O
code O
to O
collect O
crystals B
placed O
around O
the O
world B
. O

Assignment O
4 O
: O
A O
* O
Navigation O
In O
this O
assignment O
, O
students O
are O
provided O
with O
a O
game B
world B
with O
dynamic O
obstacles B
, O
in O
the O
form O
of O
walls O
that O
randomly O
appear O
and O
disappear O
throughout O
the O
environment O
( O
see O
upper O
right O
of O
Figure O
1 O
) O
. O

As O
with O
the O
previous O
assignment O
, O
an O
agent O
must O
collect O
crystals B
but O
must O
now O
account O
for O
the O
fact O
that O
any O
path B
may O
be O
blocked O
at O
any O
time B
. O

Students O
must O
implement O
the O
A O
* O
algorithm O
with O
replanning O
. O

As O
an O
optional O
component O
to O
the O
assignment O
for O
extra O
? O
Retreat B
? O
→ O
→ O
Chase O
Minion B
Kill B
Hero O
Chase O
Hero O
Kill B
Minion B
Figure O
3 O
: O
An O
example O
of O
a O
behavior O
tree O
. O

Question O
- O
marks B
are O
choice O
nodes O
that O
select O
the O
first O
child O
that O
is O
applicable O
. O

Arrows B
are O
sequence O
nodes O
that O
execute O
all O
children O
in O
the O
order O
given O
. O

credit O
, O
students O
can O
implement O
smoothing O
routines O
. O

Smoothing O
is O
a O
process O
of O
optimizing O
an O
agent O
’s O
path B
through O
a O
space O
by O
skipping O
waypoints O
and/or O
cutting O
corners O
to O
minimize O
the O
overall O
distance O
the O
agent O
has O
to O
move B
to O
arrive O
at O
a O
target B
destination I
. O

Literally O
following O
the O
arcs O
of O
a O
pathnode O
network O
results O
in B
robotic O
- I
looking I
behavior O
, O
and O
smoothing O
can O
make O
agent O
movement B
look O
more O
fluid O
and O
natural O
. O

Assignment O
5 O
: O
Finite O
State O
Machines O
In O
previous O
assignments O
, O
the O
agent O
is O
pre O
- O
programmed O
to O
collect O
crystals B
and O
the O
student O
must O
ensure O
it O
can O
get O
from O
crystal B
to O
crystal B
without O
collision O
. O

In O
this O
assignment O
, O
students O
must O
determine O
what O
behaviors O
, O
including O
moving B
and O
shooting O
, O
agents O
must O
perform O
and O
when O
. O

Students O
are O
provided O
with O
a O
MOBA O
game B
with O
only O
minion B
agents O
. O

Bases O
from O
both O
teams B
spawn O
a O
minion B
every O
few O
seconds O
. O

Students O
must O
implement O
a O
finite O
state O
machine O
that O
controls B
each O
minion B
to O
attack B
enemy B
towers I
and O
bases O
. O

The O
AI O
implementation O
must O
be O
able O
to O
destroy O
an O
enemy B
base I
with O
fewer O
than O
a O
set B
number O
of I
minions I
. O

Minion B
agents O
do O
not O
need O
to O
strictly O
reproduce O
typical O
MOBA O
minion B
behavior I
, O
and O
students O
are O
encouraged O
to O
get O
creative O
to O
see O
how O
fast O
they O
can O
destroy O
the O
enemy B
base I
and O
with O
how O
few O
minions B
. O

While O
finite O
state O
machines O
are O
often O
learned O
early B
in O
a O
computer O
science O
curriculum O
, O
the O
finite O
state O
machines O
in O
games B
are O
closely O
integrated O
with O
game B
engines O
and O
behaviors O
can O
happen O
at O
state O
transition O
time B
as O
well O
as O
at O
every O
tick O
between O
transitions O
. O

This O
assignment O
is O
fundamentally O
about O
design O
— O
the O
translation O
of O
a O
specification O
for O
minion B

 O
behavior I
over O
time B
into O
a O
finite O
state O
machine O
that O
operates O
in O
a O
real B
- I
time I
dynamic O
environment O
. O

Assignment O
6 O
: O
Behavior O
Trees O
Whereas O
assignment O
5 O
focuses B
on O
minions B
, O
assignment O
6 O
focuses B
on O
hero B
agents O
. O

Hero O
agents O
have O
different O
abilities B
, O
including O
two O
types O
of O
weapons O
and O
the O
ability B
to O
jump O
out O
of O
the O
way O
of O
incoming O
fire O
. O

They O
become O
stronger O
as O
they O
destroy O
opponents B
, O
but O
lose O
all O
enhancements O
when O
they O
die O
and O
respawn B
. O

Since O
there O
is O
only O
a O
single B
hero I
per O
team B
, O
the O
AI O
that O
controls B
a O
hero B
agent O
msut O
be O
more O
strategic O
. O

Students O
are O
asked O
to O
implement O
a O
behavior O
tree O
that O
controls B
a O
hero B
agent O
. O

A O
behavior O
tree O
is O
a O
version O
of O
real B
- I
time I
hierarchical O
planning O
, O
capable O
of O
generating O
and O
executing O
temporally O
extended O
sequences O
of O
actions O
. O

Figure O
3 O
shows O
a O
simple O
behavior O
tree O
. O

There O
are O
two O
parts O
to O
the O
assignment O
. O

First O
, O
the O
students O
must O
complete O
an O
implementation O
of O
the O
behavior O
tree O
algorithm O
, O
which O
is O
tested O
on O
a O
suite O
of O
test O
inputs B
that O
does O
not O
use O
the O
game B
engine O
. O

Second O
, O
the O
students O
must O
design O
a O
set O
of O
behaviors O
for O
use O
controlling B
a O
MOBA O
hero B
character B
. O

While O
the O
behavior O
tree O
logic O
is O
simple O
in O
theory O
, O
the O
solution O
is O
complicated O
by O
the O
demands O
of O
a O
real B
- I
time I
game I
engine O
since O
behaviors O
can O
be O
temporally O
extended O
across O
multiple O
game B
ticks O
. O

Students O
are O
asked O
to O
focus B
on O
hero B
- O
versus O
- O
hero B
combat B
. O

This O
is O
not O
a O
very O
common O
aspect O
of O
MOBA O
games B
. O

However O
, O
by O
focusing B
on O
hero B
- O
versus O
- O
hero B
combat B
, O
the O
solution O
is O
not O
dissimilar O
to O
the O
AI O
that O
might O
be O
used O
for O
a O
first O
- O
person O
shooter O
. O

In O
this O
way O
, O
students O
receive O
practical O
experience B
with O
multiple O
game B
genres O
while O
still O
building O
toward O
a O
single O
, O
complete O
MOBA O
game B
. O

Assignment O
7 O
: O
MOBA O
Competition O
The O
final O
assignment O
is O
a O
full B
MOBA O
competition B
( O
see O
Figure O
1 O
) O
, O
where O
each O
student O
must O
provide O
the O
AI O
for O
minions B
and O
heroes B
. O

Since O
minion B
AI O
was O
initially O
designed O
without O
consideration O
of O
the O
presence O
of O
heroes B
, O
and O
hero B
AI O
was O
initially O
designed O
to O
focus B
on O
hero B
- O
versus O
- O
hero B
combat B
, O
students O
must O
redesign O
both O
minion B
and O
hero B
AI O
controllers O
. O

This O
assignment O
is O
about O
design O
and O
the O
translation O
of O
design O
into O
algorithm O
. O

Because O
it O
does O
not O
introduce O
any O
new O
algorithms O
, O
this O
assignment O
is O
optional O
, O
with O
extra O
credit O
awarded O
based B
on O
competition B
performance O
. O

Using O
GAIGE O
: O
Experiences B
GAIGE O
has O
been O
used O
twice O
, O
once O
by O
the O
author O
of O
this O
paper O
, O
and O
once O
by O
another O
instructor O
at O
Georgia O
Tech O
. O

The O
first O
time B
GAIGE O
was O
used O
, O
a O
number O
of O
bugs O
were O
discovered O
in O
the O
underlying O
game B
engine O
, O
which O
were O
quickly O
patched O
. O

Despite O
the O
glitches B
, O
student O
anonymous O
opinions O
were O
unanimously O
favorable O
. O

Students O
appreciated O
the O
unified O
framework O
and O
the O
sense O
of O
building O
towards O
a O
larger O
goal B
. O

The O
course O
was O
fast O
- O
paced O
, O
with O
assignments O
every O
week O
or O
two O
. O

However O
, O
students O
did O
not O
generally O
believe O
the O
course O
to O
be O
too O
difficult O
. O

It O
should O
be O
noted O
that O
all O
students O
were O
required O
to O
have O
had O
taken O
an O
introductory O
AI O
course O
, O
which O
had O
already O
covered O
A O
* O
, O
thus O
the O
challenge O
of O
the O
A O
* O
assignment O
came O
from O
working O
with O
the O
generated O
path B
network O
, O
working O
with O
the O
run O
time B
environment O
, O
replanning O
, O
and O
smoothing O
. O

Assignment O
2 O
was O
the O
hardest B
assignment O
, O
with O
a O
distinctly O
lower O
mean O
grade O
than O
the O
other O
assignments O
( O
Figure O
4 O
) O
. O

Although O
navigation O
mesh O
generation O
is O
relatively O
straight B
- O
forward O
, O
there O
are O
many O
edge O
cases O
that O
have O
to O
be O
handled O
. O

The O
assignment O
is O
also O
longer O
than O
others O
, O
having O
two O
distinct O
phases O
: O
navigation O
mesh O
generation O
and O
path B
node O
placement O
. O

We O
have O
since O
created O
an O
Assignment O
1.5 O
, O
in O
which O
one O
must O
automatically O
create O
a O
navigation O
mesh O
from O
a O
set O
of O
pre O
- O
placed O
waypoints O
. O

The O
intention O
of O
this O
assignment O
is O
to O
familiarize O
students O
with O
path B
networks O
and O
how O
to O
determine O
which O
waypoints O
should O
be O
connected O
. O

As O
seen O
in O
Figure O
4 O
, O
students O
also O
found O
assignment O
6 O
to O
be O
more O
challenging O
than O
other O
assignments O
. O

Students O
’ O
agents O
must O
beat O
three O
, O
instructor O
- O
provided O
baseline O
hero B
agents O
in B
hero O
- I
versus I
- O
hero B
game B
play B
. O

Although O
the O
baseline O
Figure O
4 O
: O
Mean O
grades O
( O
out O
of O
10 O
points B
) O
for O
each O
of O
the O
first O
six O
assignments O
. O

agents O
were O
designed O
to O
be O
sub O
- O
optimal O
, O
one O
of O
the O
baselines O
turned B
out O
to O
be O
relatively O
difficult O
to O
beat O
; O
the O
game B
is O
simple O
enough O
that O
even O
a O
relatively O
simple O
strategy B
can O
be O
close B
to O
optimal O
. O

Future O
work O
requires O
revisions O
to O
the O
assignment O
6 O
baselines O
and O
autograder O
. O

The O
second O
time B
GAIGE O
was O
used O
a O
server B
was O
set O
up O
in O
which O
students O
could O
submit O
their O
solutions O
to O
the O
autograder O
for O
immediate O
feedback O
. O

The O
introduction O
of O
assignment O
1.5 O
improved O
the O
mean O
grade O
of O
assignment O
2 O
to O
7.5/10 O
; O
other O
assignments O
were O
in O
line O
with O
earlier B
scores O
. O

Many O
students O
chose O
to O
rely O
on O
an O
instructor O
- O
provided O
solution O
to O
assignment O
2 O
for O
subsequent O
assignments O
. O

This O
did O
not O
impair O
the O
performance O
of O
students O
since O
the O
modular O
nature O
of O
GAIGE O
meant O
that O
subsequent O
AI O
algorithms O
did O
not O
need O
to O
know O
the O
particulars O
of O
path B
network O
generation O
. O

In O
some O
cases O
, O
student O
solutions O
were O
superior O
to O
the O
instructor O
solution O
. O

Some O
students O
also O
used O
an O
instructor O
- O
provided O
A O
* O
implementation O
for O
use O
on O
assignments O
5 O
, O
6 O
, O
and O
7 O
. O

Due O
to O
the O
modular O
nature O
of O
GAIGE O
, O
AI O
behavior O
- O
control B
( O
finite O
state O
machines O
and O
behavior O
trees O
) O
do O
not O
require O
knowledge O
about O
path B
finding O
to O
work O
together O
. O

GAIGE O
makes O
it O
easy B
to O
work O
with O
to O
create O
new O
assignments O
. O

After O
students O
showed O
a O
surprising O
amount O
on O
interest O
in O
a O
lecture O
on O
binary O
space O
partitions O
, O
we O
were O
able O
to O
create O
a O
new O
assignment O
in O
less O
than O
a O
week O
. O

A O
binary O
space O
partition O
is O
a O
binary O
search O
tree O
in O
which O
nodes O
represent O
rectangular O
portions O
of O
the O
game B
world B
in O
which O
obstacles B
appear O
. O

The O
binary O
search O
tree O
can O
be O
used O
to O
filter O
the O
geometry O
of O
a O
game B
world B
when O
performing O
line O
- O
of O
- O
sight O
calculations O
, O
increasing B
computational O
efficiency O
of O
the O
game B
engine O
. O

This O
assignment O
was O
ultimately O
not O
released O
to O
the O
students O
, O
but O
may O
be O
used O
in O
future O
iterations O
of O
the O
class B
. O

We O
believe O
that O
it O
will O
be O
possible O
and O
relatively O
easy B
to O
create O
GAIGE O
structured O
assignments O
for O
procedural O
content O
generation O
, O
data O
mining O
and O
analytics O
, O
player B
modeling O
, O
and O
dynamic O
difficulty O
adjustment O
. O

In O
the O
Spring O
of O
2015 O
, O
graduate O
students O
enrolled O
in O
our O
Game O
AI O
performed O
an O
additional O
assignment O
: O
to O
develop O
an O
algorithm O
that O
procedurally O
generated O
GAIGE O
game B
maps I
based B
on O
player B
metrics O
. O

Graduate O
students O
were O
allowed O
to O
modify O
any O
aspect O
of O
the O
game B
engine O
necessary O
to O
collect O
statistics O
about O
human O
players B
’ O
in B
- I
game I
behaviors O
and O
develop O
an O
algorithm O
that O
produced O
demonstrably O
different O
map O
configurations O
based B
on O
the O
data O
collected O
. O

Conclusions O
Game O
AI O
is O
distinct O
from O
conventional O
artificial O
intelligence O
, O
yet O
both O
draw O
from O
the O
same O
algorithmic O
roots O
. O

Game O
AI O
has O
the O
additional O
appeal O
of O
using O
computer O
games B
to O
motivate O
and O
inspire O
student O
interest O
. O

Thus O
Game O
AI O
can O
act O
as O
an O
alternative O
to O
a O
standard O
course O
on O
artificial O
intelligence O
, O
or O
a O
follow O
- O
on O
course O
. O

Game O
AI O
emphasizes O
the O
relationship O
between O
design O
and O
algorithm O
. O

The O
practice O
of O
artificial O
intelligence O
always O
involves O
an O
element O
of O
design O
, O
in O
the O
form O
of O
choice O
about O
representations O
, O
data O
structures O
, O
and O
algorithms O
. O

This O
choice O
is O
often O
lost O
in O
artificial O
intelligence O
courses O
where O
problems O
are O
well O
- O
defined O
and O
packaged O
up O
for O
students O
to O
solve O
. O

In O
a O
Game O
AI O
course O
, O
there O
are O
ample O
opportunities O
to O
practice O
making O
choices O
about O
representation O
and O
algorithm O
to O
solve O
a O
real O
problem O
. O

GAIGE O
is O
a O
Python O
game B
engine O
designed O
to O
support B
a O
progression B
of O
assignments O
that O
implement O
the O
AI O
components O
of O
a O
Multi O
- O
User O
Battle O
Arena O
game B
. O

Each O
assignment O
explores O
one O
way O
in O
which O
AI O
is O
used O
in O
modern O
computer O
game B
design O
and O
development O
. O

The O
progression B
breaks O
the O
task O
of O
creating O
a O
complete O
game B
into O
well O
- O
defined O
chunks O
and O
creates O
a O
sense O
of O
accomplishment O
. O

GAIGE O
shows O
how O
artificial O
intelligence O
and O
game B
design O
can O
be O
practiced O
in O
a O
synchronized O
fashion O
without O
sacrificing O
well O
- O
defined O
metrics O
for O
success O
that O
make O
autograding O
possible O
. O

The O
presence O
of O
autograders O
means O
that O
, O
with O
GAIGE O
, O
Game O
AI O
courses O
can O
now O
be O
scaled O
up O
to O
large O
class B
sizes O
, O
which O
is O
important O
as O
computer O
science O
enrollments O
grow O
and O
online O
instruction O
becomes O
more O
prevalent O
. O

References O
[ O
DeNero O
and O
Klein O
2010 O
] O
DeNero O
, O
J. O
, O
and O
Klein O
, O
D. O
2010 O
. O

Teaching O
introductory O
artificial O
intelligence O
with O
Pacman O
. O

In O
Proceedings O
of O
the O
2010 O
AAAI O
Symposium O
Educational O
Advances O
in O
Artificial O
Intelligence O
. O

[ O
Ghallab O
, O
Nau O
, O
and O
Traverso O
2004 O
] O
Ghallab O
, O
M. O
; O
Nau O
, O
D. O
; O
and O
Traverso O
, O
P. O
2004 O
. O

Automated O
Planning O
: O
Theory O
and O
Practice O
. O

Morgan O
Kaufmann O
. O

[ O
Riedl O
and O
Bulitko O
2013 O
] O
Riedl O
, O
M. O
O. O
, O
and O
Bulitko O
, O
V. O
2013 O
. O

Interactive O
narrative O
: O
An O
intelligent O
systems O
approach O
. O

AI O
Magazine O
34(1):67–77 O
. O

[ O
Sosnowski O
et O
al O
. O

2013 O
] O
Sosnowski O
, O
S. O
; O
Ernsberger O
, O
T. O
; O
Cao O
, O
F. O
; O
and O
Ray O
, O
S. O
2013 O
. O

SEPIA O
: O
A O
scalable O
game B
environment O
for O
artificial O
intelligence O
teaching O
and O
research O
. O

In O
Proceedings O
of O
the O
2013 O
AAAI O
Symposium O
Educational O
Advances O
in O
Artificial O
Intelligence O
. O

[ O
Taylor O
2011 O
] O
Taylor O
, O
M. O
2011 O
. O

Teaching O
reinforcement O
learning O
with O
mario O
: O
An O
argument O
and O
case O
study O
. O

In O
Proceedings O
of O
the O
2011 O
AAAI O
Symposium O
Educational O
Advances O
in O
Artificial O
Intelligence O
. O

[ O
Wong O
, O
Zink O
, O
and O
Koenig O
2010 O
] O
Wong O
, O
D. O
; O
Zink O
, O
R. O
; O
and O
Koenig O
, O
S. O
2010 O
. O

Teaching O
artificial O
intelligence O
and O
robotics O
via O
games B
. O

In O
Proceedings O
of O
the O
2010 O
AAAI O
Symposium O
Educational O
Advances O
in O
Artificial O
Intelligence O
. O

Real B
- I
Time I
Video O
Highlights O
for O
Yahoo O
Esports O
Yale O
Song O
Yahoo O
Research O
New O
York O
, O
USA O
yalesong@yahoo-inc.com O
Abstract O
Esports O
has O
gained O
global O
popularity O
in O
recent O
years O
and O
several O
companies O
have O
started B
offering O
live O
streaming O
videos O
of O
esports O
games B
and O
events O
. O

This O
creates O
opportunities O
to O
develop O
large O
scale O
video O
understanding O
systems O
for O
new O
product O
features O
and O
services O
. O

We O
present O
a O
technique B
for O
detecting O
highlights B
from O
live O
streaming O
videos O
of O
esports O
game B
matches I
. O

Most O
video B
games I
use O
pronounced O
visual B
effects I
to O
emphasize O
highlight B
moments O
; O
we O
use O
CNNs O
to O
learn O
convolution O
filters O
of O
those O
visual B
effects I
for O
detecting O
highlights B
. O

We O
propose O
a O
cascaded O
prediction O
approach O
that O
allows O
us O
to O
deal O
with O
several O
challenges O
arise O
in O
a O
production O
environment O
. O

We O
demonstrate O
our O
technique B
on O
our O
new O
dataset O
of O
three O
popular O
game B
titles O
, O
Heroes O
of O
the O
Storm O
, O
League O
of O
Legends O
, O
and O
Dota O
2 O
. O

Our O
technique B
achieves O
18 O
FPS O
on O
a O
single O
CPU O
with O
an O
average O
precision O
of O
up O
to O
83.18 O
% O
. O

Part O
of O
our O
technique B
is O
currently O
deployed O
in O
production O
on O
Yahoo O
Esports O
. O

1 O
Introduction O
Esports O
is O
a O
form O
of O
competition B
on O
video B
games I
, O
where O
players B
compete O
with I
each I
other I
over O
prizes B
. O

The O
global O
esports O
market O
is O
growing O
fast O
, O
with O
an O
expected O
revenue O
of O
USD O
463 O
M O
and O
an O
audience O
of O
256 O
M O
people O
in O
2016 O
, O
which O
are O
42.6 O
% O
and O
13.3 O
% O
increases B
, O
respectively O
, O
compared O
to O
the O
previous O
year O
[ O
9 O
] O
. O

Several O
companies O
have O
recently O
launched O
websites O
dedicated O
to O
esports O
, O
e.g. O
, O
Twitch O
and O
YouTube O
Gaming O
. O

Yahoo O
launched O
Yahoo O
Esports O
in O
March O
2016 O
as O
the O
premier O
destination O
for O
delivering O
professional O
esports O
coverage O
across O
major B
games I
and O
events O
. O

The O
flourishing O
amount O
of O
esports O
videos O
calls B
for O
an O
efficient O
way O
to O
index O
and O
share O
them O
over O
various O
channels O
. O

Video O
highlighting O
is O
an O
attractive O
method O
to O
achieve O
this O
with O
many O
potential O
applications O
. O

For O
example O
, O
one O
can O
share O
highlights B
on O
social O
media O
and O
use O
them O
to O
summarize O
game B

 O
matches I
. O

In O
live O
broadcasting O
, O
highlights B
enable O
users B
to O
skim O
through O
the O
previously O
missed O
parts O
of O
a O
game B
, O
and O
allow O
companies O
to O
find O
ideal O
moments O
for O
programmatic O
ad O
placement O
. O

Previous O
research O
on O
automatic O
video O
highlighting O
and O
summarization O
have O
focused B
on O
generic O
online O
videos O
[ O
1 O
, O
12 O
, O
16 O
] O
, O
user B
generated O
content O
[ O
8 O
, O
14 O
] O
, O
and O
sports O
videos O
[ O
10 O
, O
2 O
] O
. O

Recent O
approaches O
use O
deep O
neural O
networks O
to O
develop O
an O
end O
- O
to O
- O
end O
trainable O
system O
[ O
3 O
, O
18 O
] O
. O

The O
most O
related O
to O
our O
work O
is O
sports O
video B
highlighting I
[ O
13 O
] O
; O
early B
approaches O
include O
an O
analysis O
of O
audio O
signals O
[ O
10 O
] O
and O
text O
overlays O
[ O
17 O
] O
. O

Despite O
its O
practical O
importance O
, O
esports O
video O
highlighting O
has O
received O
relatively O
little O
attention O
from O
the O
research O
community B
. O

In O
this O
paper O
, O
we O
present O
real B
- I
time I
video B
highlighting I
for O
Yahoo O
Esports O
. O

We O
observe O
that O
there O
are O
several O
pronounced O
visual B
effects I
in O
game B
highlights I
that O
set O
them O
apart O
from O
non O
- O
highlight B
game B
scenes O
, O
e.g. O
, O
splash O
of O
lights O
with O
special O
moves B
, O
which O
suggests O
there O
is O
a O
great O
opportunity O
to O
use O
the O
latest O
computer O
vision O
techniques B
to O
solve O
the O
problem O
. O

There O
are O
, O
however O
, O
several O
challenges O
that O
arise O
in O
order O
to O
deploy O
our O
system O
in O
a O
production O
environment O
, O
such O
as O
the O
real B
- I
time I
performance O
requirement O
and O
an O
ability B
to O
deal O
with O
various O
types O
of O
scenes O
in O
esports O
broadcasting O
( O
e.g. O
, O
interviews O
, O
commercials O
, O
etc O
. O

) O
that O
are O
not O
related O
to O
game B
scenes O
. O

To O
address O
these O
challenges O
, O
we O
propose O
a O
cascaded O
prediction O
approach O
that O
uses O
two O
visual O
classifiers O
in O
a O
cascaded O
manner O
. O

The O
first O
determines O
if O
a O
frame B
contains O
game B
play B
and O
stops O
further O
analyzing O
1st O
NIPS O
Workshop O
on O
Large O
Scale O
Computer O
Vision O
Systems O
( O
LSCVS O
2016 O
) O
, O
Barcelona O
, O
Spain O
. O

arXiv:1611.08780v1 O
[ O
cs O
. O

CV O
] O
27 O
Nov O
2016 O
Figure O
1 O
: O
Esports O
live O
streaming O
videos O
contain O
various O
types O
of O
scenes O
. O

This O
increases B
the O
variability O
of O
the O
input B
space O
, O
creating O
challenges O
for O
esports O
highlighting B
. O

Images O
from O
left O
to O
right O
, O
top O
( O
game B
scenes O
) O
: O
game B
play B
, O
game B
replay O
, O
game B
highlight I
, O
game B
character B
draft B
; O
and O
bottom B
( O
non B
- I
game I
scenes O
) O
: O
commentator B
, O
interview O
, O
game B
player B
, O
and O
crowds O
. O

Our O
cascaded O
prediction O
approach O
effectively O
discards O
scenes O
unrelated O
to O
game B
, O
and O
detects O
highlights B
from O
game B
play B
scenes O
only O
. O

it O
if O
not O
; O
the O
second O
determines O
if O
the O
frame B
contains O
a O
highlight B
. O

Crucial O
to O
the O
success O
of O
our O
system O
is O
a O
data O
set O
of O
over O
300 O
hours O
of O
videos O
annotated O
with O
scene O
types O
and O
highlights B
at O
the O
frame B
level B
. O

We O
present O
our O
two O
- O
stage B
annotation O
scheme O
that O
allowed O
us O
to O
collect O
our O
data O
set O
quite O
efficiently O
. O

2 O
Method O
Figure O
1 O
shows O
typical O
scenes O
that O
appear O
in O
esports O
live O
streaming O
videos O
, O
which O
can O
largely O
be O
categorized O
into O
game B
( O
top O
) O
and O
non B
- I
game I
( O
bottom B
) O
scenes O
. O

The O
former O
contains O
scenes O
that O
come O
from O
a O
computer O
game B
interface I
, O
including O
game B
play B
, O
game B
replay O
, O
and O
game B
character B
draft B
. O

The O
latter O
contains O
all O
other O
scenes O
, O
including O
commentators B
, O
player B
interviews O
, O
crowds O
, O
and O
commercials O
. O

We O
are O
primarily O
interested O
in O
detecting O
highlights B
from O
game B
play B
scenes O
only O
. O

Our O
system O
, O
therefore O
, O
needs O
to O
distinguish O
game B
play B
scenes O
from O
game B
replay O
scenes O
; O
otherwise O
, O
we O
may O
have O
duplicate O
highlights B
, O
one O
from O
game B
play B
and O
another O
from O
game B
replay O
. O

The O
non B
- I
game I
scenes O
contain O
many O
sub O
- O
categories O
, O
making O
it O
difficult O
to O
specify O
clearly O
. O

But O
we O
believe O
doing O
so O
is O
unnecessary O
in O
our O
work O
because O
we O
care O
only O
about O
the O
game B
play B
scenes O
– O
all O
others O
simply O
need O
to O
be O
filtered O
out O
. O

We O
formulate O
our O
problem O
as O
cascaded O
prediction O
with O
two O
visual O
classifiers O
: O
a O
scene O
type O
classifier O
and O
a O
highlight B
classifier O
. O

Each O
frame B
is O
first O
processed O
by O
the O
scene O
type O
classifier O
and O
categorized O
into O
one O
of O
four O
classes B
: O
game B
play B
, O
game B
replay O
, O
game B
character B
draft B
, O
and O
“ O
others O
” O
that O
include O
non B
- I
game I
scenes O
. O

If O
a O
frame B
is O
a O
game B
play B
scene O
, O
it O
is O
subsequently O
processed O
by O
the O
highlight B
classifier O
and O
categorized O
into O
either O
highlight B
or O
non O
- O
highlight B
. O

We O
take O
a O
normalized O
confidence O
score O
of O
the O
highlight B
class B
as O
the O
highlight B
score O
, O
ranged B
between O
0 O
and O
1 O
, O
and O
consider O
a O
frame B
a O
highlight B
by O
thresholding O
( O
in O
our O
experiments O
, O
we O
used O
a O
threshold O
of O
0.5 O
) O
. O

To O
achieve O
the O
real B
- I
time I
performance O
, O
we O
opted O
for O
a O
frame B
- O
based B
video O
analysis O
rather O
than O
sequencebased O
ones O
( O
e.g. O
, O
3D O
CNNs O
[ O
15 O
] O
or O
RNNs O
[ O
4 O
] O
) O
. O

We O
sample O
and O
process O
every O
5th O
frame B
of O
a O
video O
, O
and O
linearly O
interpolate O
the O
results O
to O
the O
original O
sampling O
rate O
. O

We O
can O
use O
any O
frame B
- O
based B
visual O
classifier O
as O
long O
as O
it O
provides O
good O
speed O
performance O
. O

We O
selected O
Convolutional O
Neural O
Networks O
( O
CNN O
) O
[ O
7 O
] O
as O
the O
base B
model O
for O
its O
empirical O
success O
on O
many O
visual O
classification O
tasks O
[ O
11 O
] O
. O

The O
entire O
system O
is O
implemented O
in O
C++ O
using O
the O
OpenCV O
and O
the O
CAFFE O
libraries O
. O

For O
the O
CNN O
we O
used O
the O
AlexNet O
[ O
7 O
] O
with O
batch O
normalization O
[ O
5 O
] O
after O
each O
layer O
. O

We O
trained B
each O
model O
from O
scratch O
using O
the O
ADAM O
optimizer O
[ O
6 O
] O
, O
with O
a O
mini O
batch O
of O
128 O
frames B
and O
for O
100 O
epochs O
. O

The O
training O
was O
done O
using O
the O
CaffeOnSpark O
library O
on O
Yahoo O
grid O
infrastructure O
. O

Our O
system O
is O
simple O
yet O
effective O
. O

The O
two O
classifiers O
have O
a O
clear B
separation O
of O
learning O
problems O
: O
scene O
type O
categorization O
and O
highlight B
detection O
. O

The O
system O
achieves O
18 O
FPS O
on O
a O
single O
CPU O
machine O
, O
allowing O
us O
to O
process O
videos O
in B
real O
- I
time I
( O
processing O
every O
5th O
frame B
of O
video O
requires O
a O
minimum O
of O
6 O
FPS O
) O
. O

Part O
of O
our O
technique B
is O
currently O
deployed O
in O
production O
on O
Yahoo O
Esports O
. O

3 O
Data O
Collection O
We O
collected O
a O
dataset O
of O
esports O
videos O
for O
three O
popular O
game B
titles O
: O
Heroes B
of O
the O
Storm O
( O
HotS O
) O
, O
League O
of O
Legends O
( O
LoL O
) O
, O
and O
Dota2 O
. O

Our O
dataset O
contains O
roughly O
100 O
hours O
of O
videos O
for O
each O
game B
title O
, O
with O
a O
total O
of O
about O
300 O
hours O
( O
see O
Table O
1 O
) O
. O

All O
videos O
are O
live O
recordings O
of O
major O
2 O
Game O
Title O
Non O
- O
game B
Game O
Total O
Non O
- O
highlight B
High O
. O

lvl1 O
High O
. O

lvl2 O
High O
. O

lvl3 O
HotS O
31h49m28s O
50h00m00s O
06h20m03s O
02h57m55s O
00h38m46s O
91h46m13s O
LoL O
37h18m26s O
63h23m59s O
06h42m11s O
03h05m38s O
00h31m55s O
111h02m11s O
Dota2 O
19h56m13s O
79h55m17s O
11h06m57s O
03h44m44s O
00h21m08s O
115h04m21s O
Total O
89h04m07s O
193h19m17s O
24h09m12s O
09h48m18s O
01h31m50s O
317h52m47s O
Table O
1 O
: O
The O
total O
duration O
of O
videos O
per O
scene O
type O
in O
our O
dataset O
. O

The O
non B
- I
game I
includes O
all O
kinds O
of O
scenes O
but O
game B
play B
, O
i.e. O
, O
game B
replay O
, O
game B
character B
draft B
, O
and O
“ O
others O
. O

” O
esports O
games B
and O
events O
and O
include O
scenes O
that O
appear O
in O
the O
real B
- I
world I
esports O
live O
broadcasting O
scenario O
, O
such O
as O
interviews O
, O
studio O
scenes O
, O
game B
replays O
, O
etc O
. O

Annotating O
videos O
of O
more O
than O
300 O
hours O
is O
undoubtedly O
a O
challenging O
task O
, O
especially O
when O
it O
involves O
subjective O
measurements O
such O
as O
finding O
highlights B
. O

We O
instituted O
a O
two O
- O
stage B
labeling O
scheme O
that O
is O
designed O
to O
reduce O
complexity O
in O
annotating O
video B
highlights I
. O

Scene O
type O
annotation O
. O

First O
, O
we O
categorize O
each O
part O
of O
a O
video O
into O
one O
of O
four O
scene O
types O
: O
game B
play B
, O
game B
replay O
, O
game B
character B
draft B
, O
and O
“ O
others O
” O
that O
include O
commentators B
, O
crowds O
, O
etc O
. O

We O
employ O
a O
machine O
- O
in O
- O
the O
- O
loop O
approach O
to O
do O
this O
efficiently O
. O

For O
each O
game B
title O
, O
we O
begin O
by O
annotating O
from O
scratch O
a O
small O
batch O
of O
videos O
( O
about O
10 O
hours O
) O
. O

We O
then O
use O
it O
to O
train B
a O
scene O
type O
classifier O
( O
i.e. O
, O
the O
CNN O
explained O
in O
the O
previous O
section O
) O
, O
and O
use O
the O
trained B
model O
to O
predict O
labels O
for O
the O
next O
batch O
( O
about O
20 O
hours O
) O
. O

The O
predicted O
labels O
guide O
the O
second O
round O
of O
annotation O
, O
which O
involves O
correcting O
“ O
mistakes B
” O
in O
the O
prediction O
results O
, O
rather O
than O
providing O
labels O
from O
scratch O
. O

Once O
the O
second O
batch O
is O
finished O
, O
we O
combine O
all O
annotated O
videos O
to O
train B
a O
new O
classifier O
, O
and O
use O
it O
to O
come O
up O
with O
predictions O
for O
the O
next O
batch O
( O
about O
30 O
hours O
) O
. O

We O
iterate O
this O
until O
we O
annotate O
all O
videos O
; O
each O
game B
typically O
takes O
about O
four O
rounds O
. O

Because O
this O
stage B
involves O
objective B
judgment B
, O
we O
opted O
for O
having O
one O
expert O
annotator O
go O
through O
all O
the O
videos O
in O
our O
dataset O
; O
an O
esports O
editor O
from O
Yahoo O
Esports O
volunteered O
for O
this O
role B
. O

This O
helped O
us O
maintain O
consistency O
across O
videos O
and O
obtain O
high O
quality O
labels O
. O

Highlight B
annotation O
. O

Next O
, O
we O
identify O
highlight B
moments O
from O
game B
play B
scenes O
. O

Unlike O
scene O
type O
labels O
, O
finding O
highlights B
from O
a O
video O
is O
a O
subjective O
task O
that O
can O
benefit O
from O
multiple O
measurements O
from O
different O
annotators O
. O

We O
therefore O
employ O
a O
crowdsourcing O
task O
in O
this O
stage B
. O

We O
designed O
a O
web O
interface B
that O
allows O
annotators O
to O
find O
and O
identify O
interesting O
moments O
in O
a O
video O
. O

With O
it O
, O
one O
can O
adjust O
the O
video O
playback O
speed O
and O
skip O
parts O
that O
are O
not O
game B
play B
scenes O
, O
using O
the O
scene O
type O
labels O
obtained O
from O
the O
previous O
stage B
. O

After O
many O
iterations O
and O
considering O
feedback O
received O
from O
Yahoo O
Esports O
editors O
, O
we O
opted O
for O
using O
categorical O
labels O
to O
indicate O
different O
levels B

 O
of I
highlights I
: O
level B
0 O
( O
non O
- O
highlight B
) O
, O
level B
1 O
( O
cool O
) O
, O
level B
2 O
( O
wow O
) O
, O
and O
level B
3 O
( O
OMG O
) O
. O

To O
ensure O
high O
quality O
labels O
, O
we O
had O
an O
influential O
figure O
in O
the O
esports O
community B
to O
personally O
reach O
out O
to O
the O
esports O
fans O
and O
enthusiasts O
, O
and O
recruit O
annotators O
who O
regularly O
watch O
live O
streamed O
esports O
videos O
. O

This O
allowed O
us O
to O
collect O
labels O
that O
contain O
semantic O
highlights B
( O
e.g. O
, O
main O
character B
dies O
) O
rather O
than O
mere O
low B
- O
level I
visual I
highlights B
( O
e.g. O
, O
splash O
of O
lights O
) O
. O

The O
annotators O
were O
monetarily O
compensated O
for O
their O
efforts O
. O

On O
average O
4 O
annotators O
labeled O
each O
video O
( O
min:3 O
, O
max:7 O
, O
median:4 O
) O
. O

The O
inter O
- O
rater O
reliability O
in O
terms O
of O
the O
Cronbach O
’s O
alpha O
was O
0.7627 O
. O

To O
further O
increase B
the O
quality O
of O
labels O
, O
for O
each O
video O
we O
chose O
three O
“ O
best O
” O
annotators O
who O
maximally O
agreed O
with O
each O
other O
according O
to O
the O
Cronbach O
’s O
alpha O
. O

This O
resulted O
in O
an O
increase B
in O
the O
Cronbach O
’s O
alpha O
to O
0.9225 O
. O

We O
use O
an O
average O
of O
highlight B
scores O
from O
the O
best O
3 O
annotators O
of O
each O
video O
as O
our O
final O
highlight B
label O
. O

4 O
Evaluation O
We O
treated O
each O
game B
title O
separately O
and O
performed O
three O
sets O
of O
experiments O
, O
one O
per O
game B
. O

For O
each O
game B
, O
we O
split B
our O
dataset O
so O
that O
60 O
% O
is O
used O
for O
training O
, O
20 O
% O
for O
validation O
, O
and O
20 O
% O
for O
test O
. O

We O
report B
our O
results O
in O
terms O
of O
average O
precision O
( O
AP O
) O
and O
recall O
at O
the O
frame B
- O
level B
. O

Models O
. O

We O
evaluated O
six O
approaches O
, O
largely O
grouped B
into O
Single O
and O
Cascade O
. O

The O
former O
uses O
a O
single O
model O
to O
detect O
highlights B
, O
while O
the O
latter O
uses O
two O
models O
– O
a O
scene O
type O
classifier O
and O
a O
highlight B
detector O
– O
in O
a O
cascaded O
manner O
. O

All O
the O
classification O
models O
used O
the O
softmax O
function O
. O

3 O
Model O
HotS B
LoL B
Dota2 O
AP O
Recall O
AP O
Recall O
AP O
Recall O
Single O
Random O
33.19 O
49.65 O
31.06 O
49.24 O
34.12 O
49.27 O
Binary O
62.31 O
75.14 O
38.33 O
43.93 O
57.86 O
61.40 O
Multiclass O
55.96 O
57.77 O
29.26 O
16.77 O
61.00 O
81.27 O
Cascade O
Random O
38.78 O
49.49 O
34.05 O
49.19 O
35.73 O
49.20 O
Regression O
51.72 O
19.57 O
26.67 O
9.83 O
50.00 O
44.08 O
Binary O
( O
Ours O
) O
83.18 O
86.29 O
59.66 O
56.76 O
67.90 O
77.22 O
Table O
2 O
: O
Evaluation O
results O
for O
each O
of O
the O
three O
game B
titles O
. O

Our O
approach O
outperforms O
all O
baselines O
in O
terms O
of O
AP O
and O
recall O
, O
except O
for O
recall O
on O
Dota2 O
. O

Single O
- O
Random O
produces O
random O
scores O
for O
all O
frames B
. O

Single O
- O
Binary O
uses O
a O
single O
binary O
classifier O
with O
one O
class B
representing O
game B
highlight I
and O
another O
representing O
all O
the O
other O
scene O
types O
. O

SingleMulticlass O
is O
a O
single O
classifier O
with O
5 O
categories O
: O
game B
highlight I
, O
game B
play B
, O
game B
replay O
, O
game B
character B
draft B
, O
and O
all O
the O
others O
. O

All O
cascade O
models O
shared O
one O
scene O
type O
classifier O
with O
4 O
categories O
: O
game B
play B
, O
game B
replay O
, O
game B
character B
draft B
, O
and O
all O
the O
others O
. O

We O
evaluated O
four O
variants O
of O
highlight B
detector O
. O

Cascade O
- O
Random O
produces O
random O
scores O
for O
all O
frames B
categorized O
as O
game B
play B
. O

Cascade O
- O
Regression O
directly O
estimates O
the O
highlight B
score O
; O
we O
used O
a O
Euclidean O
loss O
and O
considered O
a O
frame B
a O
highlight B
if O
the O
score O
is O
above O
1.0 O
. O

Cascade O
- O
Binary O
is O
our O
model O
that O
detects O
highlights B
using O
a O
binary O
classifier O
. O

Results O
. O

Table O
2 O
shows O
our O
experimental O
results O
in O
terms O
of O
AP O
and O
recall O
, O
for O
each O
of O
the O
three O
game B
titles O
. O

Our O
approach O
( O
Cascade O
- O
Binary O
) O
consistently O
outperforms O
all O
other O
baselines O
in O
terms O
of O
AP O
and O
recall O
. O

We O
make O
several O
interesting O
observations O
. O

We O
see O
that O
Cascade O
- O
Random O
consistently O
outperforms O
Single O
- O
Random O
in O
terms O
of O
AP O
. O

This O
shows O
the O
benefit O
of O
the O
cascaded O
prediction O
approach O
: O
Pre O
- O
filtering O
non B
- I
game I
scenes O
helps O
reduce O
making O
mistakes B
in O
highlight B
detection O
. O

It O
is O
also O
due O
to O
the O
fact O
that O
our O
scene O
type O
prediction O
model O
is O
very O
accurate O
, O
with O
above O
99 O
% O
AP O
and O
recall O
rates O
in O
all O
three O
games B
( O
it O
is O
a O
relatively O
easy B
task O
to O
discriminate O
game B
scenes O
from O
non B
- I
game I
scenes O
, O
because O
their O
pixel O
distributions O
are O
very O
different O
) O
. O

Comparing O
our O
approach O
against O
Single O
- O
Binary O
and O
Single O
- O
Multiclass O
shows O
the O
benefit O
of O
our O
cascaded O
prediction O
approach O
. O

All O
three O
models O
perform O
classification O
to O
detect O
highlights B
; O
the O
only O
difference O
is O
that O
we O
use O
an O
additional O
scene O
type O
classifier O
to O
filter O
out O
non B
- I
game I
scenes O
. O

This O
greatly O
improves O
AP O
, O
with O
a O
small O
trade O
off O
in O
speed O
performance O
( O
a O
single O
model O
achieves O
28 O
FPS O
) O
. O

Comparing O
our O
approach O
against O
Cascade O
- O
Regression O
shows O
that O
it O
is O
better O
to O
perform O
classification O
on O
binarized O
highlight B
scores O
rather O
than O
directly O
estimating O
it O
. O

The O
regression O
approach O
performs O
particularly O
worse O
in O
recall O
, O
missing O
most O
of O
the O
true O
highlight B
scenes O
. O

This O
is O
due O
in O
part O
to O
the O
heavily O
imbalanced O
sample O
distribution O
; O
the O
non O
- O
highlight B
sample O
size O
is O
larger O
than O
highlight B
level B
3 O
by O
more O
than O
100 O
times B
( O
see O
Table O
1 O
) O
. O

5 O
Conclusion O
We O
presented O
a O
cascaded O
prediction O
approach O
to O
detect O
highlights B
from O
esports O
live O
streaming O
videos O
, O
and O
showed O
the O
benefit O
of O
our O
approach O
on O
three O
popular O
game B
titles O
with O
over O
300 O
hours O
of O
videos O
. O

One O
of O
our O
primary O
goals B
in O
this O
work O
was O
to O
develop O
a O
system O
that O
is O
fast O
enough O
to O
be O
used O
in O
a O
production O
environment O
. O

We O
achieved O
the O
real B
- I
time I
performance O
via O
simple O
design O
of O
the O
system O
. O

There O
are O
, O
of O
course O
, O
many O
research O
areas B
we O
would O
like O
to O
explore O
further O
. O

One O
is O
transfer O
learning O
from O
one O
game B
to O
another O
. O

While O
there O
exists O
many O
game B
titles O
, O
there O
are O
only O
a O
few O
esports O
genres O
, O
such O
as O
real B
- I
time I
strategy I
, O
fighting O
, O
first O
- O
person O
shooter O
, O
and O
multi O
- O
player B
online O
battle B
arena O
( O
our O
three O
game B
titles O
belong O
to O
this O
genre O
) O
. O

Games B
in O
each O
genre O
share O
certain O
similarities O
in O
scene O
layouts O
and O
visual B
effects I
, O
with O
subtle O
differences O
between O
games B
. O

This O
makes O
our O
data O
set O
an O
interesting O
test O
bed O
for O
evaluating O
transfer O
learning O
techniques B
. O

Another O
is O
multimodal O
video O
processing O
. O

This O
work O
used O
visual O
signals O
only O
, O
but O
audio O
and O
text O
signals O
also O
provide O
valuable O
information O
; O
commentators B
scream O
during O
highlight B
moments O
, O
and O
game B
interfaces I
show O
text O
overlay O
during O
important O
moments O
( O
e.g. O
, O
“ O
triple O
kill B
” O
) O
. O

We O
look O
forward O
to O
using O
our O
large O
dataset O
to O
explore O
those O
areas B
in O
the O
future O
. O

4 O
Acknowledgement O
We O
thank O
Jordi O
Vallmitjana O
and O
the O
Yahoo O
Esports O
editorial O
team B
for O
their O
support B
on O
data O
collection O
, O
and O
the O
Yahoo O
Esports O
engineering O
team B
for O
their O
support B
on O
production O
deployment O
of O
our O
system O
. O

References O
[ O
1 O
] O
W.-S. O
Chu O
, O
Y. O
Song O
, O
and O
A. O
Jaimes O
. O

Video O
co O
- O
summarization O
: O
Video O
summarization O
by O
visual O
co O
- O
occurrence O
. O

In O
CVPR O
, O
2015 O
. O

[ O
2 O
] O
A. O
Ekin O
, O
A. O
M. O
Tekalp O
, O
and O
R. O
Mehrotra O
. O

Automatic O
soccer O
video O
analysis O
and O
summarization O
. O

IEEE O
Transactions O
on O
Image O
processing O
, O
2003 O
. O

[ O
3 O
] O
M. O
Gygli O
, O
Y. O
Song O
, O
and O
L. O
Cao O
. O

Video2GIF O
: O
Automatic O
generation O
of O
animated O
gifs O
from O
video O
. O

In O
CVPR O
, O
2016 O
. O

[ O
4 O
] O
S. O
Hochreiter O
and O
J. O
Schmidhuber O
. O

Long O
short O
- O
term O
memory O
. O

Neural O
computation O
, O
9(8):1735–1780 O
, O
1997 O
. O

[ O
5 O
] O
S. O
Ioffe O
and O
C. O
Szegedy O
. O

Batch O
normalization O
: O
Accelerating O
deep O
network O
training O
by O
reducing O
internal O
covariate O
shift O
. O

In O
ICML O
, O
2015 O
. O

[ O
6 O
] O
D. O
Kingma O
and O
J. O
Ba O
. O

ADAM O
: O
A O
method O
for O
stochastic O
optimization O
. O

arXiv O
preprint O
arXiv:1412.6980 O
, O
2014 O
. O

[ O
7 O
] O
A. O
Krizhevsky O
, O
I. O
Sutskever O
, O
and O
G. O
E. O
Hinton O
. O

Imagenet O
classification O
with O
deep O
convolutional O
neural O
networks O
. O

In O
NIPS O
, O
2012 O
. O

[ O
8 O
] O
Y. O
J. O
Lee O
, O
J. O
Ghosh O
, O
and O
K. O
Grauman O
. O

Discovering O
important O
people O
and O
objects B
for O
egocentric O
video O
summarization O
. O

In O
CVPR O
, O
2012 O
. O

[ O
9 O
] O
Newzoo O
. O

Global O
Esports O
Market O
Report O
: O
Revenues O
to O
Jump O
to O
$ O
463 O
M O
in O
2016 O
as O
US O
Leads B
the O
Way O
, O
Jan O
2016 O
( O
retrieved O
September O
16 O
, O
2016 O
) O
. O

[ O
10 O
] O
Y. O
Rui O
, O
A. O
Gupta O
, O
and O
A. O
Acero O
. O

Automatically O
extracting O
highlights B
for O
tv O
baseball O
programs O
. O

In O
ACM O
Multimedia O
, O
2000 O
. O

[ O
11 O
] O
O. O
Russakovsky O
, O
J. O
Deng O
, O
H. O
Su O
, O
J. O
Krause O
, O
S. O
Satheesh O
, O
S. O
Ma O
, O
Z. O
Huang O
, O
A. O
Karpathy O
, O
A. O
Khosla O
, O
M. O
Bernstein O
, O
et O
al O
. O

Imagenet O
large O
scale O
visual O
recognition O
challenge O
. O

IJCV O
, O
115(3):211–252 O
, O
2015 O
. O

[ O
12 O
] O
Y. O
Song O
, O
J. O
Vallmitjana O
, O
A. O
Stent O
, O
and O
A. O
Jaimes O
. O

TVSum O
: O
Summarizing O
web O
videos O
using O
titles O
. O

In O
CVPR O
, O
2015 O
. O

[ O
13 O
] O
J. O
Soni O
and O
P. O
Buch O
. O

Review O
on O
spontaneous O
highlight B
generation O
from O
sports O
video O
. O

Digital O
Image O
Processing O
, O
7(1):5–10 O
, O
2015 O
. O

[ O
14 O
] O
M. O
Sun O
, O
A. O
Farhadi O
, O
T.-H. O
Chen O
, O
and O
S. O
Seitz O
. O

Ranking B
highlights B
in O
personal O
videos O
by O
analyzing O
edited O
videos O
. O

IEEE O
Transactions O
on O
Image O
Processing O
, O
2016 O
. O

[ O
15 O
] O
D. O
Tran O
, O
L. O
Bourdev O
, O
R. O
Fergus O
, O
L. O
Torresani O
, O
and O
M. O
Paluri O
. O

Learning O
spatiotemporal O
features O
with O
3D O
convolutional O
networks O
. O

In O
ICCV O
, O
2015 O
. O

[ O
16 O
] O
G. O
Zen O
, O
P. O
de O
Juan O
, O
Y. O
Song O
, O
and O
A. O
Jaimes O
. O

Mouse O
activity O
as O
an O
indicator O
of O
interestingness O
in O
video O
. O

In O
ICMR O
, O
2016 O
. O

[ O
17 O
] O
D. O
Zhang O
and O
S.-F. O
Chang O
. O

Event O
detection O
in O
baseball O
video O
using O
superimposed O
caption O
recognition O
. O

In O
ACM O
Multimedia O
, O
2002 O
. O

[ O
18 O
] O
K. O
Zhang O
, O
W.-L. O
Chao O
, O
F. O
Sha O
, O
and O
K. O
Grauman O
. O

Video O
summarization O
with O
long O
short O
- O
term O
memory O
. O

2016 O
. O

5 O
Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
Anna O
Sapienza O
University O
of O
Southern O
California O
Information O
Sciences O
Institute O
annas@isi.edu O
Alessandro O
Bessi O
University O
of O
Southern O
California O
Information O
Sciences O
Institute O
bessi@isi.edu O
Emilio O
Ferrara O
University O
of O
Southern O
California O
Information O
Sciences O
Institute O
emiliofe@usc.edu O
ABSTRACT O
Multiplayer O
online O
bale O
arena O
has O
become O
a O
popular O
game B
genre O
. O

It O
also O
received O
increasing B
aention O
from O
our O
research O
community B
because O
they O
provide O
a O
wealth O
of O
information O
about O
human O
interactions O
and O
behaviors O
. O

A O
major O
problem O
is O
extracting O
meaningful O
paerns O
of O
activity O
from O
this O
type O
of O
data O
, O
in O
a O
way O
that O
is O
also O
easy B
to O
interpret O
. O

Here O
, O
we O
propose O
to O
exploit O
tensor O
decomposition O
techniques B
, O
and O
in O
particular O
Non O
- O
negative O
Tensor O
Factorization O
, O
to O
discover O
hidden O
correlated O
behavioral O
paerns O
of O
play B
in O
a O
popular O
game B
: O
League O
of O
Legends O
. O

We O
rst O
collect O
the O
entire O
gaming O
history O
of O
a O
group B
of O
about O
one O
thousand O
players B
, O
totaling O
roughly O
100 O
K O
matches B
. O

By O
applying O
our O
methodological O
framework O
, O
we O
then O
separate O
players B
into O
groups B
that O
exhibit O
similar O
features O
and O
playing B
strategies B
, O
as O
well O
as O
similar O
temporal O
trajectories O
, O
i.e. O
, O
behavioral O
progressions B
over O
the O
course O
of O
their O
gaming O
history O
: O
this O
will O
allow O
us O
to O
investigate O
how O
players B
learn O
and O
improve O
their O
skills B
. O

CCS O
CONCEPTS O
•Information O
systems→Massively O
multiplayer O
online B
games I
; O
Data O
mining O
; O
•Computing O
methodologies O
→ O
Factorization O
methods O
; O
•eory O
of O
computation O
→ O
Unsupervised O
learning O
and O
clustering O
; O
KEYWORDS O
Non O
- O
negative O
tensor O
factorization O
, O
temporal O
and O
topological O
paern O
mining O
, O
human O
behavior O
, O
multiplayer O
online B
game I
ACM O
Reference O
format O
: O
Anna O
Sapienza O
, O
Alessandro O
Bessi O
, O
and O
Emilio O
Ferrara O
. O

2017 O
. O

Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
. O

In O
Proceedings O
of O
ACM O
KDD O
2017 O
, O
Nova O
Scotia O
, O
Canada O
, O
August O
2017 O
( O
KDD’17 O
) O
, O
9 O
pages O
. O

DOI O
: O
10.475/123 O
4 O
1 O
INTRODUCTION O
Multiplayer O
Online O
Bale O
Arena O
( O
MOBA O
) O
is O
a O
genre O
of O
strategy B
online B
games I
that O
has O
drawn O
growing O
aention O
and O
has O
become O
extremely O
popular O
. O

MOBA O
consist O
of O
match B
- O
based B
games B
, O
where O
players B
, O
divided O
in O
two O
opposing B
teams I
, O
compete O
against O
each O
other O
. O

Each O
player B
during O
the O
match B
controls B
a O
single O
character B
( O
a.k.a O
. O

, O
hero B
) O
, O
having O
a O
specic O
role B
and O
abilities B
. O

Permission O
to O
make O
digital O
or O
hard B
copies O
of O
part O
or O
all O
of O
this O
work O
for O
personal O
or O
classroom O
use O
is O
granted O
without O
fee O
provided O
that O
copies O
are O
not O
made O
or O
distributed O
for O
prot O
or O
commercial O
advantage B
and O
that O
copies O
bear O
this O
notice O
and O
the O
full B
citation O
on O
the O
rst O
page O
. O

Copyrights O
for O
third O
- O
party O
components O
of O
this O
work O
must O
be O
honored O
. O

For O
all O
other O
uses O
, O
contact O
the O
owner O
/ O
author(s O
) O
. O

KDD’17 O
, O
Nova O
Scotia O
, O
Canada O
© O
2017 O
Copyright O
held O
by O
the O
owner O
/ O
author(s O
) O
. O

123 O
- O
4567 O
- O
24 O
- O
567/08/06 O
. O

. O

. O

$ O
15.00 O
DOI O
: O
10.475/123 O
4 O
e O
main O
goal B
in O
each O
match B
is O
to O
destroy O
the O
enemy B
team I
’s O
base B
, O
while O
enhancing O
the O
player B
level B
, O
increasing B
the O
abilities B
of O
the O
controlled B
character B
, O
and O
cooperating O
with O
one O
’s O
own O
teammates B
. O

is O
genre O
of O
games B
, O
including O
Heroes O
of O
the O
Storm O
, O
Dota O
2 O
, O
and O
League O
of O
Legends O
, O
has O
aracted O
researchers O
from O
dierent O
elds O
, O
especially O
because O
they O
provide O
a O
unique O
way O
to O
study O
the O
inuence O
of O
role B
- O
playing O
in O
competitive B
games I
[ O
7 O
, O
8 O
] O
, O
the O
impact B
of O
cooperation B
[ O
2 O
, O
15 O
] O
versus O
individual B
player I
aitudes O
[ O
20 O
] O
, O
social O
behaviors O
[ O
6 O
, O
16 O
, O
27 O
] O
, O
user B
commitment O
[ O
4 O
] O
, O
etc O
. O

e O
analysis O
of O
MOBA O
games B
also O
allows O
for O
the O
discovery O
of O
useful O
information O
to O
study O
the O
social O
dynamics O
of O
player B
communities B
. O

For O
example O
, O
by O
extracting O
players B
’ O
social O
activities O
and O
relations O
, O
researchers O
addressed O
issues O
such O
as O
gender O
gap O
[ O
23 O
, O
24 O
] O
and O
improved O
the O
user B
experience B
[ O
28 O
] O
. O

One O
advantage B
of O
analyzing O
players B
’ O
records O
in O
online B
games I
is O
the O
possibility O
of O
monitoring O
how O
their O
behaviors O
evolve O
over O
time B
. O

e O
temporal O
dimension O
exhibited O
by O
such O
data O
enables O
the O
study O
of O
the O
evolution B
of O
player B
performance O
, O
specically O
how O
players B
learn O
, O
adapt O
, O
and O
modify O
their O
playing O
strategies B
over O
time B
. O

We O
propose O
to O
study O
both O
the O
temporal O
and O
social O
dynamics O
of O
players B
in O
MOBA O
games B
at O
once O
. O

Here O
, O
we O
will O
focus B
on O
the O
analysis B

 O
of I
League I
of I
Legends I
( O
LoL O
) O
, O
a O
popular O
MOBA O
game B
. O

Our O
goal B
will O
be O
that O
of O
identifying O
dierent O
groups B
of O
players B
with O
common O
strategies B
, O
such O
as O
collaborative O
versus O
individualist O
players B
, O
and O
understanding O
how O
these O
groups B
of O
players B
behave O
in O
time B
, O
i.e. O
, O
how O
their O
strategies B
evolve O
over O
time B
. O

To O
this O
aim O
, O
we O
take O
advantage B
of O
Non O
- O
negative O
Tensor O
Factorization O
( O
NTF O
) O
[ O
3 O
, O
19 O
] O
techniques B
, O
which O
derive O
from O
multi O
- O
linear O
algebra O
. O

Non O
- O
negative O
tensor O
factorization O
allows O
to O
identify O
correlation O
in O
the O
data O
at O
dierent O
levels B
[ O
14 O
] O
: O
on O
the O
one O
hand O
, O
the O
application O
of O
the O
NTF O
helps O
in O
the O
identication O
of O
hidden B
topological O
structures O
in O
the O
data O
, O
like O
groups B
or O
communities B
, O
which O
are O
easy B
to O
interpret O
as O
they O
reect O
individuals O
’ O
social O
dynamics O
[ O
9 O
] O
. O

On O
the O
other O
hand O
, O
these O
topological O
structures O
share O
correlated O
activity O
paerns O
[ O
22 O
, O
26 O
] O
. O

Our O
purpose O
is O
therefore O
to O
leverage O
NTF O
to O
detect O
groups B
of O
players B
characterized O
by O
similar O
features O
and O
strategies B
as O
well O
as O
their O
temporal O
trajectories O
, O
i.e. O
, O
their O
evolution B
. O

Contributions O
rough O
the O
lens O
of O
NTF O
, O
we O
study O
the O
gaming O
history O
of O
about O
one O
thousand O
League O
of O
Legends O
players B
accounting O
for O
a O
total O
of O
roughly O
100 O
K O
matches B
. O

Our O
analysis O
will O
: O
• O
Highlight B
the O
existence O
of O
an O
underlying O
structure O
in O
the O
data O
, O
that O
allows O
us O
to O
divide O
players B
in O
groups B
characterized O
by O
similar O
features O
and O
having O
correlated O
temporal O
behaviors O
; O
arXiv:1702.05695v1 O
[ O
cs O
. O

LG O
] O
19 O
Feb O
2017 O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
Anna O
Sapienza O
, O
Alessandro O
Bessi O
, O
and O
Emilio O
Ferrara O
• O
Provide O
an O
interpretation O
of O
the O
components O
extracted O
by O
the O
NTF O
; O
• O
Validate O
the O
interpretation O
of O
the O
NTF O
results O
by O
analyzing O
the O
uncovered O
groups B
and O
their O
evolution B
over O
time B
; O
• O
Discover O
, O
by O
analyzing O
the O
temporal O
components O
, O
that O
players B
’ O
playing B
strategies B
are O
consistent O
over O
time B
; O
• O
Provide O
and O
validate O
an O
explanation O
for O
players B
behavioral O
stability O
, O
namely O
that O
the O
design O
of O
the O
game B
strongly O
impacts B
team B
formation I
in O
each O
match B
, O
thus O
manipulating O
the O
team B
’s O
probability O
of O
victory B
. O

Table O
1 O
: O
Notation O
used O
throughout O
the O
text O
. O

Notation O
Denition O
X O
constant O
x O
scalar O
x O
vector O
X O
matrix O
xij O
matrix O
entry O
X O
tensor O
xijk O
entry O
of O
a O
three O
- O
dimensional O
tensor O
◦ O
outer O
product O
2 O
METHODOLOGY O
e O
extraction O
of O
meaningful O
paerns O
of O
behavior O
in O
online B
games I
can O
be O
carried B
out O
by O
taking O
full B
advantage B
of O
tensor O
decomposition O
techniques B
. O

ese O
provide O
a O
way O
to O
disentangle O
the O
temporal O
and O
topological O
characteristics O
of O
the O
studied O
system O
. O

is O
is O
achieved O
by O
performing O
a O
dimensionality O
reduction O
on O
the O
original O
system O
. O

Let O
us O
consider O
a O
dataset O
composed O
by O
users B
( O
i.e. O
, O
players B
) O
, O
whose O
characteristics O
( O
i.e. O
, O
features O
) O
evolve O
over O
time B
. O

To O
perform O
the O
decomposition O
of O
such O
dataset O
, O
we O
need O
to O
represent O
it O
as O
a O
tensor O
( O
i.e. O
, O
a O
multi O
- O
dimensional O
array O
) O
. O

is O
can O
be O
done O
by O
assigning O
to O
each O
dimension O
of O
the O
tensor O
the O
dierent O
dimensions O
of O
the O
data O
, O
namely O
users B
, O
features O
, O
and O
time B
. O

In O
this O
context O
, O
the O
time B
axis O
corresponds O
to O
LoL B
matches I
. O

Following O
the O
notation O
reported B
in O
Table O
1 O
, O
we O
thus O
dene O
a O
three O
- O
dimensional O
array O
, O
denoted O
as O
X O
∈ O
R O
I×J×K O
, O
where O
I O
is O
the O
number O
of O
users B
, O
J O
is O
the O
number O
of O
features O
, O
and O
K O
is O
the O
number O
of O
time B
steps O
( O
i.e. O
, O
played B
matches B
) O
. O

In O
this O
formulation O
, O
the O
entry O
xijk O
of O
the O
tensor O
corresponds O
to O
the O
entry O
related O
to O
the O
i O
- O
th O
user B
in O
the O
j O
- O
th O
feature O
at O
the O
k O
- O
th O
match B
in O
her O
/ O
his O
gaming O
history O
. O

Once O
the O
dataset O
is O
represented O
in O
the O
tensor O
form O
, O
we O
can O
apply O
tensor O
decomposition O
techniques B
to O
perform O
a O
dimensionality O
reduction O
on O
the O
data O
. O

Here O
, O
we O
focus B
on O
the O
Non O
- O
negative O
Tensor O
Factorization O
( O
NTF O
) O
which O
is O
given O
by O
the O
PARAFAC O
/ O
CANDECOMP O
decomposition O
with O
non O
- O
negative O
constraints O
[ O
25 O
] O
. O

e O
NTF O
approximates O
the O
tensor O
X O
into O
the O
sum O
of O
rank B
- O
one O
tensors O
, O
called B
components O
: O
X O
≈ O
Õ O
R O
r=1 O
λr O
ar O
◦ O
br O
◦ O
cr O
, O
( O
1 O
) O
where O
R O
is O
the O
rank B
of O
the O
tensor O
, O
λr O
are O
the O
values O
of O
the O
tensor O
core O
L O
= O
diaд(λ O
) O
, O
and O
the O
outer O
product O
ar O
◦ O
br O
◦ O
cr O
identies O
the O
component O
r O
, O
with O
r O
= O
1 O
, O
. O

. O

. O

, O
R. O
e O
vectors O
ar O
, O
br O
and O
cr O
respectively O
provide O
the O
level B
of O
membership O
of O
the O
users B
to O
the O
component O
r O
, O
the O
level B
of O
membership O
of O
the O
features O
to O
the O
component O
r O
, O
and O
the O
temporal O
activation O
of O
the O
component O
r. O
ese O
vectors O
can O
be O
encoded O
in O
three O
matrices O
A O
∈ O
R O
I×R O
, O
B O
∈ O
R O
J×R O
, O
and O
C O
∈ O
R O
K×R O
, O
called B
factor O
matrices O
, O
whose O
r O
- O
th O
columns O
coincide O
to O
the O
vectors O
ar O
, O
br O
and O
cr O
. O

erefore O
, O
the O
approximation O
in O
Eq.(1 O
) O
can O
be O
rewrien O
in O
the O
Kruskal O
form O
[ O
13 O
] O
as O
X O
≈ O
JL;A O
, O
B O
, O
CK O
. O

To O
obtain O
the O
factor O
matrices O
, O
and O
thus O
the O
approximated O
tensor O
, O
we O
need O
to O
solve O
an O
optimization O
problem O
of O
the O
form O
: O
min O
kX O
− O
JL;A O
, O
B O
, O
CKk O
2 O
F O
s.t O
. O

λ O
, O
A O
, O
B O
, O
C O
≥ O
0 O
where O
k O
· O
kF O
is O
the O
Frobenius O
norm O
, O
and O
a O
non O
- O
negativity O
constraints O
has O
been O
imposed O
on O
the O
factor O
matrices O
. O

ere O
exist O
several O
algorithms O
to O
solve O
this O
optimization O
problem O
. O

Here O
, O
we O
rely O
on O
the O
Alternating O
Non O
- O
negative O
Least O
Squares O
( O
ANLS O
) O
[ O
10 O
] O
, O
combined O
with O
the O
Block O
Principal O
Pivoting O
( O
BPP O
) O
method O
, O
developed O
by O
[ O
12 O
] O
. O

To O
nd O
the O
best O
approximation O
, O
we O
run O
several O
simulations O
for O
each O
rank B
value O
. O

To O
assess O
the O
suitable O
number O
of O
components O
, O
i.e. O
, O
rank B
, O
we O
use O
the O
Core O
Consistency O
Diagnostic O
( O
CORCONDIA O
) O
by O
[ O
1 O
] O
: O
this O
method O
allows O
us O
to O
calculate O
the O
value O
of O
the O
core O
consistency O
for O
each O
simulation O
. O

In O
the O
present O
work O
, O
we O
run O
5 O
simulations O
for O
each O
rank B
and O
look O
at O
the O
change O
in O
the O
slope O
of O
the O
core O
consistency O
curve O
to O
select O
a O
suitable O
number O
of O
components O
R. O
In O
particular O
, O
the O
core O
consistency O
values O
provide O
an O
evaluation O
of O
the O
closeness O
of O
the O
computed O
decomposition O
to O
the O
ideal O
one O
. O

is O
is O
done O
by O
comparing O
the O
core O
tensor O
L O
of O
the O
decomposition O
to O
a O
super O
- O
diagonal O
tensor O
G O
, O
i.e. O
, O
a O
tensor O
having O
entries O
equal O
to O
1 O
on O
the O
diagonal O
and O
0 O
otherwise O
. O

erefore O
, O
the O
core O
consistency O
between O
L O
and O
G O
is O
dened O
as O
: O
cc O
= O
100 O
1 O
− O
ÍR O
l=1 O
ÍR O
m=1 O
ÍR O
n=1 O
( O
λlmn O
− O
дlmn O
) O
2 O
R O
! O
, O
where O
λlmn O
are O
the O
entries O
of O
the O
core O
tensor O
L O
∈ O
R O
R×R×R O
and O
дlmn O
are O
the O
entries O
of O
the O
ideal O
core O
G O
∈ O
R O
R×R×R. O
e O
core O
consistency O
has O
an O
upper O
bound O
: O
its O
values O
can O
not O
exceed O
100 O
. O

As O
a O
result O
, O
high O
values O
of O
the O
core O
consistency O
mean O
high O
similarity O
between O
the O
cores O
, O
while O
low O
values O
indicate O
that O
the O
model O
selected O
could O
be O
problematic O
. O

Finally O
, O
the O
core O
consistency O
can O
assume O
negative O
values O
, O
thus O
indicating O
that O
the O
model O
selected O
is O
inappropriate O
. O

In O
the O
Results O
section O
( O
see O
§ O
4 O
) O
we O
will O
discuss O
the O
optimal O
number O
of O
components O
as O
discovered O
by O
the O
the O
Core O
Consistency O
Diagnostic O
. O

Once O
the O
number O
of O
components O
and O
the O
corresponding O
factor O
matrices O
are O
identied O
, O
we O
can O
analyze O
the O
information O
provided O
by O
the O
factor O
matrices O
. O

By O
the O
study O
of O
the O
matrices O
A O
and O
B O
, O
we O
can O
respectively O
dene O
the O
level B
of O
membership O
of O
each O
user B
to O
a O
specic O
component O
, O
as O
well O
as O
the O
level B
of O
membership O
of O
each O
feature O
to O
the O
components O
. O

It O
is O
worth B
noting O
that O
NTF O
allows O
users B
( O
or O
features O
) O
to O
belong O
to O
several O
components O
leading B
to O
overlapping O
groups B
, O
but O
also O
allows O
users B
( O
or O
features O
) O
to O
have O
a O
low O
level B
of O
membership O
such O
that O
it O
could O
be O
not O
enough O
to O
label O
users B
( O
or O
features O
) O
as O
members O
of O
a O
specic O
component O
. O

Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
e O
are O
two O
possible O
strategies B
to O
dene O
whether O
a O
user B
( O
analogously O
for O
features O
) O
belongs O
to O
a O
component O
: O
either O
we O
accumulate O
the O
membership O
of O
a O
component O
until O
the O
95 O
% O
of O
its O
norm O
is O
reached O
[ O
26 O
] O
; O
or O
, O
we O
compute O
an O
intra O
- O
component O
k O
- O
means O
with O
k O
= O
2 O
clusters O
[ O
9 O
] O
, O
i.e. O
, O
for O
each O
component O
we O
divide O
the O
users B
in O
two O
groups B
: O
those O
who O
belong O
to O
the O
specic O
component O
, O
and O
those O
who O
do O
not O
. O

ese O
methods O
allow O
to O
identify O
user B
clusters O
that O
might O
overlap O
. O

However O
, O
the O
use O
of O
such O
methods O
could O
lead B
to O
having O
users B
that O
do O
not O
belong O
to O
any O
component O
. O

In O
this O
work O
, O
we O
use O
the O
rst O
of O
the O
two O
methods O
to O
analyze O
the O
level B
of O
membership O
of O
the O
features O
, O
provided O
by O
the O
matrix O
B. O
is O
is O
done O
to O
detect O
the O
features O
having O
a O
key O
role B
in O
the O
dierent O
components O
. O

is O
decision O
is O
justied O
by O
our O
expectation O
that O
, O
in O
League O
of O
Legends O
( O
and O
in O
MOBA O
games B
in O
general O
) O
, O
players B
can O
exhibit O
dierent O
strategies B
during O
each O
match B
, O
reecting O
dierent O
personal O
goals B
: O
for O
example O
, O
a O
user B
may O
try O
to O
kill B
as O
many O
enemies B
as O
possible O
to O
earn O
a O
great O
amount B
of I
gold I
; O
this O
however O
would O
potentially O
incur O
in O
risking O
her O
/ O
his O
hero B
’s O
death B
more O
frequently O
; O
an O
alternative O
could O
be O
avoiding O
to O
get O
the O
hero B
killed B
too O
oen O
by O
helping O
( O
assisting B
) O
other O
teammates B
, O
which O
however O
incurs O
in O
earning O
less O
gold B
at O
the O
end O
of O
the O
match B
. O

However O
, O
since O
our O
aim O
is O
to O
identify O
groups B
of O
users B
with O
dierent O
behaviors O
, O
we O
decided O
to O
study O
the O
user B
membership O
as O
follows O
. O

We O
consider O
the O
components O
of O
A O
as O
observations O
recorded O
for O
each O
user B
, O
and O
we O
t O
the O
k O
- O
means O
algorithm O
by O
imposing O
k O
= O
R O
, O
i.e. O
, O
we O
want O
to O
obtain O
one O
cluster O
of O
users B
for O
each O
component O
. O

In O
this O
way O
, O
the O
users B
are O
divided O
in O
k O
disjoint O
clusters O
. O

Finally O
, O
we O
can O
recover O
the O
temporal O
activation O
of O
each O
component O
in O
the O
columns O
of O
the O
factor O
matrix O
C. O
is O
information O
can O
be O
used O
to O
investigate O
the O
evolution B
of O
each O
extracted O
behavior O
. O

3 O
DATA O
COLLECTION O
AND O
PROCESSING O
League O
of O
Legends O
To O
extract O
meaningful O
paerns O
related O
to O
how O
people O
behave O
in O
online O
platforms O
, O
and O
in O
particular O
to O
detect O
groups B
of O
users B
characterized O
by O
similar O
aitudes O
and O
characteristics O
, O
we O
tested O
the O
non O
- O
negative O
tensor O
factorization O
on O
a O
dataset O
of O
a O
MOBA O
game B
: O
League O
of O
Legends O
( O
LoL B
) O
. O

League B
of I
legends I
is O
a O
match B
- O
based B
game B
developed O
by O
Riot O
Games O
, O
in O
which O
each O
player B
, O
i.e. O
user B
, O
controls B
a O
champion B
characterized O
by O
specic O
abilities B
and O
ght O
, O
together O
with O
other O
players B
, O
against O
a O
team B
of O
other O
players B
. O

e O
nal O
goal B
is O
to O
defeat B
the O
adversary O
team B
in O
an O
arena O
. O

Each O
champion B
starts B
the O
match B
with O
a O
low B
strength O
level I
which I
increases B
by O
killing B
adversaries O
, O
helping O
members O
of O
the O
team B
in O
kills O
, O
i.e. O
, O
assists B
, O
and O
performing O
other O
actions O
. O

During O
the O
match B
, O
each O
champion B
can O
be O
killed B
many O
times B
, O
i.e. O
, O
number O
of O
deaths B
. O

e O
player B
can O
earn O
gold B
( O
i.e. O
, O
the O
LoL O
currency O
) O
by O
performing O
some O
actions O
, O
such O
as O
killing B
or O
assisting B
in O
kills B
, O
and O
can O
use O
the O
earned O
gold B
to O
improve O
the O
abilities B
of I
the O
champion I
. O

We O
collected O
the O
LoL O
data O
by O
means O
of O
the O
Riot O
Games O
API,1 O
which O
provides O
metadata O
related O
to O
each O
match B
, O
including O
players B
’ O
performance O
, O
such O
as O
number O
of O
assists B
, O
kills B
, O
deaths B
, O
etc O
. O

Each O
player B
is O
identied O
by O
a O
unique O
label O
and O
each O
match B
is O
marked B
by O
temporal O
information O
, O
such O
as O
match B
datetime O
and O
duration O
. O

1hps://developer.riotgames.com/ O
e O
League O
of O
Legends O
Dataset O
e O
dataset O
analyzed O
in O
the O
present O
work O
consists O
of O
961 O
players B
, O
and O
the O
complete O
game B
history O
of O
their O
rst O
100 O
matches B
played B
exclusively O
in O
one O
specic O
same O
bale O
arena O
. O

We O
decided O
to O
focus B
on O
a O
specic O
bale O
arena O
to O
minimize O
the O
variability O
in O
players B
’ O
behaviors O
( O
and O
their O
evolution B
) O
induced O
by O
dierent O
game B
scenarios O
. O

To O
this O
purpose O
, O
we O
selected O
the O
most O
popular O
LoL B
bale O
arena O
, O
namely O
the O
Summoner O
’s O
Ri O
( O
map O
id=11 O
) O
. O

is O
is O
( O
by O
a O
large O
margin O
) O
the O
most O
played B
bale O
arena O
in O
the O
game B
; O
its O
choice O
provided O
us O
with O
a O
signicant O
amount O
of O
players B
who O
played B
this O
scenario O
at O
least O
100 O
times B
. O

We O
decided O
to O
set O
this O
threshold O
because O
we O
wanted O
to O
guarantee O
that O
a O
sucient O
number O
of O
matches B
were O
played B
by O
each O
single O
individual O
to O
capture B
a O
paern O
of O
temporal O
behavior O
evolution B
. O

100 O
matches B
resulted O
in O
a O
good O
trade O
- O
o O
between O
the O
number O
of O
users B
( O
nearly O
one O
thousand O
) O
and O
the O
number O
of O
total O
matches B
( O
nearly O
100 O
K O
) O
yielded O
by O
the O
selected O
threshold O
. O

Feature O
Selection O
For O
each O
match B
, O
the O
Riot O
Games O
API O
returns O
a O
very O
large O
number O
of O
features O
( O
50 O
+ O
) O
associated O
with O
the O
performance O
of O
each O
user B
involved O
in O
that O
match.2 O
Not O
all O
these O
features O
are O
predictive O
of O
player B
’s O
game B
performance O
or O
behavior O
, O
and O
many O
of O
these O
features O
are O
highly O
correlated O
one O
another O
. O

To O
identify O
informative O
features O
, O
we O
designed O
a O
simple O
machine O
learning O
task O
: O
predicting O
whether O
a O
user B
had O
won O
a O
given O
match B
, O
given O
the O
vector O
of O
50 O
+ O
features O
describing O
her O
/ O
his O
performance O
in O
that O
match B
. O

We O
use O
Decision O
Trees O
for O
this O
simple O
prediction O
task O
due O
to O
its O
ease O
of O
interpretation O
: O
feature O
can O
be O
ranked B
according O
to O
the O
information O
gain O
yielded O
by O
their O
addition O
to O
the O
model O
. O

e O
best O
model O
, O
which O
obtains O
a O
prediction O
accuracy O
above O
80 O
% O
, O
selects O
the O
following O
four O
features O
as O
responsible O
for O
over O
99 O
% O
of O
the O
information O
gain O
: O
( O
1 O
) O
number O
of O
assists B
, O
( O
2 O
) O
number O
of O
kills B
, O
( O
3 O
) O
number O
of O
deaths B
, O
and O
( O
4 O
) O
gold B
earned O
. O

We O
retain O
these O
four O
features O
and O
discard O
all O
others O
in O
the O
rest O
of O
the O
analysis O
. O

e O
nal O
tensor O
is O
composed O
by O
I O
= O
961 O
users B
, O
J O
= O
4 O
features O
, O
and O
K O
= O
100 O
time B
steps O
. O

e O
selected O
features O
are O
characterized O
by O
dierent O
ranges B
of O
values O
, O
thus O
the O
need O
to O
normalize O
them O
. O

Given O
the O
vector O
xf O
related O
to O
the O
feature O
f O
, O
we O
normalize O
each O
entry O
i O
of O
the O
vector O
as O
xˆf O
, O
i O
= O
xf O
, O
i O
− O
xmin O
xmax O
− O
xmin O
, O
where O
xmin O
and O
xmax O
respectively O
are O
the O
minimum O
and O
maximum O
values O
of O
the O
vector O
xf O
. O

4 O
RESULTS O
Our O
League O
of O
Legends O
data O
is O
now O
represented O
as O
a O
tensor O
X O
I×J×K O
, O
where O
I O
= O
961 O
, O
J O
= O
4 O
, O
and O
K O
= O
100 O
. O

erefore O
, O
the O
resulting O
threeway O
tensor O
has O
dimensions O
related O
to O
the O
players B
, O
the O
selected O
features O
, O
and O
the O
time B
steps O
, O
which O
here O
coincide O
with O
the O
matches B
. O

Once O
the O
tensor O
is O
created O
, O
we O
compute O
its O
approximation O
Xapp O
, O
by O
applying O
NTF O
. O

To O
ensure O
of O
selecting O
the O
best O
approximation O
, O
we O
run O
5 O
simulations O
of O
the O
optimization O
problem O
with O
non O
- O
negative O
constraints O
and O
we O
repeat O
this O
procedure O
while O
varying O
the O
nal O
number O
of O
components O
r O
, O
i.e. O
, O
the O
rank B
. O

We O
performed O
the O
simulations O
for O
the O
2hps://developer.riotgames.com O
/ O
api O
/ O
methods O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
Anna O
Sapienza O
, O
Alessandro O
Bessi O
, O
and O
Emilio O
Ferrara O
0 O
1 O
2 O
3 O
Feature O
0 O
1 O
2 O
Component O
# O
Feature O
membership O
0.0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
0.7 O
0.8 O
0.9 O
1.0 O
Figure O
1 O
: O
Feature O
membership O
in O
the O
components O
: O
we O
report B
the O
matrix O
B O
in O
which O
we O
zero O
- O
out O
the O
entries O
which O
are O
not O
included O
in O
the O
95 O
% O
of O
the O
norm O
of O
the O
component O
. O

Here O
, O
the O
colorbar O
indicates O
the O
level B
of O
membership O
of O
each O
feature O
to O
the O
three O
dierent O
components O
. O

e O
features O
are O
respectively O
: O
assists B
( O
0 O
) O
, O
deaths B
( O
1 O
) O
, O
kills O
( O
2 O
) O
, O
and O
gold B
( O
3 O
) O
. O

rank B
valuesr O
= O
1 O
, O
. O

. O

. O

, O
10 O
and O
selected O
the O
suitable O
rank B
on O
the O
basis O
of O
the O
results O
provided O
by O
the O
Core O
Consistency O
Diagnostic O
: O
the O
number O
of O
components O
that O
yields O
the O
largest O
knee O
in O
the O
slope O
of O
the O
core O
consistency O
curve O
is O
R O
= O
3 O
. O

We O
select O
this O
value O
of O
components O
for O
the O
following O
analysis O
. O

By O
xing O
the O
number O
of O
components O
, O
we O
then O
select O
the O
best O
approximation O
, O
by O
choosing O
the O
one O
corresponding O
to O
the O
maximum O
value O
of O
core O
consistency O
for O
the O
selected O
rank B
R. O
e O
approximated O
tensor O
is O
the O
output O
of O
the O
NTF O
and O
it O
is O
summarized O
in O
the O
factor O
matrices O
, O
as O
Xapp O
= O
JA O
, O
B O
, O
CK O
. O

We O
rst O
analyze O
the O
results O
provided O
in O
the O
matrix O
B O
, O
which O
are O
shown O
in O
Fig O
. O

1 O
. O

Here O
, O
we O
report B
the O
values O
of O
the O
matrix O
B O
that O
have O
a O
key O
role B
in O
the O
components O
. O

To O
this O
aim O
, O
for O
each O
component O
we O
sort O
their O
squared O
values O
in O
descending O
order O
, O
sum O
them O
( O
starting B
from O
the O
highest O
value O
) O
until O
we O
reach O
the O
95 O
% O
of O
the O
overall O
component O
norm O
, O
and O
set O
the O
remaining O
values O
equal O
to O
zero O
. O

e O
result O
of O
this O
procedure O
, O
shown O
in O
Fig O
. O

1 O
, O
highlights B
the O
features O
that O
are O
involved O
in O
each O
component O
. O

Here O
, O
the O
features O
are O
marked B
as O
follows O
: O
• O
0 O
: O
number O
of O
assists B
; O
• O
1 O
: O
number O
of O
deaths B
; O
• O
2 O
: O
number O
of O
kills B
; O
• O
3 O
: O
amount B
of I
earned O
gold I
. O

e O
results O
provided O
by O
studying O
the O
factor O
B O
are O
reported B
in O
Tab O
. O

2 O
, O
where O
we O
marked B
all O
the O
features O
that O
are O
involved O
in O
a O
certain O
component O
. O

As O
an O
example O
, O
the O
component O
0 O
is O
characterized O
by O
the O
features O
related O
to O
the O
assists B
and O
the O
earned O
gold B
. O

Once O
the O
components O
are O
characterized O
through O
the O
features O
having O
the O
highest O
memberships O
, O
we O
analyze O
the O
results O
related O
to O
Table O
2 O
: O
Features O
involved O
in O
each O
component O
. O

Assists O
Deaths O
Kills B
Gold O
Component O
0 O
X O
X O
Component O
1 O
X O
X O
Component O
2 O
X O
X O
X O
the O
users B
. O

e O
user B
memberships O
to O
each O
component O
are O
summarized O
in O
the O
factor O
matrix O
A. O
As O
we O
have O
explained O
in O
Sec O
. O

§ O
2 O
, O
we O
nd O
the O
users B
belonging O
to O
each O
component O
by O
taking O
advantage B
of O
a O
k O
- O
means O
method O
with O
a O
number O
of O
clusters O
equal O
to O
the O
number O
of O
components O
, O
k O
= O
3 O
in O
this O
case O
. O

is O
assumption O
is O
also O
supported B
by O
the O
Silhouee O
scores O
, O
computed O
for O
several O
values O
of O
k. O
In O
particular O
, O
for O
k O
= O
3 O
the O
score O
is O
equal O
to O
0.35 O
, O
while O
by O
increasing B
the O
number O
of O
clusters O
the O
score O
decreases O
, O
assuming O
values O
equal O
to O
0.32 O
and O
0.29 O
respectively O
for O
k O
= O
4 O
and O
k O
= O
5 O
and O
stabilizing O
below O
0.29 O
for O
k O
> O
5 O
. O

us O
, O
by O
xing O
k O
= O
3 O
the O
method O
assigns O
a O
unique O
label O
to O
each O
user B
and O
divides O
them O
into O
three O
disjoint O
groups B
. O

As O
we O
can O
observe O
from O
Fig O
. O

2(le O
) O
, O
the O
k O
- O
means O
with O
k O
= O
3 O
nds O
three O
clusters O
of O
users B
, O
which O
are O
disjoint O
in O
the O
component O
space O
, O
i.e. O
, O
given O
a O
user B
i O
its O
coordinates O
in O
the O
component O
space O
are O
given O
by O
the O
entries O
in O
the O
columns O
of O
the O
factor O
matrix O
A. O
is O
grouping B
allows O
to O
identify O
the O
users B
whose O
membership O
to O
a O
specic O
component O
is O
higher O
than O
to O
the O
others O
. O

Clusters O
0 O
, O
1 O
, O
and O
2 O
respectively O
contain O
411 O
, O
304 O
, O
and O
246 O
users B
. O

Fig O
. O

2(right O
) O
shows O
the O
Silhouee O
proles O
of O
the O
three O
clusters O
, O
proportional O
to O
their O
sizes O
. O

e O
dierence O
in O
the O
level B
of O
membership O
of O
users B
belonging O
to O
the O
dierent O
clusters O
is O
clear B
if O
we O
look O
at O
the O
values O
of O
A O
and O
how O
they O
are O
modulated O
in O
time B
. O

is O
is O
possible O
by O
computing O
for O
each O
r O
- O
th O
component O
the O
product O
P O
= O
ar O
c O
T O
r O
∈ O
R O
I×K O
, O
which O
represents O
the O
membership O
of O
each O
user B
to O
the O
r O
- O
th O
component O
modulated O
over O
time B
by O
the O
temporal O
activity O
of O
the O
r O
- O
th O
component O
. O

In O
Fig O
. O

3 O
, O
we O
report B
the O
average O
membership O
score O
over O
time B
and O
the O
related O
standard O
error O
( O
represented O
by O
error O
bars O
) O
, O
computed O
by O
taking O
into O
account O
the O
users B
belonging O
to O
the O
same O
cluster O
in O
the O
three O
components O
. O

We O
can O
observe O
how O
each O
cluster O
is O
systematically O
characterized O
by O
an O
overall O
level B
of O
membership O
to O
one O
specic O
component O
that O
is O
much O
greater O
than O
to O
the O
other O
components O
. O

Fig O
. O

3.a O
demonstrates O
the O
strong O
relation O
between O
Cluster O
0 O
and O
Component O
1 O
( O
cf O
. O

black O
squares O
) O
, O
Fig O
. O

3.b O
shows O
the O
strong O
relation O
between O
Cluster O
1 O
and O
Component O
2 O
( O
cf O
. O

dark O
red O
circles O
) O
, O
while O
Fig O
. O

3.c O
exhibits O
the O
strong O
relation O
between O
Cluster O
2 O
and O
Component O
0 O
( O
cf O
. O

, O
dark O
green B
triangles O
) O
. O

It O
is O
worth B
noting O
that O
there O
is O
an O
high O
gap O
between O
the O
average O
memberships O
over O
time B
to O
the O
cluster O
- O
related O
component O
and O
the O
remaining O
two O
components O
. O

is O
paern O
indicates O
that O
: O
• O
Users B
belonging O
to O
Cluster O
0 O
and O
thus O
to O
Component O
1 O
are O
strongly O
characterized O
by O
the O
features O
kills B
and O
earned O
gold B
; O
• O
Users B
in O
Cluster O
1 O
and O
thus O
belonging O
to O
Component O
2 O
are O
characterized O
by O
deaths B
, O
kills B
, O
and O
earned O
gold B
; O
• O
Users B
belonging O
to O
Cluster O
2 O
and O
Component O
0 O
are O
strongly O
characterized O
by O
assists B
and O
earned O
gold B
. O

rough O
the O
analysis O
of O
NTF O
results O
, O
we O
are O
able O
not O
only O
to O
identify O
the O
features O
that O
play B
a O
key O
role B
in O
a O
certain O
component O
, O
but O
we O
can O
easily O
nd O
the O
user B
membership O
to O
the O
component O
. O

is O
enables O
to O
link O
each O
user B
in O
the O
component O
to O
the O
features O
Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
0.0 O
0.02 O
0.04 O
0.06 O
0.08 O
Component O
0 O
0.0 O
0.02 O
0.04 O
0.06 O
0.08 O
Component O
1 O
Player O
membership O
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
-0.1 O
0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
coefficient O
values O
clusters O
0 O
1 O
2 O
Silhouette O
score O
Figure O
2 O
: O
K O
- O
means O
results O
: O
the O
le O
  O
gure O
shows O
the O
2-dimensional O
projection O
of O
the O
three O
clusters O
identied O
by O
the O
k O
- O
means O
. O

Each O
dot B
represents O
a O
player B
in O
its I
corresponding I
cluster O
, O
and O
the O
dot B
’s O
coordinates O
are O
given O
by O
the O
rst O
two O
columns O
of O
the O
matrix O
A. O
e O
right O
gure O
shows O
the O
Silhouette O
scores O
of O
the O
users B
belonging O
to O
the O
three O
clusters O
. O

e O
red O
line O
identies O
the O
nal O
Silhouette O
score O
of O
0.35 O
, O
and O
the O
width O
of O
the O
Silhouette O
proles O
indicates O
the O
size O
of O
the O
corresponding O
clusters O
. O

that O
characterize O
the O
strategy B
used O
in O
the O
game B
. O

An O
interpretation O
for O
these O
results O
is O
indeed O
that O
dierent O
groups B
of O
users B
, O
jointly O
identied O
by O
NTF O
and O
k O
- O
means O
, O
are O
characterized O
by O
a O
playing O
behavior O
which O
is O
dierent O
from O
group B
to O
group B
. O

In O
particular O
, O
some O
users B
, O
such O
as O
those O
related O
to O
Component O
0 O
, O
tend O
to O
collaborate O
more O
than O
others O
with O
their O
teammates B
, O
as O
they O
prefer O
to O
assists B
in O
ghting O
an O
enemy B
rather O
than O
killing B
him O
directly O
. O

Other O
users B
( O
e.g. O
, O
Component O
1 O
) O
are O
prone O
to O
perform O
individual O
actions O
, O
focusing B
on O
personal O
goals B
, O
such O
as O
earning O
a O
greater O
amount B
of I
gold I
, O
which O
can O
be O
spent O
to O
upgrade O
the O
player B
’s O
champion B
abilities B
. O

Finally O
, O
in O
Component O
2 O
we O
detect O
a O
group B
of O
users B
that O
performs O
individual O
actions O
, O
such O
as O
a O
high B
number I
of I
kills I
, O
but O
are O
signicantly O
more O
likely O
to O
cause O
their O
hero B
to O
die O
during O
these O
actions O
. O

is O
might O
pinpoint O
to O
a O
group B
of O
users B
characterized O
by O
an O
overall O
lower O
performance O
if O
compared O
with O
the O
other O
players B
. O

Validation O
To O
validate O
the O
results O
obtained O
via O
NTF O
and O
the O
related O
interpretation O
, O
we O
selected B
the O
players I
in O
each O
cluster O
, O
and O
then O
we O
computed O
the O
mean O
and O
standard O
error O
of O
the O
dierent O
feature O
values O
at O
each O
time B
step O
( O
i.e. O
, O
each O
match B
) O
. O

We O
show O
the O
results O
in O
Fig O
. O

4 O
, O
where O
each O
plot O
is O
related O
to O
a O
specic O
feature O
, O
namely O
( O
a O
) O
number O
of O
assists B
, O
( O
b O
) O
number O
of O
deaths B
, O
( O
c O
) O
number O
of O
kills B
, O
and O
( O
d O
) O
amount B
of I
earned O
gold I
. O

e O
results O
conrms O
the O
hypothesis O
and O
interpretation O
derived O
by O
the O
NTF O
analysis O
: O
Cluster O
0 O
( O
cf O
. O

, O
blue B
squares O
) O
is O
composed O
by O
players B
whose O
major O
behavior O
dynamic O
over O
time B
is O
summarized O
by O
performing O
a O
high B
number I
of I
kills I
and O
earning O
at O
the O
same O
time B
a O
greater B
amount I

 O
of I
gold I
than O
other O
players B
( O
as O
we O
mentioned O
earlier O
, O
the O
amount B
of I

 O
gold I
is O
proportional O
to O
the O
number O
of O
kills B
, O
which O
here O
serves O
as O
a O
further O
sanity O
check O
) O
. O

Cluster O
1 O
( O
cf O
. O

, O
red O
circles O
) O
consists O
of O
players B
that O
obtain O
a O
number O
of O
kills B
comparable O
to O
the O
users B
in O
Cluster O
0 O
, O
however O
their O
higher O
- O
than O
- O
average O
number O
of O
deaths B
causes O
them O
to O
systematically O
collect O
less O
gold B
( O
if O
compared O
with O
the O
other O
two O
clusters O
) O
. O

Finally O
, O
Cluster O
2 O
involves O
players B
characterized O
by O
stronger O
social O
behavior O
, O
resulting O
in O
collaboration O
with O
the O
other O
team B
members I
, O
as O
conveyed O
by O
the O
larger O
number O
of O
assists B
and O
smaller O
number O
of O
kills B
. O

is O
strategy B
allows O
players B
in O
Cluster O
2 O
to O
collect O
a O
good O
amount B
of I
gold I
and O
at O
the O
same O
time B
to O
keep O
a O
low B

 O
level I
of I
deaths B
. O

One O
of O
the O
strengths B
of O
using O
a O
technique B
such O
as O
NTF O
is O
that O
we O
can O
disentangle O
the O
topological O
characteristics O
in O
the O
data O
, O
such O
as O
group B
of O
users B
characterized O
by O
similar O
features O
, O
from O
the O
temporal O
behaviors O
. O

e O
temporal O
information O
is O
indeed O
contained O
in O
the O
matrix O
C O
, O
whose O
columns O
represent O
the O
timeseries O
of O
the O
temporal O
activation O
of O
each O
component O
, O
from O
which O
we O
can O
extract O
some O
meaningful O
interpretation O
of O
the O
evolution B
of O
players B
’ O
behaviors I
. O

We O
expect O
that O
by O
testing O
dierent O
strategies B
, O
players B
can O
modify O
or O
adapt O
their O
way O
of O
playing O
to O
achieve O
beer O
performances O
. O

is O
fact O
would O
be O
described O
by O
a O
change O
( O
such O
as O
an O
abrupt O
jump O
) O
in O
the O
temporal O
activity O
of O
a O
component O
, O
meaning O
that O
the O
component O
would O
activate O
or O
deactivate O
at O
a O
certain O
time B
. O

However O
, O
by O
the O
analysis O
of O
the O
factor O
matrix O
C O
, O
we O
can O
notice O
that O
each O
component O
is O
systematically O
active O
over O
time B
, O
i.e. O
, O
despite O
dierent O
levels B
of O
activation O
, O
no O
signicant O
behavioral O
change O
is O
noticeable O
in O
the O
behavioral O
trajectories O
of O
the O
players B
over O
the O
course O
of O
their O
100 O
matches B
. O

KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
Anna O
Sapienza O
, O
Alessandro O
Bessi O
, O
and O
Emilio O
Ferrara O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.0015 O
0.0020 O
0.0025 O
0.0030 O
0.0035 O
0.0040 O
0.0045 O
Membership O
level B
( O
a O
) O
Cluster O
0 O
Component O
0 O
Component O
1 O
Component O
2 O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.0020 O
0.0025 O
0.0030 O
0.0035 O
0.0040 O
0.0045 O
0.0050 O
0.0055 O
Membership O
level B
( O
b O
) O
Cluster O
1 O
Component O
0 O
Component O
1 O
Component O
2 O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.0015 O
0.0020 O
0.0025 O
0.0030 O
0.0035 O
0.0040 O
0.0045 O
0.0050 O
0.0055 O
Membership O
level B
( O
c O
) O
Cluster O
2 O
Component O
0 O
Component O
1 O
Component O
2 O
Figure O
3 O
: O
Membership O
modulated O
in O
time B
. O

e O
gures O
display O
the O
product O
ar O
c O
T O
r O
, O
which O
is O
the O
membership O
of O
users B
modulated O
in O
time B
. O

e O
product O
is O
computed O
by O
separately O
taking O
into O
account O
the O
users B
belonging O
to O
the O
dierent O
clusters O
. O

For O
each O
cluster O
we O
report B
the O
mean O
of O
the O
users B
membership O
to O
each O
component O
over O
time B
, O
and O
the O
related O
standard O
error O
, O
marked B
with O
an O
error O
bar O
. O

Dierent O
shades O
of O
blue B
for O
cluster O
0 O
, O
red O
for O
cluster O
1 O
, O
and O
green B
for O
cluster O
2 O
are O
assigned O
to O
distinguish O
the O
components O
. O

0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.16 O
0.18 O
0.20 O
0.22 O
0.24 O
0.26 O
0.28 O
0.30 O
Assists O
# O
( O
a O
) O
Assists B
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.20 O
0.22 O
0.24 O
0.26 O
0.28 O
0.30 O
0.32 O
0.34 O
Deaths O
# O
( O
b O
) O
Deaths B
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.06 O
0.08 O
0.10 O
0.12 O
0.14 O
0.16 O
0.18 O
0.20 O
Kills O
# O
( O
c O
) O
Kills B
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.26 O
0.27 O
0.28 O
0.29 O
0.30 O
0.31 O
0.32 O
0.33 O
0.34 O
0.35 O
Gold O
earned O
( O
d O
) O
Gold B
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
Figure O
4 O
: O
Mean O
feature O
values O
and O
related O
standard O
errors O
over O
time B
. O

e O
dierent O
subgures O
show O
the O
player B
performance O
related O
to O
a O
certain O
feature O
over O
time B
. O

We O
computed O
the O
mean O
and O
the O
standard O
error O
over O
the O
values O
related O
to O
users B
belonging O
to O
the O
same O
cluster O
. O

Clusters O
are O
marked B
by O
a O
unique O
symbol O
and O
color O
, O
which O
is O
maintained O
in O
all O
the O
gures O
, O
to O
highlight B
the O
dierent O
cluster O
characteristics O
. O

Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
0 O
20 O
40 O
60 O
80 O
100 O
Time O
step O
0.35 O
0.40 O
0.45 O
0.50 O
0.55 O
0.60 O
0.65 O
0.70 O
Component O
activation O
Temporal O
activity O
Component O
0 O
Component O
1 O
Component O
2 O
Figure O
5 O
: O
Temporal O
activity O
of O
each O
component O
. O

e O
values O
displayed O
in O
the O
gure O
are O
the O
one O
present O
in O
the O
columns O
of O
the O
factor O
matrix O
C. O
e O
dierent O
markers O
characterize O
the O
dierent O
components O
, O
accordingly O
to O
the O
clusters O
colouring O
. O

is O
result O
, O
illustrated O
in O
Fig O
. O

5 O
, O
suggests O
that O
the O
group B
of O
users B
characterized O
by O
a O
specic O
strategy B
( O
i.e. O
, O
one O
of O
the O
three O
leading B
strategies B
we O
highlighted B
above O
) O
is O
consistent O
over O
time B
; O
in O
other O
words O
, O
players B
are O
reluctant O
to O
continuously O
in O
changing O
their O
gaming O
behavior O
and O
strategy B
, O
even O
if O
that O
could O
occasionally O
entail O
a O
benet O
. O

We O
found O
an O
explanation O
for O
this O
phenomenon O
, O
which O
we O
suspect O
is O
related O
to O
the O
game B
design O
. O

League O
of O
Legends O
is O
based B
on O
a O
mathematical O
framework O
, O
that O
at O
the O
beginning O
of O
each O
match B
compares O
the O
players B
’ O
skills I
to O
create O
the O
opposing B
teams I
as O
follows O
. O

Each O
player B
in O
LoL O
is O
characterized O
by O
an O
Elo O
- O
like O
rating3 O
which O
represents O
the O
player B
skill I
level B
, O
based B
on O
the O
performances O
in O
the O
previous O
matches B
. O

us O
, O
the O
resulting O
matchmaking B
rating I
is O
used O
by O
the O
system O
in O
assembling O
the O
teams B
and O
creating O
a O
game B
in O
which O
both O
teams B
have O
an O
equal O
chance B
of O
winning.4 O
Considering O
the O
LoL O
game B
design O
, O
we O
then O
compute O
the O
probability O
distribution O
of O
match B
winning O
for O
all O
players B
in O
each O
cluster O
identied O
by O
NTF O
. O

In O
Fig O
. O

6 O
, O
we O
report B
the O
Kernel O
Density O
Estimation O
( O
KDE O
) O
for O
the O
distribution O
of O
the O
victories B
in O
each O
cluster O
. O

e O
KDE O
estimates O
the O
probability O
density O
function O
of O
the O
feature O
winner B
of O
each O
player B
involved O
in O
a O
certain O
cluster O
. O

is O
binary O
feature O
is O
equal O
to O
1 O
if O
a O
player B
wins O
a O
match B
, O
and O
0 O
otherwise O
. O

By O
looking O
at O
the O
density O
function O
for O
each O
cluster O
, O
it O
appears O
that O
the O
distributions O
are O
centered O
around O
0.5 O
, O
suggesting O
that O
each O
player B
at O
the O
beginning O
of O
each O
match B
has O
roughly O
a O
50 O
% O
probability O
of O
winning O
or O
losing O
the O
match B
. O

However O
, O
closer B
inspection O
shows O
that O
these O
distributions O
are O
slightly O
skewed O
: O
Cluster O
0 O
leans O
toward O
values O
slightly O
higher O
than O
0.5 O
, O
suggesting O
that O
the O
individualist O
strategy B
( O
aiming O
for O
a O
larger O
number O
of O
kills B
and O
less O
assists B
) O
, O
on O
the O
long O
run O
yields O
marginally O
more O
victories B
than O
the O
cooperative O
strategy B
of O
Cluster O
2 O
. O

We O
veried O
, O
using O
a O
pairwise O
two O
- O
tailed O
ttest O
, O
that O
these O
distributions O
are O
indeed O
statistically O
signicantly O
dierent O
( O
all O
p O
- O
values O
≤ O
10−3 O
) O
. O

3hps://en.wikipedia.org O
/ O
wiki O
/ O
Elo O
rating B
system I
4hps://support.riotgames.com O
/hc O
/ O
en O
- O
us O
/ O
articles/201752954-Matchmaking O
- O
Guide O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
0.7 O
0.8 O
victories B
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
density O
Kernel O
Density O
Estimation O
( O
KDE O
) O
Cluster O
0 O
Cluster O
1 O
Cluster O
2 O
Figure O
6 O
: O
Kernel O
Density O
Estimation O
( O
KDE O
) O
computed O
on O
the O
values O
related O
to O
the O
feature O
winner B
( O
binary O
feature O
equal O
to O
1 O
if O
a O
player B
wins O
the O
match B
and O
equal O
to O
0 O
if O
a O
player B
loses O
) O
. O

e O
gure O
shows O
the O
probability O
density O
function O
for O
each O
cluster O
. O

We O
maintained O
the O
color O
code O
used O
throughout O
the O
text O
to O
discriminate O
the O
dierent O
clusters O
. O

In O
conclusion O
, O
the O
League O
of O
Legends O
game B
design O
, O
and O
in O
particular O
the O
method O
used O
to O
create O
the O
opposing B
teams I
of O
a O
match B
, O
strongly O
aects O
the O
matches B
outcomes O
. O

Each O
user B
has O
roughly O
the O
same O
probability O
of O
winning O
a O
match B
, O
which O
is O
largely O
independent O
from O
the O
strategy B
used O
by O
players B
. O

Marginal O
changes O
can O
be O
noticed O
thanks O
to O
our O
NTF O
analysis O
that O
would O
otherwise O
get O
lost O
in O
the O
aggregated O
statistical O
analysis O
if O
oblivious O
of O
the O
social O
and O
temporal O
behavioral O
dynamics O
. O

We O
suggest O
that O
players B
have O
not O
incentives O
to O
change O
their O
natural O
behavior O
as O
they O
likely O
perceive O
to O
achieve O
the O
same O
winning O
performances O
of O
players B
characterized O
by O
a O
dierent O
strategy B
. O

is O
explains O
the O
temporal O
activity O
paerns O
discovered O
by O
NTF O
and O
their O
continuous O
and O
almost O
constant O
activation O
over O
time B
. O

5 O
RELATED O
WORK O
Several O
facets O
of O
League O
of O
Legends O
, O
including O
players B
behaviors I
, O
expertise O
, O
and O
features O
have O
been O
already O
investigated O
[ O
5 O
, O
17 O
] O
. O

Many O
works O
are O
focused B
on O
the O
analysis O
of O
League O
of O
Legends O
team B
composition I
. O

In O
[ O
21 O
] O
, O
the O
authors O
analyze O
player B
behaviors I
by O
developing O
a O
framework O
based B
on O
unsupervised O
learning O
techniques B
to O
discover O
behavior O
clusters O
in O
the O
data O
. O

In O
particular O
, O
they O
try O
to O
learn O
the O
optimal O
team B
composition I
and O
demonstrate O
how O
the O
result O
of O
matches B
can O
be O
predicted O
on O
the O
basis O
of O
the O
features O
characterizing O
the O
team B
. O

In O
our O
work O
, O
we O
used O
the O
winning O
prediction O
task O
to O
determine O
the O
most O
informative O
features O
that O
characterize O
players B

 O
behaviors I
. O

In O
[ O
15 O
] O
the O
authors O
study O
the O
social O
interactions O
and O
organization O
paerns O
of O
LoL O
players B
, O
to O
understand O
how O
collaboration O
arise O
during O
MOBA O
games B
. O

ey O
collected O
a O
dataset O
based B
on O
interviews O
of O
experienced O
LoL B
players I
and O
found O
that O
team B
members I
collaborate O
and O
coordinate O
to O
reach O
the O
same O
goal B
and O
increase B
their O
performance O
in O
the O
game B
. O

KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
Anna O
Sapienza O
, O
Alessandro O
Bessi O
, O
and O
Emilio O
Ferrara O
Other O
studies O
of O
LoL B
are O
focused B
on O
the O
analysis O
of O
specic O
game B
features O
, O
such O
as O
the O
usage O
of O
dierent O
characters B
( O
i.e. O
, O
champions B
) O
[ O
18 O
] O
, O
or O
the O
player B
choice O
of O
a O
specic O
role B
with O
respect O
to O
the O
one O
selected O
by O
the O
other O
team B
members I
[ O
11 O
] O
. O

e O
main O
goal B
in O
these O
studies O
is O
to O
investigate O
how O
roles B
and O
specic O
features O
have O
an O
impact B
on O
the O
players B
’ O
performance O
, O
to O
recommend O
team B
design O
and O
evaluations O
that O
can O
be O
used O
by O
players B
when O
selecting O
a O
character B
. O

Our O
study O
, O
however O
, O
discovers O
in O
an O
unsupervised O
fashion O
the O
playing O
behaviors O
and O
the O
players B
’ O
roles I
during O
the O
matches B
: O
we O
believe O
that O
our O
strategy B
could O
enrich O
the O
insights O
that O
game B
designers O
and O
analysts O
need O
to O
improve O
the O
game B
experience I
. O

It O
is O
also O
worth B
noting O
that O
most O
of O
the O
existing O
studies O
are O
mainly O
focused B
on O
the O
analysis O
of O
LoL B
from O
a O
team B
- O
based I
perspective O
, O
to O
characterize O
groups B
performance O
. O

In O
the O
present O
work O
, O
we O
investigated O
individual B
player I
behaviors O
, O
and O
how O
that O
related O
to O
player B
performances O
at O
the O
level B
of O
single O
matches B
. O

In O
our O
analysis O
, O
we O
also O
highlighted B
the O
crucial O
importance O
of O
the O
temporal O
dimension O
. O

We O
monitored O
the O
evolution B
of O
features O
over O
time B
, O
to O
determine O
whether O
and O
how O
players B
learn O
or O
modify O
their O
strategy B
, O
and O
to O
detect O
if O
a O
common O
activity O
paern O
can O
be O
found O
. O

6 O
CONCLUSIONS O
League O
of O
Legends O
is O
a O
multiplayer O
online B
bale O
game I
in O
which O
two O
teams B
ght O
each O
other O
to O
destroy O
the O
respective O
enemy B
base I
. O

We O
collected O
data O
related O
to O
League O
of O
Legends O
matches B
and O
player B
performances O
with I
the I
aim I
of O
extracting O
meaningful O
information O
about O
human O
behavioral O
paerns O
. O

For O
this O
purpose O
, O
we O
took O
advantage B
of O
tensor O
decomposition O
techniques B
, O
a O
well O
established O
mathematical O
framework O
that O
is O
successfully O
applied O
to O
many O
research O
problems O
. O

In O
particular O
, O
we O
used O
the O
non O
- O
negative O
tensor O
factorization O
( O
NTF O
) O
. O

e O
advantage B
of O
using O
such O
a O
technique B
lies O
in O
the O
opportunity O
of O
disentangling O
the O
topological O
and O
temporal O
characteristics O
in O
the O
data O
, O
and O
exploring O
and O
validating O
them O
separately O
. O

Here O
, O
we O
analyzed O
a O
dataset O
composed O
by O
nearly O
one O
thousand O
players B
, O
characterized O
by O
dierent O
features O
, O
e.g. O
number O
of O
kills B
, O
number O
of O
deaths B
, O
etc O
. O

, O
which O
varies O
over O
time B
, O
from O
match B
to O
match B
. O

We O
represented O
the O
data O
as O
a O
tensor O
and O
we O
applied O
NTF O
to O
extract O
the O
factor O
matrices O
related O
to O
the O
players B
, O
the O
features O
, O
and O
the O
temporal O
activities O
. O

e O
analysis O
of O
the O
NTF O
outcome O
and O
the O
application O
of O
clustering O
methods O
, O
such O
as O
the O
k O
- O
means O
, O
highlighted B
the O
presence O
in O
the O
data O
of O
several O
groups B
of O
players B
, O
characterized O
by O
a O
correlated O
behavior O
in O
time B
and O
topology O
. O

In O
particular O
, O
players B
belonging O
to O
the O
same O
component O
( O
cluster O
) O
are O
characterized O
by O
similar O
features O
and O
activation O
over O
time B
. O

We O
carried B
out O
the O
analysis O
of O
the O
topological O
characteristics O
of O
each O
group B
of O
player B
by O
looking O
at O
the O
features O
highlighted B
by O
NTF O
, O
and O
comparing O
the O
interpretation O
derived O
by O
these O
results O
with O
the O
original O
data O
. O

We O
found O
good O
agreement O
between O
the O
NTF O
output O
and O
the O
characteristics O
of O
the O
discovered O
groups B
of O
players B
in O
the O
original O
data O
. O

erefore O
, O
NTF O
successfully O
identied O
groups B
of O
distinct O
behaviors O
in O
the O
data O
that O
can O
be O
interpreted O
as O
dierent O
player B
strategies B
. O

It O
would O
be O
expected O
that O
dierent O
strategies B
( O
e.g. O
, O
collaborative O
vs. O
individualist O
playing O
) O
would O
lead B
to O
diverse O
performances O
( O
e.g. O
, O
aecting O
the O
winning O
/ O
losing O
ratio O
) O
. O

However O
, O
by O
investigating O
the O
temporal O
activity O
paerns O
of O
the O
player B
groups B
, O
we O
found O
that O
they O
are O
mainly O
characterized O
by O
a O
constant O
behavior O
and O
that O
are O
active O
continuously O
over O
time B
. O

us O
, O
the O
analysis O
of O
the O
temporal O
activation O
of O
the O
NTF O
components O
stressed O
the O
reluctance O
of O
players B
to O
adapt O
their O
strategy B
and O
gaming O
behavior O
over O
time B
. O

is O
nding O
might O
be O
due O
to O
the O
game B
design O
of O
League O
of O
Legends O
: O
the O
team B
formation I
in O
the O
game B
is O
based B
on O
a O
mathematical O
rule B
which O
aims O
at O
contrasting O
teams B
with O
comparable O
skills B
, O
thus O
yielding O
the O
same O
prior O
probability O
of O
victory B
to O
each O
team B
. O

We O
conrmed O
this O
fact O
by O
computing O
the O
Kernel O
Density O
Estimation O
over O
the O
feature O
winner B
for O
each O
player B
, O
divided O
by O
clusters O
. O

Only O
marginal O
, O
yet O
statistically O
signicant O
, O
dierence O
emerged O
, O
which O
are O
likely O
not O
perceivable O
by O
the O
players B
. O

us O
, O
players B
are O
not O
incentivized O
to O
changing O
their O
strategy B
with O
another O
one O
. O

In O
conclusion O
, O
the O
techniques B
and O
approaches O
used O
in O
this O
work O
are O
promising O
, O
and O
open B
new O
questions O
about O
human O
behaviors O
in O
multiplayer O
online B
games I
. O

Future O
work O
will O
be O
devoted O
to O
the O
analysis O
of O
additional O
game B
datasets O
with O
the O
aim O
of O
exploring O
behavioral O
paerns O
in O
dierent O
scenarios O
. O

e O
main O
goal B
would O
be O
to O
investigate O
if O
it O
is O
possible O
to O
nudge O
players B
to O
change O
their O
strategies B
by O
the O
use O
of O
incentives O
, O
such O
as O
rewards B
, O
and O
high O
percentage O
of O
victory B
. O

ACKNOWLEDGMENTS O
e O
authors O
are O
grateful O
to O
DARPA O
for O
support B
( O
grant O
# O
D16AP00115 O
) O
. O

is O
project O
does O
not O
necessarily O
reect O
the O
position O
/ O
policy O
of O
the O
Government O
; O
no O
ocial O
endorsement O
should O
be O
inferred O
. O

Approved O
for O
public O
release O
; O
unlimited O
distribution O
. O

REFERENCES O
[ O
1 O
] O
Rasmus O
Bro O
and O
Henk O
AL O
Kiers O
. O

2003 O
. O

A O
new O
ecient O
method O
for O
determining O
the O
number O
of O
components O
in O
PARAFAC O
models O
. O

Journal O
of O
chemometrics O
17 O
, O
5 O
( O
2003 O
) O
, O
274–286 O
. O

Denition O
of O
the O
core O
consistency O
diagnostic O
. O

[ O
2 O
] O
Barry O
Brown O
and O
Marek O
Bell O
. O

2004 O
. O

CSCW O
at O
play:’there’as O
a O
collaborative O
virtual O
environment O
. O

In O
Proceedings O
of O
the O
2004 O
ACM O
conference O
on O
Computer O
supported B
cooperative O
work O
. O

ACM O
, O
350–359 O
. O

[ O
3 O
] O
Andrzej O
Cichocki O
, O
Rafal O
Zdunek O
, O
Anh O
Huy O
Phan O
, O
and O
Shun O
- O
ichi O
Amari O
. O

2009 O
. O

Nonnegative O
matrix O
and O
tensor O
factorizations O
: O
applications O
to O
exploratory O
multiway O
data O
analysis O
and O
blind B
source O
separation O
. O

John O
Wiley O
& O
Sons O
. O

[ O
4 O
] O
Laura O
Dabbish O
, O
Robert O
Kraut O
, O
and O
Jordan O
Paon O
. O

2012 O
. O

Communication B
and O
commitment O
in O
an O
online B
game O
team I
. I
In O
Proceedings O
of O
the O
SIGCHI O
conference O
on O
human O
factors O
in O
computing O
systems O
. O

ACM O
, O
879–888 O
. O

[ O
5 O
] O
Sco O
Donaldson O
. O

2015 O
. O

Mechanics B
and O
Metagame O
Exploring O
Binary O
Expertise O
in O
League O
of O
Legends O
. O

Games O
and O
Culture O
( O
2015 O
) O
, O
1555412015590063 O
. O

[ O
6 O
] O
Nicolas O
Ducheneaut O
and O
Robert O
J O
Moore O
. O

2004 O
. O

e O
social O
side O
of O
gaming O
: O
a O
study O
of O
interaction O
paerns O
in O
a O
massively O
multiplayer O
online B
game I
. O

In O
Proceedings O
of O
the O
2004 O
ACM O
conference O
on O
Computer O
supported B
cooperative O
work O
. O

ACM O
, O
360–369 O
. O

[ O
7 O
] O
Simon O
Ferrari O
. O

2013 O
. O

From O
generative O
to O
conventional O
play B
: O
Moba O
and O
league B
of I

 O
legends I
. O

In O
Proceedings O
of O
DiGRA O
. O

1–17 O
. O

[ O
8 O
] O
Chek O
Yang O
Foo O
and O
Elina O
MI O
Koivisto O
. O

2004 O
. O

Dening O
grief O
play B
in O
MMORPGs O
: O
player B
and O
developer O
perceptions O
. O

In O
Proceedings O
of O
the O
2004 O
ACM O
SIGCHI O
International O
Conference O
on O
Advances O
in O
computer O
entertainment O
technology O
. O

ACM O
, O
245–250 O
. O

[ O
9 O
] O
Laetitia O
Gauvin O
, O
Andre O
Panisson O
, O
and O
Ciro O
Cauto O
. O

2014 O
. O

Detecting O
the O
commu- O
´ O
nity O
structure O
and O
activity O
paerns O
of O
temporal O
networks O
: O
a O
non O
- O
negative O
tensor O
factorization O
approach O
. O

PloS O
one O
9 O
, O
1 O
( O
2014 O
) O
, O
e86028 O
. O

[ O
10 O
] O
Hyunsoo O
Kim O
, O
Haesun O
Park O
, O
and O
Lars O
Elden O
. O

2007 O
. O

Non O
- O
negative O
tensor O
factor- O
´ O
ization O
based B
on O
alternating O
large O
- O
scale O
non O
- O
negativity O
- O
constrained O
least O
squares O
. O

In O
2007 O
IEEE O
7th O
International O
Symposium O
on O
BioInformatics O
and O
BioEngineering O
. O

IEEE O
, O
1147–1151 O
. O

NTF O
algorithm O
based B
on O
ANLS O
+ O
regularization O
. O

[ O
11 O
] O
Jooyeon O
Kim O
, O
Brian O
C O
Keegan O
, O
Sungjoon O
Park O
, O
and O
Alice O
Oh O
. O

2016 O
. O

e O
Prociency O
- O
Congruency O
Dilemma O
: O
Virtual O
Team O
Design O
and O
Performance O
in O
Multiplayer O
Online O
Games O
. O

In O
Proceedings O
of O
the O
2016 O
CHI O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
. O

ACM O
, O
4351–4365 O
. O

[ O
12 O
] O
Jingu O
Kim O
and O
Haesun O
Park O
. O

2012 O
. O

Fast O
nonnegative O
tensor O
factorization O
with O
an O
active O
- O
set O
- O
like O
method O
. O

In B
High O
- I
Performance I
Scientic O
Computing O
. O

Springer O
, O
Non O
- O
negative O
Tensor O
Factorization O
for O
Human O
Behavioral O
Paern O
Mining O
in O
Online O
Games O
KDD’17 O
, O
August O
2017 O
, O
Nova O
Scotia O
, O
Canada O
311–326 O
. O

NTF O
computed O
by O
ANLS O
and O
BPP O
algorithms O
. O

[ O
13 O
] O
Tamara O
Gibson O
Kolda O
. O

2006 O
. O

Multilinear O
operators O
for O
higher O
- O
order O
decompositions O
. O

United O
States O
. O

Department O
of O
Energy O
. O

Denition O
of O
Kruskal O
operator O
+ O
its O
properties O
it O
’s O
used O
to O
dene O
the O
PARAFAC O
decomposition O
. O

[ O
14 O
] O
Tamara O
G O
Kolda O
and O
Bre O
W O
Bader O
. O

2009 O
. O

Tensor O
decompositions O
and O
applications O
. O

SIAM O
review O
51 O
, O
3 O
( O
2009 O
) O
, O
455–500 O
. O

Overview O
of O
higher O
- O
order O
tensors O
and O
their O
decompositions O
. O

properties O
, O
decomposition O
models O
, O
sowares O
. O

[ O
15 O
] O
Yubo O
Kou O
and O
Xinning O
Gui O
. O

2014 O
. O

Playing B
with O
strangers O
: O
Understanding O
temporary O
teams B
in O
League O
of O
Legends O
. O

In O
Proceedings O
of O
the O
rst O
ACM O
SIGCHI O
annual O
symposium O
on O
Computer O
- O
human O
interaction O
in O
play B
. O

ACM O
, O
161–169 O
. O

[ O
16 O
] O
Yubo O
Kou O
and O
Bonnie O
Nardi O
. O

2013 O
. O

Regulating O
anti O
- O
social O
behavior O
on O
the O
Internet O
: O
e O
example O
of O
League O
of O
Legends O
. O

( O
2013 O
) O
. O

[ O
17 O
] O
Yubo O
Kou O
and O
Bonnie O
A O
Nardi O
. O

2014 O
. O

Governance O
in O
League O
of O
Legends O
: O
A O
hybrid B
system O
.. O
In O
FDG O
. O

[ O
18 O
] O
Choong O
- O
Soo O
Lee O
and O
Ivan O
Ramler O
. O

2015 O
. O

Investigating O
the O
impact B
of O
game B
features O
and O
content O
on O
champion B
usage O
in O
league B
of I
legends I
. O

Proceedings O
of O
the O
Foundation O
of O
Digital O
Games O
( O
2015 O
) O
. O

[ O
19 O
] O
Lek O
- O
Heng O
Lim O
and O
Pierre O
Comon O
. O

2009 O
. O

Nonnegative O
approximations O
of O
nonnegative O
tensors O
. O

Journal O
of O
chemometrics O
23 O
, O
7 O
- O
8 O
( O
2009 O
) O
, O
432–441 O
. O

PARAFAC O
degeneracy O
and O
the O
existence O
of O
a O
solution O
for O
the O
nonnegative O
case O
. O

[ O
20 O
] O
Tinnawat O
Nuangjumnong O
. O

2014 O
. O

e O
eects O
of O
gameplay B
on O
leadership O
behaviors O
: O
An O
empirical O
study O
on O
leadership O
behaviors O
and O
roles B
in O
multiplayer O
online O
bale O
arena O
games B
. O

In O
Cyberworlds O
( O
CW O
) O
, O
2014 O
International O
Conference O
on O
. O

IEEE O
, O
300 O
– O
307 O
. O

[ O
21 O
] O
Hao O
Yi O
Ong O
, O
Sunil O
Deolalikar O
, O
and O
Mark O
Peng O
. O

2015 O
. O

Player O
Behavior O
and O
Optimal O
Team O
Composition O
for O
Online O
Multiplayer O
Games O
. O

arXiv O
preprint O
arXiv:1503.02230 O
( O
2015 O
) O
. O

[ O
22 O
] O
Andre O
Panisson O
, O
Laetitia O
Gauvin O
, O
Marco O
aggioo O
, O
and O
Ciro O
Cauto O
. O

2014 O
. O

´ O
Mining O
concurrent O
topical O
activity O
in O
microblog O
streams O
. O

arXiv O
preprint O
arXiv:1403.1403 O
( O
2014 O
) O
. O

[ O
23 O
] O
Rabindra O
Ratan O
, O
Tracy O
Kennedy O
, O
and O
Dmitri O
Williams O
. O

2012 O
. O

League O
of O
gendered O
game B
play B
behaviors O
: O
examining O
instrumental O
vs O
identity O
- O
relevant O
avatar B
choices O
. O

Meaningful O
Play O
13 O
( O
2012 O
) O
. O

[ O
24 O
] O
Rabindra O
A O
Ratan O
, O
Nicholas O
Taylor O
, O
Jameson O
Hogan O
, O
Tracy O
Kennedy O
, O
and O
Dmitri O
Williams O
. O

2015 O
. O

Stand O
by O
your O
man O
: O
An O
examination O
of O
gender O
disparity O
in O
League O
of O
Legends O
. O

Games O
and O
Culture O
10 O
, O
5 O
( O
2015 O
) O
, O
438–462 O
. O

[ O
25 O
] O
Jean O
- O
Philip O
Royer O
, O
Nadege O
irion O
- O
Moreau O
, O
and O
Pierre O
Comon O
. O

2011 O
. O

Computing O
the O
polyadic O
decomposition O
of O
nonnegative O
third O
order O
tensors O
. O

Signal O
Processing O
91 O
, O
9 O
( O
2011 O
) O
, O
2159–2171 O
. O

Preconditionated O
nonlinear O
conjugate O
gradient O
algorithms O
. O

Add O
nonnegative O
constraints O
to O
parafac O
model O
by O
using O
adamard O
product O
of O
the O
components O
. O

[ O
26 O
] O
Anna O
Sapienza O
, O
Joseph O
Wu O
, O
Laetitia O
Gauvin O
, O
Ciro O
Cauto O
, O
and O
others O
. O

2015 O
. O

Detecting O
Anomalies O
in B
Time O
- I
varying I
Networks O
using O
Tensor O
Decomposition O
. O

In O
2015 O
IEEE O
International O
Conference O
on O
Data O
Mining O
Workshop O
( O
ICDMW O
) O
. O

IEEE O
, O
516–523 O
. O

[ O
27 O
] O
Kenneth O
B O
Shores O
, O
Yilin O
He O
, O
Kristina O
L O
Swanenburg O
, O
Robert O
Kraut O
, O
and O
John O
Riedl O
. O

2014 O
. O

e O
identication O
of O
deviance O
and O
its O
impact B
on O
retention O
in O
a O
multiplayer O
game B
. O

In O
Proceedings O
of O
the O
17th O
ACM O
conference O
on O
Computer O
supported B
cooperative O
work O
& O
social O
computing O
. O

ACM O
, O
1356–1365 O
. O

[ O
28 O
] O
Maxime O
Veron O
, O
Olivier O
Marin O
, O
and O
S O
´ O
ebastien O
Monnet O
. O

2014 O
. O

Matchmaking O
in O
´ O
multi O
- O
player B
on O
- O
line O
games B
: O
studying O
user B
traces O
to O
improve O
the O
user B
experience B
. O

In O
Proceedings O
of O
Network O
and O
Operating O
System O
Support O
on O
Digital O
Audio O
and O
Video O
Workshop O
. O

ACM O
, O
7 O
. O

Running O
head O
: O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
1 O
Player O
Skill O
Decomposition O
in O
Multiplayer O
Online O
Battle O
Arenas O
Zhengxing O
Chen1 O
* O
, O
Yizhou O
Sun2 O
* O
* O
, O
Magy O
Seif O
El O
- O
Nasr3 O
* O
, O
and O
Truong O
- O
Huy O
D. O
Nguyen4 O
* O
* O
* O
* O
Northeastern O
University O
* O
* O
University O
of O
California O
, O
Los O
Angeles O
* O
* O
* O
Texas O
A&M O
University O
- O
Commerce O
Affiliation O
arXiv:1702.06253v1 O
[ O
cs O
. O

SI O
] O
21 O
Feb O
2017 O
PLAYER O
SKILL B
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
2 O
Abstract O
Successful O
analysis O
of O
player B
skills I
in O
video B
games I
has O
important O
impacts B
on O
the O
process O
of O
enhancing O
player B
experience B
without O
undermining O
their O
continuous O
skill B
development O
. O

Moreover O
, O
player B
skill I
analysis I
becomes O
more O
intriguing O
in B
team O
- I
based I
video B
games I
because O
such O
form O
of O
study O
can O
help O
discover O
useful O
factors O
in O
effective O
team B
formation I
. O

In O
this O
paper O
, O
we O
consider O
the O
problem O
of O
skill B
decomposition O
in O
MOBA O
( O
MultiPlayer O
Online O
Battle O
Arena O
) O
games B
, O
with O
the O
goal B
to O
understand O
what O
player B
skill I
factors O
are O
essential O
for O
the O
outcome O
of O
a O
game B
match I
. O

To O
understand O
the O
construct O
of O
MOBA O
player B
skills I
, O
we O
utilize O
various O
skill B
- O
based B
predictive O
models O
to O
decompose O
player B
skills I
into O
interpretative O
parts O
, O
the O
impact B
of O
which O
are O
assessed O
in O
statistical O
terms O
. O

We O
apply O
this O
analysis O
approach O
on O
two O
widely O
known O
MOBAs B
, O
namely O
League O
of O
Legends O
( O
LoL B
) O
and O
Defense O
of O
the O
Ancients O
2 O
( O
DOTA2 O
) O
. O

The O
finding O
is O
that O
base B
skills B
of O
in B
- I
game I
avatars I
, O
base B
skills I
of I
players I
, O
and O
players B
’ O
champion B
- O
specific O
skills B
are O
three O
prominent O
skill B
components O
influencing O
LoL O
’s O
match B
outcomes O
, O
while O
those O
of O
DOTA2 O
are O
mainly O
impacted B
by O
in B
- I
game I
avatars I
’ O
base B
skills B
but O
not O
much O
by O
the O
other O
two O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
3 O
Player O
Skill O
Decomposition O
in O
Multiplayer O
Online O
Battle O
Arenas O
Introduction O
Recently O
a O
unique O
type O
of O
sports O
, O
namely O
electronic O
sports O
( O
eSports O
) O
, O
emerges O
as O
a O
popular O
genre O
of O
computer O
games B
, O
in O
which O
human O
players B
compete O
with I
one I
another I
in O
online O
, O
simulated O
environments O
governed O
by O
rules B
and O
regulations O
similar O
to O
those O
found O
in O
traditional O
forms O
of O
sports O
. O

eSports O
is O
a O
rapidly O
growing O
video B
game I
market O
attracting O
tremendous O
amounts O
of O
professional O
players B
, O
fanatical O
audiences O
and O
tournament B
organizers O
. O

A O
recent O
report B
released O
by O
SuperData O
( O
2016 O
) O
showed O
that O
the O
worldwide O
market O
for O
eSports O
, O
by O
the O
end O
of O
2015 O
, O
has O
reached O
approximately O
748 O
million O
dollars O
and O
is O
expected O
to O
grow O
to O
1.9 O
billion O
dollars O
by O
2019 O
. O

One O
of O
the O
most O
played B
game B
genres O
in O
eSports O
, O
Multiplayer O
Online O
Battle O
Arenas O
or O
MOBAs O
feature O
5-vs-5 O
competitive B
matches B
where O
each O
of O
the O
ten O
players B
selects O
a O
champion5 O
( O
or O
in B
- I
game I
avatar I
) O
to O
combat B
before O
each O
match B
starts B
. O

Each O
team B
consisting O
of O
five O
players B
has O
a O
base B
to O
defend O
and O
the O
goal B
is O
to O
attack B
the O
opposite B
teams I
’ O
champions B
and O
ultimately O
destroy O
the O
opponent B
’s O
base I
. O

MOBA O
games B
are O
typically O
characterized O
by O
rich O
choices O
of O
champions B
and O
dynamic O
strategies B
for O
both O
individual O
champion B
build O
- O
up O
and O
team B
cooperation I
, O
offering O
endless O
replayability O
to O
players B
and O
introducing O
sophisticated O
strategic O
decision O
making O
. O

MOBA O
is O
a O
highly B
skill O
- I
based I
game B
genre O
, O
in O
which O
good O
performance O
and O
high O
ranks B
can O
only O
be O
achieved O
by O
players B
with O
consistently O
superior O
gaming O
capabilities O
. O

Therefore O
, O
understanding O
how O
players B
train B
and O
build O
up O
their O
skill B
sets I
for O
competitive B
gaming O
is O
extremely O
important O
on O
many O
fronts O
. O

First O
, O
it O
helps O
game B
companies O
to O
determine O
and O
evaluate O
different O
design O
choices O
to O
ensure O
a O
fair O
and O
engaging O
environments O
for O
players B
, O
one O
of O
the O
fundamental O
aspects O
of O
game B
design O
, O
as O
suggested O
by O
Adams O
( O
2013 O
) O
. O

Understanding O
how O
a O
new O
game B
element O
affects O
players B
’ O
current O
skills B
and O
performance O
is O
crucial O
in O
the O
process O
of O
adding O
variability O
and O
updates B
to O
a O
game B
that O
is O
growing O
old O
. O

Second O
, O
better O
understanding O
of O
player B
skill I
constructs O
allows O
more O
appropriate O
player B
matching O
, O
which O
is O
currently O
an O
important O
game B
mode I
that O
most O
MOBAs O
5 O
In O
League O
of O
Legends O
, O
in B
- I
game I
avatars I
selected B
by O
players I
are O
termed O
" O
champion B
" O
. O

Other O
comparable O
MOBA O
games B
may O
phrase O
in B
- I
game I
avatars I
in O
different O
words O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
4 O
offer O
to O
players B
to O
enhance O
play B
experience B
, O
as O
pointed B
out O
by O
Nguyen O
, O
Chen O
, O
and O
El O
- O
Nasr O
( O
2015 O
) O
. O

Salen O
and O
Zimmerman O
( O
2004 O
) O
found O
out O
that O
matching B
players B
with O
too O
wide O
skill B
gaps O
leads B
to O
completely O
dominant O
wins O
that O
bore O
the O
winners B
and O
discourage O
the O
losers B
. O

Lastly O
, O
such O
knowledge O
enables O
game B
producers O
to O
identify O
and O
prevent O
cases O
of O
cheats B
and O
hacks O
, O
which O
negatively O
impact B
the O
play B
experience B
of O
other O
legitimate O
gamers B
. O

Duh O
and O
Chen O
( O
2009 O
) O
reported B
that O
the O
use O
of O
malicious O
software O
support B
has O
become O
an O
inevitable O
and O
recurring O
issue O
in O
online O
gaming O
. O

Understanding O
how O
player B
skills I
are O
formed O
and O
sharpened O
over O
time B
may O
pave O
the O
way O
towards O
more O
effective O
identification O
of O
genuine O
from O
fake O
skills B
. O

Within O
the O
scope O
of O
this O
paper O
, O
we O
address O
the O
problem O
of O
unpacking O
the O
essential O
components O
comprising O
MOBA O
games B
’ O
play B
skills B
. O

Taking O
as O
input B
a O
data O
set O
that O
records O
game B
outcome O
statistics O
, O
we O
first O
utilize O
different O
statistical O
models O
to O
capture B
skill B
decomposition O
schemes O
. O

Each O
model O
represents O
a O
hypothesis O
and O
breakdown O
of O
player B
skill I
structures O
, O
the O
effectiveness O
of O
which O
directly O
depends O
on O
how O
accurate O
it O
can O
predict O
the O
game B
outcome O
. O

By O
assessing O
the O
difference O
in O
terms O
of O
predictive O
power B
of O
the O
hypotheses O
, O
the O
contributions O
of O
skill B
components O
to O
players B
’ O
performance O
in O
the O
game B
are O
quantified O
and O
validated O
. O

This O
approach O
does O
not O
aim O
to O
generate O
a O
one O
- O
size O
- O
fits O
- O
all O
skill B
model O
; O
the O
goal B
is O
to O
inform O
users B
of O
possible O
components O
that O
make O
up O
player B
skills I
in O
a O
MOBA O
game B
and O
, O
in O
statistical O
terms O
, O
the O
interplay O
of O
these O
components O
in O
a O
player B
’s O
performance O
. O

The O
paper O
makes O
its O
contribution O
in O
the O
following O
aspects O
: O
1 O
. O

it O
provides O
a O
new O
context O
for O
skill B
analysis O
in O
the O
MOBA O
game B
setting O
, O
where O
player B

 O
skills I
are O
constituted O
from O
multi O
- O
fold O
aspects O
; O
2 O
. O

it O
details O
a O
novel O
model O
- O
based B
skill B
analysis O
approach O
based B
on O
multiple O
performance O
forecasters O
; O
and O
3 O
. O

it O
demonstrates O
the O
utility B
of O
the O
approach O
in B
real O
- I
life I
data O
sets O
, O
which O
leads B
to O
interesting O
findings O
on O
in O
two O
of O
the O
most O
popular O
MOBA O
games B
, O
i.e. O
, O
League O
of O
Legends O
( O
LoL B
) O
and O
Defense O
of O
the O
Ancients O
2 O
( O
DOTA2 O
) O
. O

In O
the O
rest O
of O
the O
paper O
, O
we O
first O
present O
research O
related O
to O
our O
work O
and O
some O
preliminary O
pertinent O
to O
the O
MOBA O
game B
context O
and O
our O
analysis O
approach O
. O

Next O
, O
the O
proposed O
skill B
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
5 O
analysis O
methodology O
is O
laid O
out O
in O
details O
, O
starting B
with O
data O
collection O
, O
skill B
structure O
hypothesis O
construction O
, O
and O
ending O
with O
hypothesis O
modeling O
and O
statistical O
comparison O
. O

The O
last O
sections O
demonstrate O
the O
utility B
of O
our O
proposed O
methodology O
using O
the O
case O
study O
of O
LoL O
and O
DOTA2 O
, O
followed O
by O
numerical O
results O
and O
discussions O
, O
before O
the O
paper O
is O
concluded O
with O
directions O
for O
future O
work O
. O

Previous O
Works O
Skill O
Rating O
and O
Modeling O
The O
study O
of O
skill B
rating I
aims O
to O
rank B
players B
after O
observing O
outcomes O
of O
their O
matchups B
. O

Bradley O
- O
Terry O
model O
proposed O
by O
Bradley O
and O
Terry O
( O
1952 O
) O
was O
developed O
to O
deal O
with O
repeated O
pairwise O
comparisons O
among O
a O
group B
of O
subjects O
. O

In B
Bradley O
- I
Terry I
model O
, O
each O
player B
is O
assumed O
to O
have O
a O
fixed O
skill B
scalar O
and O
the O
winning O
probability O
of O
a O
player B
is O
proportional O
to O
his O
skill B
in O
the O
sum O
of O
skills B
of O
both O
players B
involved O
. O

The O
Elo O
( O
1978 O
) O
system O
, O
a O
probabilistic O
model O
designed O
for O
chess O
player B
skill I
ratings I
, O
was O
developed O
later O
in O
which O
player B

 O
skills I
are O
assumed O
to O
be O
a O
random O
variable O
following O
one O
dimension O
Gaussian O
distribution O
. O

In O
the O
Elo O
system O
, O
player B
skill I
means O
get O
updated B
depending O
on O
the O
extent O
of O
agreement O
between O
expected O
outcomes O
and O
real O
outcomes O
. O

For O
example O
, O
a O
low O
skill B
player B
beating O
a O
high B
skill I

 O
player I
yields O
a O
large O
update B
in O
adjusting O
their O
skill B
means O
closer B
. O

However O
, O
the O
variance O
of O
player B
skills I
is O
assumed O
to O
be O
a O
fixed O
constant O
. O

Glicko O
system O
designed O
by O
Glickman O
( O
1999 O
) O
, O
a O
Bayesian O
ranking B
rating B
system I
, O
was O
later O
introduced O
to O
model O
the O
belief O
about O
a O
player B
’s O
skill I
. O

As O
he O
plays B
increasingly O
number O
of O
games B
, O
the O
belief O
about O
his O
skill B
becomes O
stronger O
. O

None O
of O
Bradley O
- O
Terry O
model O
, O
the O
Elo O
system O
or O
Glicko O
system O
was O
initially O
applicable O
to O
team B
- O
oriented O
games B
until O
works O
from O
Huang O
, O
Lin O
, O
and O
Weng O
( O
2004 O
) O
, O
Menke O
and O
Martinez O
( O
2008 O
) O
and O
Herbrich O
, O
Minka O
, O
and O
Graepel O
( O
2006 O
) O
generalize O
these O
models O
. O

For O
example O
, O
TrueSkill O
system O
designed O
by O
Herbrich O
et O
al O
. O

( O
2006 O
) O
extends O
the O
Elo O
system O
to O
games B
with O
flexible O
sizes O
and O
numbers O
of O
teams B
. O

While O
these O
advanced O
skill B
rating I
systems O
give O
reasonable O
player B
rankings O
in O
an O
efficient O
manner O
, O
they O
use O
relatively O
simple O
representation O
, O
usually O
single O
scalars O
, O
for O
player B
skills I
and O
are O
not O
tailored O
specifically O
for O
MOBA O
games B
. O

Skill O
modeling O
belongs O
to O
player B
modeling O
, O
which O
have O
been O
studied O
for O
long O
time B
by O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
6 O
researchers O
such O
as O
Yannakakis O
, O
Spronck O
, O
Loiacono O
, O
and O
André O
( O
2013 O
) O
, O
Bakkes O
, O
Spronck O
, O
and O
van O
Lankveld O
( O
2012 O
) O
and O
Charles O
et O
al O
. O

( O
2005 O
) O
. O

In O
player B
modeling O
, O
characteristics O
of O
human O
players B
such O
as O
strategies B
, O
preferences O
and O
skills B
are O
detected O
, O
modeled O
, O
predicted O
and O
expressed O
. O

Skill B
modeling O
differs O
with O
skill B
rating I
in O
that O
it O
aims O
to O
encompass O
player B
skills I
in O
multiple O
facets O
and O
does O
not O
necessarily O
need O
to O
rank B
players B
. O

Stanescu O
( O
2011 O
) O
and O
Chen O
and O
Joachims O
( O
2016 O
) O
modelled O
player B
skills I
in B
multi O
- I
dimensions I
( O
such O
as O
offensive O
and O
defensive O
abilities B
) O
but O
their O
works O
are O
currently O
only O
applicable O
to O
1-vs-1 O
games B
rather O
than O
team B
based I
games B
like O
MOBA O
. O

There O
are O
skill B
models O
proposed O
for O
specific O
game B
genres O
, O
such O
as O
Delalleau O
et O
al O
. O

( O
2012 O
) O
for O
FPS O
games B
and O
Avontuur O
, O
Spronck O
, O
and O
Van O
Zaanen O
( O
2013 O
) O
for O
RTS O
games B
. O

To O
our O
best O
knowledge O
, O
however O
, O
we O
have O
seen O
few O
skill B
models O
in O
MOBA O
games B
. O

Skill O
rating O
/ O
modeling O
are O
also O
intriguing O
topics O
in O
crowdsourcing O
where O
a O
large O
body O
of O
research O
focus B
on O
finding O
most O
suitable O
individuals O
to O
complete O
micro O
- O
tasks O
to O
maximize O
outcomes O
. O

Among O
those O
researches O
only O
Kittur O
( O
2010 O
) O
, O
Roy O
, O
Lykourentzou O
, O
Thirumuruganathan O
, O
Amer O
- O
Yahia O
, O
and O
Das O
( O
2015 O
) O
and O
Rahman O
, O
Thirumuruganathan O
, O
Roy O
, O
Amer O
- O
Yahia O
, O
and O
Das O
( O
2015 O
) O
consider O
team B
based I
tasks O
. O

Our O
context O
is O
still O
different O
because O
our O
" O
task O
" O
involves O
two O
teams B
, O
instead O
of O
one O
, O
in O
a O
competitive B
environment O
and O
more O
factors O
besides O
player B
skills I
can O
affect O
match B
outcomes O
. O

MOBA O
Game O
Research O
and O
Outcome O
Prediction O
Our O
paper O
replies O
on O
performance O
of O
match B
outcome O
prediction O
to O
verify O
proposed O
skill B
components O
. O

Match B
outcome O
prediction O
is O
one O
important O
area B
investigating O
the O
fundamental O
elements O
that O
determine O
results O
of O
matches B
. O

Mislak O
and O
Deja O
Myslak O
and O
Deja O
( O
2014 O
) O
tried O
to O
´ O
predict O
LoL B
outcomes O
by O
identifying O
in O
each O
team B
how O
many O
players B
play B
champions B
in O
their O
personally O
favored O
lanes B
. O

However O
, O
the O
model O
was O
only O
evaluated O
on O
partial O
matches B
where O
team B
compositions I
agree O
with O
standard O
norms O
. O

Moreover O
, O
the O
performance O
of O
the O
model O
drops B
if O
the O
two O
teams B
have O
similar O
numbers O
of O
players B
in O
their I
most I
familiar O
lanes B
. O

Pobiedina O
et O
al O
. O

Pobiedina O
, O
Neidhardt O
, O
Calatrava O
Moreno O
, O
Grad O
- O
Gyenge O
, O
and O
Werthner O
( O
2013 O
) O
and O
Pobiedina O
, O
Neidhardt O
, O
Calatrava O
Moreno O
, O
and O
Werthner O
( O
2013 O
) O
discovered O
that O
four O
factors O
contribute O
to O
team B
success O
namely O
team B
composition I
of I
champions I
, O
the O
number O
of O
friends O
, O
the O
player B
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
7 O
experience B
and O
the O
conformity O
of O
player B
nationalities O
. O

While O
their O
focus B
is O
not O
on O
player B
skill I

 O
analysis I
, O
their O
first O
finding O
, O
the O
team B
composition I
of I
champions I
, O
reconciles O
with O
our O
finding O
that O
the O
base B
skills B
of O
champions B
is O
one O
prominent O
part O
of O
player B
skill I
formation I
. O

Several O
works O
predict O
match B
outcomes O
by O
harnessing O
in B
- I
game I
information O
. O

In O
order O
to O
predict O
victory B
side O
, O
Yang O
, O
Brent O
, O
and O
Roberts O
( O
2014 O
) O
retrieved O
characteristics O
of O
in B
- I
game I
combats B
; O
Mahlmann O
, O
Schubert O
, O
and O
Drachen O
( O
2016 O
) O
utilized O
information O
from O
encounters O
( O
i.e. O
, O
situations O
" O
when O
two O
or O
more O
champions B
from O
opposing B
teams I
are O
in O
the O
range B
of O
affect O
each O
other O
" O
) O
; O
Drachen O
et O
al O
. O

( O
2014 O
) O
and O
Rioult O
, O
Metivier O
, O
Helleu O
, O
Scelles O
, O
and O
Durand O
( O
2014 O
) O
exploited O
spatio O
information O
of O
champions B
. O

However O
, O
the O
kind O
of O
information O
is O
not O
consistently O
available O
in O
the O
two O
games B
we O
are O
investigating O
( O
which O
are O
the O
most O
two O
popular O
MOBA O
games B
suggested O
by O
Minotti O
( O
2016 O
) O
) O
and O
our O
work O
is O
from O
a O
higher B
level I
to O
examine O
skill B
decomposition O
. O

Therefore O
our O
model O
does O
not O
use O
in B
- I
game I
information O
currently O
. O

PRELIMINARY O
We O
first O
introduce O
common O
rules B
of O
MOBA O
games B
especially O
those O
related O
to O
our O
context O
. O

Next O
, O
we O
describe O
the O
data O
forms O
that O
our O
proposed O
framework O
is O
designed O
to O
process O
. O

Finally O
, O
we O
show O
parameters O
and O
notations O
necessary O
for O
illustrating O
our O
model O
and O
hypotheses O
. O

MOBA O
Game O
Rules O
In O
this O
paper O
we O
focus B
on O
a O
largely O
played B
MOBA O
game B
mode I
called B
5-vs-5 O
ranked B
matches B
of O
solo B
queue I
. O

A O
typical O
such O
match B
involves O
ten O
players B
who O
form O
two O
teams B
of O
five O
. O

Each O
player B
selects O
in O
turn B
a O
champion B
to O
combat B
before O
the O
real O
match B
starts B
. O

Champions B
, O
each O
designed O
with O
unique O
attributes O
, O
are O
expected O
to O
have O
different O
play B
styles I
and O
tactics B
in O
matches B
. O

For O
example O
, O
there O
are O
champions B
called B
fighters B
who O
have O
short O
- O
ranged B
attack B
ability B
and O
excel O
at O
surviving O
combats B
. O

Another O
type O
of O
champions B
are O
called B
supports B
who O
are O
weak B
when O
alone O
but O
can O
revive O
allies B
and O
slow B
down O
opponent B
movement B
. O

Each O
team B
has O
a O
base B
to O
defend O
and O
the O
common O
victory B
goal B
is O
to O
destroy O
the O
opponents B
’ O
base I
. O

Generally O
, O
players B
adopt O
strategies B
not O
only O
based B
on O
the O
selected O
champions B
but O
also O
on O
the O
champions B
selected O
by O
teammates B
and O
the O
opponents B
. O

It O
is O
worth B
noting O
that O
each O
match B
is O
independent O
of O
each O
other O
in O
the O
sense O
that O
any O
accumulated O
golds B
, O
equipment B
or O
abilities B
of I
champions I
in O
one O
PLAYER O
SKILL B
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
8 O
match B
will O
not O
be O
transferred O
to O
any O
future O
match B
, O
i.e. O
, O
every O
player B
starts B
each O
match B
from O
scratch O
. O

Parameters O
and O
Notations O
Given O
the O
data O
set O
that O
contains O
such O
information O
as O
described O
in O
the O
previous O
section O
, O
suppose O
we O
have O
Z O
matches B
, O
in O
which O
there O
appeared O
M O
champions B
and O
N O
players B
in O
total O
. O

We O
use O
C1 O
, O
C2 O
, O
· O
· O
· O
, O
CM O
to O
index O
the O
champions B
and O
use O
P1 O
, O
P2 O
, O
· O
· O
· O
, O
PN O
to O
index O
the O
players B
. O
We I
use I
M O
= O
{ O
M1 O
, O
M2 O
, O
· O
· O
· O
, O
MZ O
} O
to O
denote O
the O
outcomes O
of O
the O
Z O
matches B
. O

The O
two O
teams B
in O
a O
match B
are O
differentiated O
by O
red O
and O
blue B
colors O
. O

Mz O
= O
1 O
if O
the O
red B
team I
won I
over O
the O
blue B
team I
in O
match B
z O
otherwise O
Mz O
= O
0 O
. O

The O
indicator O
function O
I(z O
, O
t O
, O
Ci O
, O
Pj O
) O
= O
1 O
iff O
Pj O
selected O
Ci O
in O
match B
z O
’s O
team B
t O
, O
where O
t O
can O
take O
values O
either O
r O
or O
b O
denoting O
the O
red O
or O
the O
blue B
team I
respectively O
. O

Otherwise O
, O
I(z O
, O
t O
, O
Ci O
, O
Pj O
) O
= O
0 O
. O

Note O
that O
, O
according O
to O
the O
rules B
of O
the O
ranked B
games B
we O
collected O
, O
Ci O
can O
not O
be O
chosen O
by O
two O
different O
players B
in O
a O
match B
z. O
Based B
on O
different O
hypotheses O
introduced O
in O
next O
section O
, O
we O
construct O
feature O
vector O
Dz O
∈ O
R O
K O
for O
each O
match B
z. O
We O
use O
Dz O
, O
i O
to O
denote O
the O
i O
- O
th O
component O
of O
the O
vector O
Dz O
. O

We O
use O
D O
= O
{ O
D1 O
, O
D2 O
, O
· O
· O
· O
, O
DZ O
} O
to O
denote O
the O
feature O
vectors O
for O
all O
Z O
matches B
. O

Analysis O
Methodology O
In O
this O
section O
, O
we O
describe O
the O
steps O
in O
our O
analysis O
methodology O
, O
including O
( O
1 O
) O
data O
crawling O
and O
cleaning O
, O
in O
order O
to O
acquire O
usable O
information O
of O
matches B
; O
( O
2 O
) O
hypothesis O
construction O
to O
form O
conjectures O
about O
player B
skill I
formation I
; O
( O
3 O
) O
training O
of O
predictive O
models O
constructed O
based B
on O
different O
hypotheses O
resulted O
from O
Step O
2 O
; O
and O
( O
4 O
) O
hypothesis O
evaluation O
and O
comparison O
to O
draw O
conclusions O
of O
skill B
composition O
. O

Step O
1 O
: O
Data O
Crawling O
and O
Cleaning O
Using O
commonly O
available O
match B
statistics O
data O
, O
our O
approach O
is O
to O
correlate O
different O
models O
and O
hypotheses O
of O
players B
’ O
skills I
and O
their O
match B
outcomes O
. O

Specifically O
, O
we O
are O
interested O
in O
the O
game B
outcome O
data O
recorded O
from O
5 O
vs O
5 O
MOBA O
matches B
played B
by O
a O
set O
of O
players B
in O
a O
certain O
range B
of O
time B
, O
with O
a O
set O
of O
champions B
appearing O
as O
selected O
. O

The O
matches B
should O
be O
of O
similar O
competitive B
settings I
and O
contain O
every O
match B
played B
by O
the O
player B
set O
in O
the O
period O
of O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
9 O
time B
. O

In O
more O
details O
, O
each O
match B
consists O
of O
the O
following O
information O
: O
( O
1 O
) O
IDs O
of O
the O
players B
involved O
; O
( O
2 O
) O
IDs O
of O
the O
champions B
involved O
( O
3 O
) O
Win O
/ O
loss O
match B
outcome O
. O

The O
data O
is O
readily O
available O
for O
most O
MOBA O
games B
, O
to O
the O
public O
or O
not O
, O
including O
LoL B
and O
DOTA2 O
. O

After O
the O
data O
crawling O
phase O
, O
we O
index O
matches B
by O
integers O
in O
[ O
1 O
, O
Z O
] O
and O
transform O
player B
IDs O
and O
champion B
IDs O
into O
continuous O
indices O
in O
[ O
1 O
, O
N O
] O
and O
[ O
1 O
, O
M O
] O
respectively O
. O

Step O
2 O
: O
Hypothesis O
Construction O
Next O
, O
various O
skill B
decomposition O
hypotheses O
are O
utilized O
to O
reflect O
feasible O
conjectures O
about O
player B
skill I
formation I
. O

We O
conducted O
a O
survey O
among O
8 O
MOBA O
players B
to O
extract O
helpful O
information O
in O
hypothesis O
construction O
. O

The O
survey O
included O
the O
following O
questions O
: O
1 O
. O

What O
is O
your O
experience B
in O
your O
played B
MOBA O
game B
? O
2 O
. O

Please O
list O
factors O
that O
you O
think O
could O
have O
influence O
on O
match B
outcomes O
in O
MOBA O
games B
. O

3 O
. O

Please O
elaborate O
on O
the O
factors O
you O
list O
above O
which O
are O
related O
to O
player B
skills I
. O

The O
interviewees O
include O
5 O
veteran B
players I
: O
3 O
players B
are O
from O
top B
4 O
tiers I
in O
LoL O
and O
2 O
players B

 O
with I
considerably I
high I
scores O
( O
≥ O
3500 O
) O
in O
solo B
queue I
MMR O
( O
one O
measurement O
of O
skill B
) O
in O
DOTA2 O
. O

Among O
their O
answers O
, O
we O
extracted O
three O
common O
influential O
factors O
in O
match B
outcomes O
that O
are O
associated O
with O
player B
skills I
: O
1 O
. O

Players B
’ O
general O
knowledge O
and O
experience B
in O
controlling B
match B
paces O
and O
cooperating O
in O
team B
combats I
. O

2 O
. O

Averagely O
speaking O
, O
champions B
have O
different O
curves O
to O
be O
mastered B
and O
when O
champions B
confronts O
in O
lanes B
1-on-1 O
, O
certain O
champions B
have O
advantages B
over O
the O
others O
. O

3 O
. O

Even O
veteran B
players I
can O
master B
only O
a O
couple O
of O
champions B
. O

Therefore O
champion B
- O
specific O
skills B
should O
be O
accounted O
. O

In O
the O
end O
, O
we O
propose O
3 O
skill B
components O
in O
the O
skill B
models O
: O
player B
base O
skill I
, I
champion B

 O
base I
skill I
, O
and O
champion B
- O
player B
specific O
skill I
. O

We O
construct O
7 O
hypotheses O
to O
verify O
them O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
10 O
Hypothesis O
1 O
( O
H1 O
) O
: O
The O
skill B
of O
a O
player B
is O
a O
single O
underlying O
weight O
denoting O
his O
base B
skill B

 O
level I
. I
In I
this O
hypothesis O
, O
player B
skills I
are O
regardless O
of O
selected O
champions B
. O

H2 O
: O
Player B
skill I
is O
not O
related O
to O
individual B
players I
but O
only O
related O
to O
selected O
champions B
. O

Each O
champion B
is O
associated O
with O
a O
single O
weight O
, O
which O
we O
call B
champion B
base I
skill I
, O
denoting O
the O
average O
influence O
of O
which O
the O
champion B
affects O
match B
outcomes O
. O

Champion O
base B
skill B
can O
also O
be O
interpreted O
as O
the O
average B
skill I
of I
players I
when O
selecting O
the O
same O
champions B
. O

A O
player B
’s O
skill I
is O
solely O
the O
champion B
base I
skill I
of O
his O
selected O
champion B
. O

The O
team B
skill I
is O
the O
sum O
of O
the O
weights O
of O
the O
5 O
champions B
selected O
. O

H3 O
: O
Player B
skill I
varies O
both O
across O
players B
and O
selected O
champions B
. O

Therefore O
, O
we O
need O
to O
estimate O
champion B
- O
specific O
skill B
for O
every O
player B
. O

The O
team B
skill I
is O
the O
sum O
of O
the O
5 O
player B
- O
champion B
specific O
skills B
. O

We O
also O
explore O
combinatorial O
interaction O
among O
the O
three O
types O
of O
skill B
. O

As O
above O
, O
if O
without O
mentioned O
specifically O
, O
all O
the O
hypotheses O
use O
the O
sum O
of O
individual B
skill I
compositions O
to O
represent O
team B
skill I
level I
. O

H4 O
: O
Player B
skill I
consists O
of O
player B
base O
skill I
and I
champion B
base I
skill I
. O

H5 O
: O
Player B
skill I
consists O
of O
player B
base O
skill I
and I
his O
champion B
specific O
skill B
. O

H6 O
: O
Player B
skill I
consists O
of O
champion B
base I
skill I
and O
his O
champion B
specific O
skill B
. O

H7 O
: O
Player B
skill I
consists O
of O
player B
base O
skill I
, I
champion B
base I
skill I
and O
his O
champion B
specific B

 O
skill I
. I

 I
Step O
3 O
: O
Predictive O
Models O
and O
Training O
We O
incorporate O
the O
7 O
hypotheses O
into O
multiple O
predictive O
models O
that O
predicts O
the O
game B
outcomes O
. O

The O
idea O
is O
that O
if O
a O
hypothesis O
is O
correct O
, O
the O
models O
incorporating O
corresponding O
skill B
components O
should O
demonstrate O
improved O
accuracy O
compared O
to O
those O
without O
incorporating O
those O
skill B
components O
. O

Logistic O
Regression O
. O

We O
selected O
logistic O
regression O
( O
LR O
) O
to O
model O
the O
hypotheses O
for O
the O
following O
reasons O
: O
( O
1 O
) O
LR O
is O
a O
commonly O
used O
model O
for O
binary O
classification O
problem O
with O
decent O
computation O
time B
; O
( O
2 O
) O
in O
preliminaries O
exploration O
, O
we O
also O
explored O
using O
other O
common O
supervised O
learning O
models O
such O
as O
Random O
Forest O
proposed O
byLiaw O
and O
Wiener O
PLAYER O
SKILL B
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
11 O
( O
2002 O
) O
, O
Naive O
Bayes O
and O
SVM O
proposed O
by O
Cortes O
and O
Vapnik O
( O
1995 O
) O
. O

However O
, O
the O
first O
two O
did O
not O
consistently O
outperform O
LR O
and O
SVM O
took O
too O
long O
to O
finish O
our O
predictive O
task O
; O
( O
3 O
) O
as O
discussed O
in O
the O
following O
subsection O
, O
the O
weights O
( O
model O
parameters O
to O
be O
trained B
) O
of O
logistic O
regression O
models O
can O
be O
associated O
with O
individual B
players I
’ O
skills B
. O

As O
such O
, O
logistic O
regression O
models O
team B
performance I
as O
the O
linear O
summation O
of O
individual B
skills I
, O
a O
typical O
and O
effective O
approach O
used O
by O
Rahman O
et O
al O
. O

( O
2015 O
) O
, O
Delalleau O
et O
al O
. O

( O
2012 O
) O
and O
Herbrich O
et O
al O
. O

( O
2006 O
) O
to O
model O
team B
level O
skills I
based B
on O
individuals O
. O

Logistic O
Regression O
models O
the O
conditional O
distribution O
of O
a O
match B
outcome O
given O
the O
match B
feature O
vector O
as O
a O
Bernoulli O
distribution O
, O
i.e. O
, O
Mz|Dz O
; O
θ O
∼ O
Bernoulli(φ O
) O
( O
1 O
) O
φ O
= O
p(Mz O
= O
1|Dz O
; O
θ O
) O
= O
σ(θ O
T O
Dz O
) O
( O
2 O
) O
where O
θ O
∈ O
RK O
is O
the O
model O
parameter O
of O
the O
same O
dimension O
as O
input B
Dz O
and O
σ(x O
) O
= O
1 O
1+exp(−θT O
x O
) O
is O
a O
sigmoid O
function O
. O

Logistic O
regression O
learns O
model O
parameter O
θ O
∈ O
R O
K O
that O
maximizes O
the O
log O
likelihood O
function O
: O
` O
( O
θ O
) O
= O
log O
p(M|D O
; O
θ O
) O
= O
log O
Y O
Z O
z=1 O
p(Mz|Dz O
; O
θ O
) O
= O
X O
Z O
z=1 O
Mz O
log O
p(Mz|Dz O
; O
θ)+ O
X O
Z O
z=1 O
( O
1 O
− O
Mz O
) O
log(1 O
− O
p(Mz|Dz O
; O
θ O
) O
) O
( O
3 O
) O
In O
practice O
, O
the O
objective B
function O
of O
logistic O
regression O
is O
often O
` O
( O
θ O
) O
added O
with O
a O
L2 O
regularization O
term O
C O
· O
θ O
T O
θ O
to O
prevent O
from O
the O
overfitting O
problem O
, O
where O
C O
is O
a O
configurable O
constant O
controlling B
the O
strength B
of O
regularization O
. O

For O
each O
predictive O
model O
, O
we O
will O
use O
cross O
validation O
in O
order O
to O
determine O
best O
value O
of O
C O
which O
helps O
the O
model O
achieve O
best O
accuracy O
on O
test O
datasets O
. O

Feature O
Vector O
Construction O
. O

We O
need O
to O
construct O
Dz O
, O
the O
feature O
vector O
of O
each O
match B
z O
, O
to O
feed O
as O
input B
to O
the O
predictive O
model O
. O

Each O
of O
the O
following O
paragraphs O
, O
starting B
with O
the O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
12 O
name O
of O
its O
associated O
predictive O
model O
, O
describes O
the O
way O
to O
construct O
Dz O
to O
incorporate O
the O
7 O
proposed O
hypotheses O
. O

We O
assume O
Dz O
in O
all O
models O
as O
a O
sparse O
vector O
, O
i.e. O
, O
components O
unless O
mentioned O
are O
0 O
. O

LR O
- O
P6 O
: O
Dz O
∈ O
RN O
. O

Dz O
, O
j O
= O
1 O
if O
and O
only O
if O
I(z O
, O
r O
, O
Ci O
, O
Pj O
) O
= O
1 O
and O
Dz O
, O
j O
= O
−1 O
if O
and O
only O
if O
I(z O
, O
b O
, O
Ci O
, O
Pj O
) O
= O
1 O
. O

In O
this O
way O
, O
each O
component O
of O
θ O
will O
correspond O
to O
a O
player B
’s O
skill I
weight O
and O
θ O
T O
Dz O
is O
exactly O
the O
difference O
of O
the O
player B
base O
skill I
sums I
between O
the O
two O
teams B
. O

LR O
- O
C O
: O
In O
a O
similar O
fashion O
as O
in B
LR O
- I
Player I
, O
there O
are O
five O
1 O
’s O
and O
five O
-1 O
’s O
in O
Dz O
denoting O
the O
five O
champions B
selected O
by O
the O
players B
on O
the O
red B
team O
and I
the I
blue I
team B
respectively O
. O

LR O
- O
P O
- O
C O
Dz O
∈ O
RN+M O
where O
the O
first O
N O
components O
are O
constructed O
as O
LR O
- O
P O
and O
the O
last O
M O
components O
are O
constructed O
as O
LR O
- O
C. O
LR O
- O
PC O
: O
Dz O
∈ O
R O
N×M O
( O
every O
player B
has O
M O
weights O
denoting O
his O
skills B
in O
the O
M O
champions B
) O
. O

For O
i O
, O
j O
such O
that O
I(z O
, O
r O
, O
Ci O
, O
Pj O
) O
= O
1 O
, O
set O
Dz O
, O
j·M+i O
= O
1 O
. O

Similarly O
, O
for O
i O
, O
j O
such O
that O
I(z O
, O
b O
, O
Ci O
, O
Pj O
) O
= O
1 O
, O
set O
Dz O
, O
j·M+i O
= O
−1 O
. O

LR O
- O
P O
- O
C O
- O
PC O
Dz O
∈ O
R O
N+M+N×M O
as O
the O
horizontal O
concatenation O
of O
the O
constructed O
Dz O
from O
LR O
- O
P O
, O
LR O
- O
C O
and O
LR O
- O
PC O
. O

Step O
4 O
: O
Hypothesis O
Comparison O
In O
order O
to O
verify O
the O
7 O
hypotheses O
, O
it O
is O
sufficient O
to O
use O
and O
compare O
the O
results O
of O
LR O
- O
P O
, O
LR O
- O
C O
, O
LR O
- O
P O
- O
C O
, O
LR O
- O
P O
- O
C O
- O
PC O
and O
a O
naive O
baseline O
( O
e.g. O
majority O
class B
prediction O
) O
. O

For O
example O
, O
H3 O
, O
which O
says O
player B
skills I
only O
consist O
of O
champion B
specific O
skills B
, O
can O
be O
verified O
if O
: O
( O
1 O
) O
LR O
- O
P O
- O
C O
- O
PC O
improves O
prediction O
accuracy O
significantly O
over O
LR O
- O
P O
- O
C O
; O
and O
( O
2 O
) O
neither O
LR O
- O
P O
or O
LR O
- O
C O
outperforms O
the O
naive O
baseline O
. O

More O
examples O
of O
relying O
comparisons O
to O
verify O
hypotheses O
are O
detailed O
in O
Results O
and O
Discussions O
section O
. O

We O
utilize O
model O
comparison O
metrics O
to O
reliably O
demonstrate O
the O
superiority O
of O
one O
model O
over O
the O
others O
with O
statistical O
significance O
. O

Specifically O
, O
when O
we O
compare O
two O
models O
with O
test O
accuracy O
means O
and O
standard O
deviations O
( O
mean1 O
, O
std1 O
) O
and O
( O
mean2 O
, O
std2 O
) O
respectively O
, O
we O
consider O
the O
first O
one O
has O
significant O
improvement O
over O
the O
second O
if O
6LR O
- O
P O
stands O
for O
( O
L)ogistic O
( O
R)egression O
with O
( O
P)layer O
feature O
components O
. O

In O
the O
remaining O
hypotheses O
, O
we O
also O
use O
C O
and O
PC O
to O
represent O
champion B
base I
skills I
and O
players B
’ O
champion B
specific O
skills B
respectively O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
13 O
mean1 O
− O
mean2 O
> O
2(std1 O
+ O
std2)7 O
. O

A O
type O
of O
skill B
composition O
will O
be O
identified O
as O
prominent O
if O
adding O
it O
to O
feature O
vectors O
consistently O
help O
predictive O
models O
outperform O
those O
without O
incorporating O
it O
. O

Data O
Collection O
In O
this O
paper O
, O
we O
are O
interested O
in O
analyzing O
player B
skills I
in O
two O
most O
popular O
MOBA O
games B
worldwide O
according O
to O
Minotti O
( O
2016 O
) O
, O
League O
of O
Legends O
( O
LoL B
) O
and O
Defense O
of O
Ancients O
2 O
( O
DOTA2 O
) O
. O

In O
both O
games B
, O
we O
only O
consider O
ranked B
matches B
from O
top B
tier I
players I
as O
the O
input B
data O
. O

Ranked B
matches B
are O
those O
automatically O
balanced O
by O
the O
matchmaking B
systems I
so O
as O
to O
form O
two O
teams B
with O
similar O
winning O
probability O
prior O
to O
game B
start B
. O

Ranked B
matches B
are O
usually O
played B
seriously O
because O
players B
can O
earn O
recognizable O
in B
- I
game I
points B
and O
titles O
through O
winning O
ranked B
matches B
. O

For O
this O
paper O
we O
focus B
on O
investigating O
ranked B
matches B
because O
it O
is O
the O
only O
game B
mode I
that O
provides O
public O
available O
data O
API O
in O
LoL. O
Moreover O
, O
top B
tier I
( O
most O
experienced O
and O
skilled B
) O
players I
are O
selected O
for O
two O
reasons O
: O
( O
1 O
) O
their O
skills B
are O
considered O
more O
stable O
than O
rookie O
players B
therefore O
it I
can I
reduce O
noise O
for O
our O
predictive O
model O
. O

( O
2 O
) O
As O
we O
know O
, O
in O
the O
real O
world B
, O
the O
population O
of O
players B
usually O
decreases O
exponentially O
as O
the O
skill B
levels I
goes O
up O
. O

By O
selecting O
top B
tier I
players I
we O
control B
the O
number O
of O
weights O
in O
a O
computationally O
acceptable O
range B
on O
single O
machines O
given O
that O
our O
focus B
on O
skill B
decomposition O
rather O
than O
large O
- O
scale O
skill B
rating I
. O

League O
of O
Legends O
As O
Tassi O
( O
2016 O
) O
and O
Minotti O
( O
2016 O
) O
reported B
, O
LoL O
is O
the O
most O
played B
MOBA O
game B
in O
the O
world B
, O
with O
90 O
million O
summoner B
names O
registered O
, O
27 O
million O
unique O
daily O
players B
, O
7.5 O
million O
concurrent O
users B
, O
and O
67 O
million O
monthly O
players B
. O

In O
LoL B
, O
players I
can O
be O
rewarded B
( O
or O
punished O
) O
by O
winning O
( O
or O
losing O
) O
ranked B
matches B
in O
terms O
of O
League O
Point O
. O

League O
Points B
are O
directly O
related O
to O
their O
public O
honored O
titles O
( O
termed O
tier B
) O
that O
a O
large O
number O
of O
players B
7Alternatively O
, O
one O
can O
use O
paired O
t O
- O
test O
to O
check O
the O
significance O
of O
improvement O
between O
the O
means O
of O
the O
two O
test O
accuracy O
populations O
. O

We O
found O
in O
our O
study O
both O
methods O
lead B
to O
the O
same O
conclusions O
in O
all O
pairs O
of O
model O
comparisons O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
14 O
pursue O
. O

The O
higher O
League O
Point O
a O
player B
has O
, O
the O
more O
esteemed O
tier B
he O
is O
crowned O
. O

League O
Points O
as O
well O
as O
tiers B
are O
refreshed O
at O
the O
beginning O
of O
every O
year O
when O
a O
new O
game B
season B
starts B
. O

We O
collected O
basic O
information O
of O
972 O
players B
( O
e.g. O
, O
player B
ID O
, O
registered O
name O
, O
etc O
) O
of O
the O
top O
two O
tiers B
( O
CHALLENGER O
and O
MASTER O
) O
on O
North O
America O
Server O
from O
a O
player B
database O
website8 O
in O
September O
2015 O
when O
most O
players B
have O
played B
more O
than O
half O
year O
to O
establish O
their O
tiers B
since O
the O
2015 O
season B
. O

We O
used O
the O
public O
official O
APIs9 O
to O
access O
detailed O
data O
( O
e.g. O
, O
win O
/ O
loss O
and O
selected O
champions B
) O
of O
231212 O
unique O
ranked B
matches B
played B
by O
the O
seed O
players B
in O
2015 O
. O

We O
finalized O
the O
player B
set O
of O
93,098 O
players B
by O
including O
all O
players B
appearing O
in O
the O
crawled O
matches B
. O

On O
the O
other O
hand O
, O
there O
are O
129 O
champions B
played B
in O
the O
matches B
collected O
. O

Defense O
of O
the O
Ancient O
2 O
( O
DOTA2 O
) O
DOTA2 O
is O
another O
popular O
MOBA O
game B
developed O
by O
Valve O
Corporation O
. O

In O
DOTA O
, O
the O
score O
to O
reflect O
player B
experience B
and O
skill B
is O
called B
Match B
Making O
Rating O
( O
MMR O
) O
. O

DOTA2 O
differs O
with O
LoL B
in O
that O
player B
data O
including O
MMR O
is O
by O
default O
not O
public O
available O
unless O
the O
player B
configures O
his I
profile I
to O
be O
public O
visible O
. O

Therefore O
, O
in O
a O
collected O
DOTA2 O
match B
there O
might O
be O
several O
players B
with O
one O
identity O
- O
anonymous B
player I
. O

In O
the O
training O
phase O
, O
we O
treat O
all O
anonymous B
players I
as O
one O
universal O
anonymous B
player I
. O

The O
weights O
associated O
with O
the O
universal O
anonymous B
player I
are O
considered O
as O
an O
average O
representation O
for O
all O
anonymous B

 O
players I
. O

We O
downloaded O
3.5 O
million O
DOTA2 O
matches B
played B
in O
2015 O
from O
a O
DOTA2 O
data O
statistic O
website O
10 O
. O

We O
only O
kept O
ranked B
matches B
in O
ALL O
PICK O
, O
RANDOM O
DRAFT O
or O
CAPTAIN O
game B
modes I
played B
on O
North O
America O
regions O
. O

The O
three O
game B
modes I
are O
more O
similar O
than O
others O
to O
the O
competitive B
setting I
in O
the O
LoL B
dataset O
. O

To O
filter O
the O
matches B
played B
by O
top O
players B
, O
we I
further I
selected B
matches I
with O
at O
least O
one O
player B
with O
public O
visible O
solo B
- O
queue I
MMR O
larger O
than O
4000 O
11 O
. O

Ultimately O
, O
there O
are O
117,874 O
players B
, O
1 O
universal O
anonymous O
8http://www.lolking.net/leaderboards#/na/1 O
9https://developer.riotgames.com/api/methods O
10https://yasp.co/blog/35 O
11Players O
with O
more O
than O
4000 O
MMR O
are O
approximately O
top O
10 O
% O
skilled B
players I
according O
to O
an O
online O
curated O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
15 O
player B
and O
111 O
champions B
appearing O
in O
the O
195,538 O
ranked B
matches B
. O

Results O
and O
Discussions O
The O
final O
results O
are O
shown O
in O
Table O
1 O
. O

We O
used O
grid O
search O
plus O
10-fold O
cross O
validation O
to O
determine O
the O
best O
regularization O
strength B
, O
C O
, O
of O
each O
logistic O
regression O
model O
. O

C O
in O
the O
grid O
search O
was O
sampled O
from O
log O
uniform O
distribution O
between O
0.001 O
and O
1000 O
. O

After O
properly O
tuning O
of O
C O
, O
we O
report B
each O
model O
with O
statistics O
of O
test O
accuracy O
population O
( O
mean O
and O
standard O
deviation O
) O
obtained O
in O
the O
10-fold O
cross O
validation O
. O

We O
also O
report B
the O
results O
of O
two O
baselines O
( O
TrueSkill O
and O
BL O
- O
MC O
) O
. O

In O
TrueSkill O
, O
we O
sort O
all O
matches B
in O
an O
ascending O
order O
of O
match B
created O
time B
. O

In O
the O
first O
90 O
% O
matches B
, O
we O
update B
player B
skill I
ratings I
according O
to O
TrueSkill O
algorithm O
proposed O
by O
Herbrich O
et O
al O
. O

( O
2006 O
) O
, O
a O
popular O
skill B
rating I
model O
. O

Then O
in O
the O
hold O
- O
out O
10 O
% O
latest O
matches B
, O
we O
fixed O
the O
learned O
player B
skill I
ratings I
to O
make O
outcome O
prediction O
. O

In B
BL O
- I
MC I
, O
we O
predict O
match B
outcomes O
by O
the O
majority O
labels O
in O
each O
dataset O
. O

Does O
the O
base B
skills B
of O
champions B
matter O
? O
In O
both O
games B
, O
the O
test O
accuracy O
of O
LR O
- O
C O
is O
significantly O
higher O
than O
BL O
- O
MC O
. O

We O
also O
observe O
that O
LR O
- O
P O
- O
C O
has O
noticable O
improvement O
over O
LR O
- O
P. O
Therefore O
we O
confirm O
that O
the O
base B
skills B
of O
champions B
is O
one O
source O
of O
player B
skill I

 O
composition I
. O

Does O
the O
base B
skills I
of I
players I
matter O
? O
Does O
the O
champion B
specific B
skills I
of I
players I
matter O
? O
The O
two O
questions O
can O
be O
analyzed O
together O
because O
the O
results O
show O
disagreement O
between O
the O
two O
games B
in O
both O
questions O
. O

In O
LoL O
, O
the O
two O
types O
of O
skills B
are O
indispensable O
components O
in O
player B
skill I
composition I
. O

The O
base B
skills I
of I
players I
are O
important O
because O
: O
( O
1 O
) O
LR O
- O
P O
alone O
shows O
non O
- O
trivial O
3.51 O
% O
higher O
test O
accuracy O
than O
BL O
- O
MC O
. O

( O
2 O
) O
LR O
- O
P O
- O
C O
has O
significant O
improvement O
over O
LR O
- O
C O
indicating O
that O
the O
added O
features O
, O
the O
base B
skills I
of I
players I
, O
are O
non O
- O
negligible O
besides O
the O
base B
skills B
of O
champions B
. O

( O
3 O
) O
TrueSkill O
, O
which O
purely O
models O
player B
base O
skills I
, I
has O
similar O
accuracy O
as O
LR O
- O
P. O
Players O
’ O
champion B
- O
specific O
skills B
are O
also O
important O
in O
LoL O
because O
LR O
- O
P O
- O
C O
- O
PC O
achieves O
the O
best O
accuracy O
among O
all O
models O
. O

global O
player B
MMR O
list O
: O
https://github.com/yasp-dota/yasp/wiki/MMR-Data O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
16 O
In O
DOTA2 O
, O
the O
story O
is O
much O
different O
. O

First O
, O
the O
base B
skills I
of I
players I
do O
not O
show O
explicit O
existence O
because O
( O
1 O
) O
LR O
- O
P O
has O
the O
test O
accuracy O
close B
to O
BL O
- O
MC O
, O
and O
( O
2 O
) O
LR O
- O
P O
- O
C O
shows O
no O
significant O
improvement O
over O
LR O
- O
C. O
Second O
, O
players B
’ O
champion B
specific O
skills B
do O
not O
constitute O
a O
good O
component O
of O
skill B
composition O
either O
. O

LR O
- O
P O
- O
C O
- O
PC O
has O
an O
non O
- O
significant O
0.13 O
% O
boost B
over O
test O
accuracy O
with O
LR O
- O
P O
- O
C. O
Combining O
with O
the O
fact O
that O
LR O
- O
P O
is O
merely O
close B
to O
the O
baseline O
, O
we O
conclude O
that O
in O
DOTA2 O
only O
the O
base B
skills B
of O
champions B
is O
the O
main O
source O
of O
player B
skill I
composition I
. O

Discussion O
We O
find O
that O
in O
LoL B
, O
player I
skill B
can O
be O
decomposed O
into O
three O
parts O
, O
namely O
the O
base B
skill I
of I

 O
players I
, O
the O
base B
skill B
of O
champions B
, O
and O
the O
champion B
- O
specific B
skills I
of I
players I
. O

On O
the O
other O
hand O
, O
player B
skill I
in O
DOTA2 O
is O
composed O
mainly O
of O
the O
base B
skill B
of O
champions B
. O

Next O
, O
we O
would O
like O
to O
make O
comments O
on O
the O
larger O
implication O
suggested O
by O
these O
results O
. O

To O
game B
designers O
and O
developers O
. O

The O
divergence O
of O
player B
skill I
composition I
between O
LoL O
and O
DOTA2 O
can O
be O
an O
entrance O
leading B
game B
designers O
and O
developers O
to O
think O
further O
about O
their O
design O
and O
implementation O
. O

We O
suspect O
the O
following O
reasons O
that O
potentially O
account O
for O
the O
divergence O
of O
player B
skill I
composition I
: O
( O
1 O
) O
the O
match B
making O
system O
in O
DOTA2 O
has O
a O
better O
quality O
than O
that O
in O
LoL B
, O
such O
that O
the O
base B
skills I
of I
players I
are O
hard B
to O
be O
further O
differentiated O
from O
the O
ranked B
matches B
meant O
to O
be O
fair O
, O
balancing O
and O
competitive B
. O

However O
, O
the O
suspicion O
fails O
to O
explain O
why O
champion B
specific O
skills B
are O
not O
obvious B
in O
DOTA2 O
since O
such O
kind O
skills B
are O
not O
accountable O
by O
the O
current O
match B
making O
systems O
using O
one O
dimension O
skill B
scores O
. O

( O
2 O
) O
The O
game B
design O
of O
champion B
characteristics O
may O
differ O
across O
the O
two O
games B
. O

It O
is O
worth B
noting O
that O
the O
base B
skills B
of O
champions B
has O
a O
larger O
effect B
to O
determine O
outcomes O
in O
DOTA2 O
since O
LR O
- O
C O
achieves O
more O
than O
4 O
% O
test O
accuracy O
in O
DOTA2 O
than O
in O
LoL O
and O
the O
gap O
is O
far O
beyond O
what O
the O
standard O
deviations O
of O
the O
two O
models O
can O
explain O
. O

This O
indicates O
that O
champions B
have O
a O
larger O
impact B
to O
game B
outcomes O
based B
upon O
DOTA2 O
’s O
game B
design O
. O

( O
3 O
) O
The O
anonymity O
issue O
in O
the O
DOTA2 O
dataset O
fails O
the O
estimation O
of O
player B
side O
skills I
due O
to O
the O
loss O
of O
information O
. O

To O
show O
that O
the O
failure O
is O
not O
due O
to O
the O
influence O
of O
the O
anonymous B
players I
, O
we O
randomly O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
17 O
marked B
the O
same O
ratio O
of O
the O
LoL O
players B
as O
in O
DOTA2 O
to O
be O
anonymous B
players I
. O

( O
Thus O
all O
these O
selected B
players I
are O
marked B
as O
one O
anonymous B
player I
identity O
. O

) O
We O
re O
- O
applied O
all O
our O
models O
and O
found O
that O
prediction O
improvement O
is O
still O
significant O
in O
the O
modified O
LoL B
dataset O
when O
adding O
the O
base B
skills B
and O
champion B
specific B
skills I
of I
players I
into O
the O
models O
. O

However O
, O
as O
we O
expected O
, O
the O
means O
of O
test O
accuracy O
decrease O
due O
to O
the O
loss O
of O
information O
introduced O
by O
the O
anonymity O
. O

Due O
to O
the O
limited O
space O
we O
do O
not O
list O
the O
results O
on O
this O
dataset O
. O

To O
players B
The O
results O
shown O
in O
this O
paper O
can O
be O
a O
useful O
reference O
for O
players B
regarding O
skill I
building O
and O
match B
prognosis O
. O

For O
LoL B
players I
, O
they O
can O
learn O
their O
skill B
composition O
of O
the O
three O
kinds O
of O
skills B
and O
know O
which O
part O
is O
their O
skill B
advantage B
/ O
disadvantage B
. O

Players B
can O
even O
make O
rough O
prognosis O
about O
the O
match B
right O
after O
the O
ten O
players B
have O
finished O
selecting O
champions B
. O

This O
can O
help O
players B
identify O
, O
for O
example O
, O
the O
threats O
of O
the O
opponents B
according O
to O
the O
order O
of O
the O
base B
skills B
of O
champions B
. O

Meanwhile O
there O
are O
more O
interesting O
questions O
we O
want O
to O
answer O
for O
players B
in O
the O
future O
. O

For O
example O
, O
can O
simultaneous O
choices O
of O
those O
champions B
with O
high B
base O
skills I
necessarily I
lead B
to O
higher O
win B
rate I
? O
If O
two O
teams B
have O
the O
same O
sum O
of O
player B
base O
skill I
weights I
, O
will O
the O
player B
with O
the O
highest O
skill B
weight O
determine O
the O
outcome O
? O
A O
number O
of O
intriguing O
questions O
can O
be O
derived O
from O
this O
paper O
and O
we O
aim O
to O
answer O
them O
in O
future O
works O
. O

To O
human O
resource B
managers O
and O
crowd O
source O
task O
initiators O
. O

The O
results O
also O
allude O
important O
lessons O
for O
human O
resource B
and O
crowd O
sourcing O
communities B
, O
where O
effective O
team B

 O
formation I
is O
a O
hot O
topic O
. O

In O
such O
scenario O
, O
people O
want O
to O
find O
people O
of O
different O
expertise O
to O
achieve O
a O
team B
- O
wise O
goal B
. O

It O
is O
analogous O
to O
hypothesize O
that O
team B
performance I
relies O
on O
individual O
team B
members O
skills I
forming O
from O
the O
base B
skills B
of O
tasks O
, O
the O
base B
skills B
of O
team B

 O
members I
, O
and O
their O
task O
specific O
expertise O
. O

We O
are O
looking O
for O
future O
study O
to O
verify O
the O
hypothesis O
. O

Conclusion O
and O
Future O
Works O
The O
paper O
presents O
our O
preliminary O
findings O
in O
analyzing O
player B
skill I
composition I
in O
MOBA O
games B
. O

Adopting O
a O
model O
- O
based B
analysis O
approach O
, O
we O
find O
that O
player B
skills I
in O
MOBA O
games B
could O
include O
base B
skill I
of I
player I
, O
base B
skill B
of O
champion B
and O
player B
’s O
champion B
specific O
skill B
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
18 O
as O
three O
prominent O
components O
. O

In O
addition O
, O
we O
find O
LoL O
has O
more O
diverse B
skill I
compositions O
than O
DOTA2 O
. O

The O
result O
can O
potentially O
spark O
the O
reflection O
on O
game B
design O
and O
implementation O
, O
raise O
awareness O
of O
skill B
strength B
and O
weakness B
among O
players B
and O
allude O
lessons O
for O
human O
resource B
management O
and O
crowd O
sourcing O
. O

In O
the O
future O
, O
we O
would O
like O
to O
explore O
player B
skill I
on O
team B
collaboration O
. O

For O
example O
, O
what O
is O
a O
player B
’s O
performance O
when O
teaming B
with O
players B
or O
champions B
of O
certain O
play B
styles I
? O
We O
also O
plan O
to O
investigate O
and O
compare O
player B
skill I
composition I
on O
other O
video B
game I
genres O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
19 O
References O
Adams O
, O
E. O
( O
2013 O
) O
. O

Fundamentals O
of O
game B
design O
. O

Pearson O
Education O
. O

Avontuur O
, O
T. O
, O
Spronck O
, O
P. O
, O
& O
Van O
Zaanen O
, O
M. O
( O
2013 O
) O
. O

Player O
Skill O
Modeling O
in O
Starcraft O
II O
. O

In O
Aiide O
. O

Bakkes O
, O
S. O
C. O
J. O
, O
Spronck O
, O
P. O
H. O
M. O
, O
& O
van O
Lankveld O
, O
G. O
( O
2012 O
) O
. O

Player O
behavioural O
modelling O
for O
video B
games I
. O

Entertainment O
Computing O
, O
3(3 O
) O
, O
71–79 O
. O

Bradley O
, O
R. O
A. O
, O
& O
Terry O
, O
M. O
E. O
( O
1952 O
) O
. O

Rank B
analysis O
of O
incomplete O
block O
designs O
: O
I. O
The O
method O
of O
paired O
comparisons O
. O

Biometrika O
, O
39(3/4 O
) O
, O
324–345 O
. O

Charles O
, O
D. O
, O
Kerr O
, O
A. O
, O
McNeill O
, O
M. O
, O
McAlister O
, O
M. O
, O
Black O
, O
M. O
, O
Kcklich O
, O
J. O
, O
. O

. O

. O

Stringer O
, O
K. O
( O
2005 O
) O
. O

Player B
- O
centred O
game B
design O
: O
Player B
modelling O
and O
adaptive O
digital O
games B
. O

In O
Proceedings O
of O
the O
digital O
games O
research O
conference O
( O
Vol O
. O

285 O
, O
p. O
100 O
) O
. O

Chen O
, O
S. O
, O
& O
Joachims O
, O
T. O
( O
2016 O
) O
. O

Modeling O
intransitivity O
in O
matchup B
and O
comparison O
data O
. O

In O
Proceedings O
of O
the O
ninth O
acm O
international O
conference O
on O
web O
search O
and O
data O
mining O
( O
pp O
. O

227–236 O
) O
. O

Cortes O
, O
C. O
, O
& O
Vapnik O
, O
V. O
( O
1995 O
) O
. O

Support B
- O
vector O
networks O
. O

Machine O
learning O
, O
20(3 O
) O
, O
273–297 O
. O

Delalleau O
, O
O. O
, O
Contal O
, O
E. O
, O
Thibodeau O
- O
Laufer O
, O
E. O
, O
Ferrari O
, O
R. O
C. O
, O
Bengio O
, O
Y. O
, O
& O
Zhang O
, O
F. O
( O
2012 O
, O
sep O
) O
. O

Beyond O
Skill O
Rating O
: O
Advanced O
Matchmaking B
in O
Ghost O
Recon O
Online O
. O

IEEE O
Transactions O
on O
Computational O
Intelligence O
and O
AI O
in O
Games O
, O
4(3 O
) O
, O
167–177 O
. O

Drachen O
, O
A. O
, O
Yancey O
, O
M. O
, O
Maguire O
, O
J. O
, O
Chu O
, O
D. O
, O
Wang O
, O
I. O
Y. O
, O
Mahlmann O
, O
T. O
, O
. O

. O

. O

Klabajan O
, O
D. O
( O
2014 O
, O
October O
) O
. O

Skill O
- O
based B
differences O
in B
spatio O
- I
temporal I
team B
behaviour O
in O
defence O
of O
the O
Ancients O
2 O
( O
DotA O
2 O
) O
. O

In O
2014 O
ieee O
games O
media O
entertainment O
( O
pp O
. O

1–8 O
) O
. O

IEEE O
. O

Duh O
, O
H. O
B.-L. O
, O
& O
Chen O
, O
V. O
H. O
H. O
( O
2009 O
) O
. O

Cheating B
behaviors O
in O
online O
gaming O
. O

In O
A. O
A. O
Ozok O
& O
P. O
Zaphiris O
( O
Eds O
. O

) O
, O
Online B
communities I
and O
social O
computing O
: O
Third O
international O
conference O
, O
ocsc O
2009 O
, O
held O
as O
part O
of O
hci O
international O
2009 O
, O
san O
diego O
, O
ca O
, O
usa O
, O
july O
19 O
- O
24 O
, O
2009 O
. O

proceedings O
( O
pp O
. O

567–573 O
) O
. O

Berlin O
, O
Heidelberg O
: O
Springer O
Berlin O
Heidelberg O
. O

Elo O
, O
A. O
E. O
( O
1978 O
) O
. O

The O
rating O
of O
chessplayers O
, O
past O
and O
present O
. O

Arco O
Pub O
. O

Glickman O
, O
M. O
E. O
( O
1999 O
) O
. O

Parameter O
estimation O
in O
large O
dynamic O
paired O
comparison O
PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
20 O
experiments O
. O

Applied O
Statistics O
, O
377–394 O
. O

Herbrich O
, O
R. O
, O
Minka O
, O
T. O
, O
& O
Graepel O
, O
T. O
( O
2006 O
) O
. O

Trueskill O
™ O
: O
A O
bayesian O
skill B
rating I
system O
. O

In O
( O
pp O
. O

569–576 O
) O
. O

Advances O
in O
Neural O
Information O
Processing O
Systems O
. O

Huang O
, O
T.-K. O
, O
Lin O
, O
C.-J. O
, O
& O
Weng O
, O
R. O
C. O
( O
2004 O
) O
. O

A O
generalized O
Bradley O
- O
Terry O
model O
: O
From O
group B
competition B
to O
individual B
skill I
. O

In O
Advances O
in O
neural O
information O
processing O
systems O
( O
pp O
. O

601–608 O
) O
. O

Kittur O
, O
A. O
( O
2010 O
) O
. O

Crowdsourcing O
, O
collaboration O
and O
creativity O
. O

ACM O
Crossroads O
, O
17(2 O
) O
, O
22–26 O
. O

Liaw O
, O
A. O
, O
& O
Wiener O
, O
M. O
( O
2002 O
) O
. O

Classification O
and O
regression O
by O
randomforest O
. O

R O
news O
, O
2(3 O
) O
, O
18–22 O
. O

Mahlmann O
, O
T. O
, O
Schubert O
, O
M. O
, O
& O
Drachen O
, O
A. O
( O
2016 O
) O
. O

Esports O
Analytics O
Through O
Encounter O
Detection O
. O

In O
Mit O
sloan O
sports O
analytics O
conference O
. O

Menke O
, O
J. O
E. O
, O
& O
Martinez O
, O
T. O
R. O
( O
2008 O
) O
. O

A O
Bradley O
– O
Terry O
artificial O
neural O
network O
model O
for O
individual O
ratings O
in O
group B
competitions B
. O

Neural O
computing O
and O
Applications O
, O
17(2 O
) O
, O
175–186 O
. O

Minotti O
, O
M. O
( O
2016 O
) O
. O

Comparing O
MOBAs B
: O
League O
of O
Legends O
vs. O
Dota O
2 O
vs. O
Smite O
vs. O
Heroes O
of O
the O
Storm O
. O

http://venturebeat.com/2015/07/15/comparing-mobas-league-of-legendsvs-dota-2-vs-smite-vs-heroes-of-the-storm/. O
( O
Online O
; O
accessed O
5-May-2016 O
) O
Myslak O
, O
M. O
, O
& O
Deja O
, O
D. O
( O
2014 O
) O
. O

Developing O
game B
- O
structure O
sensitive O
matchmaking B
system I
for O
´ O
massive B
- I
multiplayer I
online I
games I
. O

In O
Social O
informatics O
( O
pp O
. O

200–208 O
) O
. O

Springer O
. O

Nguyen O
, O
T.-H. O
D. O
, O
Chen O
, O
Z. O
, O
& O
El O
- O
Nasr O
, O
M. O
S. O
( O
2015 O
) O
. O

Analytics O
- O
based B
AI O
Techniques O
for O
Better O
Gaming O
Experience B
( O
Vol O
. O

2 O
; O
S. O
Rabin O
, O
Ed O
. O

) O
. O

Boca O
Raton O
, O
Florida O
: O
CRC O
Press O
. O

Pobiedina O
, O
N. O
, O
Neidhardt O
, O
J. O
, O
Calatrava O
Moreno O
, O
M. O
d. O
C. O
, O
Grad O
- O
Gyenge O
, O
L. O
, O
& O
Werthner O
, O
H. O
( O
2013 O
) O
. O

On O
Successful O
Team O
Formation O
: O
Statistical O
Analysis O
of O
a O
Multiplayer O
Online O
Game O
. O

In O
2013 O
ieee O
15th O
conference O
on O
business O
informatics O
( O
pp O
. O

55–62 O
) O
. O

IEEE O
. O

Pobiedina O
, O
N. O
, O
Neidhardt O
, O
J. O
, O
Calatrava O
Moreno O
, O
M. O
d. O
C. O
, O
& O
Werthner O
, O
H. O
( O
2013 O
) O
. O

Ranking B
factors O
of O
team B
success O
. O

In O
Proceedings O
of O
the O
22nd O
international O
conference O
on O
world B
wide O
web O
companion O
( O
pp O
. O

1185–1194 O
) O
. O

PLAYER O
SKILL O
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
21 O
Rahman O
, O
H. O
, O
Thirumuruganathan O
, O
S. O
, O
Roy O
, O
S. O
B. O
, O
Amer O
- O
Yahia O
, O
S. O
, O
& O
Das O
, O
G. O
( O
2015 O
) O
. O

Worker O
skill B
estimation O
in B
team O
- I
based I
tasks O
. O

Proceedings O
of O
the O
VLDB O
Endowment O
, O
8(11 O
) O
, O
1142–1153 O
. O

Rioult O
, O
F. O
, O
Metivier O
, O
J.-P. O
, O
Helleu O
, O
B. O
, O
Scelles O
, O
N. O
, O
& O
Durand O
, O
C. O
( O
2014 O
) O
. O

Mining O
tracks O
of O
competitive B
video O
games I
. I
In O
Aasri O
conference O
on O
sports O
engineering O
and O
computer O
science O
( O
secs O
2014 O
) O
( O
p. O
6-p O
) O
. O

Roy O
, O
S. O
B. O
, O
Lykourentzou O
, O
I. O
, O
Thirumuruganathan O
, O
S. O
, O
Amer O
- O
Yahia O
, O
S. O
, O
& O
Das O
, O
G. O
( O
2015 O
) O
. O

Task O
assignment O
optimization O
in B
knowledge O
- I
intensive I
crowdsourcing O
. O

The O
VLDB O
Journal O
, O
24(4 O
) O
, O
467–491 O
. O

Salen O
, O
K. O
, O
& O
Zimmerman O
, O
E. O
( O
2004 O
) O
. O

Rules B
of O
play B
: O
Game B
design O
fundamentals O
. O

MIT O
press O
. O

Stanescu O
, O
M. O
( O
2011 O
) O
. O

Rating O
systems O
with O
multiple O
factors O
. O

SuperData O
. O

( O
2016 O
) O
. O

eSports O
market O
brief O
: O
US O
accounts O
for O
almost O
half O
of O
total O
viewership O
. O

https://www.superdataresearch.com/blog/esports-brief/. O
( O
Online O
; O
accessed O
1-Mar-2016 O
) O
Tassi O
, O
P. O
( O
2016 O
) O
. O

Riot B
’s O
’ O
League O
of O
Legends O
’ O
Reveals O
Astonishing O
27 O
Million O
Daily O
Players O
, O
67 O
Million O
Monthly O
. O

http://www.forbes.com/sites/insertcoin/2014/01/27/riots-league-of-legends-revealsastonishing-27-million-daily-players-67-million-monthly/#26ff8e543511 O
. O

( O
Online O
; O
accessed O
5-May-2016 O
) O
Yang O
, O
P. O
, O
Brent O
, O
H. O
, O
& O
Roberts O
, O
D. O
L. O
( O
2014 O
) O
. O

Identifying O
Patterns O
in O
Combat O
that O
are O
Predictive O
of O
Success O
in O
MOBA O
Games O
. O

In O
Proceedings O
of O
foundations O
of O
digital O
games B
. O

Yannakakis O
, O
G. O
, O
Spronck O
, O
P. O
, O
Loiacono O
, O
D. O
, O
& O
André O
, O
E. O
( O
2013 O
) O
. O

Player O
Modeling O
. O

In O
Artificial O
and O
computational O
intelligence O
in O
games B
( O
pp O
. O

45–59 O
) O
. O

doi O
: O
10.4230 O
/ O
DFU.Vol6.12191.45 O
PLAYER O
SKILL B
DECOMPOSITION O
IN O
MULTIPLAYER O
ONLINE O
BATTLE O
ARENAS O
22 O
Table O
1 O
Best O
Average O
Test O
Accuracy O
by O
Each O
Model O
Model O
Test O
Acc O
± O
std O
LoL O
DOTA2 O
LR O
- O
P O
56.75 O
% O
± O
0.24 O
% O
52.62 O
% O
± O
0.47 O
% O
LR O
- O
C O
55.62 O
% O
± O
0.36 O
% O
59.16 O
% O
± O
0.41 O
% O
LR O
- O
P O
- O
C O
58.82 O
% O
± O
0.20 O
% O
59.53 O
% O
± O
0.39 O
% O
LR O
- O
P O
- O
C O
- O
PC O
60.24 O
% O
± O
0.16 O
% O
59.66 O
% O
± O
0.42 O
% O
TrueSkill O
55.24 O
% O
± O
0.16 O
% O
52.05 O
% O
± O
0.12 O
% O
BL O
- O
MC O
53.22 O
% O
± O
0.16 O
% O
52.65 O
% O
± O
0.12 O
% O
MOBA O
: O
a O
New O
Arena O
for O
Game O
AI O
Victor O
do O
Nascimento O
Silva1 O
and O
Luiz O
Chaimowicz2 O
Abstract O
— O
Games B
have O
always O
been O
popular O
testbeds O
for O
Artificial O
Intelligence O
( O
AI O
) O
. O

In O
the O
last O
decade O
, O
we O
have O
seen O
the O
rise O
of O
the O
Multiple O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
, O
which O
are O
the O
most O
played O
games B
nowadays O
. O

In O
spite O
of O
this O
, O
there O
are O
few O
works O
that O
explore O
MOBA O
as O
a O
testbed O
for O
AI O
Research O
. O

In O
this O
paper O
we O
present O
and O
discuss O
the O
main O
features O
and O
opportunities O
offered O
by O
MOBA O
games B
to O
Game O
AI O
Research O
. O

We O
describe O
the O
various O
challenges O
faced O
along O
the O
game B
and O
also O
propose O
a O
discrete O
model O
that O
can O
be O
used O
to O
better O
understand O
and O
explore O
the O
game B
. O

With O
this O
, O
we O
aim O
to O
encourage O
the O
use O
of O
MOBA O
as O
a O
novel O
research O
platform O
for O
Game O
AI O
. O

I. O
INTRODUCTION O
From O
Arthur O
Samuel O
’s O
research O
on O
Checkers O
[ O
1 O
] O
, O
through O
Deep O
Blue O
’s O
efforts O
in O
defeating B
a O
human O
chess O
champion B
[ O
2 O
] O
to O
the O
recent O
accomplishments O
of O
AlphaGo O
[ O
3 O
] O
, O
games B
have O
always O
been O
an O
important O
drive O
for O
Artificial O
Intelligence O
( O
AI O
) O
. O

Games O
provide O
a O
controlled B
test O
environment O
with O
interesting O
challenges O
, O
where O
novel O
AI O
techniques B
can O
be O
tested O
. O

Moreover O
, O
playing B
competitively O
against O
humans O
has O
always O
motivated O
AI O
researchers O
and O
has O
proved O
to O
be O
difficult O
in O
most O
games B
. O

In O
the O
last O
decade O
, O
Real O
Time O
Strategy O
( O
RTS O
) O
Games B
have O
emerged O
as O
a O
challenging O
testbed O
for O
AI O
[ O
4 O
] O
. O

More O
than O
just O
rational O
behavior O
, O
these O
games B
require O
agents O
that O
reason O
and O
act O
as O
fast O
as O
possible O
in O
partially O
known O
environments O
. O

In O
RTS O
Games O
, O
AI O
techniques B
have O
to O
deal O
with O
a O
broad O
range B
of O
variables O
, O
that O
could O
influence O
the O
chances B
of O
winning O
and O
losing O
the O
game B
. O

Real B
- I
time I
games I
also O
offer O
a O
wider B
range I
of O
actions O
to O
the O
player B
, O
and O
, O
sometimes O
, O
a O
large O
number O
of O
agents O
to O
be O
controlled B
[ O
5 O
] O
. O

One O
of O
the O
most O
successful O
testbeds O
in O
this O
context O
is O
the O
RTS O
Game O
Starcraft O
. O

Specially O
, O
the O
Brood O
War O
version O
of O
Starcraft O
became O
very O
popular O
among O
researchers O
, O
being O
used O
in O
many O
works O
and O
competitions B
[ O
6 O
] O
. O

It O
requires O
abilities B
in O
different O
areas B
such O
as O
unit B
coordination O
, O
resource B
management O
and O
combat B
skills B
and O
presents O
several O
challenges O
to O
AI O
agents O
that O
have O
to O
deal O
with O
a O
complex O
state O
space O
, O
feature O
analysis O
and O
some O
kind O
of O
intuition O
involved O
in O
Starcraft O
gameplay B
. O

Due O
to O
this O
, O
humans O
has O
historically O
performed O
better O
than O
computers O
in O
Starcraft O
[ O
7 O
] O
, O
[ O
8 O
] O
. O

We O
believe O
that O
Starcraft O
has O
become O
a O
standard O
research O
platform O
because O
of O
two O
main O
factors O
: O
a O
) O
the O
availability O
of O
a O
programming O
environment O
that O
allowed O
researchers O
to O
develop O
their O
agents O
[ O
9 O
] O
; O
and O
b O
) O
the O
large O
player B
base B
of O
the O
game B
, O
allowing O
researchers O
to O
compare O
their O
agents O
with O
humans O
. O

These O
factors O
make O
the O
research O
easier O
, O
and O
makes O
it O
more O
1V. O
N. O
Silva O
is O
with O
the O
Department O
of O
Computing O
Science O
, O
University O
of O
Alberta O
, O
Canada O
vsilva@ualberta.ca O
2Luiz O
Chaimowicz O
is O
with O
the O
Department O
of O
Computing O
Science O
, O
Federal O
University O
of O
Minas O
Gerais O
, O
Brazil O
. O

chaimo@dcc.ufmg.br O
* O
This O
work O
was O
partially O
supported B
by O
FAPEMIG O
and O
CNPq O
relevant O
, O
because O
it O
impacts B
more O
people O
. O

Moreover O
, O
it O
is O
better O
to O
perform O
user B
test O
in O
an O
environment O
that O
players B
are O
familiar O
with O
, O
because O
they O
feel O
comfortable O
, O
and O
already O
know O
how O
to O
behave O
in O
the O
game B
. O

Recently O
, O
the O
game B
community B
has O
witnessed O
the O
rise O
of O
MOBA O
( O
Multiple O
Online O
Battle O
Arena O
) O
Games O
. O

With O
titles O
such O
as O
League O
of O
Legends O
, O
Smite O
and O
Dota O
2 O
, O
MOBA O
games B
are O
responsible O
for O
almost O
30 O
% O
of O
the O
online O
gameplay B
around O
the O
world B
[ O
10 O
] O
. O

League O
of O
Legends O
alone O
consumes O
approximately O
23 O
% O
of O
this O
gameplay B
, O
and O
became O
the O
most O
played B
game B
of O
the O
world B
, O
surpassing O
the O
MMORPG O
World O
of O
Warcraft O
[ O
11 O
] O
. O

MOBA O
can O
be O
considered O
a O
sub B
- I
genre I
of I
the O
RTS I
, O
inheriting O
some O
of O
its O
characteristics O
. O

However O
, O
instead O
of O
focusing B
on O
the O
ability B
of O
maneuvering O
large O
amounts O
of O
units B
and O
building O
fortresses O
, O
a O
set B
of I
actions I
known O
as O
Macromanagement O
, O
MOBAs B
have O
a O
strong O
focus B
on O
Micromanagement O
which O
consists O
in O
mastering B
a O
set B
of I
actions I
and O
their O
best O
use O
in O
the O
control B
of O
a O
small O
set O
of O
units B
. O

In O
MOBA O
games B
, O
this O
is O
generally O
known O
as O
mechanics B
, O
and O
players B
with O
these O
fine O
abilities B
normally O
excel O
in O
the O
game B
. O

Due O
to O
their O
specific O
gameplay B
, O
MOBA O
games B
present O
a O
different O
and O
interesting O
set O
of O
challenges O
for O
Game O
AI O
. O

But O
, O
in O
spite O
of O
their O
huge O
commercial O
success O
, O
they O
have O
not O
yet O
been O
extensively O
explored O
as O
a O
testbed O
for O
game B
AI O
such O
as O
their O
RTS O
ancestors O
. O

Thus O
, O
in O
this O
paper O
we O
present O
and O
discuss O
the O
main O
challenges O
and O
opportunities O
offered O
by O
MOBA O
games B
to O
Game O
AI O
Research O
. O

We O
break O
down O
this O
discussion O
considering O
the O
specific O
phases O
of O
the O
gameplay B
and O
also O
in O
more O
general O
terms O
related O
to O
game B
mechanics I
. O

Moreover O
, O
we O
present O
a O
general O
model O
based B
on O
a O
MOBA O
abstraction O
that O
can O
facilitate O
related O
research O
in O
more O
general O
settings O
such O
as O
autonomous O
agents O
, O
computational O
intelligence O
and O
learning O
. O

The O
remainder O
of O
this O
paper O
is O
organized O
as O
follows O
: O
we O
give O
an O
overview O
of O
the O
MOBA O
genre O
in O
Section O
II O
. O

We O
follow O
by O
discussing O
the O
MOBA O
phases O
and O
their O
specific O
challenges O
in O
Section O
III O
. O

The O
mechanic B
challenges O
found O
in O
MOBA O
are O
described O
in O
Section O
IV O
while O
the O
Discrete O
Model O
proposed O
for O
the O
MOBA O
environment O
is O
presented O
in O
V. O
Finally O
, O
we O
present O
the O
related O
research O
that O
have O
been O
done O
using O
MOBA O
in O
Section O
VI O
and O
discuss O
the O
conclusions O
and O
future O
work O
in O
Section O
VII O
. O

II O
. O

MULTIPLAYER O
ONLINE O
BATTLE O
ARENA O
The O
MOBA O
genre O
has O
its O
roots O
on O
the O
RTS O
genre O
, O
and O
even O
shared O
game B
platforms O
in O
the O
early B
days O
. O

Instead O
of O
focusing B
on O
the O
management O
of O
an O
army O
, O
exploring O
maps O
and O
build O
a O
fortress O
, O
MOBA O
focuses B
on O
the O
finest O
mechanic B
abilities B
, O
such O
as O
combos O
, O
kiting O
, O
abilities B
, O
among O
others O
. O

The O
core O
gameplay B
of O
most O
MOBAs B
is O
the O
control B
of O
a O
single O
unit B
, O
commonly O
called B
hero B
. O

This O
unit B
has O
a O
set O
of O
arXiv:1705.10443v1 O
[ O
cs O
. O

AI O
] O
30 O
May O
2017 O
2 O
special B
powers I
and O
characteristics O
that O
allow O
it O
to O
combat B
other O
units B
. O

Each O
player B
receives O
his I
chosen I
hero B
in O
the O
level B
1 O
, O
and O
should O
evolve O
his O
hero B
to O
the O
max O
level B
offered O
by O
the O
game B
. O

Thus O
, O
each O
game B
is O
different O
from O
another O
, O
and O
the O
environment O
is O
not O
persistent O
. O

The O
main O
goal B
is O
to O
conquer O
a O
single O
structure O
located O
at O
the O
opposite O
side O
of O
the O
map O
. O

Another O
important O
characteristic O
of O
MOBA O
is O
that O
it O
is O
not O
played B
alone O
. O

The O
common O
game B
mode I
of O
most O
MOBAs O
requires O
two O
teams B
of O
five O
players B
. O

Each O
player B
controls B
a O
single B
hero I
that O
should O
assume O
a O
role B
in O
the O
team B
, O
composing O
the O
team B
’s O
strategy B
. O

The O
role B
consists O
of O
the O
function O
that O
the O
hero B
will O
assume O
in O
the O
match B
, O
e.g. O
Carry O
, O
Initiator O
, O
Tank O
, O
Ganker O
, O
among O
others O
. O

Each O
team B
has O
a O
base B
, O
which O
contains O
a O
structure O
that O
should O
be O
conquered O
by O
the O
opponent B
team I
. O

To O
do O
so O
, O
the O
team B
should O
fight I
against O
strong O
defensive O
structures O
, O
called B
turrets B
. O

A O
general O
MOBA O
map O
is O
composed O
by O
three O
lanes B
, O
and O
each O
lane B
can O
contain O
up O
to O
three O
turrets B
. O

To O
conquer O
a O
turret B
closer B
to O
the O
base B
, O
the O
team B
should O
firstly O
destroy O
the O
precedent O
turret B
in O
the O
same O
lane B
. O

A O
view O
of O
a O
generic O
MOBA O
map O
can O
be O
seen O
in O
Figure O
1 O
. O

Fig O
. O

1 O
. O

A O
generic O
map O
found O
on O
MOBA O
games B
. O

Red O
shapes O
represent O
a O
team B
while O
blue B
represent O
another O
. O

Circles O
represent O
turrets B
. O

The O
green B
area B
is O
the O
jungle B
and O
the O
yellow O
area B
are O
the O
lanes B
. O

The O
corner O
areas B
are O
the O
team B
’s O
base I
. O

An O
important O
part O
of O
RTS O
gameplay B
is O
to O
collect O
resources B
from O
specific O
places O
to O
build O
structures O
and O
units B
. O

For O
instance O
, O
in O
Starcraft O
this O
resource B
collection O
is O
done O
in O
sources O
of O
gas O
and O
crystals B
, O
that O
are O
later O
used O
to O
create O
units B
and O
structures O
. O

In O
the O
MOBA O
gameplay B
, O
the O
resource B
collection O
task O
is O
also O
present O
, O
but O
with O
different O
characteristics O
. O

Instead O
of O
simply O
sending O
units B
to O
collect O
the O
materials O
from O
a O
source O
, O
MOBA O
rely O
on O
the O
player B
skills I
to O
perform O
the O
so O
called B
farming B
. O

Farming O
consists O
in O
killing B
certain O
units B
known O
as O
creeps B
or O
minions B
in O
order O
to O
get O
gold B
. O

These O
units B
spawn O
in O
waves B
from O
each O
team B
base I
, O
and O
follow O
a O
lane B
fighting B
their O
way O
to O
the O
enemy B
base I
. O

Besides O
, O
it O
is O
possible O
to O
obtain O
gold B
from O
neutral O
minions B
, O
which O
live O
in O
the O
jungle B
, O
the O
free B
space O
between O
the O
lanes B
. O

Lastly O
, O
killing B
enemy B
heroes I
or O
destroying O
enemy B
structures O
also O
give O
the O
hero B
or O
the O
team B
a O
certain O
amount B
of I

 O
gold I
. O

Therefore O
, O
more O
mechanically O
skilled B
players I
can O
collect O
more O
resources B
than O
the O
least O
skilled B
ones O
. O

We O
can O
consider O
that O
collecting O
resources B
in O
MOBA O
is O
a O
harder B
task O
than O
in O
RTS O
since O
it O
involves O
skills B
like O
positioning B
, O
skill B
/ O
attack B
control B
and O
target B
selection I
, O
while O
RTS O
requires O
the O
player B
to O
simply O
send O
units B
to O
a O
specific O
place O
to O
collect O
resources B
. O

The O
gold B
in O
MOBAs O
is O
not O
used O
to O
buy O
structures O
or O
units B
. O

Instead O
, O
it O
is O
used O
to O
buy O
items B
for O
the O
controlled B
hero B
. O

These O
items B
improve O
the O
hero B
abilities B
and O
status B
, O
making O
it O
more O
powerful B
. O

Besides O
, O
there O
are O
items B
that O
give O
the O
hero B
special B

 O
powers I
and O
can O
be O
used O
during O
team B
fights I
or O
to O
disable O
enemies B
. O

Thus O
, O
instead O
of O
focusing B
in O
building O
a O
fortress O
, O
MOBA O
requires O
the O
player B
to O
improve O
his O
/ O
her O
own O
hero B
, O
preparing O
it O
to O
succeed O
in O
the O
game B
. O

Lastly O
, O
MOBA O
focuses B
on O
team B
cooperation I
to O
defeat B
an O
enemy B
team I
. O

Differently O
from O
RTS O
, O
which O
values O
players B
that O
are O
able O
to O
manage O
many O
units B
, O
explore O
the O
map O
and O
build O
structures O
, O
MOBA O
rewards B
players B
with O
higher O
mechanical O
skills B
that O
are O
also O
capable O
of O
cooperating O
with O
other O
players B
to O
execute O
a O
successful O
strategy B
. O

Therefore O
, O
besides O
individual O
abilities B
, O
cooperation B
is O
an O
important O
feature O
in O
MOBA O
gameplay B
. O

III O
. O

MOBA O
PHASES O
’ O
CHALLENGES O
A O
MOBA O
game B
can O
be O
divided O
in O
different O
phases O
, O
which O
one O
with O
specific O
challenges O
. O

Identifying O
such O
challenges O
and O
game B
features O
is O
important O
to O
better O
understand O
the O
game B
and O
to O
the O
discuss O
the O
main O
avenues O
for O
AI O
research O
on O
MOBA O
. O

In O
the O
next O
sections O
, O
for O
each O
phase O
, O
we O
describe O
its O
main B
characteristics I
and O
strategies B
and O
then O
present O
its O
main O
challenges O
in O
terms O
of O
AI O
. O

The O
phases O
are O
described O
in O
chronological O
order O
, O
starting B
before O
the O
match B
actually O
starts B
up O
to O
the O
point B
where O
the O
game B
state I
is O
more O
consolidated O
. O

A. O
Pick O
and O
Ban O
Phase O
Most O
MOBAs B
offer O
a O
large O
hero B
base B
to O
players B
, O
allowing O
them O
to O
build O
specific O
strategies B
, O
and O
to O
pick B
heroes B
that O
excel O
over O
other O
heroes B
, O
an O
act O
called B
counter O
- O
picking O
. O

Normally O
, O
there O
are O
two O
main B
hero I
picking B
modes O
: O
blind B
and O
tournament B
. O

In O
blind B
mode I
each O
team B
of O
players B
selects O
freely O
from O
the O
hero B
pool O
without O
knowing O
the O
enemy B
team I
’s O
heroes B
. O

On O
the O
other O
hand O
, O
in O
the O
tournament B
mode I
, O
each O
team B
picks B
alternately O
and I

 I
is I
allowed O
to O
perform O
bans O
on O
the O
other O
team B
, O
as O
seen O
in O
Figure O
2 O
. O

Moreover O
, O
in O
the O
tournament B
mode I
, O
the O
hero B
picked B
or O
being O
considered O
is O
seen O
by O
all O
players B
in O
the O
match B
. O

A O
hero B
can O
not O
be O
picked B
twice O
in O
the O
tournament B
mode I
, O
while O
in O
blind B
mode I
it O
can O
be O
picked B
once O
by O
each O
team B
. O

Most O
professional O
players B
and O
eSport O
staff O
agree O
that O
the O
game B
can O
be O
, O
sometimes O
, O
defined O
during O
the O
pick B
and I
ban I

 O
phase I
. O

That O
is O
because O
there O
are O
some O
heroes B
that O
stand O
out O
in O
the O
game B
strategy B
, O
especially O
in O
professional O
gameplay B
where O
players B
have O
better O
mechanical O
skills B
. O

It O
is O
also O
common O
to O
see O
bans O
towards O
specific O
heroes B
that O
players B
have O
difficulty O
to O
play B
against O
, O
or O
that O
are O
considered O
overpowered O
. O

The O
picks B
are O
normally O
made O
in O
accordance O
to O
a O
strategy B
to O
be O
executed O
by O
the O
team B
, O
such O
as O
fighting O
, O
picking B
enemies B
, O
pushing O
lanes B
, O
etc O
. O

The O
picks B
can O
also O
be O
done O
based B
on O
the O
player B
’s O
skills I
with O
a O
specific O
hero B
or O
considering O
the O
metagame B
. O

The O
metagame B
consists O
of O
a O
set O
knowledge O
from O
inside O
and O
outside O
the O
game B
that O
encompass O
a O
series O
of O
3 O
Fig O
. O

2 O
. O

A O
screenshot B
from O
the O
pick B
and O
ban O
Phase O
from O
the O
game B
Dota O
2 O
in O
tournament B
mode I
. O

guidelines O
of O
which O
strategies B
and O
heroes B
are O
more O
likely O
to O
succeed O
during O
a O
time B
period O
. O

Lastly O
, O
the O
picks B
can O
be O
done O
based B
on O
outplaying O
the O
enemy B
team I
’s O
strategy B
, O
or O
simply O
by O
picking B
heroes B
that O
counter O
- O
play B
the O
enemy B
heroes I
. O

The O
choices O
in O
the O
Pick O
and O
Ban O
Phase O
are O
key O
since O
they O
can O
determine O
the O
outcome O
of O
the O
game B
. O

So O
, O
the O
challenges O
faced O
by O
an O
Artificial O
Intelligence O
Agent O
playing B
this O
phase O
are O
of O
great O
importance O
. O

Firstly O
, O
an O
Artificial O
Intelligent O
Agent O
should O
pick B
heroes B
that O
are O
effective O
both O
playing B
alone O
and O
in O
a O
team B
also O
considering O
that O
they O
should O
be O
at O
least O
as O
good O
as O
the O
other O
team B
’s O
roster O
. O

Moreover O
, O
the O
AI O
should O
start B
the O
construction O
of O
a O
game B
strategy B
, O
planning O
which O
are O
the O
best O
actions O
for O
the O
chosen O
heroes B
. O

Lastly O
, O
it O
is O
important O
to O
perform O
some O
kind O
of O
opponent B
modeling O
in O
order O
to O
identify O
the O
possible O
strategies B
and O
roles B
to O
be O
assumed O
by O
the O
enemy B

 O
heroes I
. O

B. O
Opening O
Phase O
The O
opening O
phase O
begin O
when O
the O
match B
starts B
and O
ends O
when O
the O
creeps B
or O
minions B
reach O
the O
middle B
of I
each O
lane I
. O

This O
phase O
consists O
of O
two O
main O
tasks O
: O
buying O
the O
opening O
items B
and O
performing O
role B
allocation O
. O

Optionally O
, O
there O
may O
be O
pickoffs O
, O
level B
one O
team B
fights I
or O
jungle B
invasion I
. O

The O
act O
of O
buying O
the O
very O
first O
items B
depends O
much O
on O
the O
enemies B
that O
the O
hero B
will O
be O
facing O
. O

Nevertheless O
, O
the O
initial O
items B
are O
very O
tentative O
, O
because O
the O
player B
can O
only O
guess O
which O
hero B
it O
will O
be O
playing B
against O
. O

If O
the O
player B
has O
a O
deep O
comprehension O
of O
the O
metagame B
, O
it O
can O
predict O
which O
hero B
will O
be O
in O
each O
lane B
, O
however O
, O
this O
can O
be O
just O
a O
guess O
. O

The O
role B
allocation O
not O
only O
consists O
of O
assigning O
a O
role B
to O
a O
hero B
, O
it O
also O
consists O
of O
making O
the O
initial O
positioning B
on O
the O
game B
environment O
. O

For O
instance O
, O
the O
strategy B
defined O
can O
propose O
an O
inversion O
of O
lanes B
to O
make O
the O
hero B
to O
face O
a O
weaker B
hero I
and O
, O
therefore O
, O
maximize O
its O
chances B
of O
winning O
the O
combat B
. O

The O
jungle B
invasion I
consists O
of O
trying O
to O
kill B
neutral O
creeps B
located O
at O
the O
enemy B
’s O
jungle B
, O
denying O
gold B
and O
experience B
to O
the O
enemy B
team I
, O
especially O
if O
there O
is O
a O
jungle B
role B
. O

Performing O
this O
strategy B
can O
lead B
to O
pickoffs O
, O
where O
the O
team B
finds O
a O
vulnerable O
hero B
in O
the O
map O
and O
try O
to O
kill B
him O
, O
getting O
gold B
and O
experience B
to O
the O
team B
. O

If O
the O
strategy B
is O
based B
on O
fights B
and O
the O
opportunity O
happens O
, O
the O
team B
can O
choose O
to O
perform O
a O
level B
one O
team B
fight I
, O
that O
happens O
before O
the O
heroes B
are O
well O
developed O
. O

The O
main O
challenges O
here O
are O
the O
following O
: O
a O
) O
select O
the O
initial O
set O
of O
items B
to O
buy O
; O
b O
) O
allocate O
a O
set O
of O
five O
heroes B
to O
their O
respective O
roles B
; O
and O
c O
) O
define O
which O
lane B
will O
be O
claimed O
by O
one O
or O
more O
heroes B
. O

For O
accomplishing O
these O
, O
it O
may O
be O
necessary O
to O
develop O
a O
system O
that O
predicts O
the O
enemy B
’s O
strategy B
and O
plan O
a O
counter O
strategy B
that O
minimizes O
the O
chances B
of O
losing O
the O
game B
, O
by O
defining O
roles B
, O
lanes B
and O
items B
to O
each O
hero B
. O

C. O
Laning O
Phase O
The O
Laning O
Phase O
starts B
when O
the O
creeps B
reach O
the O
middle B
of O
each O
lane B
. O

It O
consists O
of O
mostly O
staying O
in O
the O
lane B
and O
trying O
to O
collect O
the O
maximum O
number O
of O
resources B
as O
possible O
. O

These O
resources B
come O
mainly O
from O
executing O
last O
- O
hits O
on O
enemy B
minions B
that O
come O
to O
the O
lane B
. O

This O
is O
generaly O
know O
as O
farming B
. O

Although O
this O
task O
seems O
to O
be O
simple O
, O
it O
is O
not O
, O
due O
to O
several O
factors O
. O

The O
player B
competes O
in O
damaging B
with O
its O
allied O
creeps B
, O
allied O
heroes B
and O
turrets B
. O

Moreover O
, O
the O
player B
should O
take O
care O
to O
avoid O
being O
damaged B
by O
the O
enemies B
. O

In O
some O
games B
, O
like O
Dota O
and O
Dota2 O
, O
the O
enemy B
heroes I
can O
also O
damage B
their O
allied B
creeps B
, O
making O
it O
harder B
to O
the O
player B
to O
execute O
the O
last O
hit O
. O

Besides O
the O
resource B
collection O
task O
, O
the O
player B
should O
worry O
about O
the O
positioning B
of O
the O
heroes B
. O

Enemies B
can O
take O
advantage B
of O
a O
bad O
positioning B
by O
increasing B
the O
inflicted O
damage B
or O
even O
ganking O
the O
player B
. O

The O
ganking O
action O
consists O
of O
being O
caught O
and O
killed B
by O
other O
heroes B
that O
are O
not O
currently O
combating B
the O
player B
. O

A O
common O
practice O
is O
to O
use O
the O
Fog O
of O
War O
to O
perform O
effective O
ganks B
, O
taking O
advantage B
of O
the O
partial O
information O
. O

Every O
time B
the O
hero B
is O
killed B
the O
player B
receives O
a O
time B
punishment O
, O
through O
which O
the O
player B
can O
not O
perform O
actions O
. O

This O
gank B
often O
comes O
from O
a O
player B
role I
called B
ganker O
or O
jungler B
, O
a O
role B
intended O
to O
help O
heroes B
to O
kill B
enemy B
heroes I
. O

While O
most O
players B
act O
in O
the O
lanes B
, O
the O
jungler B
spends O
the O
laning B
phase O
collecting O
resources B
in O
the O
jungle B
. O

The O
jungler B
also O
roams O
around O
the O
map O
looking O
for O
opportunities O
to O
gank B
in O
the O
lanes B
. O

Moreover O
, O
junglers B
are O
responsible O
for O
the O
collection O
of O
minions B
and O
special O
blesses O
that O
change O
the O
hero B
status I
temporarily O
, O
commonly O
called B
buffs B
. O

For O
instance O
, O
in O
the O
game B
League O
of O
Legends O
, O
the O
jungle B
creeps B
offer O
special O
buffs B
that O
helps O
the O
jungler B
to O
perform O
ganks B
or O
help O
the O
team B
. O

In O
Dota2 O
and O
League O
of O
Legends O
there O
are O
heroes B
that O
can O
recruit O
neutral O
creeps B
to O
fight B
alongside O
them O
. O

The O
Laning O
Phase O
normally O
finishes O
when O
a O
hero B
has O
enough O
resources B
to O
buy O
an O
important O
item B
or O
has O
relevant O
status B
or O
level B
difference O
to O
other O
heroes B
. O

That O
can O
be O
achieved O
through O
farming B
or O
killing B
enemies B
. O

Having O
this O
strategic B
advantage I
leads B
the O
hero B
to O
perform O
ganks B
itself O
performing O
the O
roaming O
, O
i.e. O
walking O
around O
the O
map O
in O
the O
jungle B
or O
other O
lanes B
. O

During O
the O
laning O
phase O
the O
AI O
should O
perform O
many O
reasoning O
tasks O
. O

Among O
these O
are O
: O
deciding O
whether O
or O
not O
to O
buy O
new O
items B
, O
when O
to O
fight B
or O
damage B
the O
enemy B
hero I
, O
which O
positioning B
is O
the O
best O
, O
etc O
. O

Moreover O
, O
since O
there O
are O
just O
three O
lanes B
and O
five O
heroes B
, O
the O
AI O
should O
be O
ready O
to O
4 O
cooperate O
with O
other O
players B
towards O
killing B
enemies B
or O
any O
other O
strategy B
. O

D. O
Mid O
Game O
The O
Mid O
Game O
is O
considered O
the O
phase O
in O
which O
the O
strategy B
takes O
form O
after O
the O
Laning O
Phase O
. O

Commonly O
there O
are O
three O
main O
strategy B
types O
found O
in O
most O
MOBA O
games B
: O
team B
fights I
, O
pickoff O
and O
lane B
push I
. O

The O
team B
fight I
strategy I
consists O
of O
focusing B
the O
game B
in O
getting O
strategic B
advantage I
through O
defeating B
the O
enemy B
team I
in O
a O
series O
of O
group B
fights B
. O

Teams B
that O
focus B
in O
this O
strategy B
commonly O
have O
carry B
and O
tank B
heroes B
. O

The O
first O
is O
a O
hero B
capable O
of O
inflicting O
heavy B
damage I
on O
the O
opponents B
while O
the O
second O
is O
able O
of O
taking O
a O
lot B
of I
damage I
. O

By O
winning O
those O
fights B
the O
entire O
team B
obtains O
gold B
and O
experience B
, O
while O
punishing O
the O
enemy B
team I
with O
time B
penalties I
. O

Pickoff O
is O
the O
action O
of O
selecting O
a O
vulnerable O
or O
weak B

 O
hero I
and O
trying O
to O
kill B
it O
. O

Normally O
, O
this O
strategy B
focuses B
on O
defeating B
the O
enemy B
by O
picking B
important O
heroes B
and O
killing B
them O
to O
obtain O
fight B
advantage B
or O
weakening O
a O
main B
hero I
in O
the O
group B
. O

That O
can O
grant O
advantage B
in O
team B
fights I
or O
lead B
the O
team B
to O
gank B
the O
enemy B
heroes I
one O
- O
by O
- O
one O
, O
opening B
way O
to O
victory B
. O

Lastly O
, O
the O
lane B
push I
strategy B
has O
the O
objective B
of O
picking B
a O
lane B
and O
pushing I
it O
as O
hard B
as O
possible O
, O
destroying O
all O
the O
structures O
on O
the O
path B
to O
the O
base B
. O

This O
kind O
of O
strategy B
has O
many O
variants O
, O
with O
three O
as O
the O
most O
used O
: O
split B
push I
, O
sieging O
and O
team B
push I
. O

The O
split B
push I
consists O
of O
making O
a O
distraction O
to O
the O
enemy B
team I
while O
a O
hero B
can O
destroy O
the O
enemy B
structures O
on O
other O
lanes B
. O

Sieging O
consists O
of O
pushing O
the O
enemy B
team I
hard B
toward O
their O
base B
or O
turrets B
, O
focusing B
on O
destroying O
enemy B
structures O
instead O
of O
fighting B
. O

Finally O
, O
the O
team B
push I
is O
the O
least O
used O
and O
may O
be O
seem O
as O
a O
rush B
strategy B
. O

The O
entire O
team B
goes O
for O
a O
lane B
and O
try O
to O
destroy O
all O
the O
structures O
as O
quick O
as O
possible O
, O
avoiding O
direct O
fights B
with O
the O
enemy B
team I
. O

Challenges O
found O
in O
this O
phase O
are O
very O
diverse O
, O
but O
the O
cooperation B
and O
real B
- I
time I
reasoning I
problems O
stand O
out O
. O

In O
this O
phase O
, O
regardless O
of O
the O
strategy B
being O
followed O
, O
the O
synergy O
between O
the O
team B
is O
the O
key O
. O

Furthermore O
, O
the O
mechanics B
demonstrated O
by O
the O
AI O
should O
excel O
the O
human O
mechanics B
. O

Prediction O
, O
skill B
evasion O
and O
kiting O
are O
very O
valuable O
mechanics B
in O
this O
phase O
, O
because O
players B
will O
be O
fighting B
each O
other O
in O
a O
faster O
pace O
than O
in O
previous O
phases O
. O

A O
shallow O
analysis O
demonstrates O
that O
, O
for O
instance O
, O
a O
level B
one O
hero B
commonly O
has O
a O
few O
different O
actions O
to O
perform O
against O
one O
or O
two O
enemy B

 O
heroes I
. O

On O
the O
other O
hand O
, O
in O
the O
mid B
game I
the O
hero B
has O
to O
fight B
against O
five O
other O
heroes B
and O
has O
many O
actions O
and O
action O
combinations O
. O

E. O
Late O
Game O
The O
late B
game I
is O
the O
last O
phase O
of O
a O
MOBA O
match B
. O

It O
starts B
when O
almost O
all O
heroes B
are O
missing O
one O
or O
two O
items B
to O
finish O
their O
itemization O
. O

In O
this O
phase O
the O
heroes B
are O
strong O
and O
the O
number O
of O
possible O
actions O
large O
, O
making O
combats B
more O
complex O
. O

Being O
caught O
alone O
in O
the O
late B
game I
normally O
means O
to O
be O
defeated B
. O

Moreover O
, O
the O
time B
penalty I
for O
being O
killed B
is O
higher O
at O
the O
late B
game I
, O
impairing O
the O
teams B
during O
fight I
. O

When O
heroes B
have O
filled O
their O
item B
slots O
, O
they O
might O
start B
an O
itemization O
changing O
. O

It O
consists O
in O
changing O
the O
less O
valuable O
items B
for O
better O
ones O
. O

This O
technique B
also O
helps O
the O
team B
to O
counter O
- O
play B
the O
enemy B
strategies B
by O
using O
more O
appropriate O
items B
. O

In O
the O
late B
game I
it O
is O
common O
to O
have O
the O
heroes B
at O
their O
maximum O
potential O
. O

Heroes B
that O
are O
meant O
to O
be O
tank B
can O
absorb O
great B
amounts I
of I
damage I
, O
while O
carry B
can O
inflict O
a O
huge O
amount B
of I
damage I
. O

The O
duels B
, O
or O
fights B
of O
one O
versus O
one O
, O
are O
mechanically O
driven O
, O
meaning O
that O
the O
most O
skilled B

 O
player I
will O
win O
the O
fight B
, O
considering O
that O
their O
powers B
are O
almost O
the O
same O
. O

Sieging O
and O
picking B
off O
heroes B
show O
their O
advantage B
at O
this O
time B
, O
since O
heroes B
can O
take O
turrets B
down O
faster O
. O

The O
split B
pushing O
is O
also O
very O
effective O
, O
but O
may O
be O
a O
weak B
strategy B
if O
the O
team B
has O
to O
temporarily O
fight B
without O
the O
member O
that O
is O
performing O
the O
split B
push I
. O

Finally O
, O
performing O
a O
combat B
outcome O
analysis O
at O
this O
level B
may O
be O
difficult O
, O
since O
there O
are O
many O
combinations O
of O
abilities B
and O
actions O
. O

Moreover O
, O
it O
is O
hard B
to O
predict O
the O
enemies B
actions O
, O
especially O
if O
the O
enemies B
are O
humans O
. O

The O
challenges O
found O
here O
are O
mainly O
due O
to O
the O
huge O
amount O
of O
actions O
and O
possible O
combinations O
. O

Besides O
, O
all O
the O
analysis O
should O
be O
performed O
in B
real O
- I
time I
, O
meaning O
that O
the O
reaction B
time I
is O
an O
important O
factor O
for O
the O
AI O
system O
. O

Generating O
an O
agent O
that O
is O
capable O
of O
reacting O
as O
fast O
as O
human O
players B
and O
capable O
of O
performing O
prediction O
and O
analysis O
at O
professional O
level B
is O
hard B
, O
because O
of O
the O
number O
of O
factors O
that O
impact B
these O
actions O
. O

F. O
Side O
Challenges O
Along O
with O
the O
specific O
challenges O
found O
on O
each O
game B
phase O
, O
there O
are O
some O
challenges O
that O
are O
inherent O
to O
the O
game B
as O
a O
whole O
. O

These O
challenges O
are O
related O
with O
two O
types O
of O
analyses O
found O
in O
MOBA O
: O
prediction O
and O
combat B
analysis O
. O

These O
two O
fields B
converge O
in O
some O
sense O
, O
that O
the O
combat B
analysis O
is O
prediction O
dependent O
and O
vice O
- O
versa O
. O

Here O
we O
list B
some O
of I
the I
prediction O
and O
combat B
challenges O
found O
in O
MOBA O
: O
• O
Combat B
Outcome O
from O
duels B
( O
1 O
on O
1 O
) O
and O
team B
fights I
; O
• O
Killable B
enemies B
from O
a O
set B
of I
actions I
or O
combo O
; O
• O
Strategies B
to O
be O
performed O
by O
the O
enemy B
team I
; O
• O
Winner O
of O
the O
phase O
. O

Regarding O
the O
combat B
analysis O
, O
the O
combat B
outcome O
prediction O
can O
only O
be O
performed O
by O
the O
analysis O
of O
the O
interactions O
between O
a O
group B
of O
heroes B
involved O
in O
the O
combat B
scenario I
. O

These O
interactions O
were O
previously O
studied O
in O
Dota O
2 O
[ O
12 O
] O
. O

A O
special O
case O
of O
this O
scenario O
is O
the O
Duel O
, O
where O
two O
opposing O
heroes B
fight B
each O
other O
. O

Although O
seems O
to O
be O
easy B
, O
to O
predict O
the O
winner B
of O
a O
duel B
is O
hard B
, O
because O
it O
involves O
the O
prediction O
of O
attacks B
, O
abilities B
, O
movement B
, O
items B
, O
probabilities O
of O
critical O
damage B
, O
among O
others O
. O

Besides O
, O
the O
reaction O
in O
a O
duel B
should O
be O
done O
in O
real O
time B
, O
a O
task O
that O
requires O
the O
AI O
to O
return O
its O
answers O
as O
fast O
as O
possible O
. O

5 O
Concerning O
the O
field B
of O
counter O
playing B
the O
enemy B
team I
, O
it O
is O
important O
to O
predict O
which O
items B
the O
enemy B
proposes O
to O
buy O
. O

A O
successful O
build O
will O
help O
the O
hero B
to O
outplay O
the O
enemy B
by O
having O
statuses B
that O
help O
him O
to O
make O
the O
enemy B

 O
hero I
’s O
status B
less O
relevant O
. O

Moreover O
, O
building O
items B
in O
a O
nonstatic O
way O
is O
a O
hard B
task O
, O
because O
items B
should O
be O
acquired O
on O
demand O
, O
analyzing O
the O
match B
state O
. O

For O
instance O
, O
if O
the O
team B
is O
losing O
, O
one O
may O
prefer O
to O
buy O
defensive O
items B
, O
in O
order O
to O
have O
more O
survivability O
or O
to O
risk O
and O
buy O
items B
that O
maximize O
its O
damage B
, O
trying O
to O
be O
mechanically O
perfect O
. O

These O
challenges O
could O
be O
addressed O
by O
techniques B
that O
solve O
task O
allocation O
problems O
and O
optimization O
, O
like O
Ant O
Colony O
Optimization O
, O
Genetic O
Algorithms O
, O
among O
others O
. O

IV O
. O

MOBA O
MECHANIC O
CHALLENGES O
The O
MOBA O
gameplay B
has O
various O
structures O
and O
mechanics B
that O
are O
common O
in O
most O
MOBA O
games B
. O

One O
of O
such O
structures O
are O
the O
turrets B
. O

The O
turret B
is O
a O
strong O
structure O
that O
guards O
the O
lane B
from O
being O
naturally O
advanced O
by O
the O
creeps B
. O

Besides O
, O
turrets B
provide O
a O
strong O
defensive O
area B
where O
heroes B
can O
stay O
safer B
than O
in O
the O
lane B
itself O
. O

The O
damage B
and O
behavior O
of O
the O
turrets B
provide O
a O
mechanical O
challenge O
common O
to O
the O
MOBA O
genre O
: O
the O
tower B
diving I
. O

The O
tower B
diving I
challenge O
is O
to O
go O
under O
the O
area B
of O
damage B
of O
the O
turret B
and O
kill B
one O
or O
more O
enemy B
heroes I
, O
leaving O
the O
dangerous O
area B
alive O
. O

Executing O
a O
tower B
diving I
action O
correctly O
, O
or O
even O
predicting O
its O
success O
, O
is O
challenging O
for O
both O
humans O
and O
Computers O
because O
it O
requires O
the O
analysis O
of O
a O
set B
of I
actions I
and O
preconditions O
. O

For O
example O
, O
the O
tower B
diving I
hero B
must O
be O
sure O
that O
it O
has O
sufficient O
HP O
to O
kill B
the O
enemy B
and O
is O
capable O
of O
receiving O
some O
hits O
from O
the O
turret B
without O
being O
killed B
. O

Moreover O
, O
there O
must O
be O
an O
analysis O
of O
abilities B
, O
turret B
damage B
, O
probability O
of O
another O
enemy B
come O
to O
save O
the O
allied B
under O
turret B
are O
some O
of O
these O
features O
. O

One O
hero B
could O
simply O
stun O
the O
enemy B
diver O
under O
the O
turret B
area B
, O
leaving O
him O
there O
, O
stunned O
to O
death B
by O
turret B
damage B
. O

Both O
of O
those O
problems O
could O
be O
tackled O
using O
Influence O
Maps O
, O
Graphs O
Interaction O
analysis O
[ O
12 O
] O
and O
Bayesian O
Networks O
. O

Another O
challenge O
that O
is O
common O
in O
the O
MOBA O
scenario O
is O
the O
Target O
Selection O
. O

Selecting O
the O
target B
in O
a O
fight B
is O
a O
hard B
task O
, O
especially O
in O
a O
cooperative O
mode O
. O

Humans O
, O
for O
instance O
, O
discuss O
with O
each O
other O
the O
enemy B
that O
should O
be O
the O
next O
target B
in O
their O
strategy B
. O

In O
case O
of O
AI O
controlled B
agents O
, O
this O
could O
be O
done O
by O
a O
voting O
or O
consensus O
algorithm O
, O
in O
which O
the O
agents O
would O
consider O
factors O
like O
low O
HP O
, O
high O
risk O
, O
strategy B
breaking O
, O
easy B
to O
kill B
, O
among O
others O
to O
decide O
the O
target B
. O

In O
the O
laning O
phase O
, O
the O
hero B
should O
decide O
which O
minion B
should O
be O
the O
next O
target B
among O
a O
set O
of O
creeps B
called B
wave B
. O

In O
this O
phase O
it O
is O
common O
to O
perform O
the O
last O
hitting O
, O
which O
consists O
of O
hitting O
just O
the O
creeps B
that O
are O
killable B
by O
a O
single O
attack B
. O

On O
the O
other O
hand O
, O
the O
strategy B
could O
require O
the O
agents O
to O
push O
the O
lane B
, O
thus O
, O
the O
agents O
should O
hit O
continually O
, O
while O
maximizing O
the O
farm B
. O

This O
continuous O
hitting O
could O
be O
guided O
under O
rules B
that O
distribute O
the O
hits O
using O
a O
Target O
Selection O
Strategy O
. O

The O
Target O
Selection O
should O
be O
coupled O
with O
a O
good O
positioning B
strategy B
, O
or O
this O
technique B
may O
not O
perform O
well O
. O

A O
good O
positioning B
should O
allow O
the O
hero B
to O
maximize O
its O
damage B
/ O
effectiveness O
in O
the O
action O
performed O
, O
while O
minimizing O
the O
damage B
/ O
disable O
received O
. O

An O
example O
of O
such O
positioning B
is O
to O
have O
long O
ranged B
units B
in O
the O
back O
, O
assuming O
that O
these O
units B
are O
easily O
killable B
but O
deal B
heavy O
damage I
. O

In O
the O
front O
line O
is O
interesting O
to O
position O
the O
melee B
and O
tank B
heroes B
, O
because O
they O
can O
absorb O
a O
lot B
of I
damage I
and O
retain O
the O
enemy B
advance O
, O
while O
their O
allies B
can O
inflict O
damage B
. O

The O
previous O
discussion O
about O
positioning B
raises O
another O
relevant O
question O
. O

One O
may O
plan O
its O
strategy B
in O
order O
to O
minimize O
the O
damage B
received O
, O
however O
there O
are O
heroes B
that O
are O
meant O
to O
be O
damaged B
. O

Some O
of O
these O
heroes B
are O
initiators O
, O
and O
may O
not O
start B
the O
team B
fights I
without O
receiving O
damage B
from O
enemies B
. O

Moreover O
, O
there O
are O
heroes B
that O
take O
advantage B
of O
being O
damaged B
, O
like O
damage B
buffs B
, O
and O
even O
items B
that O
buffs B
the O
hero B
based B
on O
its O
missing O
HP O
. O

Therefore O
, O
the O
damage B
minimization O
strategy B
is O
not O
always O
the O
best O
strategy B
, O
and O
that O
should O
be O
taken O
in O
account O
in O
the O
agent O
development O
. O

V. O
A O
DISCRETIZED O
MODEL O
FOR O
MOBA O
STUDY O
After O
the O
study O
of O
the O
MOBA O
structure O
, O
phases O
and O
elements O
, O
we O
see O
that O
the O
game B
is O
based B
on O
discrete O
parts O
, O
that O
can O
be O
isolated O
in O
order O
to O
perform O
a O
better O
study O
. O

Thus O
, O
we O
propose O
a O
model O
based B
on O
the O
individual O
core O
gameplay B
of O
each O
mechanic B
task O
from O
the O
MOBA O
game B
. O

Moreover O
, O
a O
testbed O
framework O
could O
be O
generated O
based B
on O
this O
model O
, O
making O
it O
possible O
to O
perform O
preliminary O
learning O
and O
testing O
in O
a O
faster O
pace O
. O

Besides O
the O
testing O
benefit O
, O
it O
is O
known O
that O
MOBA O
, O
like O
its O
predecessor O
RTS O
, O
is O
divided O
in O
abstract O
decision O
layers O
. O

These O
layers O
mean O
that O
the O
agent O
should O
perform O
decision O
making O
on O
micro O
and O
macro O
levels B
. O

These O
abstract O
levels B
allow O
the O
AI O
to O
work O
in O
an O
individual B
skill I
without O
considering O
another O
ones O
. O

Moreover O
, O
the O
model O
is O
proposed O
in O
a O
way O
that O
each O
have O
no O
dependency O
on O
another O
part O
, O
simulating O
most O
or O
all O
features O
involved O
in O
each O
individual O
piece B
. O

We O
propose O
the O
division O
of O
MOBA O
by O
the O
main O
tasks O
that O
need O
to O
be O
performed O
by O
an O
agent O
. O

The O
first O
task O
is O
the O
item B
build O
order O
, O
following O
, O
we O
propose O
models O
for O
farming B
, O
team B

 O
fight I
, O
structure O
conquering O
and O
jungle B
. O

Having O
those O
models O
in O
an O
abstract O
form O
allows O
the O
research O
to O
make O
and O
test O
agents O
in O
diverse B
skills I
individually O
, O
further O
inserting O
this O
agent O
in O
a O
real O
game B
scenario O
. O

A. O
Item O
building O
As O
presented O
, O
the O
itemization O
is O
the O
process O
of O
buying O
items B
to O
improve O
the O
hero B
’s O
status I
. O

The O
validation O
of O
this O
process O
is O
commonly O
done O
in O
two O
ways O
: O
a O
) O
effectiveness O
of O
the O
hero B
in O
the O
game B
; O
and O
b O
) O
status B
increase B
. O

It O
is O
important O
to O
stress O
that O
the O
effectiveness O
in O
the O
game B
leads B
to O
status B
increase B
, O
since O
it O
provides O
more O
gold B
and O
experience B
to O
the O
hero B
and O
the O
team B
. O

On O
the O
other O
hand O
, O
status B
increase B
not O
always O
lead B
to O
effectiveness O
, O
since O
the O
agent O
must O
be O
mechanically O
perfect O
, O
which O
includes O
many O
variables O
and O
comprehension O
of O
the O
team B
behavior O
during O
cooperative O
tasks O
. O

An O
effective O
agent O
should O
balance O
the O
items B
and O
be O
capable O
of O
reasoning O
which O
building O
order O
should O
be O
done O
in O
specific O
6 O
cases O
. O

This O
can O
be O
done O
without O
the O
need O
of O
simulating O
team B

 O
fights I
, O
because O
the O
agent O
must O
only O
receive O
the O
enemy B
team I
’s O
status B
and O
strategy B
and O
reason O
about O
which O
items B
should O
be O
bought O
to O
play B
against O
. O

This O
process O
can O
be O
done O
with O
static O
and O
dynamic O
planning O
, O
Reinforcement O
Learning O
, O
Neural O
Networks O
, O
among O
other O
techniques B
. O

The O
item B
building I
model O
, O
therefore O
, O
must O
be O
composed O
of O
all O
items B
available O
in O
the O
game B
and O
should O
provide O
a O
reasonable O
amount B
of I
gold I
relatively O
to O
the O
phase O
to O
be O
analyzed O
. O

For O
instance O
, O
a O
little O
amount B
of I
gold I
should O
be O
provided O
to O
the O
agent O
to O
perform O
the O
openings O
, O
while O
a O
greater O
amount B
of I
gold I
should O
be O
provided O
for O
the O
mid O
or O
late B
game I
representations O
. O

The O
model O
should O
also O
be O
able O
to O
represent O
the O
different O
hero B
levels B
and O
possible O
enemy B
scenarios O
as O
inputs B
. O

It O
is O
clear B
that O
this O
is O
a O
hard B
task O
due O
to O
the O
large O
number O
of O
enemy B
team I
’s O
combinations O
, O
enemy B
items B
, O
levels B
and O
game B
scenarios O
. O

B. O
Laning O
The O
laning O
phase O
is O
one O
of O
the O
most O
important O
phases O
in O
the O
MOBA O
gameplay B
. O

Winning O
this O
phase O
may O
imply O
in O
an O
important O
strategic B
advantage I
, O
besides O
more O
gold B
and O
experience B
. O

Learning O
how O
to O
lane B
is O
important O
for O
any O
AI O
agent O
, O
and O
is O
a O
difficult O
task O
since O
it O
involves O
many O
agents O
and O
structures O
. O

The O
creation O
of O
a O
laning O
model O
is O
intended O
to O
help O
the O
agent O
to O
analyze O
the O
resource B
collection O
, O
targeting O
, O
harass O
and O
tower B

 O
diving I
strategies B
. O

Therefore O
, O
it O
is O
necessary O
to O
implement O
a O
complete O
lane B
system O
, O
composed O
of O
heroes B
, O
creeps B
and O
turrets B
. O

The O
simulation O
/ O
modeling O
of O
enemies B
is O
optional O
, O
since O
the O
agent O
could O
learn O
how O
to O
fight B
in O
another O
scenario O
, O
like O
the O
team B
- O
fight I
model I
. O

This O
system O
could O
simulate O
a O
broad O
range B
of O
micromanagement O
skills B
. O

Learning O
how O
to O
control B
the O
lane B
, O
when O
to O
execute O
last O
hits O
, O
farming B
, O
and O
switching O
between O
advancing O
and O
retreating B
the O
creeps B
in O
the O
lane B
are O
essential O
skills B
to O
MOBA O
gameplay B
. O

In O
adversarial O
real B
- I
time I
gameplay B
, O
the O
system O
should O
be O
able O
to O
simulate O
multiple O
agents O
in O
the O
lane B
in O
order O
to O
show O
the O
harass O
and O
competition B
between O
agents O
for O
the O
creeps B
in O
the O
lane B
. O

C. O
Team O
fights B
The O
team B
fight I
model O
should O
allow O
the O
agents O
to O
interact O
with O
each O
other O
, O
and O
could O
be O
contained O
in O
a O
one O
lane B
map O
. O

This O
is O
intended O
to O
simulate O
the O
team B
fights I
and O
evolve O
the O
system O
to O
properly O
reason O
about O
combat B
results O
and O
possible O
interactions O
between O
agents O
. O

That O
system O
could O
be O
modelled O
both O
in O
a O
theoretical O
aspect O
as O
well O
as O
in O
a O
practical O
approach O
. O

The O
proposition O
of O
theoretical O
fight B
simulation O
is O
possible O
because O
the O
team B
fight I
consists O
of O
a O
series O
of O
interactions O
that O
is O
contained O
in O
a O
timespan O
. O

That O
could O
be O
modelled O
as O
a O
temporal O
graph O
, O
pointing B
the O
interactions O
between O
the O
agents O
. O

In O
fact O
, O
professional O
players B
and O
coaches B
already O
do O
that O
simulation O
which O
they O
call B
theorycraft B
, O
which O
consists O
of O
a O
mathematical O
analysis O
of O
the O
game B
[ O
13 O
] O
. O

After O
performing O
this O
analysis O
, O
the O
player B
formulates O
theories O
of O
which O
interactions O
and O
items B
give O
most O
advantage B
to O
his O
game B
strategy B
/ O
hero B
. O

This O
field B
could O
take O
benefit O
of O
models O
already O
established O
like O
the O
econometrics O
[ O
14 O
] O
. O

Many O
combinations O
and O
analysis O
could O
be O
made O
in O
an O
effective O
team B
fight I
model O
. O

Among O
these O
analysis O
is O
the O
hero B
balancing O
in O
the O
MOBA O
environment O
, O
a O
task O
that O
is O
done O
by O
a O
game B
designer O
team B
. O

Given O
the O
MOBA O
ecosystem O
, O
we O
observe O
that O
the O
game B
nature O
is O
clearly O
evolutionary O
, O
given O
its O
constant O
updates B
. O

Moreover O
, O
the O
combat B
outcome O
and O
the O
potential O
of O
a O
hero B
during O
all O
game B
phases O
could O
be O
analyzed O
, O
determining O
interesting O
strategies B
, O
from O
duels B
to O
5v5 O
team B
fights I
. O

D. O
A O
Theoretical O
MOBA O
model O
Besides O
the O
discretization O
of O
the O
MOBA O
game B
, O
the O
game B
as O
a O
whole O
can O
be O
analyzed O
using O
a O
theoretical O
model O
, O
as O
done O
in O
most O
sports O
such O
as O
soccer O
, O
football O
and O
rugby O
. O

This O
model O
could O
be O
also O
considered O
in O
theorycraft B
by O
the O
coaches B
and O
the O
players B
while O
planning O
the O
positioning B
, O
hero B
groups B
and O
strategies B
. O

An O
example O
is O
shown O
in O
Figure O
3 O
. O

Fig O
. O

3 O
. O

Example O
of O
a O
strategic O
analysis O
in O
a O
MOBA O
match B
. O

The O
top O
laners B
take O
different O
approaches O
for O
helping O
their O
teams B
. O

It O
may O
be O
faster O
to O
simulate O
games B
using O
graphs O
and O
theoretical O
models O
in O
order O
to O
predict O
or O
analyze O
strategic O
situations O
and O
positions O
. O

This O
analysis O
can O
even O
consider O
the O
probabilities O
of O
succeeding O
in O
the O
execution B
of O
game B
mechanics I
, O
being O
modelled O
as O
a O
probabilistic O
graphical O
model O
. O

Such O
probabilities O
may O
be O
computed O
from O
the O
performance O
obtained O
in O
the O
previous O
discrete O
parts O
, O
and O
should O
be O
identified O
in O
the O
strategy B
and O
game B
style I
adopted O
by O
a O
team B
and O
its O
strategy B
. O

Such O
model O
could O
even O
be O
used O
to O
identify O
emerging O
strategies B
that O
are O
unusual O
to O
players B
and O
teams B
, O
outperforming O
their O
common O
strategies B
. O

Lastly O
, O
modeling O
replays O
in O
a O
theoretical O
representation O
can O
help O
the O
system O
to O
classify O
the O
strategies B
, O
identifying O
their O
main B
characteristics I
. O

For O
instance O
, O
distinguishing O
split B
push I
from O
sieging O
, O
or O
team B
fight I
strategy I
from O
pickoff O
. O

This O
classification O
can O
help O
the O
system O
to O
map O
the O
strategies B
adopted O
by O
a O
team B
or O
a O
hero B
group B
. O

The O
model O
follows O
the O
conventions O
established O
by O
the O
MOBA O
map O
, O
with O
its O
structures O
and O
lanes B
. O

We O
translate O
the O
structures O
into O
nodes O
and O
the O
order O
of O
destruction O
of O
these O
structure O
into O
edges O
. O

As O
discussed O
, O
the O
outermost O
structure O
should O
be O
destroyed O
in O
order O
to O
enable O
the O
damage B
or O
destruction O
of O
an O
inner O
structure O
. O

Finally O
, O
the O
structures O
of O
an O
entire O
lane B
should O
be O
destroyed O
in O
order O
to O
enable O
the O
damage B
to O
the O
7 O
main O
structure O
. O

A O
diagram O
showing O
the O
graph O
structure O
of O
the O
model O
can O
be O
seen O
in O
Figure O
4 O
. O

Fig O
. O

4 O
. O

Diagram O
of O
a O
theoretical O
structure O
to O
represent O
a O
MOBA O
game B
. O

The O
smaller O
nodes O
represent O
turrets B
while O
the O
larger O
one O
is O
the O
main O
structure O
. O

Notice O
in O
the O
diagram O
that O
the O
interactions O
start B
in O
the O
outermost O
structure O
, O
meaning O
that O
the O
turrets B
next O
to O
the O
center O
should O
be O
destroyed O
first O
. O

The O
rectangles O
containing O
a O
subgraph O
represent O
a O
dependency O
among O
the O
structures O
, O
such O
that O
structures O
in O
different O
rectangles O
are O
independent O
from O
each O
other O
. O

Destroying O
a O
group B
of O
structures O
unlocks O
a O
way O
to O
attack B
the O
two O
turrets B
protecting O
the O
main O
structure O
. O

Lastly O
, O
after O
destroying O
these O
turrets B
, O
the O
team B
can O
attack B
the O
main O
structure O
. O

Its O
eventual O
destruction O
causes O
the O
end O
of O
the O
game B
. O

VI O
. O

RELATED O
WORK O
Despite O
the O
popularity O
of O
the O
genre O
, O
MOBA O
has O
not O
attracted O
much O
attention O
from O
researchers O
. O

This O
lack O
of O
attention O
could O
be O
mainly O
attributed O
to O
the O
commercial O
nature O
of O
these O
games B
and O
the O
lack O
of O
collaboration O
of O
industry O
and O
academia O
, O
in O
terms O
of O
open B
software O
, O
APIs O
, O
etc O
. O

Besides O
, O
MOBA O
is O
a O
relatively O
new O
game B
genre O
, O
with O
the O
main O
titles O
debuting O
in O
the O
late O
2000 O
’s O
. O

The O
MOBA O
history O
go O
back O
to O
the O
RTS O
hype O
, O
when O
modders O
used O
to O
modify O
RTS O
maps O
in O
Starcraft O
and O
Warcraft O
III O
to O
make O
unique O
gameplay B
. O

The O
RTS O
inclusively O
got O
much O
attention O
from O
the O
academia O
, O
having O
been O
widely O
studied O
and O
used O
as O
a O
competition B
platform O
for O
many O
years O
[ O
4 O
] O
, O
[ O
7 O
] O
. O

In O
that O
platform O
, O
we O
saw O
the O
use O
of O
Starcraft O
as O
a O
testbed O
for O
many O
AI O
techniques B
, O
and O
the O
development O
of O
a O
complete O
agent O
capable O
of O
defeating B
human O
players B
is O
still O
an O
open B
problem O
. O

Problems O
like O
unit B
management O
[ O
15 O
] O
, O
tactical O
gameplay B
[ O
5 O
] O
, O
kiting O
[ O
16 O
] O
, O
among O
others O
, O
were O
extensively O
studied O
in O
the O
RTS O
environment O
. O

Some O
of O
these O
problems O
have O
also O
been O
studied O
in O
the O
MOBA O
environment O
. O

One O
example O
is O
the O
Kiting O
, O
a O
mechanic B
to O
hit O
an O
run O
, O
largely O
used O
in O
the O
MOBA O
gameplay B
. O

This O
problem O
was O
tackled O
before O
Uriarte O
and O
Ontan˜on O
[ O
16 O
] O
using O
´ O
an O
ad O
- O
hoc O
approach O
, O
where O
they O
model O
the O
units B
involved O
in O
the O
Target O
Selection O
task O
using O
multiple O
features O
and O
status B
analysis O
, O
which O
is O
commonly O
called B
aggro B
. O

Moreover O
, O
selecting O
the O
target B
is O
not O
only O
related O
to O
hero B
versus O
hero B
, O
but O
also O
in O
the O
farming B
action O
. O

Later O
, O
researchers O
started B
to O
note O
that O
MOBA O
brings O
a O
different O
type O
of O
challenge O
. O

Instead O
of O
challenging O
the O
player B
to O
manage O
large O
armies O
or O
to O
optimize O
build O
orders O
, O
MOBA O
invites O
the O
player B
to O
control B
a O
single O
unit B
and O
master B
it O
. O

The O
popularity O
of O
MOBA O
expanded O
so O
fast O
that O
three O
years O
after O
its O
debut O
it O
was O
already O
the O
most O
played B
genre O
in O
the O
world B
, O
surpassing O
the O
Multiplayer O
Massively O
Online O
Real O
Playing O
Game O
, O
World O
of O
Warcraft O
. O

That O
expansion O
got O
the O
attention O
of O
some O
researchers O
that O
wanted O
to O
better O
understand O
the O
MOBA O
dynamics O
. O

The O
first O
researchers O
were O
worried O
to O
present O
to O
the O
readers O
the O
MOBA O
phenomena O
, O
its O
basic O
characteristics O
and O
gameplay B
. O

In O
[ O
17 O
] O
and O
[ O
18 O
] O
we O
can O
find O
information O
about O
basic O
MOBA O
characteristics O
. O

A O
broad O
discussion O
of O
MOBA O
as O
an O
eSport O
and O
game B
design O
can O
be O
found O
in O
[ O
19 O
] O
. O

After O
these O
descriptive O
works O
, O
some O
researchers O
wanted O
to O
go O
inside O
the O
MOBA O
gameplay B
and O
analyze O
it O
from O
the O
players B
perspective O
or O
to O
observe O
player B
’s O
performance O
. O

Because O
there O
were O
no O
tools O
to O
do O
so O
, O
most O
works O
have O
taken O
advantage B
of O
large O
replay O
databases O
to O
develop O
their O
work O
. O

The O
behavior O
of O
MOBA O
players B
during O
a O
match B
and O
its O
impacts B
into O
winning O
the O
game B
are O
explored O
in O
[ O
20 O
] O
. O

Later O
, O
the O
analysis O
of O
MOBA O
player B
’s O
performance O
is O
done O
in O
[ O
21 O
] O
, O
using O
a O
Dota2 O
replay O
database O
as O
input B
. O

On O
the O
other O
hand O
, O
the O
work O
of O
[ O
12 O
] O
is O
interested O
in O
showing O
that O
MOBA O
has O
some O
patterns O
that O
could O
lead B
a O
team B
to O
victory B
. O

It O
is O
interesting O
to O
see O
that O
both O
works O
mention O
that O
the O
knowledge O
developed O
could O
be O
used O
to O
create O
intelligent O
agents O
for O
MOBAs B
, O
which O
happened O
later O
. O

The O
urge O
of O
creating O
agents O
for O
MOBA O
games B
was O
partially O
satisfied O
with O
the O
tools O
available O
by O
S2Games O
in O
their O
game B
Heroes O
of O
Newerth O
. O

However O
, O
some O
users B
got O
frustrated O
because O
there O
was O
not O
much O
information O
about O
the O
API O
, O
besides O
the O
need O
of O
using O
some O
preprogrammed O
behaviors O
. O

In O
that O
context O
we O
find O
the O
work O
of O
[ O
22 O
] O
, O
which O
used O
Reinforcement O
Learning O
techniques B
for O
the O
implementation O
of O
a O
MOBA O
agent O
. O

Later O
, O
we O
find O
the O
work O
of O
[ O
23 O
] O
that O
uses O
third O
part O
tools O
to O
develop O
agents O
in O
the O
League O
of O
Legends O
context O
. O

VII O
. O

CONCLUSIONS O
AND O
FUTURE O
WORK O
In O
this O
paper O
we O
presented O
an O
overview O
of O
the O
Multiplayer O
Online O
Battle O
Arena O
genre O
as O
a O
testbed O
for O
Game O
AI O
research O
. O

We O
discuss O
the O
challenges O
introduced O
in O
each O
phase O
of O
the O
game B
and O
also O
in O
the O
game B
as O
a O
whole O
and O
propose O
a O
model O
that O
can O
be O
implemented O
to O
allow O
a O
better O
understanding O
of O
the O
game B
. O

We O
observe O
that O
the O
research O
interest O
in O
MOBA O
games B
is O
increasing B
, O
but O
it O
is O
still O
incipient O
. O

By O
presenting O
and O
discussing O
the O
main O
features O
and O
challenges O
introduced O
by O
MOBA O
games B
, O
we O
aim O
to O
“ O
push O
the O
lane B
” O
into O
the O
direction O
of O
a O
better O
understanding O
and O
increased B
use O
of O
MOBA O
games B
as O
a O
research O
platform O
. O

For O
future O
work O
we O
believe O
that O
the O
discrete O
framework O
proposed O
in O
this O
paper O
should O
be O
developed O
in O
order O
to O
provide O
a O
reliable O
testbed O
for O
research O
. O

Moreover O
, O
the O
knowledge O
extracted O
from O
that O
framework O
could O
be O
useful O
in O
real O
MOBA O
games B
, O
like O
Smite O
, O
Dota2 O
or O
in O
the O
RTS O
Starcraft O
. O

REFERENCES O
[ O
1 O
] O
A. O
L. O
Samuel O
, O
“ O
Some O
studies O
in O
machine O
learning O
using O
the O
game B
of O
checkers O
, O
” O
IBM O
Journal O
of O
research O
and O
development O
, O
vol O
. O

3 O
, O
no O
. O

3 O
, O
pp O
. O

210–229 O
, O
1959 O
. O

[ O
2 O
] O
M. O
Campbell O
, O
A. O
J. O
Hoane O
, O
and O
F.-h O
. O

Hsu O
, O
“ O
Deep O
blue B
, O
” O
Artificial O
intelligence O
, O
vol O
. O

134 O
, O
no O
. O

1 O
, O
pp O
. O

57–83 O
, O
2002 O
. O

[ O
3 O
] O
D. O
Silver O
, O
A. O
Huang O
, O
C. O
J. O
Maddison O
, O
A. O
Guez O
, O
L. O
Sifre O
, O
G. O
Van O
Den O
Driessche O
, O
J. O
Schrittwieser O
, O
I. O
Antonoglou O
, O
V. O
Panneershelvam O
, O
M. O
Lanctot O
et O
al O
. O

, O
“ O
Mastering B
the O
game B
of O
go O
with O
deep O
neural O
networks O
and O
tree O
search O
, O
” O
Nature O
, O
vol O
. O

529 O
, O
no O
. O

7587 O
, O
pp O
. O

484–489 O
, O
2016 O
. O

8 O
[ O
4 O
] O
M. O
Buro O
, O
“ O
Real B
- I
time I
strategy O
games I
: O
A O
new O
ai O
research O
challenge O
, O
” O
in O
IJCAI O
, O
2003 O
, O
pp O
. O

1534–1535 O
. O

[ O
5 O
] O
J. O
Hagelback O
, O
“ O
Potential O
- O
field B
based B
navigation O
in O
starcraft O
, O
” O
in O
¨ O
Computational O
Intelligence O
and O
Games O
( O
CIG O
) O
, O
2012 O
IEEE O
Conference O
on O
. O

IEEE O
, O
2012 O
, O
pp O
. O

388–393 O
. O

[ O
6 O
] O
M. O
Buro O
and O
D. O
Churchill O
, O
“ O
Real B
- I
time I
strategy O
game I
competitions B
, O
” O
AI O
Magazine O
, O
vol O
. O

33 O
, O
no O
. O

3 O
, O
p. O
106 O
, O
2012 O
. O

[ O
7 O
] O
S. O
Ontanon O
, O
G. O
Synnaeve O
, O
A. O
Uriarte O
, O
F. O
Richoux O
, O
D. O
Churchill O
, O
and O
´ O
M. O
Preuss O
, O
“ O
A O
survey O
of O
real B
- I
time I
strategy O
game I
ai O
research O
and O
competition B
in O
starcraft O
, O
” O
Computational O
Intelligence O
and O
AI O
in O
Games O
, O
IEEE O
Transactions O
on O
, O
vol O
. O

5 O
, O
no O
. O

4 O
, O
pp O
. O

293–311 O
, O
2013 O
. O

[ O
8 O
] O
E. O
Ackerman O
, O
“ O
Custom O
ai O
programs O
take O
on O
top O
ranked B
humans O
in O
starcraft O
, O
” O
December O
2015 O
, O
http://spectrum.ieee.org/automaton/robotics/artificialintelligence/custom-ai-programs-take-on-top-ranked-humans-instarcraft[Online O
; O
posted O
1-December-2015 O
] O
. O

[ O
9 O
] O
M. O
Chung O
, O
M. O
Buro O
, O
and O
J. O
Schaeffer O
, O
“ O
Monte O
carlo O
planning O
in O
rts O
games B
, O
” O
in O
CIG O
, O
2005 O
. O

[ O
10 O
] O
M. O
Murphy O
, O
“ O
Most O
played B
games B
: O
November O
2015 O
– O
fallout O
4 O
and O
black O
ops O
iii O
arise O
while O
starcraft O
ii O
shines O
, O
” O
December O
2015 O
, O
http://caas.raptr.com/most-played-games-november-2015-fallout-4-andblack-ops-iii-arise-while-starcraft-ii-shines/Raptr.com[Online O
; O
posted O
21-December-2015 O
] O
. O

[ O
11 O
] O
J. O
Gaudiosi O
, O
“ O
Riot B
games I
’ O
league B
of I
legends I
officially O
becomes O
most O
played B
pc O
game B
in O
the O
world B
, O
” O
Forbes O
. O

Jul O
, O
vol O
. O

11 O
, O
p. O
2011 O
, O
2012 O
. O

[ O
12 O
] O
P. O
Yang O
, O
B. O
Harrison O
, O
and O
D. O
L. O
Roberts O
, O
“ O
Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
moba B
games I
, O
” O
in O
Proceedings O
of O
Foundations O
of O
Digital O
Games O
, O
2014 O
. O

[ O
13 O
] O
C. O
A. O
Paul O
, O
“ O
Optimizing O
play B
: O
How O
theorycraft B
changes O
gameplay B
and O
design O
, O
” O
Game O
Studies O
, O
vol O
. O

11 O
, O
no O
. O

2 O
, O
2011 O
. O

[ O
14 O
] O
G. O
G. O
Judge O
, O
R. O
C. O
Hill O
, O
W. O
Griffiths O
, O
H. O
Lutkepohl O
, O
and O
T.-C. O
Lee O
, O
“ O
Introduction O
to O
the O
theory O
and O
practice O
of O
econometrics O
. O

” O
New O
York O
New O
York O
John O
Wiley O
and O
Sons O
1982 O
. O

, O
1988 O
. O

[ O
15 O
] O
G. O
Synnaeve O
and O
P. O
Bessiere O
, O
“ O
A O
bayesian O
model O
for O
rts O
units B
control B
applied O
to O
starcraft O
, O
” O
in O
Computational O
Intelligence O
and O
Games O
( O
CIG O
) O
, O
2011 O
IEEE O
Conference O
on O
. O

IEEE O
, O
2011 O
, O
pp O
. O

190–196 O
. O

[ O
16 O
] O
A. O
Uriarte O
and O
S. O
Onta O
n˜on O
, O
“ O
Kiting O
in O
rts O
games B
using O
influence O
maps O
, O
” O
´ O
in O
Eighth O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
Conference O
, O
2012 O
. O

[ O
17 O
] O
M. O
Nosrati O
and O
R. O
Karimi O
, O
“ O
General O
trends O
in O
multiplayer O
online B
games I
, O
” O
World O
Applied O
Programming O
, O
vol O
. O

3 O
, O
no O
. O

1 O
, O
pp O
. O

1–4 O
, O
January O
2013 O
. O

[ O
18 O
] O
F. O
Rioult O
, O
J.-P. O
Metivier O
, O
B. O
Helleu O
, O
N. O
Scelles O
, O
and O
C. O
Durand O
, O
“ O
Mining O
´ O
tracks O
of O
competitive B
video O
games I
, I
” O
AASRI O
Procedia O
, O
vol O
. O

8 O
, O
no O
. O

0 O
, O
pp O
. O

82 O
– O
87 O
, O
2014 O
, O
2014 O
AASRI O
Conference O
on O
Sports O
Engineering O
and O
Computer O
Science O
( O
SECS O
2014 O
) O
. O

[ O
19 O
] O
S. O
Ferrari O
, O
“ O
From O
generative O
to O
conventional O
play B
: O
Moba O
and O
league B
of I

 O
legends I
, O
” O
in O
DiGRA O
2013 O
- O
DeFragging O
Game O
Studies O
. O

Atlanta O
, O
GA O
, O
USA O
: O
DiGRA O
, O
August O
2013 O
. O

[ O
20 O
] O
N. O
Pobiedina O
, O
J. O
Neidhardt O
, O
M. O
d. O
C. O
C. O
Moreno O
, O
L. O
Grad O
- O
Gyenge O
, O
and O
H. O
Werthner O
, O
“ O
On O
successful O
team B
formation I
: O
Statistical O
analysis O
of O
a O
multiplayer O
online B
game I
, O
” O
in O
Business O
Informatics O
( O
CBI O
) O
, O
2013 O
IEEE O
15th O
Conference O
on O
. O

IEEE O
, O
2013 O
, O
pp O
. O

55–62 O
. O

[ O
21 O
] O
A. O
Drachen O
, O
M. O
Yancey O
, O
J. O
Maguire O
, O
D. O
Chu O
, O
I. O
Y. O
Wang O
, O
T. O
Mahlmann O
, O
M. O
Schubert O
, O
and O
D. O
Klabajan O
, O
“ O
Skill O
- O
based B
differences O
in O
spatiotemporal O
team B
behaviour O
in O
defence O
of O
the O
ancients O
2 O
( O
dota B
2 O
) O
, O
” O
in O
Proceedings O
of O
the O
IEEE O
Games O
, O
Entertainment O
, O
and O
Media O
( O
GEM O
) O
Conference O
2014 O
, O
2014 O
. O

[ O
22 O
] O
J. O
Willich O
, O
“ O
Reinforcement O
learning O
for O
heroes B
of O
newerth O
, O
” O
in O
Bachelor O
Thesis O
– O
Technische O
Universitat O
Darmstadt O
, O
2015 O
. O

[ O
23 O
] O
V. O
N. O
Silva O
and O
L. O
Chaimowicz O
, O
“ O
On O
the O
development O
of O
intelligent O
agents O
for O
moba B
games O
, O
” O
in O
Brazilian O
Symposium O
on O
Computer O
Games O
and O
Digital O
Entertainment O
( O
SBGames O
) O
, O
2015 O
. O

On O
the O
Development O
of O
Intelligent O
Agents O
for O
MOBA O
Games O
Victor O
do O
Nascimento O
Silva O
and O
Luiz O
Chaimowicz O
Department O
of O
Computer O
Science O
Universidade O
Federal O
de O
Minas O
Gerais O
- O
Brazil O
Abstract O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
is O
one O
of O
the O
most O
played B
game B
genres O
nowadays O
. O

With O
the O
increasing B
growth O
of O
this O
genre O
, O
it O
becomes O
necessary O
to O
develop O
effective O
intelligent O
agents O
to O
play B
alongside O
or O
against O
human O
players B
. O

In O
this O
paper O
we O
address O
the O
problem O
of O
agent O
development O
for O
MOBA O
games B
. O

We O
implement O
a O
two O
- O
layered O
architecture O
agent O
that O
handles O
both O
navigation O
and O
game B
mechanics I
. O

This O
architecture O
relies O
on O
the O
use O
of O
Influence O
Maps O
, O
a O
widely O
used O
approach O
for O
tactical B
analysis I
. O

Several O
experiments O
were O
performed O
using O
League O
of O
Legends O
as O
a O
testbed O
, O
and O
show O
promising O
results O
in O
this O
highly O
dynamic O
real B
- I
time I
context O
. O

Keywords O
: O
Multiplayer O
Online O
Battle O
Arena O
, O
Influence O
Maps O
, O
Tactics O
Author O
’s O
Contact O
: O
{ O
vnsilva,chaimo}@dcc.ufmg.br O
1 O
Introduction O
The O
development O
of O
tactical O
movement B
systems I
is O
still O
a O
challenging O
task O
in O
game B
AI O
. O

This O
is O
especially O
true O
when O
dealing O
with O
competitive B
or O
combat B
scenarios I
. O

Behaving O
in O
a O
logical O
way O
in O
these O
cases O
could O
be O
very O
hard B
and O
require O
heavy O
computation O
and O
domain O
knowledge O
. O

Thus O
, O
it O
is O
necessary O
to O
collect O
data O
and O
deal O
with O
uncertainty O
when O
controlling B
agents O
in O
these O
scenarios O
to O
be O
able O
to O
behave O
in O
a O
competitive B
way O
. O

Researching O
tactical B
analysis I
can O
benefit O
not O
just O
games B
but O
areas B
like O
military O
analysis O
, O
robotics O
and O
traffic O
. O

Moreover O
, O
any O
area B
with O
partial O
information O
and O
dynamic O
environment O
can O
implement O
the O
techniques B
of O
tactical B
analysis I
towards O
improving O
agent O
behavior O
. O

In O
game B
research O
, O
the O
problem O
of O
navigating O
in O
a O
combat B
scenario I
can O
be O
tackled O
by O
the O
development O
of O
specific O
strategies B
and O
tactical B
analysis I
. O

Furthermore O
, O
the O
tactical B
analysis I
field B
has O
gained O
much O
attention O
, O
as O
games B
become O
more O
complex O
. O

Tactical B
analysis I
was O
already O
performed O
in O
some O
agents O
, O
like O
Starcraft O
agents O
and O
ORTS O
competitions B
[ O
Hagelback O
and O
Johansson O
2008b],[Uriarte O
and O
¨ O
Ontan˜on O
2012 O
] O
. O

´ O
Another O
point B
that O
must O
be O
discussed O
is O
that O
MOBA O
is O
one O
of O
most O
played B
games B
of O
actuality O
, O
having O
almost O
30 O
% O
of O
online O
computing O
gameplay B
time B
1 O
. O

However O
, O
there O
is O
not O
much O
attention O
from O
researchers O
over O
this O
game B
genre O
. O

Characteristics O
like O
the O
continuous O
updates B
can O
be O
cited O
as O
one O
of O
the O
causes O
of O
the O
low O
research O
rate O
over O
this O
domain O
. O

We O
aim O
to O
encourage O
researchers O
to O
use O
MOBA O
as O
a O
domain O
as O
testbed O
for O
their O
future O
research O
, O
providing O
gathered O
knowledge O
about O
this O
domain O
. O

Games O
are O
a O
good O
option O
to O
serve O
as O
testbed O
for O
the O
development O
of O
AI O
. O

Having O
a O
limited O
environment O
that O
can O
reproduce O
almost O
all O
characteristics O
of O
the O
real O
world B
helps O
the O
research O
to O
perform O
reliable O
tests O
[ O
Adobbati O
et O
al O
. O

2001 O
] O
. O

Games B
also O
provides O
an O
environment O
where O
winning O
, O
losing O
or O
drawing O
have O
well O
- O
defined O
rules B
, O
making O
it O
easy B
for O
AI O
to O
evaluate O
these O
criteria O
. O

Developing O
and O
testing O
AI O
algorithms O
in O
a O
game B
environment O
can O
benefit O
research O
by O
reducing O
the O
costs B
of O
test O
and O
real O
world B
resources B
. O

Selecting O
a O
platform O
for O
testbed O
in O
AI O
research O
can O
be O
a O
hard B
task O
, O
especially O
when O
considering O
that O
most O
MOBA O
games B
are O
commercial O
. O

We O
considered O
several O
games B
in O
our O
initial O
research O
phase O
and O
ended O
with O
two O
main O
options O
: O
HEROES B
OF O
NEWERTH O
( O
HoN O
) O
and O
LEAGUE O
OF O
LEGENDS2 O
( O
LoL O
) O
. O

When O
choosing O
between O
these O
two O
platforms O
we O
considered O
: O
( O
a O
) O
ease O
of O
implementation O
, O
in O
terms O
of O
language O
and O
documentation O
; O
( O
b O
) O
game B
community B
, O
in O
terms O
of O
number O
of O
players B
. O

Given O
these O
characteristics O
, O
we O
chose O
to O
develop O
our O
research O
using O
LEAGUE O
OF O
LEGENDS O
as O
testbed O
. O

In O
addition O
, O
real B
- I
time I
games I
are O
gaining O
much O
attention O
in O
recent O
years O
. O

We O
observe O
the O
growing O
of O
intelligent O
agents O
development O
for O
these O
games B
, O
like O
Starcraft O
. O

Moreover O
, O
there O
have O
been O
competitions B
using O
such O
agents O
, O
like O
the O
AIIDE O
and O
the O
CIG O
Starcraft O
agents O
competition B
[ O
Ontanon O
et O
al O
. O

2013 O
] O
. O

However O
, O
in O
contrast O
to O
´ O
the O
popularity O
of O
MOBA O
, O
there O
is O
not O
a O
great O
attention O
related O
to O
this O
genre O
. O

In O
this O
paper O
we O
address O
the O
problem O
of O
tactical B
analysis I
and O
navigation O
in O
combat B
scenarios I
. O

For O
doing O
so O
, O
we O
use O
a O
widely O
popular O
MOBA O
game B
, O
League O
of O
Legends O
, O
as O
testbed O
. O

Our O
approach O
consists O
in O
the O
development O
of O
an O
Intelligent O
Agent O
that O
aims O
to O
behave O
competitively O
in O
the O
chosen O
domain O
. O

Further O
, O
we O
implement O
a O
multi O
- O
layered O
architecture O
agent O
using O
Influence O
Maps O
( O
IM O
) O
and O
real B
- I
time I
reasoning I
. O

In O
addition O
we O
collect O
and O
provide O
information O
about O
the O
MOBA O
domain O
and O
we O
encourage O
researchers O
to O
adopt O
MOBA O
as O
their O
research O
domain O
. O

In O
short O
, O
our O
contributions O
in O
this O
work O
are O
: O
1http://bit.ly/1OopRvh O
2http://www.riotgames.com/our-games O
arXiv:1706.02789v1 O
[ O
cs O
. O

AI O
] O
8 O
Jun O
2017 O
• O
We O
present O
MOBA O
games B
as O
testbed O
for O
AI O
; O
• O
Implementation O
of O
an O
game B
agent O
capable O
of O
winning O
the O
game B
; O
• O
An O
architecture O
for O
the O
game B
agent O
based B
in O
tactical B
analysis I
; O
• O
Our O
agent O
was O
capable O
of O
emerging O
the O
kiting O
behaviour O
without O
a O
dedicated O
module O
. O

The O
rest O
of O
this O
paper O
is O
organized O
as O
follows O
: O
Section O
2 O
introduces O
basic O
concepts O
about O
Tactical O
Analysis O
techniques B
, O
MOBA O
domain O
and O
LoL. O
In O
this O
section O
we O
also O
discuss O
related O
work O
in O
agent O
development O
and O
MOBA O
. O

Section O
3 O
presents O
the O
architecture O
of O
the O
agent O
implemented O
, O
showing O
its O
characteristics O
and O
layers O
. O

Experiments O
performed O
for O
evaluating O
our O
approach O
is O
presented O
in O
Section O
4 O
. O

Lastly O
, O
Section O
5 O
brings O
the O
conclusion O
and O
the O
directions O
for O
future O
work O
. O

2 O
Background O
Research O
in O
games B
requires O
collection O
of O
information O
about O
the O
domain O
to O
be O
used O
as O
guiding O
tools O
for O
the O
research O
and O
experiments O
to O
be O
developed O
. O

A O
game B
is O
composed O
of O
a O
set O
of O
rules B
during O
its O
execution B
and O
winning O
or O
losing O
criteria O
. O

Thus O
, O
understanding O
these O
characteristics O
is O
essential O
for O
implementation O
of O
agents O
, O
data O
and O
results O
collection O
. O

In O
this O
section O
we O
present O
substantial O
knowledge O
about O
techniques B
that O
were O
used O
in O
our O
research O
. O

Furthermore O
, O
we O
present O
the O
characteristics O
of O
the O
MOBA O
domain O
. O

2.1 O
Influence O
Maps O
and O
Potential O
Fields O
Tactical B
analysis I
has O
been O
studied O
by O
various O
researchers O
over O
time B
and O
was O
applied O
in O
a O
wide B
range I
of O
games B
in O
both O
academia O
and O
industry O
. O

The O
most O
common B
technique I
is O
to O
divide O
the O
game B
map I
into O
chunks O
and O
try O
to O
perform O
some O
kind O
of O
feature O
analysis O
in O
these O
chunks O
[ O
Millington O
and O
Funge O
2012 O
] O
. O

Analyzing O
the O
environment O
statically O
and O
dynamically O
in O
a O
consistent O
fashion O
can O
help O
the O
AI O
to O
behave O
in O
a O
smarter O
way O
. O

Also O
, O
the O
problem O
of O
coordinating O
agents O
in O
a O
tactical O
way O
is O
not O
new O
. O

From O
the O
work O
of O
[ O
Reynolds O
1987][Reynolds O
1999 O
] O
, O
we O
can O
observe O
the O
necessity O
of O
environment O
analysis O
when O
driving O
agents O
. O

Reynolds O
agents O
take O
into O
account O
the O
environment O
and O
other O
agents O
when O
driving O
themselves O
in O
a O
flock O
. O

Potential O
Fields O
( O
PF O
) O
, O
a O
technique B
originally O
from O
Robotics O
area B
, O
tries O
to O
do O
such O
analyses O
, O
as O
presented O
by O
[ O
Khatib O
1986 O
] O
in O
his O
approach O
for O
real B
- I
time I
obstacle B
avoidance O
and O
agent O
driving O
. O

In O
games B
, O
Influence O
Maps O
( O
IM O
) O
is O
one O
the O
main O
methods O
applied O
for O
tactical B
analysis I
of O
the O
features O
in O
a O
map O
[ O
Millington O
and O
Funge O
2012 O
] O
. O

A O
general O
definition O
of O
influence O
maps O
is O
presented O
in O
[ O
Tozour O
2001 O
] O
, O
which O
discusses O
its O
main O
concepts O
and O
advantages B
. O

Basically O
, O
the O
environment O
is O
decomposed O
in O
a O
grid O
, O
and O
each O
feature O
inserts O
points B
of O
influence O
in O
the O
cells O
where O
they O
are O
positioned O
. O

This O
influence O
is O
then O
spread O
using O
a O
proper O
algorithm O
, O
like O
Gaussian O
blur O
or O
Occlusion O
algorithm O
. O

Few O
years O
later O
, O
[ O
Hagelback O
and O
¨ O
Johansson O
2008b O
] O
presented O
the O
use O
of O
Potential O
Fields O
in O
games B
, O
showing O
a O
similar O
approach O
as O
seen O
in O
Influence O
Maps O
. O

But O
, O
in O
spite O
of O
having O
the O
same O
name O
of O
the O
technique B
used O
in O
robotics O
[ O
Khatib O
1986 O
] O
, O
the O
work O
of O
[ O
Hagelback O
and O
Johansson O
2008b O
] O
divides O
the O
¨ O
map O
in O
a O
grid O
, O
placing O
values O
over O
this O
grid O
that O
can O
be O
attractive O
or O
repulsive O
. O

The O
main O
difference O
between O
the O
approaches O
of O
IM O
and O
PF O
is O
that O
the O
first O
uses O
spread O
and O
momentum O
variables O
. O

These O
values O
can O
be O
used O
to O
modify O
the O
changes O
in O
information O
over O
time B
: O
while O
spread O
controls B
how O
far O
the O
value O
can O
reach O
, O
momentum O
controls B
the O
amount O
of O
time B
through O
which O
the O
information O
continues O
on O
the O
map O
. O

The O
use O
of O
such O
values O
allows O
IM O
to O
store O
historical O
information O
. O

Moreover O
, O
it O
turns B
IM O
into O
a O
versatile O
technique B
, O
as O
it O
can O
be O
used O
for O
both O
prediction O
and/or O
historical O
information O
just O
by O
performing O
parameter O
tuning O
[ O
Champandard O
et O
al O
. O

2011 O
] O
. O

On O
other O
hand O
, O
PF O
turns B
to O
be O
very O
static O
, O
as O
its O
only O
parameter O
is O
the O
equation O
that O
controls B
the O
influence O
over O
the O
maps O
. O

Thus O
, O
we O
observe O
that O
PF O
is O
mainly O
used O
in B
real O
- I
time I
environments O
, O
especially O
for O
navigation O
. O

2.2 O
MOBA O
Games O
In O
recent O
years O
, O
the O
popularity O
of O
Real O
- O
Time O
Strategy O
has O
grown O
substantially O
. O

With O
them O
, O
a O
community B
of O
players B
interested O
in O
producing O
new O
content O
for O
such O
games B
has O
been O
created O
. O

These O
community B
is O
commonly O
called B
Modders O
, O
after O
the O
acronym O
of O
modifiers O
, O
generating O
the O
alternative O
game B
modes I
called B
modifications O
, O
or O
” O
mods O
” O
. O

Among O
these O
, O
there O
was O
Aeon O
of O
Strife O
( O
AoS O
) O
, O
the O
first O
MOBA O
like O
game B
, O
created O
as O
a O
Starcraft O
mod O
. O

After O
the O
popularity O
of O
AoS O
, O
modders O
used O
the O
Warcraft O
III O
engine O
to O
create O
a O
similar O
game B
mode I
and O
name O
it O
Defense O
of O
the O
Ancients O
( O
DotA O
) O
. O

The O
DotA O
maps O
became O
popular O
among O
players B
, O
and O
there O
were O
many O
versions O
around O
. O

Its O
mechanics B
were O
broadly O
known O
that O
every O
other O
game B
that O
had O
similar O
gameplay B
was O
called B
” O
DotA O
like O
game B
” O
for O
a O
long O
time B
. O

The O
term O
MOBA O
was O
first O
observed O
in O
2009 O
with O
the O
debuting O
title O
League O
of O
Legends O
, O
used O
by O
Riot O
Games O
to O
describe O
their O
game B
. O

Furthermore O
, O
MOBA O
games B
evolved O
from O
the O
LAN O
games B
to O
Multiplayer O
Online O
games B
, O
and O
even O
becoming O
more O
popular O
than O
MMORPGs O
, O
like O
World B
of O
Warcraft O
[ O
Diamaranan O
2015 O
] O
. O

Nowadays O
, O
MOBA O
is O
among O
the O
most O
played B
game B
genres O
in O
the O
world B
. O

In O
terms O
of O
academic O
research O
, O
there O
is O
a O
rising O
interest O
in O
developing O
intelligent O
agents O
that O
are O
capable O
of O
playing B
games B
that O
run O
in O
real B
- I
time I
; O
have O
a O
dynamic O
environment O
; O
and O
provide O
partial O
information O
. O

The O
similarity O
of O
this O
kind O
of O
games B
to O
real B
- I
world I
problems O
is O
probably O
one O
of O
the O
main O
reasons O
for O
such O
research O
, O
as O
the O
solutions O
to O
these O
problems O
may O
be O
applied O
to O
various O
real B
- I
world I
scenarios O
[ O
Tavares O
et O
al O
. O

2014 O
] O
. O

This O
growing O
interest O
for O
game B
agents O
, O
in O
particular O
of O
RTS O
games B
, O
allowed O
the O
academia O
to O
develop O
a O
novel O
game B
research O
scenario O
, O
creating O
competitions B
of O
intelligent O
game B
agents O
[ O
Ontanon O
et O
al O
. O

2013 O
] O
. O

´ O
Overall O
, O
it O
is O
very O
common O
to O
find O
two O
types O
of O
agents O
being O
studied O
: O
the O
perfect O
agent O
aims O
to O
defeat B
all O
other O
agents O
, O
and O
is O
the O
most O
common O
type O
. O

On O
the O
other O
hand O
, O
the O
human O
- O
like O
agents O
try O
to O
behave O
as O
humans O
in O
the O
game B
environment O
. O

This O
kind O
of O
agent O
is O
especially O
popular O
a O
in O
First O
Person O
Shooter O
( O
FPS O
) O
scenario O
, O
such O
as O
the O
2kbotprize O
competition B
[ O
Cothran O
2009 O
] O
. O

In O
spite O
of O
the O
agent O
developed O
in O
this O
work O
being O
a O
MOBA O
agent O
, O
we O
also O
discuss O
the O
development O
of O
agents O
for O
RTS O
, O
since O
, O
as O
far O
as O
we O
know O
, O
there O
were O
only O
a O
few O
works O
that O
try O
to O
develop O
an O
effective O
agent O
for O
MOBA O
. O

The O
work O
presented O
by O
[ O
Weber O
et O
al O
. O

2011 O
] O
develops O
an O
human O
- O
behaving O
agent O
for O
Starcraft O
. O

In O
this O
work O
, O
the O
authors O
also O
discuss O
the O
development O
layers O
of O
an O
agent O
, O
classifying O
them O
in O
heterogeneous O
and O
homogeneous O
. O

An O
agent O
that O
is O
developed O
in O
just O
one O
layer O
is O
classified O
as O
homogeneous O
. O

This O
single B

 O
layer I
implements O
all O
features O
of O
the O
agent O
, O
like O
navigation O
and O
actuation O
over O
the O
game B
. O

The O
work O
of O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
´ O
revisits O
the O
multi O
- O
layered O
system O
, O
implementing O
an O
agent O
that O
performs O
a O
kiting O
system O
using O
Influence O
Maps O
( O
IM O
) O
. O

Moreover O
, O
the O
authors O
present O
a O
novel O
approach O
for O
tactical B
analysis I
, O
that O
was O
briefly O
discussed O
in O
[ O
Hagelback O
and O
Johansson O
2008a O
] O
. O

¨ O
In O
general O
tactical B
analysis I
, O
we O
find O
the O
work O
of O
[ O
Stanescu O
et O
al O
. O

2013 O
] O
, O
in O
which O
the O
combat B
outcome O
is O
discussed O
in O
Starcraft O
. O

In O
the O
field B
of O
knowledge O
extraction O
and O
analysis O
, O
there O
is O
the O
work O
of O
[ O
Bangay O
and O
Makin O
2014 O
] O
, O
where O
the O
attributes O
are O
analysed O
in O
a O
RTS O
game B
and O
a O
framework O
for O
balance O
analysis O
is O
proposed O
. O

Furthermore O
, O
we O
believe O
that O
this O
knowledge O
extracted O
can O
be O
used O
to O
model O
tactical B
analysis I
in O
a O
combat B
scenario I
. O

The O
MOBA O
research O
itself O
is O
a O
new O
field B
, O
with O
few O
published O
works O
. O

One O
of O
the O
reasons O
for O
this O
may O
be O
the O
lack O
of O
support B
in O
MOBA O
games B
to O
AI O
development O
. O

Most O
of O
the O
works O
found O
present O
general O
discussions O
or O
use O
collected O
data O
to O
extract O
information O
. O

The O
works O
of O
[ O
Nosrati O
et O
al O
. O

2013 O
] O
and O
[ O
Rioult O
et O
al O
. O

2014 O
] O
, O
for O
example O
, O
present O
a O
brief O
analysis O
of O
MOBA O
games B
, O
limited O
to O
features O
and O
specific O
characteristics O
. O

In O
the O
work O
of O
[ O
Ferrari O
2013 O
] O
we O
find O
a O
deep O
analysis O
of O
League O
of O
Legends O
, O
discussed O
by O
various O
aspects O
like O
Game O
Design O
, O
e O
- O
Sport O
and O
basic O
MOBA O
characteristics O
. O

When O
analysing O
the O
gameplay B
and O
player B
abilities I
, O
is O
possible O
to O
find O
the O
work O
of O
[ O
Drachen O
et O
al O
. O

2014 O
] O
, O
where O
the O
spatio O
- O
temporal O
skill B
of O
players B
is O
analyzed O
, O
focused B
in O
data O
visualization O
and O
game- O
play B
analysis O
, O
using O
Dota2 O
data O
. O

In O
the O
same O
research O
field B
, O
there O
is O
the O
work O
of O
[ O
Yang O
et O
al O
. O

2014 O
] O
where O
, O
using O
Dota2 O
matches B
log O
, O
the O
team B
- O
fights I
are I
analysed O
and O
used O
to O
learn O
patterns O
. O

These O
battles B
are O
modelled O
as O
interactivity O
graphs O
and O
later O
shaped O
in O
decision O
trees O
form O
. O

The O
model O
created O
is O
then O
used O
to O
predict O
the O
success O
of O
future O
battle B
in O
other O
matches B
. O

The O
work O
of O
[ O
Pobiedina O
et O
al O
. O

2013 O
] O
performs O
a O
quantitative O
analysis O
of O
logs O
collected O
from O
Dota2 O
, O
analyzing O
the O
relation O
between O
team B
formation I
and O
victories B
. O

It O
also O
performs O
an O
analysis O
of O
involved O
players B
, O
verifying O
that O
their O
experience B
and O
behavior O
during O
a O
match B
are O
essential O
to O
winning O
in O
MOBA O
. O

Finally O
, O
the O
work O
of O
[ O
Willich O
2015 O
] O
, O
from O
our O
knowledge O
, O
is O
the O
only O
published O
work O
that O
addresses O
the O
problem O
of O
creating O
an O
agent O
for O
MOBA O
. O

The O
author O
uses O
Reinforcement O
Learning O
in O
HoN O
domain O
to O
make O
the O
agent O
learn O
how O
to O
behave O
in O
the O
game B
environment O
. O

The O
author O
shows O
that O
the O
developed O
agent O
learns O
the O
very O
basic O
mechanics B
of O
MOBA O
, O
and O
considers O
that O
further O
research O
is O
needed O
. O

2.3 O
MOBA O
agents O
For O
knowledge O
extraction O
from O
MOBA O
games B
, O
we O
choose O
to O
separate O
and O
simplify O
the O
game B
features O
towards O
a O
general O
model O
that O
can O
be O
used O
to O
achieve O
victory B
. O

First O
, O
we O
analyze O
the O
game B
model O
and O
steps O
that O
can O
allow O
an O
agent O
to O
win O
. O

Further O
, O
we O
analyze O
each O
feature O
in O
the O
agent O
’s O
way O
to O
victory B
, O
dealing O
with O
each O
of O
them O
separately O
in O
a O
knowledge O
model O
, O
and O
then O
unifying O
them O
in O
an O
analytical O
model O
, O
in O
our O
case O
the O
Influence O
Map O
. O

MOBAs B
, O
as O
RTS O
, O
have O
a O
highly O
dynamic O
multi O
- O
agent O
environment O
. O

Instead O
of O
huge O
armies O
, O
MOBA O
focuses B
more O
in O
teamwork B
and O
strong O
micromanagement O
skills B
. O

The O
collaboration O
of O
multiple O
players B
pushes O
the O
team B
towards O
victory B
, O
although O
individual O
gameplay B
is O
also O
very O
valuable O
. O

The O
characteristics O
of O
MOBA O
, O
in O
terms O
of O
gameplay B
, O
are O
also O
very O
similar O
to O
RTS O
. O

Listing O
these O
we O
can O
cite O
: O
• O
Deterministic O
ambient O
: O
where O
the O
same O
actions O
have O
the O
same O
effect B
; O
• O
Partial O
information O
: O
there O
is O
a O
fog O
- O
of O
- O
war O
that O
covers O
areas B
out O
of O
agent O
sight O
line O
; O
• O
Non O
- O
persistent O
world B
: O
In O
contrast O
to O
most O
MMOGs O
, O
in O
MOBA O
every O
match B
the O
world B
is O
created O
; O
it O
does O
not O
exists O
when O
the O
match B
is O
over O
; O
• O
Dynamic O
/ O
Real B
- I
time I
: O
The O
game B
world B
changes O
over O
time B
and O
accordingly O
to O
agent O
actions O
. O

In O
addition O
, O
MOBA O
games B
goal I
consists O
in O
pushing O
a O
lane B
and O
destroying O
an O
enemy B
base I
. O

By O
analyzing O
various O
MOBAs B
, O
we O
can O
observe O
that O
very O
high B
- O
level I
sequence I
of O
three O
steps O
can O
be O
followed O
toward O
winning O
: O
• O
Take O
enemy B
out O
of O
lane B
; O
• O
Push O
the O
lane B
; O
• O
Destroy O
structure O
; O
Despite O
these O
high B
- O
level I
steps I
, O
its O
necessary O
a O
deep O
analysis O
to O
perform O
each O
of O
them O
. O

The O
lane B
pushing O
, O
for O
instance O
, O
requires O
tactical B

 O
analysis I
of O
creeps B
, O
risk O
analysis O
, O
among O
others O
. O

Taking O
enemy B
out O
of O
lane B
requires O
tactical B
enemy O
analysis I
, O
micromanagement O
, O
among O
others O
. O

Structure O
destroying O
, O
on O
the O
other O
hand O
, O
requires O
enemy B
analysis O
, O
damage B
analysis O
and O
others O
. O

Furthermore O
, O
all O
the O
analysis O
must O
be O
performed O
real B
- I
time I
and O
with O
partial O
information O
. O

We O
discuss O
then O
how O
this O
analysis O
can O
be O
done O
and O
combined O
, O
generating O
a O
final O
knowledge O
tactical O
model O
. O

We O
divided O
our O
analysis O
in O
three O
basic O
classes B
: O
Dynamic O
features O
, O
semi O
- O
dynamic O
features O
and O
static O
features O
. O

The O
dynamic O
features O
are O
the O
ones O
that O
can O
move B
through O
the O
scenario O
and O
perform O
actions O
. O

Semi O
- O
dynamic O
ones O
are O
units B
that O
, O
although O
do O
not O
move B
, O
can O
have O
a O
dynamic O
map O
values O
based B
in O
their O
states O
. O

Lastly O
, O
the O
static O
features O
are O
the O
ones O
that O
does O
not O
change O
through O
the O
game B
execution B
. O

We O
will O
discuss O
these O
classes B
further O
. O

Dynamic O
units B
are O
the O
ones O
that O
can O
move B
through O
the O
scenario O
performing O
changes O
in O
the O
tactical B
analysis I
very O
fast O
. O

These O
units B
also O
can O
perform O
actions O
, O
like O
attacking B
or O
casting O
spells B
. O

The O
type O
of O
units B
that O
is O
contained O
in O
this O
class B
are O
: O
heroes B
, O
neutral O
creeps B
, O
enemy B
and O
allied B
creeps B
. O

In O
MOBA O
context O
, O
creep B
is O
a O
unit B
type O
that O
heroes B
can O
interact O
with O
but O
that O
they O
can O
not O
control B
directly O
. O

This O
type O
of O
unit B
is O
AI O
- O
driven O
and O
is O
usually O
spawned O
in O
the O
team B
bases O
. O

Besides O
, O
neutral O
creeps B
are O
born O
in O
the O
jungle B
area B
, O
and O
do O
not O
move B
or O
attack B
unless O
it O
is O
injured O
by O
a O
hero B
unit I
. O

In O
some O
MOBAs B
, O
creeps B
can O
be O
also O
known O
as O
troops O
or O
minions B
. O

Heroes B
can O
be O
considered O
as O
the O
critical O
unit B
in O
dynamic O
class B
, O
they O
can O
change O
the O
game B
scenario O
very O
quickly O
, O
as O
most O
of O
their O
spells B
are O
casted O
instantly O
and O
they O
can O
be O
considered O
the O
fastest O
units B
in O
game B
. O

In O
addition O
, O
these O
units B
are O
responsible O
for O
the O
team B
fights I
and O
duels B
. O

Team B
fights I
are O
combats B
performed O
in O
groups B
of O
a O
team B
against O
another O
team B
, O
duels B
are O
fights B
that O
occur O
between O
two O
distinct O
players B
, O
one O
of O
each O
team B
. O

Allied O
and O
enemy B
creeps B
are O
born O
in O
the O
respective O
team B
base I
. O

These O
units B
go O
into O
a O
lane B
and O
battle B
enemy B
creeps B
and O
structures O
pushing O
the O
lane B
towards O
enemy B
base I
. O

In O
some O
MOBAs B
, O
like O
Dota2 O
, O
it O
is O
possible O
to O
both O
farm B
( O
kill B
enemy B
creeps B
) O
and O
deny O
( O
kill B
ally B
creeps B
) O
. O

These O
units B
normally O
come O
in O
groups B
and O
have O
strong O
effect B
in O
tactical B
analysis I
, O
as O
they O
are O
very O
important O
in O
the O
initial O
phase O
of O
game B
, O
as O
heroes B
must O
perform O
last O
hits O
to O
obtain O
gold B
from O
them O
. O

Moreover O
, O
we O
found O
that O
it O
is O
necessary O
to O
verify O
the O
creeps B
that O
are O
most O
likely O
to O
be O
killed B
first O
, O
maximizing O
the O
effects B
of O
resource B
collection O
of O
players B
. O

The O
only O
semi O
- O
dynamic O
unit B
contained O
in O
MOBA O
is O
the O
tower B
. O

The O
main B
characteristics I
of O
the O
semi O
- O
dynamic O
class B
is O
that O
it O
can O
modify O
its O
influence O
values O
based B
in O
its O
current O
aggression O
state O
. O

When O
attacking B
the O
controlled B
agent O
it O
generates O
negative O
values O
, O
as O
the O
agent O
should O
avoid O
receiving O
damage B
. O

On O
the O
other O
hand O
, O
the O
agent O
should O
attack B
the O
enemy B
towers I
, O
thus O
the O
influence O
generated O
is O
positive O
when O
there O
is O
another O
target B
being O
attacked B
by O
the O
tower B
. O

Lastly O
, O
the O
static O
features O
of O
a O
MOBA O
game B
are O
considered O
as O
the O
terrain O
. O

The O
terrain O
can O
not O
be O
modified O
over O
time B
, O
thus O
we O
can O
make O
an O
influence O
map O
that O
is O
static O
, O
performing O
the O
analysis O
just O
one O
time B
. O

Note O
that O
when O
performing O
static O
analysis O
we O
do O
not O
consider O
abilities B
of O
heroes B
that O
can O
cast O
some O
kind O
of O
collider O
or O
terrain O
modifier O
, O
considering O
these O
as O
dynamic O
characteristics O
. O

2.4 O
League O
of O
Legends O
League O
of O
Legends O
is O
a O
game B
of O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
genre O
developed O
by O
Riot O
Games O
; O
it O
was O
open B
released O
in O
2009 O
. O

MOBAs B
are O
characterized O
by O
its O
online O
competitive B
nature O
, O
where O
two O
teams B
of O
players B
compete O
against O
each O
other O
aiming O
to O
destroy O
the O
enemy B
base I
. O

A O
sample O
map O
can O
be O
seen O
in O
Figure O
1 O
. O

In O
the O
most O
common O
game B
mode I
in O
MOBAs B
, O
each O
team B
is O
composed O
of O
five O
players B
. O

The O
player B
is O
represented O
by O
a O
powerful B
unit I
called B
hero B
, O
that O
is O
chosen O
at O
the O
beginning O
of O
each O
match B
from O
a O
hero B
pool O
. O

Each O
hero B
has O
unique O
powerful B
skills I
that O
can O
be O
used O
to O
combat B
enemy B
players B
, O
neutral B
units I
and O
creeps B
. O

Each O
hero B
starts B
at O
level B
one O
and O
the O
player B
has O
to O
defeat B
enemy B
, O
AI O
driven O
, O
weak B
units B
called B
creeps B
or O
jungle B
neutral B
units I
. O

By O
defeating B
these O
units B
, O
or O
enemy B
heroes I
, O
the O
player B
can O
accumulate O
experience B
points I
developing O
skills B
and O
upgrading O
their O
hero B
level B
. O

At O
each O
level B
, O
the O
player B
can O
upgrade O
one O
skill B
, O
making O
it O
more O
powerful B
, O
and O
the O
hero B
increases B
its O
power B
in O
terms O
of O
status B
, O
e.g. O
attack B
power B
, O
health B

 O
points I
, O
mana B
points B
. O

When O
the O
player B
performs O
the O
last O
- O
hit O
in O
a O
neutral O
or O
enemy B
creep B
, O
his O
/ O
her O
hero B
gets O
gold B
, O
allowing O
the O
player B
to O
buy O
items B
that O
will O
increase B
hero B
status I
, O
making O
it O
more O
powerful B
. O

Winning O
in O
a O
MOBA O
game B
can O
be O
achieved O
just O
by O
destroying O
an O
enemy B
main O
structure O
, O
in O
League O
of O
Legends O
this O
structure O
is O
called B
Nexus O
. O

When O
a O
team B
destroys O
the O
enemy B
Nexus O
the O
game B
is O
terminated O
and O
this O
is O
the O
winner B
team B
. O

When O
choosing O
a O
hero B
, O
players B
must O
have O
in O
mind O
what O
role B
he O
/ O
she O
wants O
to O
assume O
during O
gameplay B
. O

There O
are O
heroes B
that O
can O
be O
suited O
in O
various O
functions O
inside O
the O
game B
, O
but O
most O
of O
them O
performs O
better O
when O
assuming O
the O
main O
function O
that O
the O
hero B
was O
designed O
for O
. O

Another O
concept O
that O
the O
player B
must O
take O
in O
account O
Figure O
1 O
: O
General O
Map O
of O
a O
MOBA O
Game O
is O
the O
current O
metagame B
. O

The O
metagame B
takes O
in O
account O
various O
theories O
about O
what O
the O
best O
compilation O
of O
heroes B
that O
tends O
to O
perform O
better O
in O
the O
actual O
scenario O
, O
using O
knowledge O
from O
inside O
and O
outside O
of O
the O
game B
. O

In O
addition O
, O
if O
the O
player B
is O
a O
human O
, O
his O
/ O
her O
skills B
must O
be O
considered O
. O

Due O
to O
the O
continued O
development O
and O
update B
of O
MOBAs O
, O
the O
metagame B
tends O
to O
change O
in O
every O
balance O
change O
, O
done O
by O
a O
patch O
. O

An O
example O
of O
metagame B
is O
the O
most O
common O
team B
compilation O
in O
LoL O
, O
normally O
two O
players B
go O
to O
bottom B
lane I
, O
one O
Support B
and O
one O
Attack B
Damage O
Carry O
( O
ADC O
) O
, O
in O
the O
middle B
lane I
goes O
a O
Mage O
or O
Assassin O
, O
a O
Tank O
or O
Fighter B
in O
the O
Jungle O
/ O
Ganker O
role B
and O
a O
top O
lane B
player B
that O
can O
be O
an O
Initiator O
, O
Front O
- O
liner O
or O
Tank B
. O

Metagame O
can O
be O
better O
understood O
by O
knowing O
the O
best O
functions O
of O
each O
class B
. O

An O
ADC O
is O
a O
long O
ranged B
hero B
, O
capable O
of O
dealing O
high B
damage I
to O
single B
targets I
, O
while O
a O
Support B
is O
a O
hero B
that O
can O
assist B
the O
ADC O
, O
healing B
of O
buffing B
his O
skills B
. O

Mages B
are O
champions B
that O
deal B
damage I
based B
in O
spells B
, O
and O
eventually O
are O
capable O
of O
performing O
high B
AoE O
damage I
. O

On O
the O
other O
hand O
, O
Assassins O
are O
strong O
in O
duels B
and O
killing B
a O
single O
enemy B
fast O
. O

Lastly O
, O
the O
top B
- I
laners I
are O
largely O
used O
to O
initiate O
the O
fights B
or O
block O
the O
damage B
incoming O
from O
the O
enemy B
team I
. O

Thus O
, O
the O
top O
- O
lane B
champions B
have O
high O
HP O
and O
defensive O
status B
, O
like O
armor B
. O

Tanks B
are O
champions B
capable O
of O
receiving O
high O
amounts B
of I
damage I
without O
being O
killed B
. O

Initiators O
have O
the O
responsibility O
of O
starting B
the O
team B
combats I
, O
while O
FrontLiners O
are O
responsible O
to O
stay O
between O
the O
two O
teams B
during O
the O
combat B
. O

Its O
important O
to O
notice O
that O
there O
are O
heroes B
that O
are O
classified O
in O
more O
than O
one O
function O
, O
or O
are O
suitable O
in O
more O
than O
one O
lane B
. O

For O
example O
, O
Varus O
, O
a O
LoL B
champion I
, O
can O
be O
a O
mid B
- I
laner I
or O
ADC O
, O
while O
Malphite O
can O
be O
classified O
as O
Tank B
, O
Initiator O
and O
Mage O
. O

3 O
Agent O
Architecture O
The O
use O
of O
a O
heterogeneous O
agent O
architecture O
seems O
reliable O
for O
the O
MOBA O
domain O
. O

Due O
to O
the O
multiple O
tasks O
like O
building O
, O
navigating O
, O
cooperation B
and O
combat B
, O
it O
is O
possible O
to O
use O
the O
heterogeneous O
architecture O
to O
implement O
a O
multi O
- O
tasking O
agent O
. O

Our O
agent O
interfaces B
with O
LoL B
through O
the O
use O
of O
sensors O
which O
collect O
data O
about O
the O
world B
. O

The O
agent O
also O
can O
send O
action O
to O
the O
hero B
to O
perform O
, O
modifying O
the O
game B
environment O
with O
its O
actuators O
, O
e.g. O
attacking B
. O

League O
of O
Legends O
has O
an O
application O
programming O
interface B
, O
called B
Bot O
of O
Legends O
( O
BoL O
) O
. O

BoL O
retrieves O
information O
about O
the O
game B
and O
performs O
the O
same O
actions O
as O
players B
. O

This O
tool O
is O
very O
similar O
to O
the O
Brood O
War O
API O
that O
is O
largely O
used O
in O
Starcraft O
research O
. O

Both O
tools O
have O
the O
same O
premise O
: O
they O
collect O
information O
that O
is O
available O
to O
players B
and O
just O
perform O
actions O
available O
to O
players B
. O

The O
agent O
is O
composed O
of O
two O
layers O
: O
navigation O
and O
micromanagement O
. O

Our O
primary O
goal B
is O
to O
develop O
an O
agent O
that O
could O
navigate O
Figure O
2 O
: O
Architecture O
implemented O
for O
the O
agent O
. O

and O
interact O
in O
a O
MOBA O
domain O
in O
a O
competitive B
way O
. O

That O
means O
that O
we O
are O
developing O
a O
rational O
agent O
with O
the O
goal B
of O
winning O
. O

However O
, O
as O
most O
agents O
implemented O
in O
the O
video B
games I
domain O
, O
our O
agent O
is O
still O
not O
capable O
of O
outperforming O
human O
players B
. O

3.1 O
Navigation O
Layer O
The O
base B
of O
our O
agent O
is O
modeling O
the O
information O
collected O
into O
valuable O
information O
that O
could O
be O
used O
by O
all O
layers O
. O

Thus O
, O
we O
inserted O
a O
series O
of O
hand O
- O
authored O
knowledge O
and O
data O
collected O
real B
- I
time I
. O

Further O
, O
we O
model O
the O
data O
collected O
and O
hand O
- O
authored O
over O
an O
grid O
- O
formatted O
influence O
map O
. O

A O
visual O
representation O
of O
our O
model O
can O
be O
seen O
in O
Figure O
2 O
. O

The O
influence O
map O
feeds O
the O
navigation O
layer O
that O
uses O
a O
case O
- O
based B
reasoning O
( O
CBR O
) O
system O
. O

This O
layer O
search O
for O
best O
points B
, O
taking O
in O
account O
the O
terrain O
, O
dynamic O
and O
static O
features O
from O
the O
domain O
. O

The O
influence O
maps O
uses O
single B
layer I
, O
aiming O
to O
improve O
the O
agent O
performance O
. O

The O
IM O
models O
knowledge O
about O
all O
agents O
involved O
in O
the O
MOBA O
domain O
. O

We O
use O
mathematical O
functions O
to O
create O
a O
single O
value O
base B
in O
the O
weight O
of O
individuals O
, O
generating O
a O
2D O
grid O
with O
decimal O
values O
. O

Aside O
from O
both O
the O
IM O
and O
the O
navigation O
layer O
, O
the O
micromanagement O
layer O
uses O
sensors O
to O
perceive O
the O
enemy B
and O
ally B
units B
. O

Based B
in O
the O
information O
collected O
, O
the O
micromanagement O
layer O
coordinates O
two O
CBRs O
, O
the O
target B
selector I
and O
orbwalker O
. O

In B
high O
- I
level I
, O
target B
selector I
is O
responsible O
by O
defining O
the O
unit B
that O
will O
be O
the O
current O
target B
of O
attacks B
and O
abilities B
. O

On O
the O
other O
hand O
, O
the O
orbwalker O
is O
responsible O
by O
reasoning O
between O
attack B
and O
movement B
. O

Both O
systems O
will O
be O
discussed O
further O
. O

3.1.1 O
Enemy O
Towers O
When O
analyzing O
the O
towers B
, O
in O
general O
we O
observe O
that O
it O
can O
assume O
two O
states O
: O
idle O
or O
aggressive O
. O

In O
the O
idle O
state O
, O
the O
tower B
performs O
no O
action O
, O
but O
verifies O
if O
there O
are O
enemies B
within O
its O
range B
to O
be O
attacked B
. O

As O
soon O
as O
an O
enemy B
enters O
the O
turret B
range B
, O
it O
will O
be O
attacked B
. O

In O
addition O
, O
a O
tower B
can O
just O
attack B
one O
enemy B
at O
a O
time B
, O
and O
does O
not O
inflict O
area B
- I
of I
- I
effect I
( O
AoE O
) O
damage B
. O

We O
can O
point B
a O
preference O
list O
for O
tower B
enemy B
choosing O
order O
as O
follows O
: O
i O
) O
Enemy B
hero I
attacking B
ally B
hero I
, O
ii O
) O
any O
enemy B
in O
range B
, O
iii O
) O
enemy B
creep B
and O
iv O
) O
enemy B
hero I
. O

Once O
the O
tower B
selects O
a O
target B
it O
will O
not O
change O
, O
unless O
it O
dies O
of O
comes O
in O
first O
options O
in O
the O
preference O
list O
. O

In O
some O
MOBAs B
, O
like O
Dota O
or O
Dota2 O
, O
the O
tower B
prefers O
the O
closest B
enemy B
, O
so O
the O
players B
try O
to O
stay O
after O
their O
creeps B
. O

Using O
this O
Figure O
3 O
: O
Plot O
of O
enemy B
tower I
in O
passive B
aggressive O
state O
. O

Red O
areas B
means O
more O
desirable O
positions O
, O
while O
blue B
areas I
mean O
less O
desirable O
positions O
. O

Note O
that O
red B
area I
is O
closer B
to O
the O
base B
of O
the O
agent O
’s O
team B
. O

Base O
is O
located O
in O
P(1000,1000 O
) O
knowledge O
, O
we O
were O
able O
to O
identify O
patterns O
and O
model O
an O
influence O
level B
for O
enemy B
towers I
. O

Different O
from O
RTS O
gameplay B
, O
towers B
in O
MOBA O
are O
very O
strong O
damage B
dealers O
and O
have O
high O
health O
values O
. O

They O
are O
also O
critical O
part O
of O
the O
strategy B
, O
as O
they O
protect O
the O
lanes B
and O
the O
structures O
. O

For O
dealing O
with O
enemy B
towers I
, O
we O
modeled O
two O
sub O
- O
states O
for O
the O
aggressive O
state O
: O
passive B
and O
active O
. O

The O
passive B
state O
means O
that O
there O
is O
aggression O
but O
the O
target B
of O
the O
tower B
is O
not O
our O
hero B
agent O
. O

Whereas O
the O
active O
state O
means O
that O
aggression O
target B
is O
our O
agent O
. O

The O
influence O
is O
then O
set O
to O
the O
map O
based B
in O
the O
current O
state O
. O

However O
in O
early B
tests O
we O
noticed O
that O
our O
tactical O
positioning B
was O
failing O
, O
due O
the O
resolution O
and O
that O
our O
agent O
was O
choosing O
advanced O
points B
in O
the O
map O
, O
an O
action O
called B
tower B
dive O
, O
causing O
it O
to O
die O
. O

We O
then O
introduced O
a O
weighting O
system O
for O
cells O
that O
allowed O
the O
agent O
to O
choose O
best O
positions O
based B
on O
the O
safest B
place O
. O

Our O
solution O
weighted O
the O
cells O
by O
a O
τ O
variable O
, O
that O
was O
obtained O
dividing O
the O
enemy B
unit B
to O
agent O
base B
distance O
by O
the O
IM O
cell O
distance O
to O
agent O
base B
distance O
, O
as O
seem O
in O
the O
Equation O
1 O
. O

τ O
= O
dtb O
dpb O
( O
1 O
) O
In O
addition O
, O
when O
performing O
tactical B
analysis I
, O
it O
is O
important O
to O
leave O
room O
to O
risky O
plays B
. O

Sometimes O
an O
enemy B
hero I
can O
have O
a O
very O
low O
hp O
and O
the O
agent O
could O
let O
it O
go O
just O
because O
it O
is O
in O
tower B

 O
range I
. O

Therefore O
, O
when O
weighting O
the O
tower B
we O
created O
a O
value O
ε O
that O
allows O
agent O
to O
enter O
in O
tower B
range I
if O
necessary O
. O

The O
value O
ε O
should O
be O
calculated O
as O
the O
max O
potential O
damage B
to O
be O
dealt O
by O
the O
tower B
. O

The O
rules B
for O
enemy B
tower I
influence O
can O
be O
seen O
in O
2 O
. O

wp O
= O
 O
 O
 O
( O
dpt O
, O
dpt O
< O
Hr O
− O
∆ O
τ O
∗ O
dpt O
, O
Hr O
− O
∆ O
≤ O
dpt O
≤ O
Hr O
Tr O
− O
dpt O
, O
else O
, O
α O
≥ O
3 O
 O
−Tr O
, O
dpt O
> O
ε O
−∞ O
, O
else O
, O
else O
( O
2 O
) O
Another O
problem O
faced O
by O
our O
agent O
, O
and O
often O
not O
discussed O
in O
the O
literature O
, O
is O
the O
resolution O
loss O
problem O
. O

As O
most O
games B
dedicates O
a O
few O
computation O
for O
AI O
, O
it O
is O
crucial O
to O
have O
cheap O
processing O
in O
this O
area B
[ O
Tozour O
2001 O
] O
. O

Therefore O
, O
the O
IM O
should O
be O
tuned O
aiming O
to O
deal O
a O
good O
performance O
to O
agent O
and O
does O
not O
consume O
a O
big O
computational O
time B
. O

The O
agent O
could O
then O
suffer O
from O
low O
resolution O
, O
causing O
it O
to O
perform O
poorly O
due O
to O
the O
little O
amount O
of O
data O
collected O
by O
the O
map O
. O

We O
then O
introduced O
a O
value O
∆ O
, O
that O
is O
half O
of O
the O
resolution O
used O
, O
to O
be O
used O
as O
an O
error O
correction O
. O

The O
final O
plot O
of O
equation O
in O
passive B
aggressive O
state O
can O
be O
seen O
in O
figure O
3 O
. O

Hr O
means O
hero B
range I
; O
dpt O
denotes O
the O
euclidean O
distance O
between O
the O
cell O
and O
the O
tower B
; O
Tr O
means O
the O
tower B
range I
; O
and O
ε O
denotes O
a O
limit O
area B
for O
lowest O
influence O
values O
inside O
of O
tower B
range I
. O

Figure O
4 O
: O
Plot O
of O
enemy B
creeps B
influence O
. O

Red B
areas I
means O
more O
desirable O
positions O
, O
while O
blue B
areas I
mean O
less O
desirable O
positions O
. O

Note O
that O
the O
red B
area I
is O
strongly O
influenced O
by O
the O
φ O
variable O
. O

Base O
is O
located O
in O
P(1000,1000 O
) O
3.1.2 O
Enemy O
Creeps O
Enemy O
creeps B
are O
the O
main O
resource B
collection O
approach O
for O
agents O
and O
players B
. O

Each O
time B
that O
a O
player B
kills B
an O
enemy B
creep B
the O
player B
obtains O
gold B
and O
experience B
. O

Thus O
, O
there O
must O
be O
a O
careful O
analysis B

 O
of I
these I
components I
, I
as O
they O
can O
be O
game B
changing O
. O

The O
correct O
positioning O
around O
these O
units B
will O
allow O
the O
agent O
to O
perform O
the O
last O
- O
hit O
and O
, O
as O
these O
units B
are O
highly O
dynamic O
, O
they O
must O
be O
observed O
most O
constantly O
. O

When O
calculating O
the O
creeps B
influence O
we O
are O
covering O
two O
layers O
of O
the O
agent O
architecture O
: O
movement B
and O
micromanagement O
. O

In O
this O
section O
, O
we O
discuss O
the O
movement B
analysis O
; O
the O
micromanagement O
will O
be O
discussed O
further O
, O
in O
Section O
3.3.1 O
. O

To O
compute O
the O
influence O
of O
enemy B
creeps B
we O
use O
the O
logic O
proposed O
by O
[ O
Hagelback O
and O
Johansson O
2008a O
] O
, O
staying O
away O
from O
the O
¨ O
enemy B
as O
much O
as O
possible O
but O
still O
maintaining O
it O
in O
hero B
range I
. O

As O
well O
, O
as O
discussed O
in O
enemy B
tower I
section O
, O
we O
must O
consider O
the O
resolution O
losses O
, O
creating O
a O
plateau O
in O
our O
equation O
, O
making O
it O
possible O
for O
the O
agent O
to O
hit O
the O
creep B
. O

We O
also O
consider O
the O
weighting O
relatively O
to O
the O
hero B
’s O
own O
team B
base I
, O
making O
it O
stay O
in O
the O
safest B
place O
possible O
, O
making O
the O
agent O
avoid O
the O
damage B
done O
by O
enemy B
creeps B
. O

A O
plot O
of O
the O
influence O
done O
by O
our O
model O
can O
be O
seen O
in O
Figure O
4 O
. O

In O
our O
early B
tests O
we O
noticed O
that O
just O
by O
considering O
this O
influence O
with O
weight O
was O
not O
enough O
to O
make O
the O
agent O
perform O
well O
in O
a O
prerequisite O
called B
farming3 O
. O

Making O
the O
agent O
be O
on O
the O
edge O
of O
the O
creeps B
does O
not O
guarantee O
that O
the O
agent O
will O
select O
the O
creep B
that O
will O
die O
first O
, O
or O
that O
have O
the O
lowest O
amount O
of O
health B
points I
( O
HP O
) O
. O

We O
then O
implemented O
a O
variable O
φ O
, O
that O
calculates O
the O
creep B
’s O
missing O
percent O
of O
HP O
. O

We O
inserted O
φ O
in O
the O
influence O
equation O
that O
considers O
the O
HP O
of O
a O
creep B
, O
making O
the O
agent O
to O
go O
towards O
the O
creep B
that O
has O
the O
lowest B
health I
. O

Equation O
3 O
shows O
the O
influence O
calculation O
for O
enemy B
creeps B
. O

Variable O
dpm O
denotes O
the O
euclidean O
distance O
between O
the O
cell O
and O
the O
creep B
. O

wp O
= O
 O
dpm O
, O
dpm O
< O
Hr O
− O
∆ O
max(τ O
∗ O
( O
dpm O
+ O
100 O
− O
φ O
) O
, O
wp O
) O
, O
else O
( O
3 O
) O
3.1.3 O
Ally O
Towers O
When O
weighting O
towers B
we O
observe O
that O
it O
is O
very O
defensive O
to O
be O
under O
the O
range B
of O
own O
’s O
team B
towers B
, O
but O
for O
this O
defense B
to O
be O
effective O
, O
the O
agent O
should O
be O
in O
a O
safe B
area B
. O

For O
example O
, O
the O
agent O
should O
be O
in O
range B
of O
an O
ally B
tower B
and O
out O
of O
range B
of O
enemy B
heroes I
. O

However O
, O
the O
safe B
area B
is O
very O
difficult O
to O
abstract O
due O
3Farming O
is O
the O
action O
of O
collecting O
resources B
by O
executing O
last O
- O
hits O
in O
enemy B
or O
neutral O
creeps B
. O

to O
the O
diverse O
mechanics B
found O
in O
MOBA O
games B
. O

Another O
point B
that O
must O
be O
observed O
, O
is O
that O
the O
player B
should O
avoid O
colliding O
with O
towers B
, O
as O
this O
could O
lead B
to O
time B
leaks O
in O
the O
navigation O
. O

Moreover O
, O
in O
various O
MOBAs B
, O
there O
are O
champions B
that O
can O
take O
advantage B
of O
the O
proximity O
of O
the O
hero B
to O
the O
tower B
. O

We O
then O
developed O
an O
influence O
equation O
for O
allied B
towers B
based B
on O
these O
characteristics O
. O

The O
equation O
provides O
the O
tower B
a O
linear O
decay O
of O
influence O
, O
making O
its O
best O
places O
near O
it O
and O
the O
worst O
ones O
far O
away O
from O
the O
tower B
. O

For O
avoiding O
the O
collision O
between O
the O
player B
and O
the O
tower B
, O
and O
avoiding O
to O
be O
caught O
by O
specific O
mechanics B
, O
we O
inserted O
a O
minimum O
distance O
for O
influence O
to O
be O
set O
. O

Thus O
, O
the O
agent O
will O
maintain O
distant B
from O
the O
tower B
in O
a O
secure O
way O
, O
as O
seen O
in O
Equation O
4 O
. O

wp O
= O
 O
max(Tr O
− O
dpt O
, O
wp O
) O
, O
dpt O
> O
ε O
0 O
, O
else O
( O
4 O
) O
3.1.4 O
Heroes O
Heroes O
are O
the O
most O
dynamic O
units B
in O
MOBA O
gameplay B
, O
they O
can O
move B
generally O
faster O
than O
creeps B
and O
also O
cast O
spells B
and O
attack B
. O

This O
units B
are O
the O
ones O
that O
require O
more O
attention O
when O
performing O
tactical B
analysis I
in O
the O
MOBA O
domain O
, O
due O
to O
their O
high O
potential O
of O
changing O
the O
game B
’s O
tactical O
landscape O
. O

We O
navigate O
based B
strongly O
in O
our O
ally B
heroes I
and O
try O
to O
avoid O
enemy B
damage B
. O

In O
our O
early B
research O
phase O
we O
implemented O
the O
influence O
range B
based B
in O
the O
auto B
- I
attack I
range B
of O
heroes B
. O

That O
proved O
to O
be O
a O
bad O
choice O
by O
two O
main O
reasons O
: O
( O
a O
) O
Melee O
heroes B
have O
a O
small B
range I
, O
but O
they O
can O
cast O
spells B
and O
deal B
damage I
farther O
and O
( O
b O
) O
there O
are O
heroes B
that O
have O
a O
smaller B
range I
but O
have O
a O
high O
spell B
cast O
range B
. O

Therefore O
, O
we O
had O
to O
compile O
a O
hand O
- O
authored O
knowledge O
database O
of O
all O
126 O
heroes B
contained O
in O
League O
of O
Legends O
. O

This O
database O
informed O
our O
agent O
the O
distance O
to O
be O
considered O
as O
the O
current O
hero B

 O
range I
. O

In O
addition O
to O
the O
hand O
- O
authored O
data O
, O
we O
collected O
real O
time B
information O
about O
the O
spells B
being O
cast O
, O
allowing O
the O
agent O
to O
dynamically O
modify O
the O
influence O
range B
and O
perform O
further O
analysis O
about O
the O
risk O
of O
engaging O
in O
a O
fight B
against O
an O
enemy B
. O

For O
enemy B
heroes I
we O
consider O
the O
danger B
area B
as O
a O
highly O
undesirable O
plateau O
, O
so O
we O
just O
attribute O
the O
tactical O
value O
of O
that O
hero B
, O
in O
negative O
form O
, O
in O
its O
range B
area B
. O

For O
ally B
heroes I
we O
perform O
a O
very O
similar O
approach O
, O
applying O
the O
calculated O
tactical O
value O
over O
the O
cells O
in O
range B
. O

3.2 O
Combining O
influences O
Performing O
multi O
- O
agent O
tactical B
analysis I
goes O
beyond O
the O
analysis B

 O
of I
each I
agent I
separately I
and O
running O
an O
agent O
considering O
it O
. O

It O
is O
necessary O
to O
compile O
these O
influences O
together O
, O
making O
the O
data O
collected O
make O
sense O
for O
the O
system O
. O

The O
great O
challenge O
of O
making O
a O
readable O
model O
for O
the O
system O
is O
to O
mix O
them O
in O
a O
way O
that O
is O
computationally O
efficient O
. O

The O
initial O
propose O
of O
[ O
Tozour O
2001 O
] O
is O
to O
store O
various O
influence O
map O
layers O
and O
then O
put O
them O
together O
. O

Moreover O
[ O
Champandard O
et O
al O
. O

2011 O
] O
discusses O
that O
if O
an O
additional O
buffer O
is O
not O
used O
for O
influence O
mapping O
the O
values O
can O
be O
scattered O
in O
a O
non O
- O
desirable O
way O
. O

Thus O
, O
in O
these O
two O
approaches O
we O
observe O
that O
extra O
memory O
is O
needed O
. O

When O
observing O
the O
Potential O
Fields O
approach O
proposed O
by O
[ O
Hagelback O
and O
Johansson O
2008b O
] O
we O
can O
observe O
that O
more O
¨ O
computing O
resources B
is O
needed O
, O
as O
the O
distance O
between O
the O
cells O
in O
the O
grid O
must O
be O
calculated O
. O

Our O
approach O
uses O
a O
similar O
approach O
to O
the O
proposed O
by O
Hagelback O
, O
using O
the O
distances O
to O
write O
in O
a O
2D O
grid O
. O

We O
do O
not O
¨ O
use O
extra O
buffering O
, O
and O
, O
as O
we O
do O
not O
need O
any O
historical O
information O
, O
we O
do O
not O
store O
these O
data O
in O
our O
map O
. O

All O
characteristics O
then O
drive O
our O
approach O
to O
a O
highly O
tactic B
map O
, O
analyzing O
just O
the O
actual O
information O
and O
reasoning O
based B
in O
that O
. O

After O
all O
influences O
are O
calculated O
, O
we O
need O
to O
concatenate O
them O
in O
a O
single O
2D O
grid O
. O

In O
early B
IMs O
, O
the O
task O
of O
mixing O
the O
cells O
values O
was O
done O
by O
simple O
summing O
each O
map O
layer O
, O
obtaining O
rough O
IMs O
. O

However O
, O
just O
summing O
does O
not O
provide O
a O
reliable O
technique B
for O
extracting O
information O
from O
this O
data O
. O

For O
example O
, O
summing O
can O
drive O
the O
agent O
to O
a O
local O
maximum O
when O
analyzing O
the O
enemy B
agent O
features O
, O
as O
shown O
in O
Figure O
5 O
. O

Observe O
that O
, O
while O
the O
max O
function O
allows O
each O
creep B
to O
set O
its O
own O
influence O
, O
the O
sum O
method O
tries O
to O
sum O
them O
together O
. O

However O
, O
sum O
creates O
a O
local O
maximum O
that O
could O
expose O
the O
agent O
to O
dangerous O
positioning B
during O
combat B
or O
even O
to O
cases O
where O
the O
creeps B
will O
be O
able O
to O
defeat B
the O
agent O
due O
to O
a O
local O
maximum O
. O

To O
avoid O
the O
creation O
of O
local O
maximum O
, O
as O
we O
are O
using O
an O
Influence O
Map O
based B
in O
tactical O
features O
, O
we O
classified O
the O
agents O
involved O
based B
in O
their O
importance O
. O

Doing O
so O
would O
require O
us O
to O
store O
various O
Influence O
Map O
layers O
and O
then O
perform O
operations O
between O
these O
layers O
. O

However O
, O
we O
figured O
that O
we O
could O
update B
each O
agent O
class B
one O
at O
a O
time B
, O
obtaining O
the O
same O
results O
and O
saving O
memory O
. O

The O
order O
was O
done O
by O
the O
tactical O
value O
of O
each O
unit B
. O

We O
first O
update B
creeps B
, O
then O
we O
update B
towers B
and O
lastly O
we O
update B
heroes B
. O

Operation O
selection O
was O
based B
on O
the O
goal B
of O
minimizing O
the O
local O
maximum O
generation O
and O
maximizing O
the O
outcome O
of O
the O
tactical O
goal B
related O
to O
that O
unit B
. O

For O
example O
, O
for O
creeps B
we O
aim O
to O
maximize O
the O
resource B
collection O
through O
last O
- O
hitting O
creeps B
while O
we O
try O
to O
minimize O
the O
local O
minimum O
of O
creeps B
, O
thus O
the O
exposition O
of O
the O
agent O
to O
danger B
. O

Therefore O
, O
creeps B
uses O
a O
maximum O
value O
function O
for O
mixing O
the O
values O
to O
the O
map O
. O

Our O
system O
creates O
a O
zero O
- O
valued O
IM O
in O
each O
iteration O
. O

Further O
, O
our O
system O
calculates O
the O
influence O
of O
creeps B
, O
comparing O
this O
value O
to O
the O
actual O
cell O
value O
and O
setting O
the O
maximum O
to O
current O
cell O
. O

For O
the O
setting O
of O
influence O
from O
towers B
, O
the O
second O
feature O
to O
be O
computed O
, O
we O
overwrite O
the O
values O
computed O
previously O
. O

This O
aims O
to O
put O
the O
values O
of O
towers B
over O
the O
values O
established O
by O
the O
creeps B
, O
showing O
that O
it O
is O
more O
important O
than O
the O
previous O
one O
. O

After O
setting O
tower B
values O
, O
we O
finally O
calculate O
and O
set O
the O
values O
from O
heroes B
. O

As O
these O
are O
the O
most O
dynamic O
unities O
in O
the O
game B
, O
they O
deserve O
special O
attention O
. O

Even O
more O
, O
defeating B
a O
hero B
can O
give O
the O
agent O
the O
necessary O
time B
to O
gain O
any O
kind O
of O
strategic B
advantage I
in O
the O
game B
, O
so O
the O
algorithm O
prioritizes O
them O
. O

3.3 O
Micromanagement O
Layer O
Along O
with O
a O
movement B
layer O
, O
it O
is O
necessary O
to O
implement O
a O
layer O
responsible O
for O
coordinating O
the O
abilities B
and O
attacks B
from O
hero B
. O

In O
our O
approach O
, O
as O
we O
aim O
to O
create O
a O
generic O
technique B
we O
do O
not O
focus B
on O
abilities B
, O
but O
first O
in O
simple O
auto O
attacks B
. O

In O
the O
next O
subsections O
we O
discuss O
the O
techniques B
applied O
in O
the O
micromanagement O
layer O
. O

3.3.1 O
Orbwalking O
and O
Kiting O
A O
very O
useful O
mechanic B
in O
game B
combat B
scenarios I
is O
to O
hit O
and O
run O
, O
especially O
when O
the O
coordinated O
unit B
has O
a O
higher O
range B
than O
the O
enemy B
unit B
does O
, O
this O
is O
called B
kiting O
. O

Human O
players B
very O
often O
perform O
this O
kind O
of O
mechanics B
aiming O
to O
maximize O
the O
damage B
dealt O
to O
enemies B
both O
in O
RTS O
and O
in O
MOBA O
games B
. O

Furthermore O
, O
that O
is O
an O
essential O
technique B
for O
damage B
- O
based B
carries B
and O
for O
any O
agent O
who O
aims O
to O
farm B
in O
MOBA O
. O

There O
is O
an O
algorithm O
, O
called B
orbwalker O
, O
that O
is O
responsible O
to O
inform O
when O
it O
is O
possible O
to O
attack B
or O
to O
move B
, O
very O
similar O
to O
the O
approach O
presented O
by O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
. O

However O
, O
the O
orbwalking O
algorithm O
does O
not O
per- O
´ O
form O
any O
kind O
of O
tactical B
analysis I
; O
that O
is O
why O
further O
scenario O
analysis O
is O
necessary O
when O
performing O
the O
attack B
- O
and O
- O
flee O
mechanic B
. O

In O
our O
approach O
, O
we O
did O
not O
implement O
the O
kiting O
algorithm O
as O
proposed O
by O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
; O
however O
, O
our O
agent O
per- O
´ O
formed O
this O
emerged O
behavior O
due O
to O
the O
enemy B
analysis O
and O
positioning B
system O
. O

The O
orbwalker O
itself O
informed O
the O
agent O
when O
it O
was O
possible O
to O
attack B
or O
when O
to O
walk O
, O
based B
in O
the O
animation B
time B
, O
turn B
time B
and O
latency O
of O
the O
game B
. O

The O
last O
parameter O
showed O
to O
be O
crucial O
to O
MOBA O
approach O
, O
as O
this O
genre O
is O
mostly O
based B
in O
online O
experience B
. O

As O
the O
agent O
tends O
to O
be O
as O
far O
as O
possible O
from O
enemies B
and O
maintaining O
these O
enemies B
within O
the O
agent O
range B
, O
it O
is O
clear B
that O
the O
enemies B
will O
be O
in O
the O
edge O
of O
enemy B
attacking O
field B
. O

The O
agent O
then O
receives O
the O
repositioning O
task O
while O
attack B
action O
is O
not O
available O
, O
then O
turns B
around O
and O
attack B
, O
when O
the O
attack B
is O
available O
. O

However O
, O
Figure O
5 O
: O
Comparing O
the O
max(left O
) O
to O
sum(right O
) O
mixing O
methods O
when O
analysing O
enemy B
creeps B
feature O
. O

Red B
areas I
means O
more O
desirable O
positions O
, O
while O
blue B
areas I
mean O
less O
desirable O
positions O
. O

In O
the O
right O
figure O
we O
clearly O
see O
a O
local O
maximum O
( O
in O
red O
) O
; O
in O
the O
left O
figure O
we O
see O
a O
better O
influence O
distribution O
. O

our O
agent O
does O
not O
perform O
any O
analysis O
towards O
identifying O
the O
targets B
whose O
kiting O
can O
be O
performed O
, O
as O
the O
approach O
presented O
in O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
does O
. O

´ O
3.3.2 O
Target O
Selection O
The O
target B
selection I
problem O
is O
an O
open B
challenge O
in O
most O
strategic O
games B
, O
including O
RTS O
and O
MOBA O
. O

The O
work O
of O
[ O
Hagelback O
and O
¨ O
Johansson O
2008a O
] O
address O
this O
problem O
in O
a O
very O
early B
tactical O
approach O
. O

Later O
the O
work O
of O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
discuss O
this O
´ O
problem O
in O
greated O
detail O
, O
assigning O
tactical O
values O
for O
each O
enemy B

 O
based I
in O
its O
distance O
, O
manually O
- O
assigned O
tactical O
threat O
and O
DPS O
. O

They O
show O
that O
it O
is O
an O
essential O
part O
of O
tactical O
behavior O
to O
select O
the O
right O
enemy B
to O
attack B
. O

The O
work O
of O
[ O
Liu O
et O
al O
. O

2014 O
] O
presents O
an O
automatic O
target B
selection I
approach O
using O
Genetic O
Algorithms O
. O

However O
, O
their O
concept O
of O
Target O
Selection O
, O
although O
still O
tactic B
, O
shows O
a O
different O
concept O
from O
[ O
Uriarte O
and O
Ontan˜on O
2012 O
] O
. O

Rather O
´ O
than O
using O
tactical O
values O
or O
DPS O
it O
just O
considers O
the O
health B
points I
of O
each O
nearby O
enemy B
for O
target B
selection I
. O

In O
our O
approach O
we O
collect O
data O
and O
combine O
them O
with O
handauthored O
content O
, O
building O
a O
target B
selector I
that O
analyses O
the O
tactical O
properties O
of O
each O
unit B
. O

We O
use O
a O
preference O
model O
based B
in O
HP O
, O
Unit O
Tactic B
characteristics O
and O
Unit B
danger B
potential O
, O
also O
known O
as O
aggro B
. O

This O
mechanism B
does O
not O
have O
any O
actuator O
, O
it O
just O
provides O
information O
for O
the O
Micromanagement O
layer O
, O
that O
actuates O
in O
the O
agent O
state O
. O

As O
presented O
, O
our O
approach O
considers O
the O
danger B
first O
, O
targeting B
dangerous O
enemies B
, O
aiming O
to O
survive O
as O
long O
as O
possible O
. O

Secondly O
, O
it O
prioritizes O
resource B
collection O
, O
farming B
as O
much O
as O
possible O
. O

As O
discussed O
before O
, O
the O
selection O
of O
creeps B
to O
be O
farmed B
is O
is O
a O
responsibility O
shared O
by O
the O
micromanagement O
and O
movement B
. O

Moving B
next O
to O
low O
HP O
creeps B
allows O
the O
agent O
to O
put O
them O
in O
range B
, O
allowing O
the O
target B
selector I
to O
aim O
them O
in O
the O
micromanagement O
layer O
and O
successfully O
perform O
the O
last O
hit O
. O

Lastly O
, O
we O
verify O
the O
existence O
of O
a O
secure O
scenario O
to O
take O
aim O
at O
enemy B
towers I
, O
focusing B
on O
goal B
- O
controlling O
and O
on O
raising O
the O
advantage B
gained O
by O
the O
agent O
. O

By O
doing O
so O
, O
the O
agent O
is O
aiming O
to O
push O
the O
lane B
towards O
the O
enemy B
base I
, O
finally O
defeating B
the O
enemy B
by O
destroying O
the O
main O
structure O
. O

4 O
Empirical O
Evaluation O
In O
order O
to O
evaluate O
our O
approach O
we O
performed O
three O
sets O
of O
experiments O
in O
the O
domain O
of O
League O
of O
Legends O
. O

The O
first O
experiment O
tests O
the O
effectiveness O
of O
our O
agent O
, O
testing O
if O
the O
agent O
is O
capable O
of O
winning O
in O
a O
scenario O
where O
it O
plays B
alone O
. O

Secondly O
, O
we O
test O
the O
efficiency O
of O
this O
agent O
, O
measuring O
its O
performance O
in O
terms O
of O
resource B
collection O
. O

Lastly O
we O
test O
the O
agent O
’s O
performance O
in O
a O
match B
against O
human O
players B
. O

These O
experiments O
and O
their O
results O
will O
be O
discussed O
in O
the O
next O
subsections O
. O

4.1 O
Experiment O
1 O
: O
Winning O
alone O
in O
MOBA O
In O
this O
experiment O
we O
run O
the O
agent O
in O
a O
scenario O
where O
it O
is O
in O
a O
match B
without O
ally B
or O
enemy B
heroes I
. O

This O
experiment O
shows O
the O
effectiveness O
of O
the O
agent O
and O
the O
success O
of O
the O
sequence O
of O
steps O
presented O
to O
win O
in O
a O
MOBA O
game B
. O

Our O
baseline O
is O
the O
work O
of O
[ O
Willich O
2015 O
] O
, O
however O
we O
were O
not O
capable O
of O
replicating O
the O
author O
experiments O
, O
as O
code O
is O
not O
available O
. O

In O
addition O
, O
the O
author O
uses O
HoN O
as O
game B
domain O
, O
while O
in O
this O
work O
we O
use O
LoL B
as O
testbed O
. O

For O
this O
experiment O
we O
use O
a O
map O
where O
there O
is O
just O
one O
lane B
. O

In O
[ O
Willich O
2015 O
] O
the O
author O
uses O
a O
similar O
approach O
, O
although O
they O
use O
a O
three B
- I
lane I
map I
but O
considers O
just O
one O
. O

We O
executed O
20 O
matches B
with O
a O
hero B
randomly O
selected O
from O
the O
LoL O
champion B
pool O
. O

Our O
agent O
was O
capable O
of O
winning O
all O
matches B
. O

In O
addition O
, O
our O
agent O
always O
showed O
a O
KDA O
factor O
of O
0 O
, O
meaning O
that O
it O
never O
died O
and O
did O
not O
present O
errors O
. O

Also O
for O
testing O
the O
local O
maximum O
problem O
we O
monitored O
the O
agent O
also O
collecting O
the O
time B
that O
the O
matches B
lasted O
. O

The O
agent O
was O
capable O
of O
winning O
in O
an O
average O
of O
22.6 O
minutes O
with O
a O
standard O
deviation O
of O
5 O
minutes O
. O

Further O
, O
as O
we O
were O
using O
random B
heroes I
, O
the O
agent O
shows O
consistency O
for O
driving O
various O
characters B
. O

We O
then O
wanted O
to O
measure O
how O
well O
it O
performed O
alone O
in O
the O
scenario O
. O

A O
good O
measure O
for O
both O
the O
positioning B
is O
the O
resource B
collection O
. O

That O
leads B
us O
to O
the O
next O
experiment O
. O

4.2 O
Experiment O
2 O
: O
Resource O
Collection O
Resource B
collection O
in O
MOBA O
is O
different O
from O
RTS O
, O
as O
it O
is O
performed O
by O
killing B
enemy B
or O
neutral O
creeps B
, O
champions B
and O
structures O
. O

Furthermore O
, O
its O
important O
to O
maximize O
the O
resource B
collection O
. O

In O
this O
test O
we O
first O
run O
our O
approach O
without O
the O
φ O
variable O
, O
that O
represents O
the O
health O
of O
creeps B
. O

Then O
we O
compare O
the O
performance O
of O
resource B
collected O
to O
the O
approach O
with O
φ O
. O

In O
MOBA O
community B
there O
is O
a O
common O
sense O
that O
the O
best O
farm B
score O
, O
as O
performed O
by O
professional O
players B
, O
is O
to O
last O
hit O
10 O
creeps B
per O
minute O
, O
we O
use O
that O
sense O
as O
our O
baseline O
. O

We O
first O
ran O
10 O
matches B
with O
the O
agent O
using O
the O
φ O
variable O
for O
improving O
positioning B
and O
resource B
collection O
, O
then O
we O
run O
10 O
matches B
without O
using O
the O
φ O
variable O
. O

The O
tests O
were O
executed O
in O
the O
same O
conditions O
from O
Experiment O
1 O
, O
a O
single O
lane B
map O
without O
enemy B
heroes I
. O

Our O
experiments O
with O
the O
health B
factor I
shows O
an O
average O
of O
92,24 O
% O
efficiency O
in O
farming B
( O
see O
Table O
1 O
) O
, O
showing O
that O
our O
agent O
is O
capable O
of O
efficiently O
collecting O
resources B
. O

Further O
, O
Table O
1 O
: O
Performance O
presented O
by O
the O
agent O
during O
resource B
collection O
Method O
Creeps O
per O
Minute O
Efficiency(% O
) O
Baseline O
10 O
100 O
% O
φ O
disabled O
6.084 O
60.84 O
% O
φ O
enabled O
9.224 O
92.24 O
% O
this O
performance O
can O
be O
compared O
to O
the O
resource B
collection O
rate O
of O
professional O
players B
. O

In O
contrast O
to O
the O
previous O
performance O
, O
in O
matches B
where O
the O
health B
factor I
was O
disabled O
the O
efficiency O
was O
lowered O
to O
an O
average O
of O
60,84 O
% O
of O
resource B
collection O
. O

With O
this O
experiment O
we O
show O
that O
our O
agent O
is O
capable O
of O
efficiently O
collecting O
resources B
in O
a O
highly O
dynamic O
environment O
. O

It O
is O
important O
to O
stress O
that O
despite O
the O
lack O
of O
enemies B
or O
allies B
in O
the O
match B
, O
the O
agent O
has O
to O
compete O
with O
allied O
creeps B
for O
last O
hits O
. O

Moreover O
, O
this O
experiment O
shows O
the O
consistency O
between O
the O
two O
layers O
implemented O
, O
demonstrating O
that O
the O
combination O
of O
navigation O
and O
micromanagement O
system O
is O
effective O
. O

Further O
, O
we O
show O
that O
with O
the O
φ O
variable O
we O
were O
capable O
of O
improve O
the O
resource B
collection O
task O
by O
more O
than O
30 O
% O
. O

4.3 O
Experiment O
3 O
: O
Evaluation O
Against O
Human O
Players O
This O
experiment O
deals O
with O
a O
real O
match B
in O
League O
of O
Legends O
. O

We O
selected O
the O
mode O
called B
All O
Random O
All O
Mid O
( O
ARAM O
) O
to O
be O
used O
as O
testbed O
for O
this O
experiment O
. O

In O
this O
mode O
the O
map O
just O
has O
one O
lane B
and O
all O
heroes B
are O
selected O
at O
random O
for O
players B
that O
have O
the O
chance B
to O
exchange O
them O
between O
allies B
during O
the O
hero B
selection I
phase O
. O

This O
modes O
also O
provides O
more O
gold B
at O
the O
beginning O
of O
the O
match B
and O
the O
heroes B
starts B
at O
level B
three O
instead O
of O
one O
. O

We O
performed O
10 O
matches B
in O
ARAM O
mode O
for O
evaluation O
and O
collected O
the O
hero B
performance O
and O
win O
/ O
lose O
from O
our O
agent O
. O

We O
observe O
that O
when O
it O
plays B
melee B
characters B
the O
performance O
is O
greatly O
affected O
, O
doing O
much O
worse O
than O
when O
playing B
ranged B
heroes B
. O

The O
hero B
shows O
a O
high O
amount O
of O
deaths B
when O
playing B
melee B
champions B
. O

When O
playing B
ranged O
champions B
it O
shows O
low B
death O
levels I
, I
but O
does O
not O
show O
a O
high O
amount O
of O
kills B
. O

In O
both O
melee B
and O
ranged B
, O
our O
agent O
shows O
a O
high O
amount O
of O
assists B
, O
showing O
that O
it O
helps O
another O
players B
to O
get O
kills B
. O

In O
spite O
of O
assist B
characteristics O
, O
its O
not O
possible O
to O
affirm O
that O
it O
behaves O
in O
a O
cooperative O
fashion O
, O
as O
assists B
can O
be O
granted O
by O
just O
casting O
an O
spell B
or O
auto O
attacking O
. O

Lastly O
, O
as O
with O
most O
agents O
playing B
against O
humans O
, O
our O
agent O
performed O
poorly O
with O
most O
heroes B
. O

Due O
the O
fast O
dynamics O
of O
MOBA O
, O
our O
agent O
reaction B
time I
was O
not O
sufficient O
to O
play B
in O
a O
team B
. O

5 O
Conclusion O
and O
Future O
Work O
In O
this O
work O
we O
presented O
an O
approach O
for O
tactical B
analysis I
and O
knowledge O
modeling O
in B
real O
- I
time I
scenario O
using O
Influence O
Maps O
. O

For O
doing O
so O
, O
we O
used O
League O
of O
Legends O
, O
one O
of O
the O
most O
played B
games B
in O
the O
world B
nowadays O
. O

Our O
experiments O
results O
show O
that O
this O
is O
a O
promising O
approach O
. O

We O
implemented O
the O
game B
agent O
using O
an O
heterogeneous O
architecture O
composed O
of O
two O
layers O
: O
navigation O
and O
micromanagement O
. O

Furthermore O
, O
our O
approach O
show O
that O
it O
is O
possible O
, O
and O
necessary O
, O
to O
strongly O
connect O
these O
layers O
, O
as O
presented O
by O
the O
parameters O
that O
connects O
movement B
and O
micromanagement O
towards O
resource B
collection O
. O

We O
also O
demonstrate O
that O
simple O
features O
, O
like O
a O
health B

 O
factor I
, O
can O
greatly O
increase B
the O
performance O
of O
the O
agent O
. O

Moreover O
, O
further O
tactical O
investigation O
in O
MOBA O
domain O
is O
required O
to O
behave O
well O
against O
humans O
. O

Lastly O
, O
we O
obtained O
a O
kiting O
behavior O
by O
combining O
heterogeneous O
controls B
. O

In O
previous O
research O
this O
behavior O
required O
a O
dedicated O
module O
. O

A O
problem O
that O
can O
be O
tackled O
in O
future O
work O
is O
the O
competition B
of O
the O
current O
agent O
against O
human O
players B
. O
It I
is I
known O
that O
current O
state O
of O
the O
arts O
agents O
show O
poor B
performance O
when O
playing B
against O
human O
players B
, O
and O
an O
interesting O
question O
to O
be O
answered O
is O
whether O
this O
happens O
in O
MOBA O
. O

Moreover O
, O
whether O
the O
agent O
performs O
competitively O
cooperating O
with O
human O
players B
, O
as O
MOBA O
requires O
multiple O
players B
to O
control B
multiple O
agents O
. O

Another O
problem O
that O
can O
be O
addressed O
is O
the O
development O
of O
multiple O
agents O
and O
the O
interactions O
between O
them O
. O

Applying O
approaches O
of O
distributing O
tasks O
in O
games B
, O
like O
the O
presented O
in O
[ O
Tavares O
et O
al O
. O

2014 O
] O
, O
looks O
very O
promising O
when O
applied O
to O
the O
MOBA O
context O
, O
as O
this O
game B
genre O
is O
strongly O
oriented O
to O
player B
roles I
[ O
Yang O
et O
al O
. O

2014 O
] O
. O

6 O
Acknowledgements O
We O
would O
like O
to O
thank O
the O
BoL O
Community O
, O
especially O
Bilbao O
and O
Kenect O
, O
that O
supported B
us O
in O
the O
development O
using O
BoL O
environment O
. O

We O
also O
thank O
the O
reviewers O
, O
that O
provided O
insightful O
feedback O
and O
corrections O
to O
our O
paper O
. O

This O
work O
was O
supported B
by O
CAPES O
, O
CNPq O
and O
FAPEMIG O
. O

References O
ADOBBATI O
, O
R. O
, O
MARSHALL O
, O
A. O
N. O
, O
SCHOLER O
, O
A. O
, O
TEJADA O
, O
S. O
, O
KAMINKA O
, O
G. O
A. O
, O
SCHAFFER O
, O
S. O
, O
AND O
SOLLITTO O
, O
C. O
2001 O
. O

Gamebots B
: O
A O
3d O
virtual O
world B
test O
- O
bed O
for O
multi O
- O
agent O
research O
. O

In O
Proceedings O
of O
the O
second O
international O
workshop O
on O
Infrastructure O
for O
Agents O
, O
MAS O
, O
and O
Scalable O
MAS O
, O
vol O
. O

5 O
, O
Montreal O
, O
Canada O
. O

BANGAY O
, O
S. O
, O
AND O
MAKIN O
, O
O. O
2014 O
. O

Generating O
an O
attribute O
space O
for O
analyzing O
balance O
in O
single O
unit B
rts O
game B
combat B
. O

In O
Computational O
Intelligence O
and O
Games O
( O
CIG O
) O
, O
2014 O
IEEE O
Conference O
on O
, O
IEEE O
, O
1–8 O
. O

CHAMPANDARD O
, O
A. O
, O
DILL O
, O
K. O
, O
AND O
ISLA O
, O
D. O
2011 O
. O

Lay O
of O
the O
land O
: O
Smarter O
ai O
through O
influence O
maps O
. O

In O
Game O
developers O
conference O
, O
vol O
. O

2011 O
. O

COTHRAN O
, O
J. O
2009 O
. O

Winning O
the O
2k O
bot O
prize B
with O
a O
long O
- O
term O
memory O
database O
using O
sqlite O
. O

Available O
on O
AiGameDev O
. O

com O
at O
http://aigamedev O
. O

com O
/ O
open B
/ O
articles O
/ O
sqlite O
- O
bot O
. O

DIAMARANAN O
, O
A. O
, O
2015 O
. O

Most O
played B
games B
: O
May O
2015 O
the O
witcher O
debuts O
, O
world B
of O
warcraft O
stumbles O
, O
June O
. O

http://caas.raptr.com/most-played-games-may-2015-thewitcher-debuts-world-of-warcraft-stumbles/ O
Raptr.com[Online O
; O
posted O
26-June-2015 O
] O
. O

DRACHEN O
, O
A. O
, O
YANCEY O
, O
M. O
, O
MAGUIRE O
, O
J. O
, O
CHU O
, O
D. O
, O
WANG O
, O
I. O
Y. O
, O
MAHLMANN O
, O
T. O
, O
SCHUBERT O
, O
M. O
, O
AND O
KLABAJAN O
, O
D. O
2014 O
. O

Skill O
- O
based B
differences O
in B
spatio O
- I
temporal I
team B
behaviour O
in O
defence O
of O
the O
ancients O
2 O
( O
dota B
2 O
) O
. O

In O
Games O
Media O
Entertainment O
( O
GEM O
) O
, O
2014 O
IEEE O
, O
IEEE O
, O
1–8 O
. O

FERRARI O
, O
S. O
2013 O
. O

From O
generative O
to O
conventional O
play B
: O
Moba O
and O
league B
of I
legends I
. O

In O
Proceedings O
of O
DiGRA O
2013 O
: O
DeFragging O
Game O
Studies O
, O
vol O
. O

1 O
, O
1–17 O
. O

HAGELBACK O
¨ O
, O
J. O
, O
AND O
JOHANSSON O
, O
S. O
J. O
2008 O
. O

The O
rise O
of O
potential O
fields B
in O
real O
time B
strategy B
bots O
. O

In O
AAAI O
Conference O
on O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
. O

HAGELBACK O
¨ O
, O
J. O
, O
AND O
JOHANSSON O
, O
S. O
J. O
2008 O
. O

Using O
multiagent O
potential O
fields B
in B
real O
- I
time I
strategy B
games B
. O

In O
Proceedings O
of O
the O
7th O
international O
joint O
conference O
on O
Autonomous O
agents O
and O
multiagent O
systems O
- O
Volume O
2 O
, O
International O
Foundation O
for O
Autonomous O
Agents O
and O
Multiagent O
Systems O
, O
631–638 O
. O

KHATIB O
, O
O. O
1986 O
. O

Real B
- I
time I
obstacle B
avoidance O
for O
manipulators O
and O
mobile O
robots O
. O

The O
international O
journal O
of O
robotics O
research O
5 O
, O
1 O
, O
90–98 O
. O

LIU O
, O
S. O
, O
LOUIS O
, O
S. O
J. O
, O
AND O
BALLINGER O
, O
C. O
2014 O
. O

Evolving O
effective O
micro O
behaviors O
in O
rts O
game B
. O

In O
Computational O
Intelligence O
and O
Games O
( O
CIG O
) O
, O
2014 O
IEEE O
Conference O
on O
, O
IEEE O
, O
1–8 O
. O

MILLINGTON O
, O
I. O
, O
AND O
FUNGE O
, O
J. O
2012 O
. O

Artificial O
intelligence O
for O
games B
. O

CRC O
Press O
. O

NOSRATI O
, O
M. O
, O
KARIMI O
, O
R. O
, O
AND O
HARIRI O
, O
M. O
2013 O
. O

General O
trends O
in O
multiplayer O
online B
games I
. O

World O
Applied O
Programming O
3 O
. O

ONTANON O
´ O
, O
S. O
, O
SYNNAEVE O
, O
G. O
, O
URIARTE O
, O
A. O
, O
RICHOUX O
, O
F. O
, O
CHURCHILL O
, O
D. O
, O
AND O
PREUSS O
, O
M. O
2013 O
. O

A O
survey O
of O
real B
- I
time I

 O
strategy I
game B
ai O
research O
and O
competition B
in O
starcraft O
. O

Computational O
Intelligence O
and O
AI O
in O
Games O
, O
IEEE O
Transactions O
on O
5 O
, O
4 O
, O
293–311 O
. O

POBIEDINA O
, O
N. O
, O
NEIDHARDT O
, O
J. O
, O
CALATRAVA O
MORENO O
, O
M. O
D. O
C. O
, O
GRAD O
- O
GYENGE O
, O
L. O
, O
AND O
WERTHNER O
, O
H. O
2013 O
. O

On O
successful O
team B
formation I
: O
Statistical O
analysis O
of O
a O
multiplayer O
online B
game I
. O

In O
Business O
Informatics O
( O
CBI O
) O
, O
2013 O
IEEE O
15th O
Conference O
on O
, O
IEEE O
, O
55–62 O
. O

REYNOLDS O
, O
C. O
W. O
1987 O
. O

Flocks O
, O
herds O
and O
schools O
: O
A O
distributed O
behavioral O
model O
. O

In O
ACM O
Siggraph O
Computer O
Graphics O
, O
vol O
. O

21 O
, O
ACM O
, O
25–34 O
. O

REYNOLDS O
, O
C. O
W. O
1999 O
. O

Steering O
behaviors O
for O
autonomous O
characters B
. O

In O
Game O
developers O
conference O
, O
vol O
. O

1999 O
, O
763–782 O
. O

RIOULT O
, O
F. O
, O
METIVIER O
´ O
, O
J.-P. O
, O
HELLEU O
, O
B. O
, O
SCELLES O
, O
N. O
, O
AND O
DURAND O
, O
C. O
2014 O
. O

Mining O
tracks O
of O
competitive B
video O
games I
. I
AASRI O
Procedia O
8 O
, O
82–87 O
. O

STANESCU O
, O
M. O
, O
HERNANDEZ O
, O
S. O
P. O
, O
ERICKSON O
, O
G. O
, O
GREINER O
, O
R. O
, O
AND O
BURO O
, O
M. O
2013 O
. O

Predicting O
army O
combat B
outcomes O
in O
starcraft O
. O

In O
AIIDE O
. O

TAVARES O
, O
A. O
R. O
, O
AZPURUA O
´ O
, O
H. O
, O
AND O
CHAIMOWICZ O
, O
L. O
2014 O
. O

Evolving O
swarm O
intelligence O
for O
task O
allocation O
in O
a O
real O
time B
strategy B
game B
. O

In O
Computer O
Games O
and O
Digital O
Entertainment O
( O
SBGAMES O
) O
, O
2014 O
Brazilian O
Symposium O
on O
, O
IEEE O
, O
99–108 O
. O

TOZOUR O
, O
P. O
2001 O
. O

Influence O
mapping O
. O

Game B
programming O
gems O
2 O
, O
287–297 O
. O

URIARTE O
, O
A. O
, O
AND O
ONTAN˜ O
ON O
´ O
, O
S. O
2012 O
. O

Kiting O
in O
rts O
games B
using O
influence O
maps O
. O

In O
Eighth O
AAAI O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
Conference O
. O

WEBER O
, O
B. O
G. O
, O
MATEAS O
, O
M. O
, O
AND O
JHALA O
, O
A. O
2011 O
. O

Building O
human O
- O
level B
ai O
for O
real B
- I
time I
strategy O
games I
. O

In O
AAAI O
Fall O
Symposium O
: O
Advances O
in O
Cognitive O
Systems O
, O
vol O
. O

11 O
, O
01 O
. O

WILLICH O
, O
J. O
2015 O
. O

Reinforcement O
Learning O
for O
Heroes O
of O
Newerth O
. O

Bachelor O
thesis O
, O
Technische O
Universitat O
Darmstadt O
. O

YANG O
, O
P. O
, O
HARRISON O
, O
B. O
, O
AND O
ROBERTS O
, O
D. O
L. O
2014 O
. O

Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
MOBA O
games B
. O

In O
Proceedings O
of O
Foundations O
of O
Digital O
Games O
. O

Dynamic O
Difficulty O
Adjustment O
on O
MOBA O
Games O
Mirna O
Paula O
Silva O
, O
Victor O
do O
Nascimento O
Silva O
and O
Luiz O
Chaimowicz O
Department O
of O
Computer O
Science O
Universidade O
Federal O
de O
Minas O
Gerais O
( O
UFMG O
) O
Belo O
Horizonte O
, O
Brazil O
Abstract O
This O
paper O
addresses O
the O
dynamic O
difficulty O
adjustment O
on O
MOBA O
games B
as O
a O
way O
to O
improve O
the O
players B
entertainment O
. O

Although O
MOBA O
is O
currently O
one O
of O
the O
most O
played B
genres O
around O
the O
world B
, O
it O
is O
known O
as O
a O
game B
that O
offer O
less O
autonomy O
, O
more O
challenges O
and O
consequently O
more O
frustration O
. O

Due O
to O
these O
characteristics O
, O
the O
use O
of O
a O
mechanism B
that O
performs O
the O
difficulty O
balance O
dynamically O
seems O
to O
be O
an O
interesting O
alternative O
to O
minimize O
and/or O
avoid O
that O
players B
experience B
such O
frustrations O
. O

In O
this O
sense O
, O
this O
paper O
presents O
a O
dynamic O
difficulty O
adjustment O
mechanism B
for O
MOBA O
games B
. O

The O
main O
idea O
is O
to O
create O
a O
computer O
controlled B
opponent B
that O
adapts O
dynamically O
to O
the O
player B
performance O
, O
trying O
to O
offer O
to O
the O
player B
a O
better O
game B
experience I
. O

This O
is O
done O
by O
evaluating O
the O
performance O
of O
the O
player B
using O
a O
metric O
based B
on O
some O
game B
features O
and O
switching O
the O
difficulty O
of O
the O
opponent B
’s O
artificial O
intelligence O
behavior O
accordingly O
. O

Quantitative O
and O
qualitative O
experiments O
were O
performed O
and O
the O
results O
showed O
that O
the O
system O
is O
capable O
of O
adapting O
dynamically O
to O
the O
opponent B
’s O
skills B
. O

In O
spite O
of O
that O
, O
the O
qualitative O
experiments O
with O
users B
showed O
that O
the O
player B
’s O
expertise O
has O
a O
greater O
influence O
on O
the O
perception O
of O
the O
difficulty O
level B
and O
dynamic O
adaptation O
. O

Keywords O
: O
Artificial O
Intelligence O
, O
Digital O
Games O
, O
Dynamic O
Difficulty O
Adjustment O
, O
Dynamic O
Difficulty O
Balance O
, O
Entertainment O
, O
MOBA O
1 O
. O

Introduction O
The O
game B
industry O
is O
growing O
at O
a O
fast O
pace O
, O
globally O
generating O
more O
revenue O
than O
film O
and O
music O
industries O
( O
Thompson O
et O
al O
. O

, O
2015 O
) O
. O

Games O
are O
considered O
a O
great O
source O
of O
entertainment O
( O
Nareyek O
, O
2004 O
) O
and O
, O
due O
Preprint O
submitted O
to O
Entertainment O
Computing O
June O
12 O
, O
2017 O
arXiv:1706.02796v1 O
[ O
cs O
. O

AI O
] O
8 O
Jun O
2017 O
to O
that O
, O
the O
industry O
is O
increasingly O
investing O
more O
resources B
in O
research O
and O
development O
. O

This O
allows O
developers O
to O
create O
realistic O
graphics O
, O
deep O
narratives O
and O
complex O
artificial O
intelligence O
( O
AI O
) O
, O
leading B
to O
games B
even O
closer B
to O
reality O
( O
Machado O
et O
al O
. O

, O
2011 O
; O
Smith O
et O
al O
. O

, O
2011 O
) O
. O

The O
development O
of O
realistic O
games B
results O
in O
an O
improved O
player B
immersion O
which O
, O
in O
general O
, O
increases B
their O
satisfaction O
( O
Bowman O
and O
McMahan O
, O
2007 O
) O
. O

Although O
this O
is O
a O
well O
explored O
approach O
, O
it O
is O
not O
the O
only O
way O
to O
make O
games B
more O
attractive O
. O

According O
to O
Yannakakis O
and O
Hallam O
( O
2007 O
) O
, O
the O
player B
’s O
psychological O
factor O
makes O
direct O
influence O
to O
this O
attractiveness O
, O
requiring O
the O
game B
to O
maintain O
the O
player B
interested O
on O
it O
. O

An O
approach O
to O
captivate O
the O
player B
into O
the O
game B
experience I
is O
to O
make O
the O
challenges O
directly O
associated O
to O
the O
player B
’s O
skill I
( O
de O
Araujo O
and O
Feij´o O
, O
2013 O
) O
. O

However O
, O
a O
game B
may O
not O
suit O
the O
expectation O
of O
players B
with O
different O
skills B
. O

While O
a O
player B
may O
have O
a O
hard B
time B
in O
final O
levels B
of O
a O
game B
, O
there O
may O
be O
another O
player B
that O
can O
not O
win O
the O
initial O
ones O
. O

This O
scenario O
requires O
that O
the O
game B
dynamically O
adjusts O
itself O
presenting O
challenges O
that O
suit O
the O
needs O
and O
skills B
of O
each O
player B
. O

This O
game B
adjustment O
can O
be O
performed O
by O
a O
technique B
called B
dynamic O
difficulty O
adjustment O
( O
DDA O
) O
or O
dynamic O
difficulty O
balancing O
. O

In O
spite O
of O
different O
studies O
in O
DDA O
( O
Stanley O
et O
al O
. O

, O
2005 O
; O
Spronck O
et O
al O
. O

, O
2006 O
; O
Togelius O
et O
al O
. O

, O
2007 O
; O
Bakkes O
et O
al O
. O

, O
2009 O
; O
Wheat O
et O
al O
. O

, O
2015 O
) O
, O
none O
of O
them O
tackles O
MOBA O
( O
Multiplayer O
Online O
Battle O
Arena O
) O
games B
, O
which O
are O
one O
of O
the O
most O
played B
games B
genres O
nowadays O
, O
having O
almost O
30 O
% O
of O
online O
computing O
gameplay B
time1 O
. O

Although O
they O
are O
very O
popular O
among O
gamers B
, O
there O
is O
not O
much O
attention O
from O
researchers O
over O
this O
game B
genre O
. O

This O
can O
be O
related O
to O
the O
inherent O
challenges O
of O
developing O
competitive B
artificial O
intelligent O
agents O
for O
MOBA O
games B
as O
well O
as O
the O
constant O
updates B
and O
changes O
on O
these O
games B
. O

This O
paper O
presents O
a O
dynamic O
difficulty O
adjustment O
mechanism B
for O
MOBA O
games B
. O

The O
main O
idea O
is O
to O
create O
a O
computer O
controlled B
opponent B
that O
adapts O
dynamically O
to O
the O
player B
performance O
, O
trying O
to O
offer O
to O
the O
player B
a O
better O
game B
experience I
. O

This O
is O
done O
by O
evaluating O
the O
performance O
of O
the O
player B
using O
a O
metric O
based B
on O
some O
game B
features O
and O
switching O
the O
difficulty O
of O
the O
opponent B
’s O
artificial O
intelligence O
behavior O
accordingly O
. O

This O
idea O
was O
initially O
proposed O
in O
( O
Silva O
et O
al O
. O

, O
2015 O
; O
Silva O
, O
2015 O
) O
, O
and O
here O
we O
revisit O
the O
mechanism B
, O
giving O
more O
details O
about O
its O
implementation O
and O
performing O
1http://goo.gl/zgKjJL O
2 O
a O
set O
of O
experiments O
with O
human O
players B
in O
order O
to O
have O
a O
qualitative O
evaluation O
. O

We O
also O
present O
and O
discuss O
the O
main B
characteristics I
and O
challenges O
of O
MOBA O
games B
, O
trying O
to O
encourage O
other O
researchers O
to O
use O
them O
as O
testbeds O
in O
their O
future O
work O
. O

This O
paper O
is O
organized O
as O
follows O
: O
in O
Section O
2 O
we O
present O
the O
related O
work O
and O
background O
on O
difficulty O
balance O
; O
Section O
3 O
covers O
MOBA O
games B
aspects O
, O
history O
and O
challenges O
, O
as O
well O
as O
the O
game B
DotA O
used O
as O
testbed O
in O
this O
work O
; O
Section O
4 O
addresses O
the O
methodology O
and O
the O
proposed O
mechanism B
; O
Section O
5 O
discusses O
the O
performed O
experiments O
of O
agents O
versus O
agents O
and O
the O
obtained O
results O
; O
Section O
6 O
presents O
the O
experiments O
performed O
with O
users B
and O
the O
collected O
results O
; O
and O
finally O
, O
Section O
7 O
brings O
the O
conclusion O
and O
directions O
for O
future O
work O
. O

2 O
. O

Difficulty O
Balance O
Difficulty O
balance O
, O
or O
difficulty O
adjustment O
, O
consists O
on O
doing O
modifications O
to O
parameters O
, O
scenarios O
and/or O
game B
behaviors O
in O
order O
to O
avoid O
the O
player B
’s O
frustration O
when O
facing O
the O
game B
challenges O
( O
de O
Araujo O
and O
Feij´o O
, O
2013 O
; O
Koster O
, O
2010 O
) O
. O

According O
to O
Mateas O
( O
2002 O
) O
and O
Hunicke O
( O
2005 O
) O
, O
it O
is O
possible O
to O
adjust O
all O
game B
features O
using O
the O
correct O
algorithms O
, O
from O
storytelling O
to O
maps O
and O
level B
layouts O
, O
all O
online O
. O

These O
adjustments O
allow O
the O
game B
to O
adapt O
itself O
to O
each O
player B
, O
making O
he O
/ O
she O
entertained O
throughout O
the O
game B
. O

To O
make O
this O
possible O
, O
Andrade O
et O
al O
. O

( O
2005 O
) O
describes O
that O
the O
dynamic O
difficulty O
adjustment O
must O
attend O
three O
basic O
requirements O
. O

First O
of O
all O
, O
the O
game B
must O
automatically O
identify O
the O
players B
’ O
skills I
and O
adapt O
to O
it O
as O
fast O
as O
possible O
. O

Second O
, O
the O
game B
must O
track O
the O
player B
’s O
improvement O
and O
regressions O
, O
as O
the O
game B
must O
keep O
balance O
according O
to O
the O
player B
’s O
skill I
. O

At O
last O
, O
the O
adaptive O
process O
must O
not O
be O
explicitly O
perceived O
by O
players B
, O
keeping O
game B
states I
coherent O
to O
previous O
ones O
. O

However O
, O
before O
applying O
the O
dynamic O
difficulty O
adjustment O
, O
it O
is O
necessary O
to O
understand O
the O
meaning O
of O
difficulty O
. O

The O
meaning O
of O
difficulty O
is O
abstract O
in O
many O
ways O
and O
some O
aspects O
should O
be O
taken O
into O
account O
to O
evaluate O
and O
measure O
difficulty O
. O

For O
this O
measuring O
, O
we O
can O
consider O
level B
design O
characteristics O
( O
Bartle O
, O
2004 O
) O
, O
amount O
of O
resource B
or O
enemies B
( O
Hunicke O
, O
2005 O
) O
, O
amount O
of O
victories B
or O
losses O
( O
Poole O
, O
2004 O
; O
Xavier O
, O
2010 O
) O
, O
among O
other O
metrics O
. O

Nevertheless O
, O
dynamic O
difficulty O
adjustment O
is O
not O
as O
simple O
as O
just O
giving O
player B
additional O
health O
items B
when O
in O
trouble O
. O

This O
problem O
requires O
estimation O
of O
time B
and O
intervention O
in O
the O
3 O
right O
moment O
, O
since O
maintaining O
the O
player B
entertained O
is O
a O
complex O
task O
in O
an O
interactive O
context O
( O
Hunicke O
, O
2005 O
) O
. O

A O
wide B
range I
of O
tasks O
and O
challenge O
levels B
can O
be O
found O
in O
games B
. O

For O
example O
, O
tasks O
that O
require O
high O
skill B
and O
synchronism O
( O
First O
Person O
Games O
) O
, O
tasks O
that O
require O
logic O
and O
problem O
solving O
skills B
( O
Puzzles O
) O
, O
tasks O
related O
to O
planning O
( O
Strategy O
games B
) O
, O
and O
so O
on O
( O
Klimmt O
et O
al O
. O

, O
2009 O
) O
. O

According O
to O
Klimmt O
et O
al O
. O

( O
2009 O
) O
, O
there O
is O
evidence O
that O
the O
completion O
of O
tasks O
and O
challenge O
overcoming O
are O
directly O
related O
to O
player B
satisfaction I
and O
fun O
. O

Yannakakis O
( O
2008 O
) O
developed O
a O
study O
about O
the O
most O
popular O
approaches O
for O
player B
modeling O
during O
interaction O
with O
entertainment O
systems O
. O

According O
to O
this O
study O
, O
most O
qualitative O
approaches O
proposed O
for O
player B
entertainment O
modeling O
tends O
to O
be O
based B
in O
conceptual O
definitions O
proposed O
by O
Malone O
( O
1981 O
) O
and O
Csikszentmihalyi O
( O
1991 O
) O
. O

Malone O
( O
1981 O
) O
defended O
the O
need O
for O
a O
specific O
motivation O
during O
gameplay B
to O
entertain O
the O
player B
. O

The O
necessary O
features O
to O
reach O
such O
motivation O
are O
: O
fantasy O
, O
control B
, O
challenges O
and O
curiosity O
. O

The O
use O
of O
fantasy O
as O
part O
of O
game B
world B
could O
improve O
player B
motivation O
, O
creating O
objects B
, O
scenarios O
or O
situations O
that O
the O
player B
could O
explore O
. O

Control B
is O
a O
player B
feeling O
through O
which O
he O
/ O
she O
is O
part O
of O
game B
control B
. O

Given O
the O
interaction O
of O
games B
, O
all O
of O
them O
make O
the O
player B
feel O
involved O
in O
game B
control B
and O
the O
control B
levels B
can O
change O
from O
game B
to O
game B
. O

Challenge O
implies O
that O
the O
game B
should O
pursue O
tasks O
and O
goals B
in O
an O
adequate O
level B
, O
making O
the O
player B
feel O
challenged O
to O
his O
/ O
her O
limits O
. O

The O
uncertainty O
of O
completing O
tasks O
or O
goals B
provided O
by O
game B

 O
mechanics I
encourages O
the O
player B
motivation O
. O

Finally O
, O
curiosity O
suggests O
that O
game B
information O
must O
be O
complex O
and O
unknown O
, O
to O
encourage O
exploration O
and O
reorganization O
of O
information O
by O
players B
. O

Games B
must O
pursue O
multiple O
situations O
or O
scenarios O
from O
the O
main O
course O
since O
it O
helps O
to O
stimulate O
the O
player B
to O
explore O
the O
unknown O
( O
Malone O
, O
1981 O
; O
Egenfeldt O
- O
Nielsen O
et O
al O
. O

, O
2013 O
) O
. O

The O
qualitative O
approach O
proposed O
by O
Csikszentmihalyi O
( O
1991 O
) O
is O
called B
flow O
theory O
or O
flow O
model O
. O

According O
to O
the O
author O
, O
flow O
is O
a O
mental O
state O
experienced B
when O
the O
user B
is O
executing O
an O
activity O
in O
which O
he O
/ O
she O
is O
immersed O
, O
feeling O
focused B
, O
completely O
involved O
and O
fulfilled O
during O
task O
execution B
. O

So O
, O
this O
model O
takes O
into O
account O
the O
psychological O
steps O
that O
players B
reach O
during O
gameplay B
. O

In O
this O
sense O
, O
the O
main O
goal B
is O
controlling B
the O
challenge O
levels B
aiming O
to O
maintain O
the O
player B
inside O
the O
flow O
, O
avoiding O
to O
reach O
boredom O
( O
no O
challenges O
at O
all O
) O
or O
frustration O
( O
challenges O
are O
too O
hard B
) O
. O

Figure O
1 O
show O
a O
graph O
of O
flow O
theory O
presented O
by O
Csikszentmihalyi O
( O
1991 O
) O
. O

The O
model O
presented O
by O
Csikszentmihalyi O
shows O
how O
a O
task O
difficulty O
is O
4 O
Figure O
1 O
: O
Diagram O
of O
flow O
theory O
, O
by O
Csikszentmihalyi O
. O

directly O
related O
to O
the O
perception O
of O
who O
is O
executing O
it O
. O

The O
flow O
channel O
illustrates O
that O
difficulty O
can O
be O
progressively O
improved O
, O
since O
there O
exists O
time B
to O
the O
player B
to O
learn O
and O
improve O
his O
/ O
her O
skills B
to O
overcome O
this O
challenge O
( O
Csikszentmihalyi O
, O
2000 O
) O
. O

Thereby O
, O
this O
model O
avoids O
frustration O
of O
very O
hard B
situations O
or O
boredom O
caused O
by O
very O
easy B
situations O
. O

Furthermore O
, O
Csikszentmihalyi O
and O
Nakamura O
( O
2010 O
) O
go O
beyond O
and O
determine O
that O
the O
ratio O
of O
challenges O
to O
skills B
should O
be O
around O
50/50 O
in O
order O
to O
produce O
enjoyable O
experiences B
. O

On O
the O
other O
hand O
, O
there O
are O
some O
studies O
that O
question O
if O
the O
ratio O
of O
challenges O
to O
skills B
is O
really O
a O
measure O
of O
flow O
. O

Løvoll O
and O
Vittersø O
( O
2014 O
) O
, O
for O
instance O
, O
present O
a O
work O
with O
some O
empirical O
evidence O
that O
contests O
the O
idea O
that O
flow O
is O
produced O
when O
challenges O
and O
skills B
are O
harmonized O
. O

According O
to O
them O
, O
the O
interaction O
between O
challenges O
and O
skills B
as O
independent O
variables O
does O
not O
support B
the O
challenge O
skill B
ratio O
proposed O
by O
Csikszentmihalyi O
and O
Nakamura O
. O

In O
a O
different O
approach O
, O
if O
we O
can O
balance O
the O
fantasy O
, O
control B
, O
challenge O
and O
curiosity O
proposed O
by O
Malone O
( O
1981 O
) O
and O
associate O
it O
to O
the O
progressive O
development O
of O
difficulty O
presented O
by O
Csikszentmihalyi O
( O
1991 O
) O
, O
it O
is O
possible O
that O
the O
resulting O
game B
can O
entertain O
the O
player B
. O

However O
, O
using O
just O
these O
features O
is O
not O
sufficient O
to O
show O
if O
game B
challenges O
are O
compatible O
with O
player B
skills I
. O

So O
, O
it O
is O
necessary O
measuring O
techniques B
to O
define O
when O
and O
how O
difficulty O
should O
be O
adjusted O
. O

5 O
2.1 O
. O

Evaluating O
the O
Difficulty O
Level O
According O
to O
Andrade O
et O
al O
. O

( O
2005 O
) O
, O
there O
are O
some O
different O
approaches O
to O
dynamically O
balance O
the O
difficulty O
level B
of O
a O
game B
. O

However O
, O
all O
of O
these O
approaches O
require O
measuring O
, O
implicitly O
or O
explicitly O
, O
the O
difficulty O
level B
that O
the O
player B
is O
facing O
on O
that O
moment O
. O

This O
measurement O
can O
be O
done O
by O
using O
heuristics O
, O
for O
example O
the O
success O
rate O
of O
skill B
landing O
, O
the O
capture B
of O
enemy B
points B
, O
the O
time B
used O
to O
complete O
a O
task O
or O
any O
other O
metric O
that O
can O
evaluate O
the O
player B
. O

Missura O
and O
G¨artner O
( O
2009 O
) O
made O
a O
relation O
between O
game B
runtime I
, O
health O
and O
score O
in O
a O
way O
that O
it O
composes O
an O
evaluation O
criteria O
that O
performs O
the O
game B
difficulty O
adjustment O
. O

Demasi O
and O
Adriano O
( O
2003 O
) O
developed O
a O
heuristic O
function O
called B
“ O
Challenge O
Function O
” O
that O
is O
responsible O
for O
describing O
the O
game B
state I
, O
and O
tries O
to O
show O
how O
hard B
the O
game B
is O
for O
the O
player B
in O
a O
given O
time B
. O

Another O
way O
to O
track O
difficulty O
levels B
is O
using O
some O
physiological O
signs O
, O
informally O
called B
body O
language O
. O

Van O
Den O
Hoogen O
et O
al O
. O

( O
2008 O
) O
mentions O
that O
the O
body O
language O
of O
a O
player B
could O
be O
related O
to O
his O
/ O
her O
experience B
during O
play B
. O

According O
to O
the O
authors O
, O
there O
are O
evidences O
that O
show O
that O
specific O
postures O
, O
facial O
expressions O
, O
eye O
movements B
, O
stress O
over O
mouse O
/ O
keyboard O
/ O
joystick O
, O
and O
others O
, O
could O
evidence O
experiences B
like O
interest O
, O
excitement O
, O
frustration O
and O
boredom O
. O

For O
the O
evaluation O
of O
player B
experience B
, O
authors O
created O
a O
monitoring O
ambient O
, O
placing O
pressure O
sensors O
at O
different O
devices O
( O
mouse O
, O
chair O
, O
etc O
) O
. O

Also O
cameras O
were O
placed O
to O
register O
movements B
and O
facial O
expression O
. O

The O
results O
of O
this O
experiment O
show O
that O
the O
behaviors O
observed O
are O
directly O
related O
to O
the O
excitement O
level B
and O
dominance O
felt O
during O
the O
game B
. O

Nacke O
and O
Lindley O
( O
2008 O
) O
, O
besides O
using O
cameras O
to O
capture B
body O
language O
, O
also O
used O
electrodes O
to O
track O
mental O
reaction O
from O
players B
during O
a O
First O
Person O
Shooter O
( O
FPS O
) O
match B
. O

The O
results O
obtained O
during O
player B
monitoring O
were O
based B
on O
the O
flow O
theory O
proposed O
by O
Csikszentmihalyi O
( O
1991 O
) O
, O
therefore O
, O
authors O
could O
observe O
if O
the O
players B
were O
inside O
the O
flow O
, O
anxious O
or O
bored O
during O
the O
gameplay B
. O

Although O
the O
explicit O
measuring O
( O
external O
monitoring O
) O
of O
difficulty O
levels B
could O
provide O
fine O
results O
related O
to O
game B
fitness O
to O
player B
’s O
skill I
, O
it O
is O
impracticable O
to O
the O
dynamic O
difficulty O
adjustment O
. O

Not O
all O
players B
have O
measuring O
tools O
at O
home O
and O
using O
such O
tools O
could O
be O
intrusive O
, O
since O
this O
could O
make O
the O
player B
uncomfortable O
by O
being O
monitored O
. O

Implicit O
approaches O
( O
metrics O
and O
heuristics O
) O
do O
not O
need O
external O
equipment B
, O
therefore O
these O
approaches O
are O
more O
popular O
among O
game B
developers O
. O

Besides O
, O
they O
contribute O
to O
the O
6 O
fact O
that O
players B
must O
not O
perceive O
that O
difficulty O
is O
being O
adjusted O
during O
gameplay B
. O

This O
paper O
tries O
to O
perform O
a O
dynamic O
difficulty O
adjustment O
through O
the O
development O
of O
a O
mechanism B
that O
switches O
between O
three O
distinct O
levels B
of O
artificial O
intelligence O
in O
order O
to O
provide O
an O
opponent B
that O
better O
suits O
the O
player B
’s O
abilities I
. O

The O
mechanism B
performs O
several O
evaluations O
during O
the O
match B
, O
detecting O
the O
moments O
in O
which O
the O
game B
is O
unbalanced O
, O
and O
then O
executes O
the O
difficulty O
adjustment O
. O

3 O
. O

Multiplayer O
Online O
Battle O
Arena O
Multiplayer O
Online O
Battle O
Arena O
, O
also O
known O
as O
Action O
Real B
- I
time I
Strategy I
, O
or O
simply O
as O
MOBA O
, O
is O
a O
genre O
originated O
from O
Real O
- O
Time O
Strategy O
( O
RTS O
) O
, O
as O
a O
modification O
of O
the O
original O
game B
. O

The O
first O
known O
MOBA O
game B
is O
Aeon O
of O
Strife O
, O
created O
from O
the O
game B
Starcraft O
. O

In O
this O
game B
, O
the O
player B
should O
choose O
an O
unit B
and O
work O
his O
/ O
her O
way O
to O
conquer O
the O
enemy B
’s O
base I
with O
the O
chosen O
unit B
and O
its O
special B
powers I
. O

This O
game B
structure O
was O
maintained O
through O
the O
improvement O
of O
MOBAs B
. O

One O
interesting O
fact O
is O
that O
these O
games B
came O
up O
from O
simple O
fan O
made O
games B
to O
become O
one O
of O
the O
most O
played B
genres O
in O
the O
world B
, O
as O
will O
be O
discussed O
later O
in O
this O
section O
. O

Another O
interesting O
fact O
is O
that O
according O
to O
Johnson O
et O
al O
. O

( O
2015 O
) O
and O
Kwak O
et O
al O
. O

( O
2015 O
) O
, O
MOBA O
games B
were O
found O
to O
offer O
less O
autonomy O
, O
more O
frustration O
and O
more O
challenges O
to O
players B
. O

These O
findings O
with O
respect O
to O
autonomy O
seems O
most O
likely O
to O
be O
a O
function O
of O
the O
fact O
that O
MOBA O
games B
involve O
fairly O
focused B
competition B
with O
other O
players B
. O

Moreover O
, O
the O
greater O
levels B
of O
frustration O
experienced B
may O
also O
be O
a O
function O
of O
the O
focused O
competition B
that O
occurs O
in O
MOBA O
games B
and O
the O
steep O
learning O
curve O
. O

With O
less O
focus B
on O
the O
qualities O
of O
the O
game B
and O
greater O
focus B
on O
competing O
and O
cooperating O
with O
others O
, O
there O
is O
more O
potential O
for O
frustration O
with O
the O
performance O
of O
other O
players B
. O

This O
interpretation O
is O
supported B
by O
players B
reporting B
a O
greater O
challenge O
when O
playing B
MOBA O
games B
( O
Silva O
and O
Chaimowicz O
, O
2015b O
) O
. O

Due O
to O
these O
characteristics O
, O
the O
use O
of O
a O
mechanism B
that O
performs O
the O
difficulty O
balance O
dynamically O
seems O
to O
be O
a O
viable O
alternative O
to O
minimize O
and/or O
avoid O
that O
such O
frustrations O
be O
experienced B
by O
the O
players B
. O

In O
this O
section O
we O
present O
the O
MOBA O
history O
, O
its O
characteristics O
and O
gameplay B
. O

Then O
, O
we O
discuss O
the O
unique O
features O
present O
in O
DotA O
, O
the O
selected O
platform O
to O
be O
used O
as O
testbed O
in O
this O
work O
, O
and O
why O
it O
is O
so O
hard B
to O
develop O
AI O
agents O
to O
play B
against O
humans O
on O
these O
games B
. O

7 O
3.1 O
. O

History O
As O
Real B
- I
Time I
Strategy O
( O
RTS O
) O
games B
became O
popular O
, O
we O
observed O
the O
urge O
of O
the O
players B
to O
create O
their O
own O
maps O
and O
gameplay B
styles I
. O

This O
phenomena O
resulted O
in O
a O
community B
of O
developers O
that O
later O
came O
to O
be O
known O
as O
modders O
. O

Their O
work O
was O
known O
as O
mods O
, O
an O
acronym O
to O
the O
word O
“ O
modification O
” O
. O

In O
those O
mods O
, O
players B
could O
use O
the O
original O
game B
environment O
to O
play B
by O
their O
own O
rules B
, O
allowing O
them O
to O
create O
a O
fantasy O
world B
beyond O
the O
limits O
of O
the O
original O
game B
. O

Released O
in O
1998 O
, O
Aeon O
of O
Strife O
was O
the O
first O
mod O
to O
present O
a O
unique O
characteristic O
that O
caught O
RTS O
players B
attention O
. O

Instead O
of O
focusing B
on O
resource B
collection O
and O
base B
construction O
, O
the O
mod O
valued O
the O
player B
ability I
to O
control B
a O
single O
unit B
, O
an O
ability B
known O
as O
micromanagement O
. O

This O
characteristic O
invited O
players B
to O
duel B
against O
each O
other O
into O
single O
or O
teams B
battles B
, O
showing O
to O
be O
a O
successful O
approach O
to O
get O
players B
involved O
in O
the O
game B
. O

Aeon O
of O
Strife O
inspired O
many O
other O
mods O
that O
followed O
its O
guidelines O
. O

Later O
in O
2005 O
, O
one O
of O
those O
mods O
stood O
out O
in O
the O
crowd O
: O
Defense B
of O
the O
Ancients O
( O
DotA O
) O
. O

The O
platform O
was O
not O
Starcraft O
anymore O
, O
but O
another O
game B
developed O
by O
Blizzard O
: O
Warcraft O
III O
. O

The O
game B
had O
the O
perfect O
environment O
and O
an O
open B
API O
that O
allowed O
the O
modders O
to O
do O
their O
job O
. O

Therefore O
, O
they O
created O
the O
DotA O
map O
where O
players B
would O
assume O
the O
control B
of O
a O
single O
unit B
called B
Hero O
and O
develop O
this O
unit B
by O
defeating B
enemies B
, O
just O
like O
in O
a O
Role B
Playing O
Game O
( O
RPG O
) O
. O

Every O
hero B
has O
a O
single O
set O
of O
attributes O
and O
powers B
, O
characterizing O
them O
in O
a O
role B
. O

The O
player B
could O
then O
choose O
a O
hero B
based B
on O
its O
team B
needs O
or O
on O
its O
own O
gameplay B
style I
. O

The O
game B
story O
of O
the O
DotA O
map O
were O
also O
inherited O
from O
the O
Warcraft O
myth O
: O
the O
war O
between O
two O
races O
in O
the O
Warcraft O
world B
, O
the O
Night O
Elf O
and O
the O
Undead B
. O

Thereby O
, O
players B
were O
invoked O
to O
defend O
a O
main O
structure O
called B
Ancient O
, O
which O
must O
be O
destroyed O
in O
order O
to O
achieve O
the O
game B
goal I
. O

The O
game B
is O
divided O
into O
two O
teams B
with O
five O
players B
each O
: O
The O
Sentinel O
, O
having O
the O
Tree O
of O
Life O
as O
Ancient O
; O
and O
The O
Scourge O
, O
having O
the O
Frozen O
Throne O
as O
Ancient O
. O

Screen O
shots O
of O
the O
team B
bases O
can O
be O
found O
in O
Figure O
2 O
. O

The O
DotA O
popularity O
among O
players B
resulted O
in O
behavioral O
changes O
in O
the O
general O
gameplay B
. O

Instead O
of O
just O
playing B
on O
LANs O
, O
players B
were O
excited O
about O
playing B
on O
the O
Internet O
. O

At O
that O
time B
, O
the O
broadband O
was O
expanding O
all O
around O
the O
world B
and O
players B
wanted O
to O
test O
it O
, O
as O
well O
as O
test O
their O
skills B
challenging O
others O
around O
the O
world B
. O

There O
were O
platforms O
like O
Garena O
that O
had O
dedicated O
servers B
only O
for O
DotA O
matches B
. O

DotA O
’s O
gameplay B
became O
so O
famous O
that O
it O
inspired O
the O
game B
industry O
to O
create O
professional O
games B
based B
8 O
Figure O
2 O
: O
The O
Sentinel O
base B
( O
top O
) O
and O
the O
Scourge O
base B
( O
bottom B
) O
. O

on O
this O
play B
style I
. O

In O
2009 O
, O
Riot O
Games O
released O
a O
game B
called B
League O
of O
Legends O
( O
LoL O
) O
( O
Games O
and O
Games O
, O
2009 O
) O
, O
with O
characteristics O
very O
similar O
to O
DotA. O
This O
company O
created O
the O
term O
MOBA O
, O
referring O
to O
their O
debuting O
title O
as O
a O
Multiplayer O
Online O
Battle O
Arena O
. O

Later O
, O
Valve O
has O
released O
its O
own O
game B
, O
known O
as O
Dota2 O
, O
that O
immediately O
caught O
the O
attention O
of O
the O
world B
and O
media O
because O
of O
the O
1 O
million O
Dollar O
tournament B
. O

Lastly O
, O
around O
2010 O
, O
S2 O
games B
have O
released O
its O
own O
title O
, O
Heroes O
of O
Newerth O
, O
that O
has O
similar O
characteristics O
to O
DotA O
, O
Dota2 O
and O
LoL. O
Nowadays O
, O
there O
are O
other O
titles O
, O
such O
as O
Strife O
and O
Heroes O
of O
the O
Storm O
, O
but O
they O
did O
not O
get O
many O
players B
as O
the O
games B
released O
before O
. O

In O
numbers O
, O
we O
can O
see O
that O
MOBA O
genre O
is O
a O
world B
success O
, O
as O
shown O
in O
Figure O
3 O
. O

By O
2012 O
, O
the O
game B
League O
of O
Legends O
has O
overcome O
World B
of O
Warcraft O
as O
the O
most O
played B
game B
in O
the O
world B
( O
Gaudiosi O
, O
2012 O
) O
. O

In O
November O
2015 O
, O
as O
reported B
by O
Raptr O
, O
League O
of O
Legends O
alone O
represented O
more O
than O
22 O
% O
of O
the O
worldwide O
gameplay1 O
. O

There O
are O
international O
eSports O
competitions B
involving O
those O
games B
and O
millionaire O
prizes B
. O

In O
2015 O
, O
for O
example O
, O
1http://goo.gl/zgKjJL O
9 O
Figure O
3 O
: O
Ten O
most O
played B
games B
of O
2015 O
. O

Source O
: O
Raptr O
/ O
Statista O
the O
official O
Valve O
’s O
World B
Tournament B
of O
Dota2 O
called B
“ O
The O
International O
” O
distributed O
a O
total O
of O
US$ O
18 O
million2 O
. O

3.2 O
. O

Gameplay O
and O
Characteristics O
To O
provide O
challenges O
that O
suit O
the O
player B
’s O
skills I
it O
is O
necessary O
to O
comprehend O
the O
gameplay B
that O
involves O
the O
game B
. O

The O
MOBA O
game B
can O
be O
summarized O
into O
two O
teams B
playing B
against O
each O
other O
: O
Team B
1 O
and O
Team O
2 O
. O

Players B
on O
the O
Team O
1 O
are O
based B
at O
the O
southwest O
corner O
of O
the O
map O
, O
and O
those O
on O
the O
Team O
2 O
are O
based B
at O
the O
northeast O
corner O
. O

Each O
base B
is O
defended O
by O
towers B
and O
waves B
of O
NPC O
units B
( O
called B
creeps B
) O
that O
guard O
the O
main O
paths B
leading B
to O
their O
base B
, O
called B
lanes B
. O

In O
the O
center O
of O
each O
base B
there O
is O
one O
main O
structure O
. O

This O
structure O
is O
the O
goal B
of O
the O
game B
, O
which O
the O
enemy B
must O
destroy O
in O
order O
to O
win O
the O
match B
. O

The O
teams B
are O
composed O
by O
five O
players B
, O
where O
each O
player B
controls B
one O
specific O
and O
powerful B
unit I
with O
unique O
abilities B
, O
which O
is O
called B
Hero O
or O
Champion O
. O

In O
most O
MOBAs B
, O
players B
on O
each O
team B
choose O
one O
from O
dozens O
of O
heroes B
, O
each O
with O
different O
abilities B
and O
tactical O
advantages B
over O
the O
others O
. O

2http://goo.gl/6iXfMD O
10 O
Figure O
4 O
: O
General O
map O
layout O
from O
MOBA O
games B
. O

The O
scenario O
is O
highly B
team O
- I
oriented I
: O
it O
is O
difficult O
for O
one O
player B
to O
lead B
the O
team B
to O
victory B
by O
himself O
/ O
herself O
. O

Since O
the O
gameplay B
goes O
around O
strengthening O
individual O
heroes B
, O
it O
does O
not O
require O
focus B
on O
resource B
management O
and O
base B
- I
building I
, O
unlike O
most O
traditional O
RTS O
games B
. O

When O
killing B
enemy B
or O
neutral B
units I
, O
the O
player B
gains O
experience B
points I
and O
when O
enough O
experience B
is O
accumulated O
the O
player B
increases B
his O
/ O
her O
level B
. O

Leveling B
up O
improves O
the O
hero B
’s O
toughness O
and O
the O
damage B
it O
inflicts O
, O
allowing O
players B
to O
upgrade O
spells B
or O
skills B
. O

In O
addition O
to O
accumulating O
experience B
, O
players B
also O
manage O
a O
single O
resource B
of O
gold B
that O
can O
be O
used O
to O
buy O
items B
such O
as O
armory B
, O
potions B
, O
among O
others O
. O

Besides O
a O
small O
periodic O
income O
, O
heroes B
can O
earn O
gold B
by O
killing B
hostile O
units B
, O
towers B
, O
base B
structures O
, O
and O
enemy B
heroes I
. O

With O
gold B
, O
players B
can O
buy O
items B
to O
strengthen O
their O
hero B
and O
gain O
abilities B
. O

Also O
, O
certain O
items B
can O
be O
combined O
with O
recipes O
to O
create O
more O
powerful B
items I
. O

Buying O
items B
that O
suit O
the O
chosen O
hero B
is O
an O
important O
tactical O
element O
of O
the O
game B
. O

3.3 O
. O

Map O
The O
map O
is O
segmented O
into O
three O
different O
lanes B
, O
the O
top O
, O
the O
bottom B
, O
and O
the O
middle B
lane I
. O

Each O
one O
of O
these O
lanes B
leads B
to O
the O
other O
team B
’s O
base I
, O
guarded O
by O
towers B
along O
the O
way O
. O

Figure O
4 O
represents O
a O
general O
MOBA O
map O
with O
its O
lanes B
, O
bases O
and O
towers B
along O
each O
lane B
. O

11 O
The O
map O
area B
located O
between O
the O
lanes B
is O
called B
jungle B
. O

This O
is O
where O
neutral O
creeps B
can O
be O
found O
, O
which O
can O
be O
killed B
for O
gathering O
more O
gold B
and O
experience B
points I
. O

It O
is O
possible O
to O
level B
up O
by O
killing B
creeps B
in O
the O
jungle B
instead O
of O
in O
the O
lanes B
. O

This O
practice O
is O
called B
jungling O
. O

During O
the O
early B
laning O
phase O
of O
the O
game B
, O
most O
gameplay B
is O
centered O
around O
“ O
farming B
” O
. O

It O
means O
that O
players B
focus B
on O
collecting O
resources B
and O
leveling B
up O
their O
heroes B
by O
defeating B
enemy B
units B
, O
like O
creeps B
or O
heroes B
. O

In O
the O
case O
of O
the O
junglers B
, O
they O
walk O
through O
the O
jungle B
and O
kill B
neutral B
units I
. O

Further O
, O
the O
junglers B
and O
their O
team B
seek O
for O
failures O
on O
the O
enemy B
teams I
’ O
strategy B
, O
looking O
for O
catching O
them O
in O
traps B
or O
performing O
gang O
killing O
, O
the O
so O
called B
ganks B
. O

Lastly O
, O
there O
is O
the O
late B
game I
phase O
, O
when O
the O
gameplay B
is O
commonly O
focused B
on O
teamfights B
, O
i.e. O
, O
teams B
use O
their O
heroes B
to O
fight B
in O
groups B
, O
looking O
for O
weakening O
the O
enemy B
team I
and O
pushing O
the O
lane B
towards O
the O
enemy B
’s O
base I
. O

Each O
team B
has O
defensive O
towers B
placed O
along O
the O
lanes B
leading B
to O
the O
Ancient O
. O

Those O
towers B
inflict O
heavy O
single B
target I
damage B
to O
heroes B
and O
creeps B
. O

In O
the O
early B
stages B
of O
the O
game B
, O
a O
hero B
can O
only O
take O
a O
few O
hits O
from O
a O
tower B
before O
dying O
, O
so O
one O
must O
be O
careful O
as O
to O
not O
get O
in O
a O
bad O
positioning O
relatively O
to O
the O
towers B
until O
they O
have O
gained O
enough O
strength B
. O

In O
the O
Figure O
4 O
the O
towers B
are O
represented O
by O
little O
circles O
placed O
in O
the O
lanes B
. O

3.4 O
. O

Defense B
of O
the O
Ancients O
The O
game B
Defense O
of O
the O
Ancients O
( O
DotA O
) O
is O
a O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
mod O
version O
of O
the O
game B
Warcraft O
III O
: O
Reign O
of O
Chaos O
and O
later O
to O
its O
expansion O
, O
Warcraft O
III O
: O
The O
Frozen O
Throne O
. O

The O
scenario O
objective B
is O
for O
each O
team B
to O
destroy O
the O
opponents B
’ O
Ancient O
, O
heavily O
guarded O
structures O
located O
at O
opposing O
corners O
of O
the O
map O
. O

Players B
use O
powerful B
units I
known O
as O
heroes B
, O
and O
are O
assisted B
by O
allied O
heroes B
( O
played B
by O
other O
users B
) O
and O
AI O
- O
controlled B
fighters B
known O
as O
creeps B
. O

As O
in B
role O
- I
playing I
games B
, O
players B
level B
up O
their O
heroes B
and O
use O
gold B
to O
buy O
items B
and O
equipment B
during O
the O
match B
. O

Moreover O
, O
since O
DotA O
is O
a O
mod O
of O
the O
game B
Warcraft O
III O
: O
Reign O
of O
Chaos O
, O
it O
becomes O
easier B
to O
modify O
because O
we O
can O
use O
the O
tools O
made O
to O
edit O
Warcraft O
maps O
to O
do O
it O
. O

Therefore O
, O
the O
game B
Defense O
of O
the O
Ancients O
( O
DotA O
) O
was O
chosen O
to O
be O
the O
testbed O
of O
this O
work O
. O

12 O
3.5 O
. O

Game B
Adaptations O
To O
use O
the O
game B
Defense O
of O
the O
Ancients O
as O
a O
testbed O
, O
some O
adaptations O
were O
made O
in O
order O
to O
better O
suit O
the O
needs O
of O
this O
work O
. O

The O
original O
game B
allows O
the O
player B
to O
choose O
his O
/ O
her O
hero B
among O
110 O
different O
options O
. O

But O
, O
for O
this O
work O
, O
we O
chose O
to O
restrict O
this O
quantity O
to O
only O
10 O
heroes B
, O
equally O
distributed O
between O
both O
teams B
. O

Each O
hero B
has O
distinct O
characteristics O
, O
behaviors O
and O
abilities B
. O

Thereby O
, O
to O
better O
focus B
on O
the O
strategies B
and O
the O
development O
of O
abilities B
, O
we O
designed O
our O
artificial O
intelligence O
agent O
to O
control B
one O
specific O
hero B
. O

The O
selection O
performed O
was O
random O
and O
the O
chosen O
character B
is O
Lion O
- O
The O
Demon O
Witch O
. O

Given O
this O
choice O
, O
it O
became O
possible O
to O
classify O
which O
abilities B
and O
behaviors O
should O
be O
implemented O
so O
that O
the O
artificial O
intelligence O
agent O
would O
work O
with O
a O
consistent O
behavior O
during O
the O
game B
match I
. O

Figure O
5 O
shows O
a O
screenshot B
of O
the O
character B
Lion O
- I
The I
Demon O
Witch O
during O
a O
game B
match I
. O

Figure O
5 O
: O
Screenshot B
of O
the O
hero B
during O
a O
match B
. O

The O
DotA O
game B
also O
offers O
a O
variety O
of O
game B
modes I
, O
selected O
by O
the O
game B
host B
at O
the O
beginning O
of O
the O
match B
. O

The O
game B
modes I
dictate O
the O
difficulty O
of O
the O
scenario O
, O
as O
well O
as O
whether O
people O
can O
choose O
their O
hero B
or O
are O
assigned O
one O
randomly O
. O

Many O
game B
modes I
can O
be O
combined O
, O
allowing O
more O
flexible O
options O
. O

In O
this O
work O
we O
restrict O
the O
game B
mode I
to O
single O
selection O
, O
that O
means O
that O
a O
player B
does O
not O
receive O
a O
random B
hero I
, O
but O
is O
allowed O
to O
select O
among O
nine O
others O
, O
because O
Lion O
is O
automatically O
picked B
by O
the O
AI O
agent O
. O

13 O
3.6 O
. O

The O
Challenges O
of O
Developing O
a O
MOBA O
agent O
Developing O
agents O
capable O
of O
defeating B
competitive B
human O
players B
in O
MOBA O
games B
remains O
an O
open B
research O
challenge O
. O

According O
to O
Buro O
( O
2003 O
) O
and O
Weber O
et O
al O
. O

( O
2010 O
) O
, O
improving O
the O
capabilities O
of O
computer O
opponents B
would O
increase B
the O
game B
playing O
experience I
and O
provide O
several O
interesting O
research O
questions O
for O
the O
artificial O
intelligence O
community B
. O

However O
, O
developing O
an O
AI O
agent O
to O
play B
MOBA O
games B
is O
not O
a O
simple O
task O
( O
Silva O
and O
Chaimowicz O
, O
2015a O
) O
. O

Although O
there O
are O
several O
AI O
agents O
for O
all O
the O
different O
MOBA O
distributions O
, O
none O
of O
them O
can O
perform O
as O
well O
as O
expert O
human O
players B
. O

One O
of O
the O
reasons O
for O
this O
is O
due O
to O
the O
inability O
of O
AI O
systems O
to O
learn O
from O
experience B
. O

Human O
players B
only O
need O
a O
couple O
of O
matches B
to O
identify O
opponents B
’ O
weaknesses B
and O
use O
them O
in O
their O
favor O
in O
upcoming O
games B
. O

Current O
machine O
learning O
approaches O
in O
this O
area B
are O
not O
good O
enough O
when O
compared O
to O
expert O
humans O
skills B
( O
Buro O
, O
2003 O
; O
Weber O
et O
al O
. O

, O
2011 O
) O
. O

Yet O
, O
according O
to O
Buro O
( O
2004 O
) O
, O
some O
commercial O
game B
AI O
systems O
may O
outperform O
human O
players B
and O
may O
even O
create O
challenging O
encounters O
, O
but O
they O
do O
not O
advance O
our O
understanding O
on O
how O
to O
create O
intelligent O
entities B
, O
since O
it O
cheats B
to O
compensate O
its O
lack O
of O
sophistication O
by O
using O
map O
revealing O
and O
faster O
resource B
gathering O
. O

Since O
MOBA O
games B
are O
originated O
from O
Real O
- O
Time B
Strategy O
( O
RTS O
) O
genre O
, O
many O
of O
the O
challenges O
that O
surround O
RTS O
games B
can O
also O
be O
applied O
to O
MOBA O
. O

A O
case O
study O
for O
real B
- I
time I
AI O
problems O
in O
the O
context O
of O
RTS O
games B
can O
be O
found O
in O
Buro O
( O
2003 O
, O
2004 O
) O
; O
Buro O
and O
Furtak O
( O
2004 O
) O
. O

As O
discussed O
before O
, O
MOBA O
provides O
a O
complex O
environment O
, O
populated O
with O
dynamic O
and O
static O
features O
. O

Moreover O
, O
MOBA O
characteristics O
tends O
to O
make O
the O
game B
more O
dynamic O
than O
its O
precursor O
, O
the O
RTS O
genre O
. O

Fights B
, O
duels B
, O
and O
actions O
happen O
in O
a O
short O
time B
, O
all O
requiring O
the O
computation O
of O
complex O
algorithms O
to O
analyze O
the O
scenario O
and O
to O
reason O
about O
it O
. O

For O
instance O
, O
teamfights B
normally O
last O
a O
few O
seconds O
and O
the O
agent O
has O
to O
perform O
a O
large O
amount O
of O
computation O
in O
a O
short O
time B
, O
to O
reason O
about O
allies B
, O
enemies B
, O
and O
strategies B
. O

Even O
for O
humans O
, O
it O
is O
difficult O
to O
maintain O
the O
total O
control B
of O
the O
situation O
during O
these O
fights B
. O

Although O
not O
having O
the O
macromanagement O
that O
occurs O
in O
RTS O
games B
, O
MOBA O
matches B
require O
the O
player B
to O
reason O
about O
thousands O
of O
combinations O
of O
spells B
and O
items B
. O

The O
spell B
leveling O
order O
, O
item B
buying O
and O
building O
order O
matter O
, O
because O
each O
spell B
and O
each O
item B
has O
its O
own O
characteristics O
, O
making O
a O
special B
ability I
that O
highlights B
the O
hero B
early B
in O
the O
game B
. O

Such O
combinations O
should O
take O
into O
account O
the O
enemy B
that O
is O
being O
faced O
by O
the O
agent O
, O
the O
14 O
opponent B
team I
in O
general O
, O
the O
items B
and O
combinations O
from O
its O
own O
team B
, O
among O
many O
other O
features O
in O
the O
game B
. O

Even O
more O
, O
those O
features O
are O
not O
aways O
clear B
to O
be O
translated O
in O
a O
language O
that O
can O
be O
easily O
understood O
by O
the O
agent O
, O
since O
it O
requires O
experience B
and O
sometimes O
knowledge O
that O
goes O
beyond O
the O
game B
itself O
. O

Being O
a O
commercial O
game B
genre O
, O
MOBA O
provides O
a O
rich O
hero B
pool O
, O
allowing O
the O
player B
to O
choose O
among O
hundreds O
of O
heroes B
. O

Performing O
combinations O
of O
heroes B
on O
the O
team B
can O
lead B
to O
success O
or O
defeat B
even O
in O
the O
hero B
picking B
phase O
of O
the O
game B
. O

Selecting O
the O
right O
hero B
to O
be O
played B
against O
another O
hero B
, O
or O
a O
set O
of O
heroes B
that O
can O
face O
the O
opponent B
’s O
set O
is O
a O
difficult O
task O
. O

This O
choice O
requires O
knowledge O
about O
the O
teammates B
’ O
heroes B
, O
the O
development O
curve O
, O
the O
hero B
classification O
and O
trying O
to O
predict O
the O
enemy B
’s O
team I
strategy B
. O

Moreover O
, O
each O
hero B
in O
a O
MOBA O
game B
is O
designed O
with O
a O
role B
. O

That O
means O
that O
a O
hero B
will O
be O
better O
developed O
if O
it O
is O
played B
in O
the O
role B
that O
it O
was O
designed O
. O

Picking B
the O
right O
heroes B
for O
the O
right O
roles B
requires O
all O
the O
knowledge O
cited O
above O
, O
and O
it O
is O
a O
hard B
task O
for O
the O
AI O
agent O
, O
since O
it O
requires O
knowledge O
that O
goes O
beyond O
of O
the O
game B
scope O
, O
commonly O
denominated O
metagame B
. O

For O
instance O
, O
in O
a O
situation O
where O
the O
team B
composed O
by O
five O
weak B
, O
low B
- O
damage I
dealers O
are O
fighting B
against O
a O
team B
composed O
by O
five O
strong O
, O
high B
- O
damage I
dealers O
sounds O
like O
a O
bad O
choice O
, O
since O
the O
first O
team B
will O
struggle O
on O
all O
battles B
against O
the O
opponent B
during O
the O
match B
. O

Lastly O
, O
MOBA O
games B
, O
as O
RTS O
, O
provide O
a O
partially O
observable O
environment O
. O

Dealing O
with O
the O
uncertainty O
of O
this O
situation O
is O
hard B
for O
most O
agents O
, O
because O
it O
requires O
sophisticated O
motion O
planning O
algorithms O
, O
and O
real B
- I
time I
reasoning O
about O
the O
environment O
. O

There O
are O
some O
MOBAs B
, O
like O
Heroes O
of O
the O
Storm O
, O
that O
even O
integrate O
a O
bush B
in O
the O
game B
scenario O
, O
providing O
spots B
where O
the O
player B
can O
not O
be O
seen O
if O
his O
/ O
her O
hero B
is O
inside O
a O
bush B
. O

This O
allows O
players B
to O
perform O
a O
wide B
range I
of O
tactical O
plays B
, O
like O
traps B
, O
faking O
and O
ganking O
. O

Reasoning O
about O
these O
fast O
- O
paced O
plays B
is O
not O
trivial O
, O
and O
therefore O
, O
requires O
predictions O
and O
especial O
research O
efforts O
. O

4 O
. O

Methodology O
Our O
difficulty O
adjustment O
mechanism B
consists O
in O
the O
development O
of O
three O
different O
levels B
of O
artificial O
intelligence O
that O
will O
be O
chosen O
during O
the O
match B
in O
order O
to O
present O
challenges O
that O
suit O
the O
player B
’s O
skills I
. O

To O
select O
the O
right O
opponent B
, O
a O
difficulty O
evaluation O
is O
performed O
during O
the O
game B
and O
if O
it O
indicates O
that O
the O
players B
are O
not O
evolving O
in O
the O
same O
pace O
, O
it O
executes O
15 O
the O
necessary O
adjustment O
. O

Throughout O
this O
section O
, O
we O
address O
the O
artificial O
intelligence O
agent O
developed O
, O
the O
game B
features O
, O
the O
difficulty O
evaluation O
process O
, O
and O
the O
mechanism B
to O
dynamically O
adjust O
the O
presented O
difficulty O
during O
a O
match B
. O

4.1 O
. O

Artificial O
Intelligence O
Agent O
To O
be O
able O
to O
provide O
an O
opponent B
that O
can O
face O
different O
skilled B
players I
, O
the O
artificial O
intelligence O
agent O
must O
be O
implemented O
with O
distinct O
ability B
levels B
to O
simulate O
the O
most O
different O
behaviors O
played B
. O

Since O
the O
agent O
must O
simulate O
an O
opponent B
player I
, O
the O
developed O
algorithm O
implements O
actions O
and O
behaviors O
to O
a O
hero B
unit I
. O

During O
a O
game B
match I
, O
this O
hero B
should O
follow O
the O
player B
’s O
performance O
, O
so O
if O
the O
player B
is O
having O
a O
good O
evolution B
, O
the O
hero B
controlled B
by O
artificial O
intelligence O
must O
be O
able O
to O
also O
do O
the O
same O
. O

However O
, O
if O
the O
player B
is O
not O
evolving O
enough O
or O
if O
his O
/ O
her O
development O
start B
to O
decrease O
, O
the O
AI O
agent O
that O
controls B
the O
hero B
must O
lower O
its O
pace O
and O
keep O
up O
with O
its O
opponent B
. O

The O
hero B
behavior O
was O
divided O
into O
three O
categories O
: O
easy B
mode I
, O
regular O
mode O
and O
hard B
mode I
. O

Each O
one O
of O
these O
categories O
has O
singular O
aspects O
that O
aim O
to O
be O
suitable O
to O
players B
with O
different O
abilities B
. O

These O
are O
described O
below O
. O

Easy O
Mode O
. O

In O
the O
easy B
mode I
, O
the O
hero B
performs O
regular B
attacks I
every O
time B
an O
enemy B
enters O
in O
its O
attack B
range B
. O

When O
an O
allied B
tower I
is O
under O
attack B
, O
the O
hero B
detects O
the O
need O
for O
defense B
and O
moves B
towards O
the O
attacked B
ally B
in O
order O
to O
defend O
it O
. O

Another O
strategic O
action O
is O
how O
the O
hero B
chooses O
the O
enemy B
tower I
to O
be O
its O
main B
target I
. O

Every O
time B
the O
hero B
starts B
a O
moving B
action O
, O
it O
analyses O
which O
of O
the O
enemy B
’s O
towers I
has O
taken O
more O
damage B
and O
is O
closer B
to O
be O
defeated B
. O

Once O
it O
finds O
, O
the O
hero B
sets O
that O
tower B
as O
the O
main B

 O
target I
and O
goes O
in O
that O
direction O
. O

It O
is O
important O
to O
mention O
that O
, O
in O
the O
easy B

 O
mode I
, O
all O
the O
attacking B
actions O
that O
the O
hero B
performs O
are O
basic O
attacks B
. O

The O
hero B
also O
retreats B
as O
a O
defense B
strategy B
. O

So O
when O
its O
health B
points I
are O
below O
30 O
% O
, O
it O
starts B
to O
retreat B
towards O
its O
base B
, O
where O
it O
can O
recover O
its O
health O
when O
it O
reaches O
a O
specific O
recovery B
building O
. O

The O
easy B
mode I
was O
created O
for O
beginners B
or O
some O
less O
skilled B
players I
, O
where O
the O
implemented O
strategies B
are O
not O
very O
complex O
and O
do O
not O
use O
any O
special O
character B
skill B
( O
also O
known O
as O
spells B
) O
. O

16 O
Regular O
Mode O
. O

In O
the O
regular O
mode O
, O
besides O
the O
strategies B
implemented O
for O
the O
easy B
mode I
, O
the O
hero B
also O
starts B
to O
manipulate O
items B
. O

The O
item B
manipulation O
is O
very O
helpful O
to O
improve O
the O
hero B
’s O
attributes O
and O
also O
to O
recover O
some O
attributes O
that O
have O
been O
decreased O
, O
for O
example O
, O
items B
to O
recover O
health B

 O
points I
or O
mana B
. O

Likewise O
, O
there O
are O
items B
to O
increase B
attributes O
like O
strength B
, O
speed O
, O
intelligence O
, O
among O
others O
. O

As O
part O
of O
the O
defense B
strategy B
, O
if O
the O
hero B
’s O
health B
points I
reach O
30 O
% O
or O
less O
, O
it O
will O
first O
use O
some O
health O
potion B
to O
recover O
it O
and O
if O
these O
items B
are O
over O
, O
then O
the O
hero B
starts B
to O
retreat B
towards O
its O
base B
. O

The O
regular O
mode O
was O
created O
to O
match B
those O
players B
that O
have O
already O
some O
experience B
and O
know O
how O
to O
use O
some O
of O
the O
game B
functionalities O
in O
his O
/ O
her O
favor O
but O
are O
not O
experts O
yet O
. O

Hard O
Mode O
. O

The O
hard B
mode I
has O
all O
the O
strategies B
implemented O
on O
both O
preceding O
modes O
, O
besides O
its O
own O
specific O
actions O
. O

Here O
, O
the O
hero B
goes O
beyond O
item B
manipulation O
and O
starts B
to O
learn O
, O
improve O
and O
cast O
spells B
. O

Spells B
are O
unique O
skills B
that O
each O
hero B
has O
. O

These O
spells B
can O
give O
a O
more O
effective O
damage B
on O
the O
enemy B
, O
can O
boost B
the O
recovery B
of O
its O
own O
attributes O
( O
like O
mana B
or O
health B
points I
) O
, O
can O
give O
some O
kind O
of O
advantage B
to O
allied B
units B
( O
like O
freezing O
the O
enemies B
) O
, O
among O
other O
possibilities O
. O

Every O
time B
the O
hero B
gains O
a O
new O
level B
it O
also O
gains O
one O
attribute O
point B
to O
distribute O
among O
its O
spells B
. O

So O
in O
this O
mode O
, O
besides O
the O
regular B
attack I
, O
the O
hero B
also O
casts O
spells B
to O
attack B
enemies B
or O
defend O
allies B
. O

Here O
we O
also O
decided O
to O
implement O
a O
new O
strategy B
for O
a O
head O
- O
to O
- O
head O
combat B
. O

In O
order O
to O
avoid O
losing O
the O
combat B
against O
another O
hero B
, O
the O
artificial O
intelligence O
agent O
algorithm O
keeps O
monitoring O
the O
area B
around O
its O
hero B
. O

Therefore O
, O
if O
an O
enemy B
hero I
enters O
the O
monitored O
area B
, O
the O
hero B
controlled B
by O
the O
agent O
will O
take O
advantage B
on O
that O
and O
will O
begin O
to O
attack B
it O
. O

The O
strategies B
to O
defend O
allied B
towers B
and O
to O
retreat B
are O
the O
same O
developed O
on O
regular O
mode O
. O

The O
hard B
mode I
was O
created O
to O
match B
those O
players B
that O
have O
more O
experience B
on O
the O
DotA O
game B
and O
also O
know O
how O
to O
use O
the O
game B
functionalities O
in O
their O
favor O
. O

This O
kind O
of O
player B
may O
be O
an O
expert O
on O
the O
game B
or O
a O
quick O
learner O
. O

The O
table O
displayed O
in O
Figure O
6 O
summarizes O
all O
the O
developed O
difficulty O
modes O
and O
their O
strategies B
. O

4.2 O
. O

Difficulty O
Evaluation O
Process O
A O
difficulty O
evaluation O
process O
was O
developed O
to O
be O
performed O
during O
the O
game B
and O
indicates O
when O
the O
players B
are O
not O
evolving O
at O
the O
same O
pace O
. O

For O
that O
, O
it O
was O
necessary O
to O
observe O
which O
game B
features O
should O
be O
analyzed O
and O
17 O
Figure O
6 O
: O
Summary O
of O
the O
developed O
strategies B
for O
each O
artificial O
intelligence O
agent O
. O

how O
to O
properly O
use O
the O
information O
from O
each O
one O
of O
them O
. O

The O
analyzed O
features O
and O
the O
evaluation O
process O
are O
described O
below O
. O

4.2.1 O
. O

Game B
Features O
To O
evaluate O
a O
game B
match I
, O
it O
is O
crucial O
to O
identify O
which O
features O
can O
represent O
the O
players B
’ O
performance O
and O
are O
relevant O
to O
the O
evaluation O
. O

In O
our O
testbed O
, O
we O
identified O
three O
important O
features O
that O
can O
illustrate O
the O
player B
’s O
behavior I
during O
a O
DotA O
match B
. O

These O
features O
are O
: O
Hero O
’s O
Level O
, O
Hero O
’s O
Death O
and O
Towers O
Destroyed O
. O

Each O
one O
of O
these O
features O
will O
be O
described O
in O
the O
following O
paragraphs O
: O
Hero O
’s O
Level O
. O

This O
feature O
represents O
the O
player B
’s O
evolution B
during O
a O
match B
, O
where O
the O
greater O
is O
the O
level B
value O
, O
the O
stronger O
is O
the O
character B
. O

Although O
this O
feature O
represents O
the O
evolution B
, O
it O
should O
not O
be O
the O
only O
analyzed O
feature O
because O
it O
is O
possible O
that O
the O
player B
increases O
his I
/ I
her O
hero B
’s O
level B
without O
really O
increasing B
his O
/ O
her O
abilities B
. O

For O
example O
, O
the O
player B
can O
keep O
the O
hero B
closer O
to O
battles B
without O
engaging O
in O
any O
fight B
and O
, O
by O
doing O
that O
, O
it O
will O
gain O
some O
experience B
points I
that O
are O
shared O
among O
the O
allies B
that O
are O
closer B
to O
the O
battle B
and O
will O
help O
the O
hero B
to O
evolve O
its O
level B
. O

Thereby O
, O
even O
if O
all O
players B
have O
heroes B
with O
equivalent O
levels B
, O
this O
feature O
alone O
does O
not O
give O
a O
real O
track O
on O
the O
game B
balance O
. O

Hero O
’s O
Death O
. O

This O
feature O
counts O
how O
many O
times B
the O
hero B
has O
died O
during O
a O
DotA O
match B
. O

Differently O
from O
all O
other O
features O
, O
the O
hero B
’s O
death B
may O
represent O
the O
player B
’s O
performance O
and O
the O
level B
of O
difficulty O
that O
he O
/ O
she O
is O
18 O
facing O
more O
accurately O
. O

For O
example O
, O
an O
inexperienced O
player B
, O
even O
having O
a O
hero B
with O
a O
high B
level I
, O
may O
have O
a O
high O
death B
rate O
, O
since O
he O
/ O
she O
may O
not O
know O
how O
to O
use O
more O
properly O
the O
characteristics O
and O
peculiarities O
of O
his O
/ O
her O
character B
as O
well O
as O
a O
possible O
lack O
of O
game B
strategies B
. O

Thereby O
, O
this O
feature O
seems O
to O
represent O
more O
accurately O
how O
well O
the O
player B
is O
facing O
the O
game B
challenges O
. O

Towers O
Destroyed O
. O

This O
feature O
is O
the O
amount O
of O
enemy B
’s O
towers I
destroyed O
by O
the O
allied B
team B
. O

It O
represents O
the O
team B
expansion O
and O
dominance O
over O
the O
map O
. O

Although O
this O
feature O
is O
not O
directly O
related O
to O
the O
player B
’s O
performance O
, O
since O
other O
allies B
can O
also O
destroy O
towers B
, O
it O
gives O
us O
a O
good O
notion O
of O
the O
game B
’s O
progress O
and O
team B
expansion O
over O
the O
map O
. O

Therefore O
, O
if O
a O
team B
is O
quickly O
progressing O
over O
the O
map O
, O
it O
may O
represent O
that O
the O
game B
is O
unbalanced O
. O

4.2.2 O
. O

Tracking O
Player O
’s O
Performance O
In O
order O
to O
perform O
a O
dynamic O
difficulty O
adjustment O
, O
it O
is O
necessary O
to O
evaluate O
the O
game B
from O
time I
to O
time B
and O
verify O
if O
the O
game B
is O
presenting O
challenges O
suitable O
to O
the O
player B
’s O
performance O
. O

If O
the O
player B
is O
having O
a O
poor B
performance O
, O
the O
game B
should O
be O
capable O
to O
identify O
that O
and O
reduce O
its O
difficulty O
. O

In O
the O
same O
way O
, O
if O
the O
player B
evolves O
faster O
than O
the O
challenges O
presented O
, O
the O
game B
should O
increase B
its O
difficulty O
. O

Once O
we O
have O
defined O
the O
game B
features O
that O
must O
be O
analyzed O
, O
this O
process O
can O
be O
summarized O
into O
the O
creation O
of O
an O
heuristic O
function O
that O
will O
keep O
track O
on O
the O
player B
’s O
performance O
and O
inform O
when O
it O
is O
necessary O
to O
adjust O
the O
difficulty O
. O

This O
heuristic O
function O
will O
be O
our O
evaluation O
method O
during O
the O
game B
match I
and O
from O
now O
on O
it O
will O
be O
called B
as O
evaluation O
function O
. O

So O
, O
considering O
the O
features O
mentioned O
before O
and O
the O
impact B
that O
each O
one O
represents O
on O
the O
player B
’s O
performance O
, O
we O
have O
: O
P(xt O
) O
= O
Hl O
− O
Hd O
+ O
Td O
, O
( O
1 O
) O
where O
P(xt O
) O
is O
the O
performance O
function O
of O
player B
x O
on O
time B
t. O
Hl O
is O
the O
hero B
’s O
level B
, O
Hd O
is O
the O
hero B
’s O
death B
count O
and O
Td O
is O
the O
number O
of O
towers B
destroyed O
. O

It O
is O
important O
to O
mention O
that O
the O
values O
of O
these O
features O
are O
related O
to O
the O
player B
and O
his I
/ I
her O
hero B
. O

Computing O
the O
difference O
between O
the O
measurements O
at O
two O
consecutive O
times B
t O
and O
t−1 O
, O
it O
is O
possible O
to O
calculate O
the O
current O
evolution B
of O
the O
player B
, O
as O
shown O
in O
the O
equation O
below O
: O
P O
0 O
( O
x O
) O
= O
P(xt O
) O
− O
P(xt−1 O
) O
. O

( O
2 O
) O
19 O
Figure O
7 O
: O
The O
verification O
performed O
by O
the O
adjustment O
mechanism B
. O

Once O
the O
performance O
function O
was O
calculated O
for O
both O
players B
( O
x O
and O
y O
) O
the O
evaluation O
value O
can O
be O
obtained O
by O
: O
α O
= O
P O
0 O
( O
x O
) O
− O
P O
0 O
( O
y O
) O
, O
( O
3 O
) O
where O
α O
is O
the O
difference O
between O
performances O
. O

We O
should O
mention O
that O
player B
x O
is O
the O
one O
that O
we O
are O
analyzing O
and O
player B
y O
is O
the O
one O
controlled B
by O
the O
artificial O
intelligence O
system O
. O

Therefore O
, O
the O
player B
y O
is O
the O
one O
that O
will O
have O
its O
difficulty O
adjusted O
during O
the O
game B
. O

It O
is O
important O
to O
measure O
the O
opponent B
’s O
performance O
in O
order O
to O
evaluate O
if O
its O
progress O
is O
compatible O
or O
not O
with O
the O
players B
. O

4.3 O
. O

Dynamic O
Difficulty O
Adjustment O
Mechanism O
The O
proposed O
mechanism B
is O
the O
key O
to O
make O
the O
adjustment O
work O
properly O
during O
the O
game B
. O

Until O
now O
we O
have O
only O
showed O
how O
to O
verify O
if O
the O
player B
’s O
performance O
is O
balanced O
to O
a O
certain O
opponent B
or O
not O
. O

Thereby O
, O
the O
main O
task O
of O
the O
implemented O
mechanism B
is O
to O
analyze O
the O
α O
value O
and O
perform O
or O
not O
the O
difficulty O
adjustment O
at O
the O
game B
time I
t. O
The O
mechanism B
works O
by O
evaluating O
the O
α O
variable O
and O
constantly O
verifying O
if O
this O
variable O
is O
within O
the O
β O
’s O
range B
, O
where O
β O
represents O
the O
limit O
value O
of O
the O
evaluation O
function O
. O

This O
value O
means O
how O
far O
a O
player B
can O
perform O
better O
than O
the O
other O
player B
, O
without O
considering O
the O
game B
unbalanced O
. O

If O
the O
value O
of O
|β| O
is O
a O
large O
number O
, O
then O
the O
adjustment O
will O
occur O
with O
less O
frequency O
, O
since O
it O
may O
take O
some O
time B
to O
α O
overcome O
β O
. O

Likewise O
, O
if O
|β| O
is O
a O
small O
number O
, O
then O
the O
adjustment O
will O
occur O
more O
frequently O
, O
since O
it O
may O
overcome O
β O
more O
easily O
. O

And O
if O
α O
stays O
inside O
the O
limits O
values O
of O
−β O
and O
β O
, O
it O
means O
that O
both O
players B
are O
having O
a O
similar O
performance O
and O
therefore O
, O
the O
match B
is O
currently O
balanced O
. O

The O
Figure O
7 O
illustrates O
this O
approach O
. O

In O
Section O
5 O
, O
several O
experiments O
were O
made O
in O
order O
to O
find O
the O
best O
limit O
value O
for O
β O
. O

20 O
5 O
. O

Experiments O
: O
Agents O
vs O
Agents O
In O
order O
to O
verify O
the O
effectiveness O
of O
the O
proposed O
mechanism B
, O
a O
series O
of O
experiments O
was O
performed O
. O

The O
players B
’ O
performance O
were O
analyzed O
along O
with O
the O
behavior O
of O
their O
heroes B
. O

The O
dynamic O
adjustment O
mechanism B
was O
also O
observed O
, O
as O
well O
as O
its O
variations O
and O
the O
impact B
caused O
on O
the O
matches B
. O

On O
each O
experiment O
, O
we O
performed O
20 O
matches B
of O
the O
game B
with O
the O
static O
artificial O
intelligence O
agent O
controlling B
one O
team B
against O
the O
dynamic O
artificial O
intelligence O
agent O
controlling B
the O
other O
one O
( O
Figure O
16 O
, O
in O
the O
end O
of O
this O
section O
, O
shows O
a O
summary O
of O
the O
matches B
) O
. O

The O
β O
limits O
for O
triggering B
the O
artificial O
intelligence O
switch O
were O
set O
to O
−1 O
and O
1 O
. O

Therefore O
, O
every O
time B
the O
difference O
among O
performances O
( O
α O
) O
exceeds O
the O
β O
limits O
, O
the O
difficulty O
of O
the O
dynamic O
AI O
should O
be O
modified O
accordingly O
. O

These O
values O
were O
determined O
empirically O
, O
after O
executing O
a O
large O
number O
of O
tests O
and O
observing O
the O
results O
contained O
in O
the O
gamelogs O
, O
and O
were O
not O
changed O
during O
these O
experiments O
. O

5.1 O
. O

Baseline O
First O
, O
we O
performed O
an O
unbalanced O
match B
in O
order O
to O
stipulate O
a O
baseline O
to O
compare O
with O
the O
obtained O
results O
from O
all O
three O
experiments O
. O

This O
baseline O
match B
is O
set O
by O
two O
different O
AI O
agent O
players B
with O
static O
behavior O
. O

One O
of O
them O
is O
on O
easy B
mode I
, O
representing O
a O
player B
without O
experience B
, O
and O
the O
second O
one O
is O
on O
hard B
mode I
, O
representing O
a O
very O
experienced O
player B
. O

The O
results O
of O
this O
match B
are O
shown O
in O
Figure O
8 O
, O
where O
the O
difference O
among O
both O
performances O
can O
be O
noticed O
. O

Figure O
8 O
: O
Baseline O
values O
obtained O
from O
a O
match B
with O
static O
difficulty O
for O
both O
players B
. O

21 O
The O
player B
/ O
agent O
performance O
is O
measured O
taking O
into O
account O
its O
current O
state O
during O
the O
match B
. O

The O
positive O
peaks O
represents O
moments O
where O
the O
agent O
improved O
its O
performance O
when O
compared O
to O
its O
last O
state O
. O

Likewise O
, O
negative O
peaks O
mean O
that O
the O
agent O
had O
its O
performance O
decreased O
based B
on O
its O
last O
game B
state I
. O

Converting O
these O
to O
game B
situations O
, O
when O
a O
hero B
gains O
a O
level B
or O
the O
team B
manage O
to O
destroy O
a O
tower B
, O
then O
this O
will O
impact B
positively O
in O
its O
development O
, O
increasing B
the O
player B
’s O
current O
performance O
. O

Similarly O
, O
if O
a O
hero B
dies O
this O
will O
result O
in O
a O
negative O
impact B
in O
its O
development O
decreasing O
the O
player B
’s O
current O
performance O
. O

During O
this O
game B
match I
, O
the O
hard B
mode I
player B
kept O
increasing B
his O
performance O
, O
presenting O
only O
one O
time B
of O
regression O
in O
his O
development O
. O

Meanwhile O
, O
the O
easy B
mode I
player B
performance O
was O
very O
unstable O
, O
with O
several O
moments O
of O
regression O
in O
his O
development O
. O

Therefore O
, O
we O
can O
consider O
that O
a O
match B
will O
be O
balanced O
if O
the O
difference O
among O
both O
performances O
were O
not O
divergent O
. O

So O
, O
examining O
once O
again O
the O
graph O
of O
Figure O
8 O
, O
it O
is O
possible O
to O
observe O
that O
each O
performance O
peak O
shows O
itself O
as O
an O
appropriate O
moment O
to O
execute O
a O
difficulty O
balance O
in O
order O
to O
get O
the O
players B
’ O
performance O
closer O
to O
each O
other O
. O

Figure O
9 O
shows O
the O
cumulative O
performance O
value O
for O
each O
player B
during O
this O
particular O
match B
. O

On O
this O
graph O
, O
it O
becomes O
clear B
that O
the O
hard B
mode I
player B
evolves O
much O
faster O
than O
the O
easy B
mode I
player B
. O

This O
greater O
performance O
evolution B
can O
be O
related O
to O
the O
fact O
that O
the O
hero B
increases B
his O
level B
rapidly O
and O
has O
a O
low O
amount O
of O
deaths B
. O

On O
the O
other O
hand O
, O
the O
easy B
mode I
player B
had O
his I
performance I
lowered O
by O
a O
large O
number O
of O
deaths B
, O
in O
spite O
of O
having O
a O
good O
development O
. O

This O
led B
to O
a O
poor B
performance O
when O
compared O
to O
the O
hard B
mode I
player B
. O

Therefore O
, O
due O
to O
that O
difference O
between O
them O
, O
the O
adjustment O
appears O
to O
be O
necessary O
in O
order O
to O
minimize O
this O
disparity O
among O
their O
behaviors O
and O
present O
a O
more O
fair O
and O
competitive B
game I
. O

After O
setting O
a O
baseline O
of O
an O
unbalanced O
match B
, O
we O
performed O
a O
set O
of O
experiments O
to O
compare O
the O
performance O
of O
a O
player B
using O
the O
adaptive O
AI O
against O
an O
opponent B
using O
a O
static O
AI O
, O
fixed O
at O
one O
of O
the O
predetermined O
modes O
. O

Specifically O
, O
for O
player B
A O
we I
used I
a O
static O
artificial O
intelligence O
agent O
in O
order O
to O
simulate O
the O
possible O
skills B
of O
a O
human O
player B
. O

For O
player B
B O
we I

 I
applied O
the O
proposed O
mechanism B
, O
so O
that O
this O
player B
should O
keep O
its O
performance O
equivalent O
to O
player B
A O
and O
for O
that O
it O
should O
perform O
a O
dynamic O
difficulty O
adjustment O
. O

These O
experiments O
are O
mentioned O
on O
the O
following O
subsections O
. O

22 O
Figure O
9 O
: O
Cumulative O
player B
’s O
performance O
with O
static O
difficulty O
for O
both O
players B
. O

5.2 O
. O

Easy B
x O
Adaptive O
In O
this O
first O
set O
of O
experiments O
, O
player B
A O
was O
set O
with O
the O
easy B
mode I
, O
simulating O
a O
novice B
player I
, O
while O
player B
B O
was O
set O
with O
the O
adaptive O
AI O
, O
starting B
with O
the O
regular O
mode O
and O
switching O
to O
try O
to O
match B
the O
other O
player B
performance O
. O

Figure O
10 O
shows O
the O
performance O
of O
both O
players B
during O
this O
match B
( O
P O
0 O
) O
, O
while O
Figure O
11 O
shows O
the O
results O
of O
the O
evaluation O
function O
( O
α O
) O
and O
the O
difficulty O
adjustments O
made O
during O
the O
game B
. O

Figure O
10 O
: O
Performance O
of O
Easy O
x O
Adaptive O
players B
during O
one O
match B
. O

As O
mentioned O
before O
, O
the O
player B
’s O
performance O
is O
measured O
by O
taking O
into O
account O
his O
/ O
her O
current O
state O
during O
the O
match B
. O

The O
positive O
peaks O
represent O
moments O
where O
the O
player B
had O
improved O
and O
negative O
peaks O
mean O
that O
the O
player B
had O
decreased O
based B
on O
his O
/ O
her O
last O
game B
state I
. O

Figure O
10 O
shows O
23 O
that O
the O
adaptive O
artificial O
intelligence O
agent O
( O
player B
B O
) O
managed O
to O
keep O
its O
performance O
similar O
to O
its O
opponent B
, O
the O
easy B
mode I
player B
A. O
On O
Figure O
11 O
, O
we O
can O
track O
how O
well O
the O
adaptive O
player B
( O
player B
B O
) O
managed O
to O
be O
compatible O
with O
player B
A O
during O
the O
match B
. O

When O
the O
evaluation O
function O
shows O
negative O
peaks O
, O
it O
means O
that O
the O
difficulty O
should O
be O
adjusted O
and O
decreased O
by O
one O
level B
. O

Likewise O
, O
if O
there O
are O
positive O
peaks O
, O
the O
difficulty O
of O
the O
adaptive O
player B
should O
be O
increased B
by O
one O
level B
. O

Moments O
where O
the O
evaluation O
function O
remains O
constant O
( O
equals O
0 O
) O
means O
that O
the O
performance O
of O
both O
players B
are O
very O
similar O
and O
due O
to O
that O
no O
adjustment O
is O
necessary O
at O
this O
time B
. O

Therefore O
, O
the O
difficulty O
can O
be O
maintained O
. O

Figure O
11 O
: O
Difficulty O
adjustments O
performed O
by O
the O
mechanism B
during O
one O
match B
. O

It O
is O
important O
to O
mention O
that O
the O
difficulty O
adjustment O
is O
performed O
by O
increasing B
or O
decreasing O
one O
level B
of O
each O
time B
. O

With O
this O
approach O
we O
minimize O
the O
possibility O
of O
the O
opponent B
player I
noticing O
the O
behavior O
change O
. O

After O
analyzing O
this O
set O
of O
experiments O
and O
studying O
the O
gamelogs O
obtained O
from O
each O
one O
, O
we O
observed O
that O
in O
85 O
% O
of O
the O
matches B
, O
the O
adaptive O
player B
B O
managed O
to O
keep O
the O
game B
balanced O
and O
as O
result O
of O
each O
match B
, O
player B
A O
won O
60 O
% O
of O
the O
matches B
and O
player B
B O
won O
40 O
% O
. O

5.3 O
. O

Regular O
x O
adaptive O
On O
the O
second O
set O
of O
experiments O
, O
we O
kept O
using O
the O
artificial O
intelligence O
agents O
developed O
to O
control B
two O
players B
, O
one O
from O
each O
team B
. O

Here O
, O
we O
manage O
to O
simulate O
an O
intermediary O
player B
with O
player B
A O
using O
a O
static O
artificial O
intelligence O
agent O
on O
regular O
mode O
. O

For O
player B
B O
we I
applied I
the O
proposed O
mechanism B
, O
starting B
it O
on O
regular O
mode O
. O

Figure O
12 O
shows O
the O
performance O
24 O
of O
both O
players B
( O
P O
0 O
) O
during O
one O
single O
match B
. O

Likewise O
, O
Figure O
13 O
shows O
the O
results O
of O
the O
evaluation O
function O
( O
α O
) O
during O
the O
game B
and O
the O
difficulty O
adjustments O
made O
along O
the O
match B
. O

Figure O
12 O
: O
Performance O
of O
Regular O
x O
Adaptive O
players B
during O
one O
match B
. O

The O
analysis O
performed O
in O
this O
set O
of O
experiments O
is O
pretty O
similar O
to O
the O
previous O
one O
. O

The O
positive O
peaks O
represent O
moments O
where O
the O
player B
had O
improved O
and O
negative O
peaks O
mean O
that O
the O
player B
had O
decreased O
its O
performance O
. O

In O
Figure O
12 O
, O
we O
can O
observe O
that O
the O
adaptive O
artificial O
intelligence O
agent O
( O
player B
B O
) O
tried O
to O
follow O
its O
opponent B
’s O
performance O
( O
player B
A O
) O
presenting O
similar O
peaks O
at O
close B
time B
periods O
. O

On O
Figure O
13 O
, O
we O
can O
follow O
all O
the O
adjustments O
made O
during O
the O
match B
. O

The O
adaptive O
player B
spent O
most O
of O
its O
time B
alternating O
between O
the O
regular O
mode O
and O
the O
hard B
mode I
. O

This O
variation O
can O
be O
understood O
as O
moments O
where O
player B
B O
was O
having O
a O
poor B
development O
when O
compared O
to O
player B
A O
, O
and O
the O
need O
to O
increase B
the O
difficulty O
was O
perceived O
. O

Similarly O
, O
when O
player B
’s O
B O
behavior O
were O
standing O
out O
, O
the O
need O
for O
reducing O
the O
difficulty O
could O
also O
be O
seen O
. O

The O
graphic O
also O
shows O
that O
player B
B O
stayed O
balanced O
during O
the O
game B
. O

Furthermore O
, O
after O
analyzing O
this O
second O
set O
of O
experiments O
and O
studying O
all O
gamelogs O
collected O
, O
we O
observed O
that O
the O
players B
had O
a O
compatible O
performance O
in O
90 O
% O
of O
the O
matches B
. O

The O
results O
of O
the O
matches B
were O
50 O
% O
of O
victories B
for O
each O
player B
. O

5.4 O
. O

Hard B
x O
adaptive O
On O
the O
last O
set O
of O
experiments O
, O
we O
managed O
to O
simulate O
an O
expert B
player I
( O
player B
A O
) O
against O
the O
adaptive O
player B
( O
player B
B O
) O
. O

As O
we O
mentioned O
before O
, O
25 O
Figure O
13 O
: O
Difficulty O
adjustments O
performed O
by O
the O
mechanism B
during O
one O
match B
. O

the O
adaptive O
player B
started B
on O
regular O
mode O
and O
changed O
its O
behavior O
during O
the O
match B
in O
order O
to O
keep O
the O
game B
balanced O
. O

Figure O
14 O
shows O
the O
performance O
( O
P O
0 O
) O
of O
both O
players B
during O
one O
match B
. O

Likewise O
, O
Figure O
15 O
shows O
the O
results O
of O
the O
evaluation O
function O
( O
α O
) O
during O
the O
game B
and O
the O
difficulty O
adjustments O
made O
along O
the O
match B
. O

Figure O
14 O
: O
Performance O
of O
Hard O
x O
Adaptive O
players B
during O
one O
match B
. O

Analyzing O
the O
results O
from O
Figure O
14 O
, O
the O
adaptive O
player B
started B
developing O
a O
better O
performance O
than O
player B
A O
in O
the O
beginning O
of O
the O
match B
. O

Therefore O
, O
it O
was O
detected O
that O
the O
difficulty O
should O
be O
reduced O
in O
order O
to O
keep O
the O
balance O
( O
Figure O
15 O
) O
. O

After O
that O
, O
they O
kept O
their O
performances O
very O
close B
and O
the O
difficulty O
kept O
alternating O
between O
easy B
mode I
and O
regular O
mode O
until O
player B
A O
can O
present O
itself O
better O
/ O
stronger O
than O
player B
B. O
The O
opposite O
can O
also O
be O
seen O
, O
when O
player B
B O
kept O
alternating O
between O
regular O
mode O
and O
hard B
mode I
in O
order O
to O
reach O
player B
’s O
A O
performance O
. O

26 O
Figure O
15 O
: O
Difficulty O
adjustments O
performed O
by O
the O
mechanism B
during O
one O
match B
. O

Furthermore O
, O
after O
analyzing O
the O
gamelogs O
collected O
from O
each O
game B
, O
we O
observed O
that O
the O
adaptive O
player B
( O
player B
B O
) O
changed O
its O
difficulty O
and O
succeed O
to O
keep O
the O
match B
balanced O
on O
80 O
% O
of O
the O
experiments O
. O

As O
result O
of O
the O
battles B
, O
player B
A O
won O
45 O
% O
of O
the O
matches B
. O

5.5 O
. O

Discussion O
Despite O
the O
good O
results O
, O
not O
all O
the O
cases O
presented O
the O
expected O
results O
, O
which O
has O
resulted O
in O
unbalanced O
matches B
. O

To O
get O
to O
this O
conclusion O
, O
we O
observed O
all O
the O
executed O
matches B
and O
studied O
all O
the O
collected O
gamelogs O
. O

These O
gamelogs O
kept O
track O
of O
the O
game B
on O
every O
15 O
seconds O
, O
recording O
the O
current O
situation O
of O
both O
teams B
, O
the O
related O
features O
, O
their O
values O
, O
among O
other O
information O
. O

Once O
the O
game B
was O
finished O
, O
we O
started B
to O
translate O
those O
collected O
information O
, O
comparing O
the O
values O
from O
both O
heroes B
and O
making O
the O
necessary O
assumptions O
. O

Considering O
all O
the O
performed O
experiments O
10 O
% O
of O
them O
were O
unbalanced O
because O
the O
mechanism B
took O
too O
long O
to O
perform O
each O
adjustment O
, O
leading B
to O
a O
great O
difference O
between O
the O
players B
performance O
. O

So O
, O
when O
the O
players B
were O
getting O
closer B
to O
a O
balance O
, O
the O
match B
has O
ended O
. O

On O
the O
other O
hand O
, O
5 O
% O
of O
the O
executed O
experiments O
were O
unbalanced O
due O
to O
an O
excess O
of O
adjustments O
. O

In O
these O
scenarios O
, O
the O
adjustments O
were O
being O
performed O
too O
quickly O
, O
causing O
player B
B O
to O
not O
evolve O
properly O
during O
the O
match B
, O
which O
resulted O
in O
an O
easy B
game B
for O
player B
A. O
After O
performing O
all O
the O
experiments O
, O
it O
was O
possible O
to O
summarize O
the O
obtained O
final O
results O
from O
the O
game B
matches I
. O

Figure O
16 O
shows O
the O
amount B

 O
of I
victories I
and O
losses O
of O
the O
adaptive O
AI O
against O
the O
easy B
, O
regular O
and O
hard B
27 O
Figure O
16 O
: O
Final O
results O
from O
all O
matches B
performed O
on O
the O
experiments O
. O

modes O
. O

These O
results O
show O
that O
both O
players B
had O
a O
similar O
amount O
of O
victories B
, O
demonstrating O
that O
the O
adaptive O
player B
offers O
a O
more O
balanced B
game I
experience B
. O

6 O
. O

Experiments O
: O
Agents O
vs O
Users B
We O
also O
performed O
tests O
with O
users B
to O
assess O
qualitatively O
the O
efficacy O
of O
the O
implemented O
mechanism B
. O

The O
objective B
of O
these O
tests O
consists O
of O
verifying O
, O
according O
to O
the O
perception O
of O
the O
player B
, O
if O
the O
mechanism B
can O
keep O
the O
difficulty O
of O
the O
game B
balanced O
facing O
their O
skills B
and O
if O
this O
approach O
stimulates O
his O
/ O
her O
entertainment O
. O

To O
make O
the O
tests O
more O
objective B
, O
we O
only O
enrolled O
users B
who O
have O
played B
the O
game B
Defense O
of O
the O
Ancients O
( O
DotA O
) O
at O
least O
once O
, O
in O
order O
to O
avoid O
problems O
in O
understanding O
interface B
elements O
and/or O
the O
game B
’s O
mechanics I
. O

The O
tests O
involved O
inviting O
the O
users B
to O
play B
two O
matches B
of O
the O
game B
Defense O
of O
the O
Ancients O
( O
DotA O
) O
, O
where O
their O
main O
goal B
was O
to O
defeat B
the O
opposing B
team I
. O

However O
, O
it O
was O
explained O
to O
them O
that O
the O
outcome O
of O
the O
match B
was O
not O
crucial O
for O
the O
experiment O
, O
since O
we O
were O
more O
interested O
in O
behavioral O
issues O
generated O
by O
the O
game B
. O

We O
provided O
two O
types B
of I
maps I
. O

On O
map O
A O
, O
users B
faced O
the O
dynamic O
agent O
as O
opponent B
and O
on O
map O
B O
they O
faced O
a O
static O
agent O
. O

With O
these O
two O
types B
of I
maps I
, O
we O
evaluated O
if O
the O
difficulty O
adjustment O
can O
be O
perceived O
during O
the O
match B
and O
whether O
or O
not O
this O
dynamic O
adjustment O
can O
really O
impact B
on O
the O
player B
entertainment O
. O

Note O
that O
each O
volunteer O
played B
two O
times B
on O
the O
same O
map O
, O
not O
changing O
maps O
. O

We O
performed O
the O
tests O
on O
this O
way O
because O
we O
were O
trying O
to O
avoid O
the O
placebo O
effect B
on O
the O
users B
. O

Once O
the O
user B
knows O
that O
he O
was O
playing B
in O
both O
game B
versions O
he O
could O
assume O
that O
the O
adjustment O
was O
happening O
even O
though O
it O
may O
not O
be O
true O
. O

Therefore O
, O
we O
managed O
to O
ask O
only O
the O
expert O
users B
to O
play B
against O
both O
opponents B
( O
dynamic O
agent O
and O
static O
agent O
) O
, O
in O
28 O
order O
to O
really O
assess O
if O
the O
adjustment O
can O
be O
detected O
or O
not O
. O

We O
choose O
to O
ask O
only O
the O
expert B
players I
to O
play B
in O
both O
maps O
because O
they O
have O
more O
knowledge O
and O
expertise O
in O
the O
game B
mechanics I
, O
this O
makes O
them O
able O
to O
identify O
quickly O
any O
abnormal O
behavior O
on O
the O
opponent B
. O

A O
total O
of O
eleven O
users B
participated O
on O
the O
tests O
. O

All O
tests O
were O
guided O
by O
the O
following O
script B
: O
1 O
. O

The O
participant O
was O
instructed O
about O
how O
the O
test O
was O
going O
to O
occur O
and O
signed O
an O
agreement O
to O
participate O
on O
the O
test O
; O
2 O
. O

A O
quick O
presentation O
of O
the O
game B
mechanics I
was O
carried B
out O
and O
its O
rules B
in O
case O
the O
user B
did O
not O
remember O
it O
; O
3 O
. O

A O
preliminary O
interview O
was O
made O
, O
whose O
main O
objective B
was O
to O
let O
us O
know O
the O
user B
’s O
profile O
and O
background O
regarding O
game B
playing O
; O
4 O
. O

The O
participant O
played B
two O
game B
matches I
of O
DotA O
on O
the O
same O
map O
; O
5 O
. O

A O
post O
- O
test O
interview O
was O
made O
, O
whose O
objective B
was O
to O
gather O
the O
user B
’s O
opinions O
and O
perceptions O
regarding O
the O
presented O
system O
. O

The O
results O
from O
these O
tests O
are O
discussed O
in O
the O
following O
sections O
. O

6.1 O
. O

Pre O
- O
test O
evaluation O
During O
the O
tests O
, O
all O
users B
were O
instructed O
to O
answer O
a O
series O
of O
questions O
to O
make O
it O
possible O
to O
establish O
profiles O
according O
to O
their O
habits O
, O
preferences O
and O
experiences B
. O

These O
questions O
were O
placed O
on O
the O
pre O
- O
test O
questionnaire O
and O
its O
main O
goal B
is O
to O
try O
to O
identify O
similarities O
among O
the O
volunteers O
’ O
behavior O
during O
the O
experiments O
. O

A O
total O
of O
eleven O
male O
users B
participated O
on O
the O
experiments O
. O

Among O
them O
one O
is O
less O
than O
18 O
years O
old O
, O
two O
are O
from O
18 O
to O
21 O
years O
old O
, O
four O
are O
from O
22 O
to O
25 O
years O
old O
and O
the O
last O
four O
are O
between O
26 O
and O
29 O
years O
old O
. O

Regarding O
their O
education O
level B
, O
one O
was O
still O
in O
high O
school O
, O
seven O
were O
studying O
or O
already O
concluded O
a O
higher O
education O
and O
one O
had O
master B
’s O
degree O
. O

To O
verify O
their O
experience B
with O
digital O
games B
, O
we O
asked O
which O
are O
the O
game B
genres O
that O
the O
users B
have O
recently O
played B
the O
most O
. O

The O
most O
popular O
genres O
were O
platform O
games B
and O
RPG O
selected O
by O
90.9 O
% O
of O
the O
volunteers O
, O
followed O
by O
First O
Person O
Shooter O
( O
FPS O
) O
and O
sports O
with O
81.8 O
% O
of O
the O
users B
and O
on O
third O
place O
there O
is O
Real O
- O
Time B
Strategy O
( O
RTS O
) O
and O
racing O
with O
72.2 O
% O
. O

Observe O
that O
this O
percentages O
do O
not O
sum O
up O
, O
because O
users B
could O
select O
more O
than O
one O
option O
. O

Figure O
17 O
shows O
all O
the O
genres O
selected O
by O
the O
volunteers O
. O

29 O
Figure O
17 O
: O
Most O
popular O
game B
genres O
among O
the O
volunteers O
. O

Regarding O
the O
frequency O
on O
which O
they O
play B
, O
63.6 O
% O
of O
the O
users B
play B
every O
day O
and O
36.4 O
% O
from O
1 O
to O
3 O
times B
per O
week O
. O

The O
players B
were O
also O
asked O
on O
which O
devices O
they O
are O
currently O
playing B
more O
and O
90.9 O
% O
of O
the O
users B
selected O
mobile B
devices I
, O
followed O
by O
PC O
with O
81.8 O
% O
and O
consoles O
with O
36.4 O
% O
. O

Figure O
18 O
presents O
the O
chart O
with O
the O
most O
popular O
devices O
. O

Figure O
18 O
: O
Most O
popular O
gaming O
platforms O
among O
the O
volunteers O
. O

Now O
, O
regarding O
MOBA O
games B
knowledge I
, O
all O
users B
said O
that O
they O
had O
played B
DotA O
at O
least O
once O
, O
on O
which O
five O
rated O
themselves O
as O
beginners B
, O
four O
as O
intermediary O
players B
and O
two O
as O
experts O
. O

Among O
the O
eleven O
volunteers O
, O
30 O
only O
eight O
said O
they O
had O
played B
another O
MOBA O
game B
: O
100 O
% O
of O
them O
played B
League O
of O
Legends O
and O
37.5 O
% O
played B
Heroes O
of O
Newerth O
. O

About O
their O
expertise O
in O
MOBA O
games B
in O
general O
, O
25 O
% O
self O
- O
proclaimed O
as O
beginners B
, O
37.5 O
% O
as O
intermediary O
players B
and O
37.5 O
% O
as O
expert B
players I
. O

6.2 O
. O

Post O
- O
test O
evaluation O
After O
answering O
all O
pre O
- O
test O
questions O
, O
the O
participants O
were O
asked O
to O
play B
two O
matches B
against O
our O
agents O
. O

Therefore O
, O
we O
created O
two O
different O
maps O
( O
A O
and O
B O
) O
and O
asked O
the O
volunteer O
to O
play B
both O
matches B
in O
only O
one O
map O
. O

Both O
maps O
contain O
the O
same O
game B
structure O
and O
what O
differ O
them O
is O
that O
map O
A O
hosts B
the O
dynamic O
artificial O
intelligence O
agent O
while O
map O
B O
hosts B
the O
static O
artificial O
intelligence O
agent O
. O

These O
agents O
are O
the O
same O
used O
on O
the O
previous O
experiment O
. O

Among O
the O
participants O
, O
six O
of O
them O
played B
on O
map O
A O
and O
five O
of O
them O
played B
on O
map O
B. O
Once O
both O
matches B
were O
concluded O
, O
the O
user B
was O
instructed O
to O
answer O
the O
post O
- O
test O
questionnaire O
, O
which O
tackles O
various O
aspects O
related O
to O
how O
he O
/ O
she O
perceived O
the O
game B
experience I
. O

Questions O
related O
to O
experience B
and O
immersion O
of O
the O
user B
during O
the O
match B
came O
from O
a O
selection O
of O
questionnaires O
about O
user B
experience B
in O
games B
( O
Takatalo O
et O
al O
. O

, O
2015 O
; O
Fox O
and O
Brockmyer O
, O
2013 O
; O
Jennett O
et O
al O
. O

, O
2008 O
; O
IJsselsteijn O
et O
al O
. O

, O
2007 O
) O
. O

They O
were O
presented O
as O
affirmatives O
so O
that O
the O
user B
could O
choose O
how O
much O
he O
/ O
she O
agrees O
to O
it O
following O
the O
5 O
points B
of O
Likert O
’s O
classification O
( O
Norman O
, O
2010 O
) O
. O

The O
scale O
goes O
from O
1 O
- O
“ O
Strongly O
disagree O
” O
; O
2 O
- O
“ O
Partially O
disagree O
” O
; O
3 O
- O
“ O
Indifferent O
” O
; O
4 O
- O
“ O
Partially O
agree O
” O
; O
and O
5 O
- O
“ O
Strongly O
agree O
” O
. O

The O
first O
set O
of O
affirmatives O
addresses O
aspects O
of O
the O
player B
immersion O
during O
the O
match B
: O
55 O
% O
described O
as O
indifferent O
when O
asked O
if O
they O
did O
not O
realize O
the O
time B
running O
while O
playing B
and O
73 O
% O
of O
them O
strongly O
agree O
that O
they O
worked O
hard B
to O
get O
good O
results O
in O
the O
game B
. O

When O
asked O
if O
there O
were O
moments O
where O
they O
wanted O
to O
quit O
the O
game B
, O
73 O
% O
strongly O
disagree O
with O
this O
affirmative O
. O

This O
may O
suggest O
that O
although O
half O
of O
the O
volunteers O
said O
they O
were O
indifferent O
regarding O
the O
game B
time I
lapse O
, O
the O
majority O
stated O
that O
they O
put O
some O
effort O
to O
accomplish O
the O
main O
goal B
and O
they O
did O
not O
want O
to O
quit O
( O
Figure O
19 O
) O
. O

Thereby O
, O
we O
can O
assume O
that O
the O
game B
was O
able O
to O
offer O
a O
considerable O
level B
of O
immersion O
to O
the O
player B
. O

The O
next O
set O
of O
affirmatives O
addresses O
the O
game B
challenge O
provided O
by O
the O
agent O
during O
the O
match B
. O

When O
asked O
if O
the O
game B
kept O
them O
motivated O
to O
keep O
playing B
, O
18 O
% O
strongly O
agree O
and O
46 O
% O
partially O
agree O
. O

Now O
, O
regarding O
the O
difficulty O
, O
27 O
% O
strongly O
agree O
that O
the O
game B
is O
too O
challenging O
for O
them O
, O
31 O
Figure O
19 O
: O
Affirmatives O
that O
address O
aspects O
of O
the O
player B
immersion O
during O
the O
match B
. O

while O
18 O
% O
strongly O
agree O
that O
the O
game B
is O
suitably O
challenging O
for O
them O
. O

Finally O
, O
36 O
% O
said O
that O
the O
game B
is O
not O
challenging O
at O
all O
( O
Figure O
20 O
) O
. O

These O
very O
distinct O
opinions O
can O
be O
observed O
due O
to O
the O
different O
level B
of O
expertise O
of O
each O
player B
. O

Those O
that O
consider O
themselves O
as O
beginners B
in O
DotA O
or O
general O
MOBA O
, O
found O
that O
the O
game B
was O
too O
challenging O
for O
them O
. O

Using O
the O
same O
analysis O
, O
those O
that O
consider O
themselves O
as O
experts O
found O
the O
game B
too O
easy B
because O
they O
know O
more O
advanced O
strategies B
that O
goes O
beyond O
the O
agent O
’s O
algorithm O
. O

The O
following O
set O
of O
affirmatives O
addressed O
the O
player B
ability I
/ O
competence O
during O
the O
game B
. O

64 O
% O
of O
the O
players B
felt O
successful O
at O
the O
end O
of O
the O
game B
and O
won O
the O
match B
. O

Also O
, O
82 O
% O
agree O
that O
they O
were O
making O
progress O
during O
the O
course O
of O
the O
game B
. O

About O
the O
player B
enjoyment O
during O
the O
game B
, O
73 O
% O
liked O
to O
play B
against O
our O
agent O
and O
would O
recommend O
this O
game B
to O
others O
. O

64 O
% O
said O
that O
he O
would O
play B
this O
game B
again O
( O
Figure O
21 O
) O
. O

Therefore O
, O
although O
the O
agents O
were O
not O
always O
very O
challenging O
to O
all O
players B
, O
we I
can I
assume O
that O
most O
of O
them O
enjoyed O
the O
match B
against O
it O
. O

The O
last O
set O
of O
affirmatives O
addresses O
the O
agent O
opponent B
level B
. O

Here O
, O
all O
the O
opinions O
got O
divided O
where O
we O
have O
that O
27.3 O
% O
strongly O
agree O
and O
9.1 O
% O
partially O
agree O
that O
the O
opponent B
plays B
a O
lot O
better O
than O
they O
. O

This O
opinion O
were O
given O
probably O
by O
beginners B
, O
since O
they O
may O
had O
a O
more O
difficult O
time B
trying O
to O
win O
the O
match B
. O

Regarding O
that O
the O
opponent B
plays B
in O
the O
same O
level B
as O
them O
, O
27.3 O
% O
strongly O
agree O
with O
this O
affirmation O
, O
which O
may O
indicate O
that O
these O
players B
are O
intermediaries O
or O
maybe O
beginners B
that O
managed O
to O
win O
the O
match B
. O

Finally O
, O
we O
have O
that O
36.4 O
% O
of O
the O
players B
strongly O
agree O
that O
the O
opponent B
plays B
much O
worse O
than O
them O
. O

This O
answer O
was O
given O
by O
experts O
and O
intermediaries O
players B
that O
managed O
to O
win O
the O
match B
without O
putting O
too O
32 O
Figure O
20 O
: O
Affirmatives O
that O
address O
the O
game B
challenge O
provided O
by O
the O
agent O
during O
the O
match B
. O

much O
effort O
on O
this O
accomplishment O
. O

Thereby O
, O
we O
can O
observe O
that O
the O
player B
opinion O
about O
the O
opponent B
reflects O
directly O
on O
his O
/ O
her O
expertise O
, O
where O
the O
agent O
with O
the O
same O
behavior O
can O
be O
too O
challenging O
for O
some O
and O
not O
very O
challenging O
for O
another O
. O

To O
analyse O
the O
player B
’s O
perception O
during O
the O
game B
, O
we O
asked O
them O
some O
questions O
about O
whether O
or O
not O
they O
have O
noticed O
any O
changes O
on O
the O
agent O
’s O
behavior O
during O
the O
match B
. O

We O
made O
these O
questions O
to O
all O
the O
players B
regardless O
the O
map O
where O
they O
played B
. O

All O
of O
them O
said O
they O
did O
not O
notice O
the O
opponent B
getting O
harder O
or O
easier B
. O

They O
also O
mentioned O
not O
noticing O
the O
agent O
adapting O
its O
behavior O
in O
order O
to O
be O
more O
suitable O
to O
them O
. O

Therefore O
, O
based B
on O
these O
answers O
we O
can O
assume O
that O
the O
mechanism B
were O
successful O
in O
at O
least O
one O
of O
its O
goals B
by O
changing O
its O
behavior O
without O
making O
it O
noticeable O
to O
the O
player B
. O

This O
is O
the O
first O
premise O
of O
the O
dynamic O
difficulty O
adjustment O
which O
tries O
to O
guarantee O
that O
the O
player B
will O
not O
feel O
cheated O
or O
disappointed O
during O
the O
game B
. O

33 O
Figure O
21 O
: O
Affirmatives O
that O
address O
the O
enjoyment O
during O
the O
match B
. O

6.3 O
. O

Discussion O
The O
main O
goal B
of O
these O
experiments O
were O
to O
evaluate O
qualitatively O
the O
efficacy O
of O
the O
implemented O
mechanism B
, O
verifying O
if O
it O
can O
keep O
the O
game B
difficulty O
balanced O
to O
each O
player B
, O
and O
whether O
or O
not O
this O
can O
really O
impact B
on O
the O
player B
entertainment O
. O

Therefore O
, O
we O
provided O
two O
types B
of I
maps I
, O
one O
with O
the O
dynamic O
agent O
and O
another O
with O
the O
static O
agent O
. O

After O
analysing O
the O
results O
from O
all O
players B
, O
we I
asked I
informally O
for O
both O
expert O
volunteers O
to O
also O
play B
on O
the O
other O
map O
, O
so O
that O
we O
could O
assess O
their O
perceptions O
regarding O
both O
agents O
. O

After O
comparing O
the O
experience B
they O
had O
on O
each O
map O
, O
they O
said O
that O
the O
opponent B
from O
map O
A O
( O
dynamic O
agent O
) O
presented O
a O
more O
fluid O
behavior O
and O
they O
preferred O
to O
play B
against O
it O
. O

They O
also O
stated O
that O
both O
maps O
were O
not O
very O
challenging O
to O
them O
and O
maybe O
it O
could O
be O
more O
suitable O
for O
players B
with O
little O
experience B
. O

With O
that O
feedback O
we O
believe O
that O
, O
in O
the O
current O
state O
, O
our O
agent O
has O
enough O
expertise O
to O
play B
against O
novice B
players I
. O

Thereby O
, O
the O
novice B
players I
can O
be O
entertained O
while O
learning O
the O
game B
style I
. O

By O
observing O
the O
data O
regarding O
the O
player B
immersion O
during O
the O
match B
, O
we O
can O
consider O
that O
the O
game B
provided O
a O
satisfactory O
level B
of O
immersion O
, O
since O
none O
of O
the O
players B
said O
they I
wanted I
to O
quit O
the O
game B
or O
found O
it O
too O
long O
. O

As O
well O
as O
the O
questions O
related O
to O
the O
player B
enjoyment O
during O
the O
match B
, O
although O
the O
agents O
were O
not O
always O
very O
challenging O
against O
all O
users B
, O
we O
can O
assume O
that O
most O
of O
them O
enjoyed O
playing B
against O
it O
. O

However O
, O
after O
considering O
the O
responses O
related O
to O
the O
game B
challenge O
provided O
, O
it O
was O
observed O
that O
, O
for O
experts O
, O
the O
developed O
artificial O
intelligence O
agent O
turned B
out O
to O
be O
weak B
and O
not O
very O
challenging O
. O

We O
believed O
that O
this O
occurred O
due O
to O
the O
absence O
of O
more O
complex O
strategies B
during O
its O
34 O
development O
, O
once O
it O
is O
not O
a O
simple O
task O
to O
develop O
such O
agents O
. O

As O
mentioned O
on O
Section O
3.6 O
, O
developing O
agents O
capable O
of O
defeating B
competitive B
human O
players B
in O
MOBA O
remains O
an O
open B
research O
challenge O
and O
we O
can O
attribute O
our O
agents O
flaws O
to O
this O
. O

Some O
of O
the O
intermediary O
players B
also O
found O
both O
agents O
not O
very O
difficult O
to O
defeat B
. O

As O
for O
the O
entry O
- O
level B
players B
, O
based B
on O
their O
responses O
we O
could O
state O
that O
they O
found O
both O
opponents B
too O
challenging O
, O
because O
probably O
they O
had O
a O
more O
difficult O
time B
trying O
to O
win O
the O
match B
although O
the O
agent O
had O
kept O
the O
same O
pace O
as O
his O
. O

7 O
. O

Conclusion O
The O
dynamic O
difficulty O
adjustment O
consists O
in O
an O
alternative O
towards O
the O
definition O
of O
the O
game B
challenge O
levels B
. O

This O
adjustment O
is O
dynamically O
performed O
, O
making O
it O
possible O
to O
track O
the O
player B
’s O
skills I
and O
adjust O
itself O
during O
game B
runtime I
. O

The O
presented O
work O
aimed O
to O
increase B
the O
player B
’s O
entertainment O
by O
providing O
a O
mechanism B
that O
adjusts O
the O
game B
AI O
agent O
according O
to O
the O
player B
’s O
skills B
. O

This O
mechanism B
was O
implemented O
on O
a O
MOBA O
game B
, O
called B
Defense O
of O
the O
Ancient O
( O
DotA O
) O
. O

After O
performing O
experiments O
that O
simulate O
the O
three O
main O
player B
’s O
behaviors I
( O
beginner B
, O
regular O
and O
experienced B
) O
, O
it O
was O
possible O
to O
verify O
that O
the O
dynamic O
difficulty O
adjustment O
mechanism B
was O
able O
to O
keep O
up O
with O
the O
player B
’s O
abilities I
on O
85 O
% O
of O
all O
experiments O
. O

On O
the O
remaining O
experiments O
that O
failed O
to O
suit O
the O
player B
’s O
skill I
, O
10 O
% O
of O
it O
occurred O
because O
the O
adjustment O
mechanism B
spent O
too O
much O
time B
to O
perform O
each O
needed O
adjustment O
which O
resulted O
in O
a O
great O
difference O
between O
the O
players B
performance O
. O

And O
the O
last O
5 O
% O
of O
it O
occurred O
due O
to O
an O
excess O
of O
adjustments O
that O
were O
performed O
too O
quickly O
, O
without O
giving O
enough O
time B
to O
the O
game B
to O
evolve O
properly O
. O

Given O
the O
presented O
results O
, O
we O
can O
conclude O
that O
the O
proposed O
mechanism B
behaved O
as O
expected O
and O
is O
capable O
to O
offer O
a O
game B
match I
compatible O
with O
the O
simulated O
player B
’s O
performance O
. O

Also O
, O
after O
observing O
all O
obtained O
results O
, O
we O
can O
state O
that O
the O
key O
to O
a O
balanced B
game I
is O
to O
keep O
changing O
the O
difficulty O
of O
the O
adaptive O
player B
in O
order O
to O
follow O
the O
performance O
of O
the O
human O
player B
and O
avoid O
boredom O
and O
frustration O
. O

As O
future O
work O
, O
the O
dynamic O
difficulty O
adjustment O
mechanism B
will O
be O
improved O
in O
order O
to O
decrease O
the O
amount O
of O
cases O
where O
the O
balance O
did O
not O
work O
properly O
. O

We O
believe O
that O
we O
must O
implement O
a O
general O
AI O
, O
capable O
of O
handling O
almost O
all O
heroes B
available O
in O
DotA O
, O
in O
order O
to O
offer O
diversity O
35 O
of O
strategies B
and O
helping O
the O
player B
to O
learn O
how O
to O
play B
against O
different O
heroes B
. O

Another O
improvement O
that O
could O
be O
done O
is O
testing O
the O
activation O
or O
deactivation O
of O
features O
, O
like O
item B
buy O
, O
spell B
casting O
and O
combos O
, O
tracking O
the O
player B
behavior I
and O
trying O
to O
imitate O
his O
/ O
her O
knowledge O
. O

In O
machine O
learning O
field B
, O
it O
is O
possible O
to O
try O
to O
learn O
the O
players B
preference O
, O
in O
order O
to O
improve O
the O
knowledge O
of O
the O
intelligent O
agent O
, O
making O
it O
to O
follow O
the O
player B
’s O
behavior I
. O

Moreover O
, O
machine O
learning O
could O
track O
which O
skills B
the O
player B
has O
developed O
and O
push O
new O
knowledge O
into O
the O
agent O
in O
order O
to O
try O
to O
stimulate O
the O
player B
to O
explore O
the O
game B
and O
learn O
what O
the O
agent O
is O
doing O
. O

We O
could O
also O
track O
the O
player B
preferences O
in O
order O
to O
try O
to O
classify O
his O
/ O
her O
play B
style I
and O
select O
heroes B
that O
match B
his O
/ O
her O
preferences O
, O
making O
the O
learning O
curve O
smoother O
. O

Finally O
we O
want O
to O
performe O
qualitative O
tests O
with O
a O
larger O
number O
of O
players B
in O
order O
to O
get O
more O
insights O
about O
the O
proposed O
mechanism B
. O

8 O
. O

Acknowledgements O
This O
work O
was O
partially O
supported B
by O
CAPES O
, O
CNPq O
and O
Fapemig O
. O

We O
would O
like O
to O
thank O
all O
the O
volunteers O
that O
participated O
in O
our O
qualitative O
tests O
. O

References O
References O
Andrade O
, O
G. O
, O
Ramalho O
, O
G. O
, O
Santana O
, O
H. O
, O
Corruble O
, O
V. O
, O
2005 O
. O

Extending O
reinforcement O
learning O
to O
provide O
dynamic O
game B
balancing O
. O

In O
: O
Proceedings O
of O
the O
Workshop O
on O
Reasoning O
, O
Representation O
, O
and O
Learning O
in O
Computer O
Games O
, O
19th O
International O
Joint O
Conference O
on O
Artificial O
Intelligence O
( O
IJCAI O
) O
. O

pp O
. O

7–12 O
. O

Bakkes O
, O
S. O
, O
Spronck O
, O
P. O
, O
van O
den O
Herik O
, O
J. O
, O
2009 O
. O

Rapid O
and O
reliable O
adaptation O
of O
video B
game I
AI O
. O

Computational O
Intelligence O
and O
AI O
in O
Games O
, O
IEEE O
Transactions O
on O
1 O
( O
2 O
) O
, O
93–104 O
. O

Bartle O
, O
R. O
A. O
, O
2004 O
. O

Designing O
virtual O
worlds B
. O

New O
Riders O
. O

Bowman O
, O
D. O
A. O
, O
McMahan O
, O
R. O
P. O
, O
2007 O
. O

Virtual O
reality O
: O
how O
much O
immersion O
is O
enough O
? O
Computer O
40 O
( O
7 O
) O
, O
36–43 O
. O

36 O
Buro O
, O
M. O
, O
2003 O
. O

Real B
- I
time I
strategy O
games I
: O
A O
new O
AI O
research O
challenge O
. O

In O
: O
IJCAI O
. O

pp O
. O

1534–1535 O
. O

Buro O
, O
M. O
, O
2004 O
. O

Call B
for O
ai O
research O
in O
rts O
games B
. O

In O
: O
Proceedings O
of O
the O
AAAI-04 O
Workshop O
on O
Challenges O
in O
Game O
AI O
. O

pp O
. O

139–142 O
. O

Buro O
, O
M. O
, O
Furtak O
, O
T. O
, O
2004 O
. O

RTS O
games B
and O
real B
- I
time I
AI O
research O
. O

In O
: O
Proceedings O
of O
the O
Behavior O
Representation O
in O
Modeling O
and O
Simulation O
Conference O
( O
BRIMS O
) O
. O

Vol O
. O

6370 O
. O

Csikszentmihalyi O
, O
M. O
, O
1991 O
. O

Flow O
. O

HarperCollins O
. O

Csikszentmihalyi O
, O
M. O
, O
2000 O
. O

Beyond O
boredom O
and O
anxiety O
. O

Jossey O
- O
Bass O
. O

Csikszentmihalyi O
, O
M. O
, O
Nakamura O
, O
J. O
, O
2010 O
. O

Effortless O
attention O
in O
everyday O
life O
: O
A O
systematic O
phenomenology O
. O

In O
: O
Bruya O
, O
B. O
( O
Ed O
. O

) O
, O
Effortless O
Attention O
: O
A O
New O
Perspective O
in O
the O
Cognitive O
Science O
of O
Attention O
and O
Action O
. O

MIT O
Press O
, O
pp O
. O

179–189 O
. O

de O
Araujo O
, O
B. O
B. O
P. O
L. O
, O
Feij´o O
, O
B. O
, O
Oct. O
2013 O
. O

Evaluating O
dynamic O
difficulty O
adaptivity O
in O
shoot’em O
up O
games B
. O

In O
: O
Proceedings O
of O
the O
XII O
Brazilian O
Symposium O
on O
Games O
and O
Digital O
Entertainment O
- O
SBGames O
2013 O
. O

S˜ao O
Paulo O
, O
Brazil O
, O
pp O
. O

229 O
– O
238 O
. O

Demasi O
, O
P. O
, O
Adriano O
, O
J. O
d. O
O. O
, O
2003 O
. O

On O
- O
line O
coevolution O
for O
action O
games B
. O

International O
Journal O
of O
Intelligent O
Games O
& O
Simulation O
2 O
( O
2 O
) O
. O

Egenfeldt O
- O
Nielsen O
, O
S. O
, O
Smith O
, O
J. O
H. O
, O
Tosca O
, O
S. O
P. O
, O
2013 O
. O

Understanding O
video B

 O
games I
: O
The O
essential O
introduction O
. O

Routledge O
. O

Fox O
, O
C. O
M. O
, O
Brockmyer O
, O
J. O
H. O
, O
2013 O
. O

The O
development O
of O
the O
game B
engagement O
questionnaire O
: O
A O
measure O
of O
engagement O
in O
video B
game I
playing B
: O
Response O
to O
reviews O
. O

Interacting O
with O
Computers O
, O
iwt003 O
. O

Games O
, O
R. O
, O
Games O
, O
W. O
R. O
, O
2009 O
. O

League O
of O
legends O
. O

Computer O
Game O
. O

Gaudiosi O
, O
J. O
, O
2012 O
. O

Riot B
games I
’ O
league B
of I
legends I
officially O
becomes O
most O
played B
pc O
game B
in O
the O
world B
. O

Forbes O
. O

Jul O
11 O
, O
2011 O
. O

Hunicke O
, O
R. O
, O
2005 O
. O

The O
case O
for O
dynamic O
difficulty O
adjustment O
in O
games B
. O

In O
: O
Proceedings O
of O
the O
2005 O
ACM O
SIGCHI O
International O
Conference O
on O
Advances O
in O
computer O
entertainment O
technology O
. O

ACM O
, O
pp O
. O

429–433 O
. O

37 O
IJsselsteijn O
, O
W. O
, O
De O
Kort O
, O
Y. O
, O
Poels O
, O
K. O
, O
Jurgelionis O
, O
A. O
, O
Bellotti O
, O
F. O
, O
2007 O
. O

Characterising O
and O
measuring O
user B
experiences B
in O
digital O
games B
. O

In O
: O
International O
conference O
on O
advances O
in O
computer O
entertainment O
technology O
. O

Vol O
. O

2 O
. O

p. O
27 O
. O

Jennett O
, O
C. O
, O
Cox O
, O
A. O
L. O
, O
Cairns O
, O
P. O
, O
Dhoparee O
, O
S. O
, O
Epps O
, O
A. O
, O
Tijs O
, O
T. O
, O
Walton O
, O
A. O
, O
2008 O
. O

Measuring O
and O
defining O
the O
experience B
of O
immersion O
in O
games B
. O

International O
journal O
of O
human O
- O
computer O
studies O
66 O
( O
9 O
) O
, O
641–661 O
. O

Johnson O
, O
D. O
, O
Nacke O
, O
L. O
E. O
, O
Wyeth O
, O
P. O
, O
2015 O
. O

All O
about O
that O
base B
: O
differing O
player B
experiences B
in O
video B
game I
genres O
and O
the O
unique O
case O
of O
moba B

 O
games I
. O

In O
: O
Proceedings O
of O
the O
33rd O
Annual O
ACM O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
. O

ACM O
, O
pp O
. O

2265–2274 O
. O

Klimmt O
, O
C. O
, O
Blake O
, O
C. O
, O
Hefner O
, O
D. O
, O
Vorderer O
, O
P. O
, O
Roth O
, O
C. O
, O
2009 O
. O

Player O
performance O
, O
satisfaction O
, O
and O
video B
game I
enjoyment O
. O

In O
: O
Entertainment O
Computing O
– O
ICEC O
2009 O
. O

Springer O
, O
pp O
. O

1–12 O
. O

Koster O
, O
R. O
, O
2010 O
. O

Theory O
of O
fun O
for O
game B
design O
. O

O’Reilly O
Media O
, O
Inc. O
Kwak O
, O
H. O
, O
Blackburn O
, O
J. O
, O
Han O
, O
S. O
, O
2015 O
. O

Exploring O
cyberbullying O
and O
other O
toxic O
behavior O
in O
team B
competition I
online O
games I
. O

Social O
Dynamics O
22 O
( O
28 O
) O
, O
47 O
. O

Løvoll O
, O
H. O
S. O
, O
Vittersø O
, O
J. O
, O
2014 O
. O

Can O
balance O
be O
boring O
? O
a O
critique O
of O
the O
challenges O
should O
match B
skills B
hypotheses O
in O
flow O
theory O
. O

Social O
indicators O
research O
115 O
( O
1 O
) O
, O
117–136 O
. O

Machado O
, O
M. O
C. O
, O
Fantini O
, O
E. O
P. O
, O
Chaimowicz O
, O
L. O
, O
2011 O
. O

Player B
modeling O
: O
Towards O
a O
common O
taxonomy O
. O

In O
: O
Computer O
Games O
( O
CGAMES O
) O
, O
2011 O
16th O
International O
Conference O
on O
. O

IEEE O
, O
pp O
. O

50–57 O
. O

Malone O
, O
T. O
W. O
, O
1981 O
. O

Toward O
a O
theory O
of O
intrinsically O
motivating O
instruction O
* O
. O

Cognitive O
science O
5 O
( O
4 O
) O
, O
333–369 O
. O

Mateas O
, O
M. O
, O
2002 O
. O

Interactive O
drama O
, O
art O
and O
artificial O
intelligence O
. O

Ph.D. O
thesis O
, O
Carnegie O
Mellon O
University O
, O
Pittsburgh O
, O
PA O
, O
USA O
, O
aAI3121279 O
. O

Missura O
, O
O. O
, O
G¨artner O
, O
T. O
, O
2009 O
. O

Player B
modeling O
for O
intelligent O
difficulty O
adjustment O
. O

In O
: O
Discovery O
Science O
. O

Springer O
, O
pp O
. O

197–211 O
. O

38 O
Nacke O
, O
L. O
, O
Lindley O
, O
C. O
A. O
, O
2008 O
. O

Flow O
and O
immersion O
in B
first O
- I
person I
shooters O
: O
measuring O
the O
player B
’s O
gameplay B
experience B
. O

In O
: O
Proceedings O
of O
the O
2008 O
Conference O
on O
Future O
Play O
: O
Research O
, O
Play O
, O
Share O
. O

ACM O
, O
pp O
. O

81–88 O
. O

Nareyek O
, O
A. O
, O
2004 O
. O

AI O
in O
computer O
games B
. O

Queue O
1 O
( O
10 O
) O
, O
58 O
. O

Norman O
, O
G. O
, O
2010 O
. O

Likert O
scales O
, O
levels B
of O
measurement O
and O
the O
laws O
of O
statistics O
. O

Advances O
in O
health O
sciences O
education O
15 O
( O
5 O
) O
, O
625–632 O
. O

Poole O
, O
S. O
, O
2004 O
. O

Trigger B
happy O
: O
Videogames B
and O
the O
entertainment O
revolution O
. O

Arcade O
Publishing O
. O

Silva O
, O
M. O
P. O
, O
2015 O
. O

Inteligˆencia O
artificial O
adaptativa O
para O
ajuste O
dinˆamico O
de O
dificuldade O
em O
jogos O
digitais O
. O

Master O
’s O
thesis O
, O
Universidade O
Federal O
de O
Minas O
Gerais O
( O
UFMG O
) O
, O
Belo O
Horizonte O
/ O
MG O
, O
Brazil O
. O

Silva O
, O
M. O
P. O
, O
Silva O
, O
V. O
d. O
N. O
, O
Chaimowicz O
, O
L. O
, O
2015 O
. O

Dynamic O
difficulty O
adjustment O
through O
an O
adaptive O
AI O
. O

In O
: O
Brazilian O
Symposium O
on O
Games O
and O
Entertainment O
( O
SBGames O
) O
, O
2015 O
SBC O
14th O
. O

pp O
. O

52–59 O
. O

Silva O
, O
V. O
d. O
N. O
, O
Chaimowicz O
, O
L. O
, O
2015a O
. O

On O
the O
development O
of O
intelligent O
agents O
for O
moba B
games I
. O

In O
: O
Brazilian O
Symposium O
on O
Games O
and O
Entertainment O
( O
SBGames O
) O
, O
2015 O
SBC O
14th O
. O

Silva O
, O
V. O
d. O
N. O
, O
Chaimowicz O
, O
L. O
, O
2015b O
. O

A O
tutor O
agent O
for O
moba B
games I
. O

In O
: O
Brazilian O
Symposium O
on O
Games O
and O
Entertainment O
( O
SBGames O
) O
, O
2015 O
SBC O
14th O
. O

Smith O
, O
A. O
M. O
, O
Lewis O
, O
C. O
, O
Hullett O
, O
K. O
, O
Smith O
, O
G. O
, O
Sullivan O
, O
A. O
, O
2011 O
. O

An O
inclusive O
taxonomy O
of O
player B
modeling O
. O

University O
of O
California O
, O
Santa O
Cruz O
, O
Tech O
. O

Rep. O
UCSC O
- O
SOE-11 O
- O
13 O
. O

Spronck O
, O
P. O
, O
Ponsen O
, O
M. O
, O
Sprinkhuizen O
- O
Kuyper O
, O
I. O
, O
Postma O
, O
E. O
, O
2006 O
. O

Adaptive O
game B
ai O
with O
dynamic O
scripting O
. O

Machine O
Learning O
63 O
( O
3 O
) O
, O
217–248 O
. O

Stanley O
, O
K. O
O. O
, O
Bryant O
, O
B. O
D. O
, O
Miikkulainen O
, O
R. O
, O
2005 O
. O

Evolving O
neural O
network O
agents O
in O
the O
nero O
video B
game I
. O

Proceedings O
of O
the O
IEEE O
, O
182–189 O
. O

Takatalo O
, O
J. O
, O
H¨akkinen O
, O
J. O
, O
Nyman O
, O
G. O
, O
2015 O
. O

Understanding O
presence O
, O
involvement O
, O
and O
flow O
in O
digital O
games B
. O

In O
: O
Game O
User O
Experience O
Evaluation O
. O

Springer O
, O
pp O
. O

87–111 O
. O

39 O
Thompson O
, O
P. O
, O
Parker O
, O
R. O
, O
Cox O
, O
S. O
, O
2015 O
. O

Interrogating O
creative O
theory O
and O
creative O
work O
: O
Inside O
the O
games B
studio O
. O

Sociology O
, O
0038038514565836 O
. O

Togelius O
, O
J. O
, O
De O
Nardi O
, O
R. O
, O
Lucas O
, O
S. O
M. O
, O
2007 O
. O

Towards O
automatic O
personalised O
content O
creation O
for O
racing O
games B
. O

In O
: O
Computational O
Intelligence O
and O
Games O
, O
2007 O
. O

CIG O
2007 O
. O

IEEE O
Symposium O
on O
. O

IEEE O
, O
pp O
. O

252–259 O
. O

Van O
Den O
Hoogen O
, O
W. O
, O
Ijsselsteijn O
, O
W. O
, O
de O
Kort O
, O
Y. O
, O
2008 O
. O

Exploring O
behavioral O
expressions O
of O
player B
experience B
in O
digital O
games B
. O

In O
: O
Proceedings O
of O
the O
workshop O
on O
Facial O
and O
Bodily O
Expression O
for O
Control O
and O
Adaptation O
of O
Games O
ECAG O
2008 O
. O

pp O
. O

11–19 O
. O

Weber O
, O
B. O
G. O
, O
Mateas O
, O
M. O
, O
Jhala O
, O
A. O
, O
2010 O
. O

Applying O
goal B
- O
driven O
autonomy O
to O
starcraft O
. O

In O
: O
AIIDE O
. O

Weber O
, O
B. O
G. O
, O
Mateas O
, O
M. O
, O
Jhala O
, O
A. O
, O
2011 O
. O

Building O
human O
- O
level B
AI O
for O
realtime O
strategy B
games B
. O

In O
: O
AAAI O
Fall O
Symposium O
: O
Advances O
in O
Cognitive O
Systems O
. O

Vol O
. O

11 O
. O

p. O
01 O
. O

Wheat O
, O
D. O
, O
Masek O
, O
M. O
, O
Lam O
, O
C. O
P. O
, O
Hingston O
, O
P. O
, O
2015 O
. O

Dynamic O
difficulty O
adjustment O
in O
2d O
platformers O
through O
agent O
- O
based B
procedural O
level B
generation O
. O

In O
: O
Systems O
, O
Man O
, O
and O
Cybernetics O
( O
SMC O
) O
, O
2015 O
IEEE O
International O
Conference O
on O
. O

IEEE O
, O
pp O
. O

2778–2785 O
. O

Xavier O
, O
G. O
, O
2010 O
. O

A O
condi¸c˜ao O
eletrol´udica O
: O
Cultura O
visual O
nos O
jogos O
eletrˆonicos O
. O

Teres´opolis O
: O
Novas O
ideias O
. O

Yannakakis O
, O
G. O
N. O
, O
2008 O
. O

How O
to O
model O
and O
augment O
player B
satisfaction I
: O
a O
review O
. O

In O
: O
Proceedings O
of O
the O
1st O
Workshop O
on O
Child O
, O
Computer O
and O
Interaction O
( O
ICMI’08 O
) O
, O
ACM O
Press O
, O
Montreal O
, O
Canada O
. O

Yannakakis O
, O
G. O
N. O
, O
Hallam O
, O
J. O
, O
2007 O
. O

Towards O
optimizing O
entertainment O
in O
computer O
games B
. O

Applied O
Artificial O
Intelligence O
. O

40 O
Appendix O
A. O
User O
Experiments O
Questionnaire O
Figure O
A.22 O
: O
Pre O
- O
test O
questionnaire O
. O

41 O
Figure O
A.23 O
: O
Pre O
- O
test O
questionnaire O
. O

42 O
Figure O
A.24 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
immersion O
. O

43 O
Figure O
A.25 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
challenge O
. O

44 O
Figure O
A.26 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
player B
skills I
. O

45 O
Figure O
A.27 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
enjoyment O
during O
the O
match B
. O

46 O
Figure O
A.28 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
the O
opponent B
level B
. O

47 O
Figure O
A.29 O
: O
Post O
- O
test O
questionnaire O
: O
Questions O
about O
the O
player B
’s O
perception O
during O
the O
game B
match I
. O

48 O
A O
Tutor O
Agent O
for O
MOBA O
Games O
Victor O
do O
Nascimento O
Silva O
and O
Luiz O
Chaimowicz O
Departament O
of O
Computer O
Science O
, O
Universidade O
Federal O
de O
Minas O
Gerais O
, O
Brazil O
Abstract O
Digital O
games B
have O
become O
a O
key O
player B
in O
the O
entertainment O
industry O
, O
attracting O
millions O
of O
new O
players B
each O
year O
. O

In O
spite O
of O
that O
, O
novice B
players I
may O
have O
a O
hard B
time B
when O
playing B
certain O
types O
of O
games B
, O
such O
as O
MOBAs O
and O
MMORPGs O
, O
due O
to O
their O
steep O
learning O
curves O
and O
not O
so O
friendly O
online B
communities I
. O

In O
this O
paper O
, O
we O
present O
an O
approach O
to O
help O
novice B
players I
in O
MOBA O
games B
overcome O
these O
problems O
. O

An O
artificial O
intelligence O
agent O
plays B
alongside O
the O
player B
analysing O
his I
/ I
her O
performance O
and O
giving O
tips O
about O
the O
game B
. O

Experiments O
performed O
with O
the O
game B
League O
of O
Legends O
show O
the O
potential O
of O
this O
approach O
. O

Keywords O
: O
Artificial O
Intelligence O
; O
Game O
Tutorials O
; O
Game O
Learning O
Author O
’s O
Contact O
: O
{ O
vnsilva,chaimo}@dcc.ufmg.br O
1 O
Introduction O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
have O
become O
very O
popular O
in O
recent O
years O
, O
reaching O
millions O
of O
users B
everyday O
. O

On O
a O
first O
sight O
, O
MOBA O
games B
present O
a O
simple B
game I
concept O
: O
the O
player B
controls B
a O
hero B
that O
should O
use O
its O
abilities B
to O
fight B
enemy B
heroes I
and O
creeps B
, O
destroy O
structures O
and O
conquer O
an O
enemy B
base I
. O

Despite O
this O
simple O
concept O
, O
this O
game B
genre O
have O
a O
complex B
gameplay I
and O
requires O
a O
lot O
of O
knowledge O
that O
may O
not O
be O
so O
familiar O
to O
novice B
players[Yang O
et O
al O
. O

2014 O
] O
. O

They O
should O
understand O
how O
to O
walk O
, O
cast O
spells B
, O
last O
hit O
creeps B
, O
harass O
enemies B
, O
fight B
neutral O
creeps B
, O
attack B
, O
perform O
itemization O
, O
among O
other O
things O
. O

Resource B
management O
must O
be O
done O
, O
like O
using O
mana B
, O
energy B
and O
gold B
, O
and O
players B
should O
understand O
how O
to O
build O
his O
/ O
her O
hero B
and O
plan O
strategies B
based B
on O
these O
features O
. O

Games O
often O
present O
a O
tutorial B
where O
the O
main B
gameplay I
is O
presented O
, O
aiming O
to O
teach O
newcomers O
how O
to O
play B
. O

Requiring O
the O
player B
to O
complete O
tutorial B
matches B
or O
playing B
against O
bots O
is O
also O
a O
very O
common O
practice O
, O
trying O
to O
assure O
that O
the O
player B
understands O
the O
concepts O
presented O
. O

However O
, O
in O
MOBA O
games B
, O
there O
are O
some O
features O
or O
roles B
that O
require O
the O
cooperation B
of O
more O
than O
one O
player B
. O

To O
group B
players B
of O
similar O
skills B
in O
a O
team B
, O
these O
games B
implement O
a O
matchmaking B
system I
[ O
Mylak O
and O
Deja O
2014 O
] O
, O
but O
, O
in O
a O
cooperative O
scenario O
, O
this O
approach O
may O
not O
perform O
well O
. O

The O
performance O
can O
be O
particularly O
poor B
in O
initial O
matches B
, O
as O
the O
combination O
of O
players B
with O
low O
skill B
or O
no O
knowledge O
of O
some O
features O
in O
the O
same O
team B
will O
not O
help O
them O
in O
learning O
the O
game B
. O

In O
this O
paper O
we O
propose O
an O
approach O
to O
help O
novice B
players I
to O
learn O
the O
basics O
of O
MOBA O
games B
, O
improving O
their O
entertainment O
. O

The O
idea O
is O
to O
develop O
an O
Artificial O
Intelligence O
Agent O
that O
will O
play B
alongside O
a O
human O
team B
acting O
as O
a O
tutor O
. O

This O
agent O
will O
provide O
tips O
and O
support B
players B
, O
aiming O
at O
improving O
their O
experience B
and O
gameplay B
while O
also O
benefiting O
the O
game B
by O
reducing O
the O
number O
of O
dropouts O
. O

This O
tutor O
will O
replace O
one O
of O
the O
human O
players B
and O
will O
go O
unnoticed O
by O
others O
, O
which O
will O
consider O
it O
a O
regular O
player B
. O

Despite O
of O
the O
fact O
that O
the O
tutor O
supports B
all O
team B
, O
it O
is O
focused B
in O
supporting B
a O
specific O
player B
, O
that O
it O
identifies O
as O
partner O
. O

The O
rest O
of O
this O
paper O
is O
organized O
as O
follows O
: O
Section O
2 O
presents O
some O
definitions O
related O
to O
MOBA O
games B
and O
their O
player B
community B
while O
in O
Section O
3 O
we O
discuss O
some O
related O
works O
. O

Section O
4 O
describes O
the O
mechanisms B
used O
for O
the O
tutor O
development O
and O
Section O
5 O
presents O
the O
experiments O
and O
obtained O
results O
. O

Finally O
, O
Section O
6 O
brings O
the O
conclusion O
and O
directions O
for O
future O
work O
. O

2 O
MOBA O
Games O
Before O
describing O
our O
approach O
, O
it O
is O
important O
to O
present O
the O
gameplay B
, O
characteristics O
and O
features O
of O
MOBA O
games B
. O

We O
also O
discuss O
the O
profile O
of O
the O
gamer B
that O
generally O
plays B
this O
game B
genre O
, O
presenting O
its O
behavior O
in O
community B
. O

2.1 O
General O
Description O
Multiplayer O
Online O
Battle O
Arena O
is O
a O
genre O
originated O
from O
Real O
Time O
Strategy O
( O
RTS O
) O
games B
, O
in O
which O
players B
control B
an O
unit B
( O
Hero O
) O
in O
a O
map O
, O
taking O
part O
in O
a O
battle B
to O
destroy O
an O
enemy B
base I
, O
like O
a O
capture B
the O
flag O
game B
. O

As O
the O
popularity O
of O
RTS O
games B
increased B
in O
the O
last O
decade O
, O
players B
began O
to O
develop O
custom B
maps I
and O
features O
to O
games B
, O
called B
MODs O
, O
using O
tools O
distributed O
with O
the O
game B
. O

Thus O
, O
a O
custom B
map I
from O
Starcraft O
was O
the O
start B
point B
of O
MOBA O
games B
, O
giving O
rise O
to O
Aeon O
of O
Strife O
. O

The O
popularity O
of O
this O
genre O
grew O
and O
users B
from O
similar O
games B
began O
to O
develop O
similar O
maps O
. O

Defense B
of O
the O
Ancients O
( O
DotA O
) O
was O
one O
of O
these O
maps O
, O
originated O
from O
Warcraft O
III O
. O

DotA O
was O
so O
successful O
that O
its O
name O
was O
used O
to O
describe O
the O
genre O
for O
a O
long O
time B
, O
the O
DotA O
like O
games B
. O

The O
term O
MOBA O
just O
came O
up O
in O
2009 O
, O
when O
Riot O
Games O
used O
it O
to O
describe O
its O
debuting O
title O
League O
of O
Legends O
( O
LoL O
) O
[ O
Ferrari O
2013 O
] O
. O

Therefore O
, O
MOBA O
became O
very O
popular O
and O
originated O
several O
other O
titles O
like O
Dota O
2 O
, O
Heroes O
of O
the O
Storm O
, O
Heroes O
of O
Newerth O
and O
Strife O
. O

Data O
from O
Riot O
Games O
shows O
that O
67 O
millions O
of O
users B
plays B
matches B
every O
month O
, O
with O
27 O
millions O
of O
players B
daily O
and O
7.5 O
millions O
of O
player B
simultaneously O
[ O
Games O
2015 O
] O
. O

MOBA O
gameplay B
generally O
consist O
of O
matches B
where O
ten O
players B
compete O
, O
five O
in O
each O
team B
. O

At O
the O
beginning O
of O
a O
match B
, O
each O
player B
must O
choose O
a O
hero B
, O
which O
he O
/ O
she O
will O
be O
playing B
during O
the O
entire O
match B
. O

The O
hero B
is O
a O
powerful B
unit I
with O
an O
unique O
spell B
set O
that O
can O
be O
used O
to O
build O
complex O
game B
strategies B
and O
goals B
. O

Controlling B
that O
hero B
, O
the O
player B
should O
join O
his O
teammates B
in O
a O
competition B
to O
destroy O
an O
enemy B
base I
, O
which O
is O
heavily O
guarded O
by O
turrets B
and O
structures O
[ O
Groen O
2012 O
] O
. O

A O
MOBA O
map O
is O
generally O
composed O
of O
three O
lanes B
: O
bottom B
, O
mid O
and O
top O
, O
where O
AI O
driven O
creeps B
spawns O
. O

By O
defeating B
these O
creeps B
, O
neutral O
creeps B
or O
enemy B

 O
heroes I
the O
player B
can O
gather O
gold B
and O
experience B
. O

While O
gold B
can O
be O
used O
to O
buy O
items B
in O
order O
to O
improve O
hero B
status I
, O
experience B
makes O
the O
hero B
gain O
level B
, O
getting O
stronger O
and O
learning O
new O
abilities B
. O

2.2 O
Community O
Real O
Time O
Strategy O
players B
are O
widely O
known O
by O
their O
expertise O
in O
handling O
lots O
of O
information O
simultaneously O
and O
for O
also O
being O
very O
“ O
toxic O
” O
, O
raging O
at O
novice B
players I
[ O
Kwak O
et O
al O
. O

2015 O
] O
. O

In O
Warcraft O
III O
it O
is O
very O
common O
to O
kick O
players B
from O
matches B
when O
they O
are O
identified O
as O
novice B
. O

Since O
DotA O
evolved O
from O
Warcraft O
III O
it O
is O
also O
possible O
to O
kick O
players B
from O
games B
. O

When O
DotA O
became O
popular O
in O
online O
environments O
, O
the O
common O
sandbox O
were O
hold O
in O
a O
P2P O
architecture O
, O
where O
one O
player B
is O
the O
host B
and O
other O
players B
are O
able O
to O
connect O
to O
his O
match B
. O

As O
result O
, O
most O
players B
were O
able O
to O
join O
matches B
where O
there O
are O
players B
that O
already O
know O
the O
game B
environment O
and O
mechanics B
very O
well O
. O

Instead O
of O
being O
friendly O
and O
patient O
to O
newcomers O
, O
experienced O
players B
usually O
bully O
them O
[ O
Kwak O
et O
al O
. O

2015],[Blackburn O
and O
Kwak O
2014 O
] O
. O

Raging O
characteristics O
from O
RTS O
players B
seems O
to O
be O
connected O
to O
modern O
MOBA O
games B
as O
most O
of O
them O
migrated O
from O
RTS O
to O
MOBA O
. O

Valve O
approach O
to O
fight B
toxic O
behaviour O
in O
DotA O
2 O
uses O
a O
player B
report B
system I
. O

Riot O
also O
noticed O
toxic O
behavior O
amongst O
LoL O
players B
, O
having O
initially O
created O
a O
system O
where O
players B
could O
judge O
cases O
based B
on O
reports B
[ O
Lin O
2013 O
] O
. O

However O
they O
recently O
updated B
their O
system O
, O
applying O
Machine O
Learning O
Techniques B
to O
arXiv:1706.02832v1 O
[ O
cs O
. O

AI O
] O
9 O
Jun O
2017 O
identify O
toxic O
behavior[Lin O
2015 O
] O
. O

As O
presented O
, O
MOBA O
community B
its O
not O
very O
friendly O
[ O
Blackburn O
and O
Kwak O
2014 O
] O
, O
and O
that O
could O
imply O
in O
players B
feeling O
frustrated O
more O
often O
. O

Some O
online B
games I
have O
similar O
behavior O
amongst O
their O
players B
, O
and O
try O
to O
solve O
the O
problem O
by O
encouraging O
experienced O
players B
to O
help O
newcomers O
. O

In O
these O
games B
, O
to O
avoid O
the O
experience B
level B
gap O
, O
developers O
or O
even O
the O
community B
implements O
an O
” O
adopt O
a O
newbie B
” O
program O
, O
where O
an O
experienced O
player B
is O
encouraged O
to O
help O
a O
newcomer O
finishing O
quests O
, O
gathering O
items B
and O
getting O
levels B
. O

When O
the O
newcomer O
becomes O
experienced B
both O
, O
the O
godfather O
and O
the O
newcomer O
, O
can O
be O
gifted O
in O
game B
with O
special O
items B
, O
gold B
or O
titles O
in O
the O
community B
. O

Csikszentmihalyi O
presents O
a O
model O
for O
identifying O
if O
players B
are O
in O
flow O
[ O
Csikszentmihalyi O
1991 O
] O
. O

Being O
in O
flow O
state O
means O
that O
players B
must O
not O
feel O
frustrated O
or O
bored O
, O
following O
in O
a O
way O
that O
the O
entertainment O
is O
reached O
, O
having O
a O
good B
game I
environment O
and O
community B
can O
help O
players B
to O
be O
in O
flow O
state O
. O

However O
MOBA O
games B
involves O
much O
more O
than O
just O
the O
difficulty O
presented O
by O
game B
mechanics I
. O

Some O
human O
factors O
, O
like O
toxic O
behavior O
, O
can O
drive O
players B
to O
frustration O
. O

Playing B
in O
team B
requires O
cooperation I
and O
patience O
, O
and O
that O
is O
what O
the O
matchmaking B
system I
from O
MOBA O
games B
tries O
to O
obtain O
[ O
Mylak O
and O
Deja O
2014 O
] O
. O

Matching B
players B
of O
similar O
skills B
could O
provide O
a O
friendly O
environment O
, O
where O
players B
could O
grow O
together O
, O
in O
skills B
and O
knowledge O
, O
but O
that O
hardly O
happens O
if O
there O
is O
no O
one O
that O
can O
teach O
. O

It O
is O
not O
difficult O
to O
find O
players B
appealing O
to O
video O
tutorials B
or O
pro O
player B
guides O
trying O
to O
learn O
game B
features O
. O

Also O
it O
is O
hard B
to O
reach O
flow O
state O
if O
players B
need O
to O
leave O
game B
environment O
to O
watch O
or O
read O
tips O
. O

It O
is O
even O
harder B
to O
reach O
flow O
when O
there O
is O
another O
player B
flaming B
at O
you O
by O
your O
mistakes B
or O
inexperience O
. O

3 O
Related O
Work O
Tutorials B
are O
the O
main O
tool O
to O
interactively O
teach O
game B
features O
to O
players B
in O
modern O
games B
. O

Normally O
, O
these O
teaching O
approaches O
are O
simpler O
to O
apply O
since O
the O
game B
provides O
an O
environment O
for O
practicing O
what O
the O
player B
has O
learned O
[ O
Silva O
and O
Nascimento O
2012 O
] O
. O

Although O
most O
game B
tutorials I
are O
successful O
to O
show O
simple B
game I
features O
, O
it O
is O
not O
hard B
to O
find O
players B
that O
claim O
to O
have O
learned O
less O
than O
necessary O
or O
did O
not O
understand O
the O
features O
at O
all O
. O

This O
can O
be O
justified O
by O
the O
complex B
gameplay I
and O
the O
broad O
range B
of O
mechanics B
found O
in O
modern O
games B
. O

MOBAs B
also O
have O
joined O
the O
tutorial B
phenomena O
, O
as O
they O
are O
generally O
harder B
to O
learn O
than O
simple B
games I
. O

In O
Dota O
2 O
, O
for O
example O
, O
there O
is O
a O
tutorial B
in O
which O
the O
player B
has O
a O
tutor O
hero B
that O
guides O
him O
/ O
her O
through O
the O
main O
game B
features O
and O
mechanics B
such O
as O
turrets B
, O
creep B
farming B
and O
denying O
. O

Strife O
and O
Heroes O
of O
Newerth O
both O
offer O
similar O
environments O
of O
learning O
main B
gameplay I
. O

By O
defeating B
a O
boss O
, O
that O
is O
a O
champion B
, O
the O
player B
completes O
the O
dungeon B
like O
tutorials B
. O

League O
of O
Legends O
offers O
the O
player B
a O
tutorial B
that O
shows O
the O
game B
features O
in O
an O
single O
lane B
dungeon B
. O

All O
tutorials B
cited O
above O
require O
and/or O
invite O
the O
player B
to O
join O
a O
set O
of O
matches B
against O
AI O
in O
order O
to O
improve O
his O
/ O
her O
skills B
and O
be O
able O
to O
play B
against O
other O
players B
. O

There O
is O
also O
some O
academic O
research O
on O
how O
to O
use O
AI O
mechanisms B
to O
help O
players B
to O
learn O
complex O
games B
. O

For O
example O
, O
in O
Cunha O
et O
. O

al O
. O

[ O
Cunha O
et O
al O
. O

2015 O
] O
, O
authors O
implemented O
an O
advisory O
system O
that O
gives O
hints O
based B
on O
the O
current O
status B
of O
the O
game B
and O
the O
performance O
of O
the O
player B
. O
They I
tested I
their O
approach O
in O
Wargus O
, O
a O
RTS O
game B
with O
a O
complex O
scenario O
. O

On O
a O
more O
broad O
sense O
, O
a O
general O
survey O
on O
the O
use O
of O
real O
time B
AI O
based B
teammates B
in O
several O
games B
genres O
can O
be O
found O
in O
[ O
McGee O
and O
Abraham O
2010 O
] O
. O

AI O
teammates B
can O
be O
used O
with O
different O
purposes O
, O
including O
to O
helping O
the O
players B
to O
learn O
a O
game B
, O
as O
we O
do O
in O
this O
paper O
. O

The O
basics O
of O
game B
features O
must O
be O
presented O
to O
players B
in O
every O
MOBA O
game B
, O
and O
some O
of O
these O
games B
only O
show O
the O
mechanics B
in O
a O
very O
high B
level I
. O

Thereby O
, O
most O
players B
claim O
that O
the O
tutorials B
are O
not O
sufficient O
to O
teach O
the O
basics O
of O
the O
game B
. O

Moreover O
, O
it O
normally O
takes O
time B
to O
initially O
start B
playing B
a O
tutorial B
, O
then O
playing B
with O
bots O
and O
finally O
Player O
versus O
Player O
( O
PvP O
) O
. O

So O
, O
players B
generally O
become O
impatient O
and O
go O
straight B
to O
the O
PvP O
game B
. O

And O
it O
is O
not O
difficult O
to O
find O
player B
that O
gets O
frustrated O
by O
meeting O
experienced O
players B
playing B
in O
low B
level I
accounts I
, O
called B
“ O
smurfs O
” O
. O

So O
most O
players B
tend O
to O
search O
for O
partners O
who O
can O
teach O
them O
the O
game B
properly O
, O
others O
does O
not O
even O
keep O
trying O
, O
just O
stop O
playing B
. O

Instead O
of O
just O
giving O
a O
set O
of O
instructions O
to O
the O
player B
, O
our I
approach I
introduces O
an O
AI O
agent O
that O
plays B
alongside O
him O
/ O
her O
. O

The O
agent O
aims O
to O
teach O
the O
player B
the O
general O
game B
features O
giving O
instructions O
while O
he O
/ O
she O
plays B
the O
game B
. O

At O
first O
that O
may O
just O
seems O
like O
a O
regular O
game B
tutorial I
but O
, O
in O
our O
approach O
, O
the O
agent O
analyses O
the O
player B
skills I
and O
teaches O
him O
/ O
her O
specific O
features O
verifying O
the O
player B
performance O
. O

Furthermore O
, O
the O
agent O
is O
capable O
of O
helping O
the O
player B
to O
improve O
his O
/ O
her O
skills B
in O
game B
using O
a O
game B
character B
. O

This O
approach O
reduces O
the O
player B
frustration O
by O
ensuring O
that O
he O
/ O
she O
is O
playing B
with O
a O
reliable O
, O
non O
- O
toxic O
partner O
. O

4 O
Implementing O
a O
Tutor O
Bot O
for O
MOBA O
Games B
The O
first O
step O
in O
our O
implementation O
was O
to O
find O
a O
suitable O
MOBA O
platform O
. O

Most O
of O
them O
are O
commercial O
games B
and O
do O
not O
provide O
tools O
that O
allow O
developers O
to O
implement O
novel O
game B
features O
or O
mods O
. O

After O
doing O
a O
broad O
research O
we O
found O
that O
the O
best O
option O
was O
to O
implement O
our O
system O
in O
a O
game B
that O
is O
popular O
and O
stable O
, O
so O
we O
chose O
League O
of O
Legends O
( O
LoL B
) O
as O
our O
main O
platform O
. O

We O
then O
chose O
to O
use O
third O
party O
tools O
to O
develop O
the O
tutor O
system O
. O

Thereby O
we O
develop O
our O
approach O
using O
Bot O
of O
Legends O
( O
BoL O
) O
a O
third O
party O
tool O
that O
injects O
scripts B
written O
in O
Lua O
in O
the O
game B
. O

BoL O
is O
maintained O
by O
an O
open B
developers O
community B
, O
and O
only O
has O
access O
to O
the O
information O
the O
players B
have O
. O

( O
cheating O
is O
n’t O
allowed O
) O
. O

In O
MOBA O
there O
are O
different O
roles B
the O
heroes B
can O
assume O
, O
each O
one O
aiming O
to O
fulfill O
some O
needs O
of O
the O
team B
, O
given O
a O
strategy B
. O

We O
identified O
that O
the O
best O
role B
to O
help O
players B
improve O
their I
skills I
is O
support B
. O

This O
role B
is O
characterized O
by O
the O
presence O
of O
utility B
spells B
, O
such O
as O
cure O
, O
shields B
or O
disables O
, O
that O
can O
improve O
the O
ally B
hero I
survival O
potential O
. O

The O
support B
is O
normally O
responsible O
for O
helping O
a O
carry B
( O
champion B
that O
does O
a O
great B
amount I
of I
damage I
) O
in O
the O
bottom B
lane I
, O
in O
the O
current O
strategy B
of O
LoL. O
After O
the O
lane B
phase O
, O
it O
is O
responsible O
by O
helping O
all O
players B
in O
team B
fights I
and O
lane B
pushing O
. O

We O
chose O
Soraka O
, O
a O
LoL B
support O
champion I
that O
easily O
fills O
the O
needs O
of O
a O
novice B
player I
, O
providing O
support B
and O
healing B
while O
he O
/ O
she O
learns O
how O
to O
play B
the O
game B
. O

Soraka O
is O
focused B
in O
high O
crowd O
control B
, O
having O
spells B
that O
slow B
, O
root O
and O
silence O
enemies B
in B
area O
- I
of I
- O
effect B
. O

She O
also O
has O
spells B
that O
can O
cure O
close B
allies B
and O
, O
lastly O
, O
her O
ultimate O
spell B
can O
cure O
the O
entire O
team B
. O

Soraka O
passive B
spell B
allows O
her O
to O
run O
faster O
towards O
injured O
allies B
, O
making O
her O
helpful O
not O
just O
for O
one O
player B
, O
but O
for O
the O
entire O
team B
. O

We O
then O
developed O
a O
script B
that O
selects O
the O
bottom B
lane I
player B
, O
as O
supporters O
normally O
go O
in O
the O
bottom B
lane I
, O
and O
follows O
the O
player B
from O
an O
adjustable O
distance O
, O
avoiding O
collisions O
with O
him O
/ O
her O
. O

This O
system O
uses O
a O
two O
layered O
architecture O
: O
the O
first O
layer O
is O
responsible O
for O
driving O
the O
agent O
to O
follow O
the O
player B
while O
the O
second O
layer O
controls B
the O
use O
of O
skills B
to O
support B
the O
player B
. O

The O
support B
system O
only O
helps O
the O
player B
, O
supporting B
him O
during O
the O
entire O
game B
. O

However O
the O
support B
system O
does O
not O
give O
tips O
to O
player B
about O
mechanics B
and O
techniques B
. O

The O
tutor O
system O
tracks O
the O
player B
mechanics B
and O
gives O
tips O
to O
improve O
it O
. O

We O
should O
stress O
that O
our O
goal B
is O
to O
develop O
an O
agent O
capable O
of O
helping O
players B
to O
develop O
their O
skills B
, O
thereby O
is O
not O
our O
current O
goal B
to O
develop O
an O
agent O
that O
is O
Turing O
acceptable O
. O

We O
developed O
a O
message B
system O
that O
is O
integrated O
with O
LoL O
smart O
ping B
system O
. O

It O
emits O
messages B
and O
smart O
signals O
warning O
the O
player B
when O
he I
/ I
she O
has O
low B
health I
or O
bad O
positioning B
. O

A O
rule B
based B
system O
is O
used O
to O
evaluate O
the O
data O
collected O
, O
getting O
the O
tips O
from O
a O
lookup O
table O
. O

The O
table O
tip O
was O
developed O
based B
on O
the O
video O
guides O
from O
Riot O
Games O
and O
also O
on O
texts O
from O
LoL B
champion I
pages O
. O

For O
positioning B
, O
we O
give O
tips O
to O
the O
player B
when O
he I
/ I
she O
is O
going O
to O
be O
focused B
by O
towers B
, O
enemies B
or O
minions B
. O

These O
specific O
tips O
are O
given O
to O
all O
players B
in O
the O
team B
. O

For O
developing O
the O
agent O
we O
extracted O
domain O
knowledge O
from O
expert B
players I
, O
in B
- I
game I
tutorials B
, O
videos O
and O
streaming O
from O
expert B

 O
players I
. O

We O
then O
divided O
this O
information O
in O
tactical O
and O
positioning B
, O
attributing O
them O
to O
movement B
and O
mechanism B
layer O
respectively O
. O

The O
movement B
system I
was O
developed O
using O
the O
knowledge O
Figure O
1 O
: O
Behavior O
Tree O
implemented O
by O
the O
agent O
. O

The O
Sequencers O
are O
represented O
by O
arrow B
rectangles O
, O
Selectors O
by O
question O
circles O
. O

Actions O
and O
questions O
are O
represented O
by O
texts O
inside O
rectangles O
. O

modelled O
in O
a O
Behavior O
Tree O
, O
than O
can O
be O
seen O
in O
Figure O
1 O
, O
used O
to O
decision O
making O
. O

Real O
time B
reasoning O
was O
used O
to O
cause O
stimuli O
in O
the O
Behavior O
Tree O
and O
perform O
the O
decision O
making O
strategy B
. O

5 O
Experiments O
5.1 O
Experiment O
1 O
: O
Playing B
alongside O
human O
players B

 O
We I
run I
it O
in O
a O
low B
level I
account I
, O
playing B
in O
cooperative O
versus O
AI O
mode O
, O
cooperating O
with O
4 O
human O
players B
. O

League O
of O
Legends O
provides O
players B
a O
match B
history O
, O
where O
you O
can O
retrieve O
matches B
from O
accounts O
by O
visiting O
their O
profile O
. O

It O
is O
also O
possible O
to O
record O
information O
from O
the O
after O
match B
screen O
, O
where O
score O
information O
is O
shown O
for O
each O
player B
who O
took O
part O
in O
the O
match B
. O

A O
brand O
new O
, O
level B
1 O
account O
was O
created O
assuring O
that O
LoL O
matchmaking B
would O
select B
players I
with O
zero O
or O
low O
game B
knowledge I
. O

For O
data O
collection O
, O
we O
tracked O
the O
player B
that O
partnered O
with O
the O
tutor O
. O

We O
compared O
the O
results O
of O
three O
matches B
played B
prior O
and O
three O
matches B
played B
right O
after O
the O
match B
played B
with O
the O
tutor O
. O

Also O
, O
we O
ran O
3 O
matches B
using O
only O
the O
supporting B
system O
and O
3 O
matches B
with O
supporting B
and O
tip O
system O
enabled O
. O

All O
matches B
were O
played B
in O
Summoners O
Rift O
, O
Coop O
vs. O
AI O
, O
Introductory O
solo B
queue I
, O
with O
matchmaking B
selected O
team B
. O

The O
data O
has O
been O
collected O
using O
LoL O
patch O
5.7 O
in O
2015 O
. O

As O
a O
performance O
metric O
, O
we O
chose O
the O
Kill O
/ O
Death B
/ O
Assists O
( O
KDA O
) O
factor O
. O

It O
consists O
of O
analysing O
the O
number O
of O
enemies B
killed B
, O
the O
number O
of O
times B
the O
player B
helped O
his I
teammates I
to O
kill B
an O
enemy B
and O
how O
many O
times B
he O
/ O
she O
died O
. O

This O
factor O
is O
broadly O
used O
to O
analyse O
the O
gameplay B
and O
performance O
of O
players B
, O
both O
in O
amateur O
, O
competitive B
or O
e O
- O
Sports O
scenario O
. O

The O
KDA O
factor O
function O
is O
presented O
in O
equation O
1 O
. O

f(KDA O
) O
= O
 O
K+A O
D O
, O
D O
> O
0 O
K O
+ O
A O
, O
else O
( O
1 O
) O
We O
performed O
experiments O
with O
6 O
different O
players B
( O
A O
- O
F O
) O
. O

Players B
A O
, O
B O
and O
C O
played B
matches B
with O
the O
support B
system O
only O
, O
while O
D O
, O
E O
and O
F O
played B
with O
support B
and O
tip O
system O
enabled O
. O

For O
each O
player B
, O
we O
computed O
the O
average O
and O
standard O
deviation O
of O
the O
KDA O
factor O
from O
3 O
matches B
before O
and O
after O
the O
match B
played B
with O
the O
tutor O
. O

The O
results O
are O
displayed O
in O
Figure O
2 O
. O

When O
comparing O
the O
matches B
using O
only O
the O
support B
system O
( O
A O
, O
B O
, O
C O
) O
with O
support B
plus O
tip O
system O
( O
D O
, O
E O
, O
F O
) O
we O
can O
observe O
that O
players B
which O
used O
the O
tip O
system O
had O
a O
larger O
performance O
improvement O
in O
matches B
played B
right O
after O
playing B
with O
the O
tutor O
. O

This O
large O
improvements O
are O
not O
observed O
when O
only O
the O
support B
system O
is O
used O
( O
A O
, O
B O
, O
C O
) O
. O

In O
the O
match B
played B
with O
the O
tutor O
( O
gray O
bar O
) O
, O
all O
-5 O
0 O
5 O
10 O
15 O
20 O
25 O
30 O
35 O
40 O
45 O
A O
B O
C O
D O
E O
F O
KD O
A O
Average O
Before O
Match O
Along O
Tutor O
After O
Figure O
2 O
: O
KDA O
performance O
of O
six O
players B
( O
A O
- O
F O
) O
before O
and O
after O
playing B
with O
the O
tutor O
. O

players B
, O
except O
C O
, O
showed O
better O
than O
average O
performance O
, O
supporting B
our O
hypothesis O
that O
the O
companion O
of O
a O
more O
knowledgeable O
player B
improves O
performance O
. O

We O
can O
also O
observe O
that O
players B
D O
, O
E O
, O
F O
showed O
much O
better O
performance O
when O
playing B
along O
our O
tutor O
, O
showing O
that O
the O
tip O
and O
support B
system O
together O
is O
helpful O
to O
new O
players B
. O

Overall O
, O
we O
observe O
that O
all O
players B
, O
except O
by O
C O
, O
showed O
performance O
improvements O
after O
playing B
along O
the O
tutor O
, O
showing O
that O
the O
tutor O
may O
help O
them O
to O
improve O
their O
gameplay B
. O

Player O
C O
presents O
a O
slightly O
decrease O
in O
performance O
, O
and O
that O
can O
be O
caused O
by O
various O
factors O
, O
such O
as O
bad O
connection B
, O
player B
experience B
and O
others O
. O

We O
also O
observed O
that O
some O
players B
presented O
a O
high O
standard O
deviation O
prior O
to O
play B
a O
match B
along O
the O
tutor O
, O
meaning O
their O
performance O
was O
unstable O
. O

As O
all O
players B
were O
under O
level B
10 O
, O
that O
can O
be O
considered O
normal O
, O
as O
these O
players B
are O
learning O
how O
to O
play B
. O

Results O
shown O
that O
players B
B O
, O
C O
and O
E O
reduced O
the O
standard O
deviation O
, O
meaning O
they O
became O
more O
stable O
in O
their O
gameplay B
. O

On O
the O
other O
hand O
, O
we O
observed O
that O
players B
A O
, O
D O
and O
F O
showed O
higher O
standard O
deviations O
after O
playing B
along O
the O
tutor O
. O

Standard O
deviation O
, O
in O
our O
data O
collection O
, O
can O
be O
influenced O
by O
many O
factors O
like O
the O
performance O
increase B
, O
player B
experience B
, O
connection B
problems O
, O
and O
others O
, O
that O
means O
it O
should O
not O
be O
taken O
as O
a O
conclusive O
evaluation O
factor O
in O
this O
study O
. O

Finally O
, O
League O
of O
Legends O
provides O
a O
chat B
room I
in O
game B
and O
after O
it O
. O

It O
also O
provides O
an O
honor O
system O
, O
in O
which O
partners O
can O
honor O
teammates B
for O
being O
helpful O
, O
friendly O
or O
showing O
teamwork B
. O

Moreover O
, O
LoL O
lets O
users B
add O
others O
as O
friends O
, O
so O
they O
can O
invite O
others O
to O
play B
matches B
together O
. O

After O
the O
three O
matches B
with O
tip O
system O
our O
tutor O
got O
three O
honors O
, O
one O
for O
being O
helpful O
and O
two O
for O
teamwork B
. O

It O
also O
got O
one O
friend O
request O
and O
some O
chat B
compliments O
. O

Table O
1 O
: O
Score O
of O
the O
agent O
as O
helper O
for O
new O
players B
Score O
Number O
of O
answers O
Percentage O
10 O
1 O
16.6 O
% O
8 O
3 O
50 O
% O
7 O
2 O
33.3 O
% O
Table O
2 O
: O
Answers O
to O
: O
” O
How O
do O
you O
evaluate O
the O
frequency O
of O
tips O
? O
” O
Answer O
Number O
of O
answers O
Percentage O
Good O
frequency O
4 O
66.7 O
% O
Little O
low O
frequency O
1 O
16.6 O
% O
Low O
frequency O
1 O
16.6 O
% O
5.2 O
Experiment O
2 O
: O
Survey O
and O
play B
with O
controlled B
players B
For O
evaluation O
of O
our O
approach O
we O
performed O
some O
experiments O
with O
players B
. O
We I
invited I
random O
players B
to O
join O
our O
agent O
in O
two O
matches B
. O

In O
the O
first O
one O
the O
player B
would O
play B
alongside O
the O
agent O
without O
the O
tip O
system O
. O

In O
the O
second O
match B
, O
the O
player B
would O
play B
with O
the O
tips O
on O
. O

For O
information O
collection O
we O
applied O
two O
surveys1 O
. O

First O
survey O
was O
applied O
before O
the O
matches B
was O
actually O
played B
, O
aiming O
to O
collect O
information O
about O
our O
testers O
. O

We O
performed O
early B
tests O
with O
two O
users B
for O
evaluation O
and O
fixing O
possible O
problems O
, O
the O
results O
were O
discarded O
. O

We O
have O
a O
set O
of O
six O
players B
, O
all O
male O
with O
ages O
ranged B
from O
18 O
to O
24 O
years O
old O
. O

In O
the O
first O
survey O
we O
collected O
data O
from O
their O
experience B
with O
MOBA O
and O
League O
of O
Legends O
. O

The O
experience B
of O
these O
players B
could O
reveal O
how O
well O
they O
could O
evaluate O
our O
system O
. O

Also O
, O
we O
have O
asked O
players B
about O
their I
experience I
with O
MOBAs B
others O
than O
LoL O
, O
evaluating O
the O
overall O
expertise O
of O
these O
players B
. O

All O
players B
were O
familiar O
with O
MOBA O
, O
three O
of O
them O
were O
advanced O
, O
two O
intermediate O
and O
one O
beginner B
. O

We O
notice O
that O
the O
majority O
or O
players B
are O
advanced O
, O
and O
we O
expect O
these O
players B
could O
provide O
us O
valuable O
feedback O
about O
the O
tutor O
. O

In O
addition O
, O
all O
of O
the O
players B
surveyed O
were O
main O
players B
of O
LoL B
and O
had O
experience B
with O
at O
least O
one O
MOBA O
game B
out O
of O
LOL O
. O

Further O
we O
asked O
players B
about O
their I
experience I
in O
LoL B
with O
the O
initial O
game B
tutorial I
. O

We O
asked O
players B
to O
attribute O
a O
score O
to O
the O
tutorial B
ranged B
from O
0 O
to O
10 O
, O
where O
0 O
means O
very O
unsatisfied O
and O
10 O
very O
satisfied O
. O

Our O
results O
found O
that O
most O
players B
consider O
the O
tutorial B
of O
median O
to O
poor B
value O
. O

We O
had O
two O
players B
that O
did O
not O
do O
the O
tutorial B
, O
two O
that O
attributed O
a O
score O
of O
four O
and O
two O
that O
gave O
a O
score O
of O
five O
to O
the O
LoL B
tutorial B
. O

In O
addition O
we O
asked O
if O
the O
player B
considers O
that O
the O
tutorial B
is O
sufficient O
for O
teaching O
a O
new O
player B
the O
basic O
concepts O
of O
the O
game B
obtaining O
. O

We O
also O
asked O
questions O
about O
the O
hero B
, O
the O
game B
and O
player B
experience B
, O
however O
we O
will O
not O
include O
all O
details O
in O
this O
work O
due O
to O
paper O
size O
. O

After O
the O
matches B
played B
alongside O
our O
agent O
, O
we O
asked O
the O
player B
to O
answer O
a O
survey O
about O
his O
experience B
with O
the O
agent O
alone O
and O
the O
agent O
plus O
the O
tip O
system O
. O

The O
survey O
revealed O
that O
none O
of O
the O
players B
has O
played B
with O
a O
system O
like O
the O
one O
we O
present O
in O
this O
work O
. O

We O
asked O
players B
question O
about O
the O
performance O
of O
the O
agent O
in O
the O
match B
. O

We O
also O
asked O
players B
how O
they I
thought I
that O
the O
agent O
would O
help O
novice B
players I
and O
how O
good O
the O
agent O
was O
, O
shown O
in O
Table O
1 O
. O

Further O
, O
we O
asked O
about O
the O
tip O
system O
and O
the O
tips O
frequency O
, O
Table O
2 O
. O

Lastly O
, O
we O
asked O
players B
about O
the O
actions O
that O
the O
agent O
performed O
during O
the O
match B
, O
classifying O
him O
as O
robotic O
or O
human O
. O

Four O
of O
these O
players B
considered O
the O
agent O
more O
robotic O
than O
human O
, O
one O
considered O
it O
very O
robotic O
and O
one O
considered O
it O
nor O
robotic O
nor O
human O
. O

6 O
Conclusions O
and O
Future O
Work O
In O
this O
paper O
we O
addressed O
the O
problem O
of O
helping O
novice B
players I
with O
the O
features O
and O
mechanics B
of I
MOBA O
games I
aiming O
to O
im1Survey O
1 O
: O
http://bit.ly/1HJtBDg O
Survey O
2 O
: O
http://bit.ly/1g5HvqA O
prove O
their O
entertainment O
. O

For O
this O
, O
we O
develop O
an O
Artificial O
Intelligence O
driven O
agent O
that O
plays B
along O
players B
guiding O
and O
tipping O
them O
. O

We O
ran O
the O
agent O
in O
League O
of O
Legends O
live O
servers B
within O
a O
brand O
new O
account O
aiming O
to O
find O
novice B
players I
joining O
the O
game B
. O

This O
approach O
showed O
that O
players B
improve O
their I
performance I
when O
playing B
with O
friendly O
partners O
that O
are O
willing O
to O
teach O
them O
the O
game B
features O
. O

In O
addition O
, O
we O
ran O
our O
agent O
alongside O
selected B
players I
, O
with O
set B
size O
of I
six I
. O

These O
players B
answered O
two O
surveys O
related O
to O
their O
previous O
experiences B
related O
to O
MOBA O
and O
experience B
with O
the O
tutor O
. O

They O
were O
also O
invited O
to O
play B
two O
matches B
along O
the O
agent O
. O

Results O
show O
promising O
results O
towards O
helping O
new O
players B
, O
as O
these O
players B
evaluate O
that O
the O
agent O
performed O
well O
as O
support B
. O

They O
also O
reported B
that O
the O
frequency O
of O
the O
tips O
are O
well O
balanced O
in O
most O
cases O
. O

For O
future O
work O
, O
we O
intend O
to O
perform O
more O
experiments O
, O
both O
qualitative O
and O
quantitative O
. O

Also O
, O
a O
deeper O
analysis O
can O
be O
done O
by O
tracking O
players B
’ O
progression B
using O
the O
tutor O
studying O
the O
period O
needed O
to O
learn O
game B
features O
and O
agent O
lifetime O
. O

Another O
analysis O
that O
can O
be O
performed O
is O
the O
gameplay B
improvement O
of O
the O
entire O
team B
, O
measuring O
tutor O
efficiency O
as O
a O
multiple O
supporter O
. O

Finally O
, O
we O
want O
to O
improve O
this O
tutor O
to O
teach O
advanced O
features O
, O
training O
experienced O
players B
. O

This O
would O
help O
players B
that O
already O
understand O
the O
game B
to O
improve O
their O
skills B
having O
a O
personal O
coach B
agent O
. O

References O
BLACKBURN O
, O
J. O
, O
AND O
KWAK O
, O
H. O
2014 O
. O

Stfu O
noob B
! O
: O
predicting O
crowdsourced O
decisions O
on O
toxic O
behavior O
in O
online B
games I
. O

In O
Proceedings O
of O
the O
23rd O
international O
conference O
on O
World O
wide O
web O
, O
877–888 O
. O

CSIKSZENTMIHALYI O
, O
M. O
1991 O
. O

Flow O
. O

HarperCollins O
. O

CUNHA O
, O
R. O
, O
MACHADO O
, O
M. O
, O
AND O
CHAIMOWICZ O
, O
L. O
2015 O
. O

Rtsmate O
: O
Towards O
an O
advice O
system O
for O
rts O
games B
. O

Computers O
in O
Entertainment O
( O
CIE O
) O
11 O
, O
4 O
, O
1 O
. O

FERRARI O
, O
S. O
2013 O
. O

From O
generative O
to O
conventional O
play B
: O
Moba O
and O
league B
of I
legends I
. O

In O
DiGRA O
2013 O
. O

GAMES O
, O
R. O
, O
2015 O
. O

Our O
games B
: O
League B
of I
legends I
. O

http:// O
www.riotgames.com/our-games O
. O

[ O
Online O
; O
accessed O
21- O
April-2015 O
] O
. O

GROEN O
, O
A. O
, O
2012 O
. O

Ask O
GR O
Anything O
: O
What O
’s O
a O
MOBA O
? O
http://www.gamesradar.com/ O
ask O
- O
gr O
- O
anything O
- O
whats O
- O
moba/. O
[ O
Online O
; O
accessed O
21-April-2015 O
] O
. O

KWAK O
, O
H. O
, O
BLACKBURN O
, O
J. O
, O
AND O
HAN O
, O
S. O
2015 O
. O

Exploring O
cyberbullying O
and O
other O
toxic O
behavior O
in O
team B
competition B
online B

 O
games I
. O

Social O
Dynamics O
22 O
, O
28 O
, O
47 O
. O

LIN O
, O
J. O
2013 O
. O

The O
science O
behind O
shaping O
player B
behavior I
in O
online B

 O
games I
. O

In O
annual O
Game O
Developers O
Conference O
. O

LIN O
, O
J. O
2015 O
. O

More O
science O
behind O
shaping O
player B
behavior I
in O
online B
games I
. O

In O
Game O
Developers O
Conference O
. O

MCGEE O
, O
K. O
, O
AND O
ABRAHAM O
, O
A. O
T. O
2010 O
. O

Real B
- I
time I
team B
- O
mate O
ai O
in O
games B
: O
A O
definition O
, O
survey O
, O
& O
critique O
. O

In O
Foundations O
of O
Digital O
Games O
, O
124–131 O
. O

MYLAK O
, O
M. O
, O
AND O
DEJA O
, O
D. O
2014 O
. O

Developing O
game B
- O
structure O
sensitive O
matchmaking B
system I
for O
massive B
- I
multiplayer I
online I

 O
games I
. O

In O
Exploration O
on O
Games O
and O
Gamers O
Workshop O
, O
SocInfo O
2014 O
. O

SILVA O
, O
V. O
N. O
, O
AND O
NASCIMENTO O
, O
M. O
N. O
2012 O
. O

Investigac¸ao˜ O
da O
melhoria O
do O
aprendizado O
de O
alunos O
do O
ensino O
medio O
da O
rede O
´ O
publica O
de O
ensino O
atrav O
´ O
es O
do O
uso O
de O
programac¸ O
´ O
ao O
, O
rob O
˜ O
otica O
e O
jo- O
´ O
gos O
digitais O
. O

In B
SBGames O
- I
Brazilian I
Symposium O
of O
Games O
and O
Digital O
Entertainment O
. O

YANG O
, O
P. O
, O
HARRISON O
, O
B. O
, O
AND O
ROBERTS O
, O
D. O
L. O
2014 O
. O

Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
moba B

 O
games I
. O

In O
Foundations O
of O
Digital O
Games O
. O

Video O
Highlight O
Prediction O
Using O
Audience O
Chat O
Reactions O
Cheng O
- O
Yang O
Fu O
, O
Joon O
Lee O
, O
Mohit O
Bansal O
, O
Alexander O
C. O
Berg O
UNC O
Chapel O
Hill O
{ O
cyfu O
, O
joonlee O
, O
mbansal O
, O
aberg}@cs.unc.edu O
Abstract O
Sports O
channel O
video O
portals O
offer O
an O
exciting O
domain O
for O
research O
on O
multimodal O
, O
multilingual O
analysis O
. O

We O
present O
methods O
addressing O
the O
problem O
of O
automatic O
video B
highlight I
prediction O
based B
on O
joint O
visual O
features O
and O
textual O
analysis O
of O
the O
real B
- I
world I
audience O
discourse O
with O
complex O
slang O
, O
in O
both O
English O
and O
traditional O
Chinese O
. O

We O
present O
a O
novel O
dataset O
based B
on O
League O
of O
Legends O
championships O
recorded O
from O
North O
American O
and O
Taiwanese O
Twitch.tv O
channels O
( O
will O
be O
released O
for O
further O
research O
) O
, O
and O
demonstrate O
strong O
results O
on O
these O
using O
multimodal O
, O
character B
- I
level I
CNN O
- O
RNN O
model O
architectures O
. O

1 O
Introduction O
On O
- O
line O
eSports O
events O
provide O
a O
new O
setting O
for O
observing O
large O
- O
scale O
social O
interaction O
focused B
on O
a O
visual O
story O
that O
evolves O
over O
time B
— O
a O
video B

 O
game I
. O

While O
watching O
sporting O
competitions B
has O
been O
a O
major O
source O
of O
entertainment O
for O
millennia O
, O
and O
is O
a O
significant O
part O
of O
today O
’s O
culture O
, O
eSports O
brings O
this O
to O
a O
new O
level B
on O
several O
fronts O
. O

One O
is O
the O
global O
reach O
, O
the O
same O
games B
are O
played B
around O
the O
world B
and O
across O
cultures O
by O
speakers O
of O
several O
languages O
. O

Another O
is O
the O
scale O
of O
on O
- O
line O
text O
- O
based B
discourse O
during O
matches B
that O
is O
public O
and O
amendable O
to O
analysis O
. O

One O
of O
the O
most O
popular O
games B
, O
League O
of O
Legends O
, O
drew O
43 O
million O
views O
for O
the O
2016 O
world B
series O
final O
matches B
( O
broadcast O
in O
18 O
languages O
) O
and O
a O
peak O
concurrent O
viewership O
of O
14.7 O
million1 O
. O

Finally O
, O
players B
interact O
through O
what O
they O
see O
on O
screen O
while O
fans O
( O
and O
researchers O
) O
can O
see O
exactly O
the O
same O
views O
. O

1 O
http://www.lolesports.com/en_US/articles/ O
2016-league O
- O
legends O
- O
world B
- O
championship O
- O
numbers O
( O
a O
) O
Twitch B
( O
b O
) O
Youtube O
( O
c O
) O
Facebook O
Figure O
1 O
: O
Pictures O
of O
Broadcasting O
platforms:(a O
) O
Twitch B
: O
League O
of O
Legends O
Tournament O
Broadcasting O
, O
( O
b O
) O
Youtube O
: O
News O
Channel O
, O
( O
c)Facebook O
: O
Personal O
live O
sharing O
This O
paper O
builds O
on O
the O
wealth O
of O
interaction O
around O
eSports O
to O
develop O
predictive O
models O
for O
match B
video B
highlights I
based B
on O
the O
audience O
’s O
online O
chat B
discourse O
as O
well O
as O
the O
visual O
recordings O
of O
matches B
themselves O
. O

ESports O
journalists O
and O
fans O
create O
highlight B
videos O
of O
important O
moments O
in O
matches B
. O

Using O
these O
as O
ground O
truth O
, O
we O
explore O
automatic O
prediction O
of O
highlights B
via O
multimodal O
CNN+RNN O
models O
for O
multiple O
languages O
. O

Appealingly O
this O
task O
is O
natural O
, O
as O
the O
community B
already O
produces O
the O
ground O
truth O
and O
is O
global O
, O
allowing O
multilingual O
multimodal O
grounding O
. O

Highlight O
prediction O
is O
about O
capturing B
the O
exciting O
moments O
in O
a O
specific O
video O
( O
a O
game B
match I
in O
this O
case O
) O
, O
and O
depends O
on O
the O
context O
, O
the O
state O
of O
play B
, O
and O
the O
players B
. O

This O
task O
of O
predicting O
the O
exciting O
moments O
is O
hence O
different O
from O
summarizing O
the O
entire O
match B
into O
a O
story O
summary O
. O

Hence O
, O
highlight B
prediction O
can O
benefit O
from O
the O
available O
real B
- I
time I
text O
commentary O
from O
fans O
, O
which O
is O
valuable O
in O
exposing O
more O
abstract O
background O
context O
, O
that O
may O
not O
be O
accessible O
with O
arXiv:1707.08559v1 O
[ O
cs O
. O

CL O
] O
26 O
Jul O
2017 O
computer O
vision O
techniques B
that O
can O
easily O
identify O
some O
aspects O
of O
the O
state O
of O
play B
. O

As O
an O
example O
, O
computer O
vision O
may O
not O
understand O
why O
Michael O
Jordan O
’s O
dunk O
is O
a O
highlight B
over O
that O
of O
another O
player B
, O
but O
concurrent O
fan O
commentary O
might O
reveal O
this O
. O

We O
collect O
our O
dataset O
from O
Twitch.tv O
, O
one O
of O
the O
live O
- O
streaming O
platforms O
that O
integrates O
comments O
( O
see O
Fig O
. O

1 O
) O
, O
and O
the O
largest O
live O
- O
streaming O
platform O
for O
video B
games I
. O

We O
record O
matches B
of O
the O
game B
League O
of O
Legends O
( O
LOL O
) O
, O
one O
of O
the O
largest O
eSports O
game B
in O
two O
subsets O
, O
1 O
) O
the O
spring O
season B
of O
the O
North O
American O
League O
of O
Legends O
Championship O
Series O
( O
NALCS O
) O
, O
and O
2 O
) O
the O
League O
of O
Legends O
Master O
Series O
( O
LMS O
) O
hosted B
in O
Taiwan O
/ O
Macau O
/ O
HongKong O
, O
with O
chat B
comments O
in O
English O
and O
traditional O
Chinese O
respectively O
. O

We O
use O
the O
community B
created O
highlights B
to O
label O
each O
frame B
of O
a O
match B
as O
highlight B
or O
not O
. O

In O
addition O
to O
our O
new O
dataset O
, O
we O
present O
several O
experiments O
with O
multilingual O
characterbased O
models O
, O
deep O
- O
learning O
based B
vision O
models O
either O
per O
- O
frame B
or O
tied O
together O
with O
a O
videosequence O
LSTM O
- O
RNN O
, O
and O
combinations O
of O
language O
and O
vision O
models O
. O

Our O
results O
indicate O
that O
while O
surprisingly O
the O
visual O
models O
generally O
outperform O
language O
- O
based B
models O
, O
we O
can O
still O
build O
reasonably O
useful O
language O
models O
that O
help O
disambiguate O
difficult O
cases O
for O
vision O
models O
, O
and O
that O
combining O
the O
two O
sources O
is O
the O
most O
effective O
model O
( O
across O
multiple O
languages O
) O
. O

2 O
Related O
Work O
We O
briefly O
discuss O
a O
small O
sample O
of O
the O
related O
work O
on O
language O
and O
vision O
datasets O
, O
summarization O
, O
and O
highlight B
prediction O
. O

There O
has O
been O
a O
surge O
of O
vision O
and O
language O
datasets O
focusing B
on O
captions O
over O
the O
last O
few O
years O
, O
( O
Rashtchian O
et O
al O
. O

, O
2010 O
; O
Ordonez O
et O
al O
. O

, O
2011 O
; O
Lin O
et O
al O
. O

, O
2014 O
) O
, O
followed O
by O
efforts O
to O
focus B
on O
more O
specific O
parts O
of O
images O
( O
Krishna O
et O
al O
. O

, O
2016 O
) O
, O
or O
referring O
expressions O
( O
Kazemzadeh O
et O
al O
. O

, O
2014 O
) O
, O
or O
on O
the O
broader O
context O
( O
Huang O
et O
al O
. O

, O
2016 O
) O
. O

For O
video O
, O
similar O
efforts O
have O
collected O
descriptions O
( O
Chen O
and O
Dolan O
, O
2011 O
) O
, O
while O
others O
use O
existing O
descriptive O
video O
service O
( O
DVS O
) O
sources O
( O
Rohrbach O
et O
al O
. O

, O
2015 O
; O
Torabi O
et O
al O
. O

, O
2015 O
) O
. O

Beyond O
descriptions O
, O
other O
datasets O
use O
questions O
to O
relate O
images O
and O
language O
( O
Antol O
et O
al O
. O

, O
2015 O
; O
Yu O
et O
al O
. O

, O
2015 O
) O
. O

This O
approach O
is O
extended O
to O
movies O
in O
Tapaswi O
et O
al O
. O

( O
2016 O
) O
. O

The O
related O
problem O
of O
visually O
summarizing O
videos O
( O
as O
opposed O
to O
finding O
the O
highlights B
) O
has O
produced O
datasets O
of O
holiday O
and O
sports O
events O
with O
multiple O
users B
making O
summary O
videos O
( O
Gygli O
et O
al O
. O

, O
2014 O
) O
and O
multiple O
users B
selecting O
summary O
key O
- O
frames B
( O
de O
Avila O
et O
al O
. O

, O
2011 O
) O
from O
short O
videos O
. O

For O
language O
- O
based B
summarization O
, O
Extractive O
models O
( O
Filippova O
and O
Altun O
, O
2013 O
; O
Filippova O
et O
al O
. O

, O
2015 O
) O
generate O
summaries O
by O
selecting O
important O
sentences O
and O
then O
assembling O
these O
, O
while O
Abstractive O
models O
( O
Chopra O
et O
al O
. O

, O
2016 O
; O
Mei O
et O
al O
. O

, O
2016 O
; O
Nallapati O
et O
al O
. O

, O
2016 O
; O
See O
et O
al O
. O

, O
2017 O
) O
generate O
/ O
rewrite O
the O
summaries O
from O
scratch O
. O

Closer B
to O
our O
setting O
, O
there O
has O
been O
work O
on O
highlight B
prediction O
in O
football O
( O
soccer O
) O
and O
basketball O
based B
on O
audio O
of O
broadcasts O
( O
Cheng O
and O
Hsu O
, O
2006 O
) O
( O
Wang O
et O
al O
. O

, O
2004 O
) O
where O
commentators B
may O
have O
an O
outsized O
impact B
or O
visual O
features O
( O
Bertini O
et O
al O
. O

, O
2005 O
) O
. O

In O
the O
spirit B
of O
our O
study O
, O
there O
has O
been O
work O
looking O
at O
tweets O
during O
sporting O
events O
( O
Hsieh O
et O
al O
. O

, O
2012 O
) O
, O
but O
the O
tweets O
are O
not O
as O
immediate O
or O
as O
well O
aligned O
with O
the O
games B
as O
the O
eSports O
comments O
. O

More O
closely O
related O
to O
our O
work O
, O
Song O
( O
2016 O
) O
collects O
videos O
for O
Heroes O
of O
the O
Storm O
, O
League O
of O
Legends O
, O
and O
Dota2 O
on O
online O
broadcasting O
websites O
of O
around O
327 O
hours O
total O
. O

They O
also O
provide O
highlight B
labeling O
annotated O
by O
four O
annotators O
. O

Our O
method O
, O
on O
the O
other O
hand O
, O
has O
a O
similar O
scale O
of O
data O
, O
but O
we O
use O
existing O
highlights B
, O
and O
we O
also O
employ O
textual O
audience O
chat B
commentary O
, O
thus O
providing O
a O
new O
resource B
and O
task O
for O
Language O
and O
Vision O
research O
. O

In O
summary O
, O
we O
present O
the O
first O
languagevision O
dataset O
for O
video B
highlighting I
that O
contains O
audience O
reactions O
in O
chat B
format O
, O
in O
multiple O
languages O
. O

The O
community B
produced O
ground O
truth O
provides O
labels O
for O
each O
frame B
and O
can O
be O
used O
for O
supervised O
learning O
. O

The O
language O
side O
of O
this O
new O
dataset O
presents O
interesting O
challenges O
related O
to O
real B
- I
world I
Internet O
- O
style O
slang O
. O

3 O
Data O
Collection O
Our O
dataset O
covers O
218 O
videos O
from O
NALCS O
and O
103 O
from O
LMS O
for O
a O
total O
of O
321 O
videos O
from O
week O
1 O
to O
week O
9 O
in O
2017 O
spring O
series O
from O
each O
tournament B
. O

Each O
week O
there O
are O
10 O
matches B
for O
NALCS O
and O
6 O
matches B
for O
LMS O
. O

Matches B
are O
best O
of O
3 O
, O
so O
consist O
of O
two O
games B
or O
three O
games B
. O

The O
first O
and O
third O
games B
are O
used O
for O
training O
. O

The O
second O
games B
in O
the O
first O
4 O
weeks O
are O
used O
as O
valida- O
Frame O
	  O
48-dim O
	 O
vector O
( O
a O
) O
Feature O
vector O
of O
frame B
Highlight O
	
 O
Template O
	 O
Matching B
	
 O
video O
		
 O
Video O
	 O
Matching B
	 O
Result O
		
 O
Similarity O
	 O
Response O
	
 O
( O
b O
) O
Template O
Matching O
Figure O
2 O
: O
Highlight O
Labeling O
: O
( O
a O
) O
The O
feature O
representation O
of O
each O
frame B
is O
calculated O
by O
averaging O
each O
color O
channel O
in O
each O
subregion O
. O

( O
b O
) O
After O
template O
matching O
, O
the O
top O
bar O
shows O
the O
maximum O
of O
similarity O
matching O
of O
each O
frame B
in O
the O
highlight B
and O
the O
bottom B
bar O
is O
the O
labeling O
result O
of O
the O
video O
. O

tion O
and O
the O
remainder O
of O
second O
games B
are O
used O
as O
test O
. O

Table O
1 O
lists O
the O
numbers O
of O
videos O
in O
train B
, O
validation O
, O
and O
test O
subsets O
. O

Dataset O
Train B
Val O
Testing O
Total O
NALCS O
128 O
40 O
50 O
218 O
LMS O
57 O
18 O
28 O
103 O
Table O
1 O
: O
Dataset O
statistics O
( O
number O
of O
videos O
) O
. O

Each O
game B
’s O
video O
ranges B
from O
30 O
to O
50 O
minutes O
in O
length O
which O
contains O
image O
and O
chat B
data O
linked O
to O
the O
specific O
timestamp O
of O
the O
game B
. O

The O
average O
number O
of O
chats B
per O
video O
is O
7490 O
with O
a O
standard O
deviation O
of O
4922 O
. O

The O
high O
value O
of O
standard O
deviation O
is O
mostly O
due O
to O
the O
fact O
that O
NALCS O
simultaneously O
broadcasts O
matches B
in O
two O
different O
channels O
( O
nalcs12 O
and O
nalcs23 O
) O
which O
often O
leads B
to O
the O
majority O
of O
users B
watching O
the O
channel O
with O
a O
relatively O
more O
popular O
team B
causing O
an O
imbalance O
in O
the O
number O
of O
chats B
. O

If O
we O
only O
consider O
LMS O
which O
broadcasts O
with O
a O
single O
channel O
, O
the O
average O
number O
of O
chats B
are O
7210 O
with O
standard O
deviation O
of O
2719 O
. O

The O
number O
of O
viewers B
for O
each O
game B
averages O
about O
21526 O
, O
and O
the O
number O
of O
unique O
users B
who O
type O
in O
chat B
is O
on O
average O
2185 O
, O
i.e. O
, O
roughly O
10 O
% O
of O
the O
viewers B
. O

Highlight B
Labeling O
For O
each O
game B
, O
we O
collected O
community B
generated O
highlights B
ranging B
from O
5 O
minutes O
to O
7 O
minutes O
in O
length O
. O

For O
the O
purpose O
of O
consistency O
within O
our O
data O
, O
we O
collected O
the O
highlights B
from O
a O
single O
Youtube O
channel O
, O
2 O
https://www.twitch.tv/nalcs1 O
3 O
https://www.twitch.tv/nalcs2 O
Onivia,4 O
which O
provided O
highlights B
for O
both O
championship O
tournaments B
in O
a O
consistent O
arrangement O
. O

We O
expect O
such O
consistency O
will O
aid O
our O
model O
to O
better O
pick B
up O
characteristics O
for O
determining O
highlights B
. O

We O
next O
need O
to O
align O
the O
position O
of O
the O
frames B
from O
the O
highlight B
video O
to O
frames B
in O
the O
full B
game I
video O
. O

For O
this O
, O
we O
adopted O
a O
template O
matching O
approach O
. O

For O
each O
frame B
in O
the O
video O
and O
the O
highlight B
, O
we O
divide O
it O
into O
16 O
regions O
of O
4 O
by O
4 O
and O
use O
the O
average O
value O
of O
each O
color O
channel O
in O
each O
region O
as O
the O
feature O
. O

The O
feature O
representation O
of O
each O
frame B
ends O
up O
as O
a O
48-dim O
vector O
as O
shown O
in O
Figure O
2a O
. O

For O
each O
frame B
in O
the O
highlight B
, O
we O
can O
find O
the O
most O
similar O
frame B
in O
the O
video O
by O
calculating O
distance O
between O
these O
two O
vectors O
. O

However O
, O
matching B
a O
single O
frame B
to O
another O
suffers O
from O
noise O
. O

Therefore O
, O
we O
alternatively O
concatenate O
the O
following O
frames B
to O
form O
a O
window O
and O
use O
template O
matching B
to O
find O
the O
best O
matching O
location B
in O
the O
video O
. O

We O
found O
out O
that O
when O
the O
window O
size O
is O
60 O
frames B
, O
it O
gives O
consistent O
and O
high O
quality O
results O
. O

For O
each O
frame B
, O
the O
result O
contains O
not O
only O
the O
best O
matching O
score O
but O
also O
the O
location B
of O
that O
match B
in O
the O
video.5 O
Figure O
2b O
illustrates O
this O
matching O
process O
. O

4 O
Model O
In O
this O
section O
, O
we O
explain O
the O
proposed O
models O
and O
components O
. O

We O
first O
describe O
the O
notation O
and O
definition O
of O
the O
problem O
, O
plus O
the O
evaluation O
metric O
used O
. O

Next O
, O
we O
explain O
our O
vision O
model O
VCNN O
- O
LSTM O
and O
language O
model O
L O
- O
Char O
- O
LSTM O
. O

Finally O
, O
we O
describe O
the O
joint O
multimodal O
model O
lv O
- O
LSTM O
. O

Problem O
Definition O
Our O
basic O
task O
is O
to O
determine O
if O
a O
frame B
of O
the O
full B
input B
video O
should O
be O
labeled O
as O
being O
part O
of O
the O
output O
highlight B
or O
not O
. O

To O
simplify O
our O
notation O
, O
we O
use O
X O
= O
{ O
x1 O
, O
x2 O
, O
... O
, O
xt O
} O
to O
denote O
a O
sequence O
of O
features O
for O
frames B
. O

Chats B
are O
expressed O
as O
C O
= O
{ O
( O
c1 O
, O
ts1 O
) O
, O
... O
, O
( O
cn O
, O
tsn O
) O
} O
. O

where O
each O
chat B
c O
comes O
with O
a O
timestamp O
ts O
. O

Methods O
take O
the O
image O
features O
and/or O
chats B
and O
predict O
labels O
for O
the O
frames B
, O
Y O
= O
{ O
y1 O
, O
y2 O
, O
... O
, O
yt O
} O
. O

Evaluation O
Metric O
: O
We O
refer O
to O
the O
set O
of O
frames B
with O
positive O
ground O
truth O
label O
as O
Sgt O
and O
the O
set O
4 O
https://www.youtube.com/channel/ O
UCPhab209KEicqPJFAk9IZEA O
5When O
the O
window O
contains O
a O
moment O
of O
clip O
transition O
in O
highlights B
, O
the O
best O
matching O
score O
appears O
low O
. O

This O
is O
used O
to O
separate O
all O
clips O
in O
the O
highlight B
. O

Then O
we O
can O
use O
the O
starting O
and O
end O
locations B
of O
each O
clip O
to O
label O
the O
video O
. O

Video O
Prediction O
… O
… O
ResNet-34 O
ResNet-34 O
Prediction O
( O
a O
) O
V O
- O
CNN O
Video O
Image O
Window O
Size O
Prediction O
… O
LSTM O
LSTM O
… O
LSTM O
ResNet-34 O
ResNet-34 O
ResNet-34 O
( O
b O
) O
V O
- O
CNN O
- O
LSTM O
Video O
Concatenated O
Chat O
String O
Text O
Window O
Size O
Prediction O
Chat B
LSTM O
LSTM O
LSTM O
LSTM O
… O
… O
T O
H O
E O
S O
E O
… O
C O
O O
O O
L O
! O
1-hot O
1-hot O
1-hot O
1-hot O
( O
c O
) O
L O
- O
Char O
- O
LSTM O
Video O
LSTM O
LSTM O
LSTM O
… O
… O
ResNet-34 O
… O
LSTM O
MLP O
Prediction O
… O
Concatenated O
Chat O
String O
Chat O
T O
H O
E O
… O
C O
O O
O O
L O
! O
1-hot O
1-hot O
1-hot O
ResNet-34 O
LSTM O
ResNet-34 O
LSTM O
( O
d O
) O
Full B
model O
: O
lv O
- O
LSTM O
Figure O
3 O
: O
Network O
architecture O
of O
proposed O
models O
. O

of O
predicted O
frames B
with O
a O
positive O
label O
as O
Spred O
. O

Following O
( O
Gygli O
et O
al O
. O

, O
2014 O
; O
Song O
et O
al O
. O

, O
2015 O
) O
, O
we O
use O
the O
harmonic O
mean O
F O
- O
score O
in O
Eq.2 O
widely O
used O
in O
video O
summarization O
task O
for O
evaluation O
: O
P O
= O
Sgt O
∩ O
Spred O
|Spred| O
, O
R O
= O
Sgt O
∩ O
Spred O
|Sgt| O
( O
1 O
) O
F O
= O
2P O
R O
P O
+ O
R O
× O
100 O
% O
( O
2 O
) O
V O
- O
CNN O
We O
use O
the O
ResNet-34 O
model O
( O
He O
et O
al O
. O

, O
2016 O
) O
to O
represent O
frames B
, O
motivated O
by O
its O
strong O
results O
on O
the O
ImageNet O
Challenge O
( O
Russakovsky O
et O
al O
. O

, O
2015 O
) O
. O

Our O
naive O
V O
- O
CNN O
model O
( O
Figure O
3a O
) O
uses O
features O
from O
the O
pre O
- O
trained O
version O
of O
this O
network O
6 O
directly O
to O
make O
prediction O
at O
each O
frame B
( O
which O
are O
resized O
to O
224x224 O
) O
. O

V O
- O
CNN O
- O
LSTM O
In O
order O
to O
exploit O
visual O
video O
information O
sequentially O
over O
time B
, O
we O
use O
a O
memory O
- O
based B
LSTM O
- I
RNN I
on O
top O
of O
the O
image O
features O
, O
so O
as O
to O
model O
long O
- O
term O
dependencies O
. O

All O
of O
our O
videos O
are O
30FPS O
. O

As O
the O
difference O
between O
consecutive O
frames B
is O
usually O
minor O
, O
we O
run O
prediction O
every O
10th O
frame B
during O
evaluation O
and O
interpolate O
predictions O
between O
these O
frames B
. O

During O
training O
, O
due O
to O
the O
GPU O
memory O
constraints O
, O
we O
unfold O
the O
LSTM O
cell O
16 O
times B
. O

Therefore O
the O
image O
window O
size O
is O
around O
5-seconds O
( O
16 O
samples O
every O
10th O
frame B
from O
30fps O
video O
) O
. O

The O
hidden O
state O
from O
the O
last O
cell O
is O
used O
as O
the O
V O
- O
CNNLSTM O
feature O
. O

This O
process O
is O
shown O
in O
Figure O
3b O
. O

L O
- O
Word O
- O
LSTM O
and O
L O
- O
Char O
- O
LSTM O
Next O
, O
we O
discuss O
our O
language O
- O
based B
models O
using O
the O
audience O
chat B
text O
. O

Word O
- O
level B
LSTM O
- O
RNN O
models O
( O
Sutskever O
et O
al O
. O

, O
2014 O
) O
are O
a O
common O
approach O
to O
embedding O
sentences O
. O

Unfortunately O
, O
this O
does O
not O
fit O
our O
Internet O
- O
slang O
style O
language O
with O
irregularities O
, O
“ O
mispelled O
” O
words O
( O
hapy O
, O
happppppy O
) O
, O
emojis O
( O
ˆ O
ˆ O
) O
, O
abbreviations O
( O
LOL O
) O
, O
marks B
( O
? O
! O
? O
! O
? O
! O
? O
! O
) O
, O
or O
onomatopoeic O
cases O
6 O
https://github.com/pytorch/pytorch O
( O
e.g. O
, O
4 O
which O
sounds O
like O
yes O
in O
traditional O
Chinese O
) O
. O

People O
may O
type O
variant O
length O
of O
4 O
, O
e.g. O
, O
, O
4444444 O
to O
express O
their O
remarks O
. O

Therefore O
, O
alternatively O
, O
we O
model O
the O
audience O
chat B
with O
a O
character B
- I
level I
LSTM O
- O
RNN O
model O
( O
Graves O
, O
2013 O
) O
. O

Characters B
of O
the O
language O
, O
Chinese O
, O
English O
, O
or O
Emojis O
, O
are O
expanded O
to O
multiple O
ASCII O
characters B
according O
to O
the O
two O
- O
character B
Unicode O
or O
other O
representations O
used O
on O
the O
chat B
servers B
. O

We O
encode O
a O
1-hot O
vector O
for O
each O
ASCII O
input B
character B
. O

For O
each O
frame B
we O
use O
all O
chats B
that O
occur O
in O
the O
next O
Wt O
seconds O
which O
are O
called B
text O
window O
size O
to O
form O
the O
input B
for O
L O
- O
CharLSTM O
. O

We O
concatenate O
all O
the O
chats B
in O
a O
window O
, O
separating O
them O
by O
a O
special O
stop O
character B
, O
and O
then O
fed O
to O
a O
3-layer O
L O
- O
Char O
- O
LSTM O
model.7 O
This O
model O
is O
shown O
in O
Figure O
3c O
. O

Following O
the O
setting O
in O
Sec O
. O

5 O
, O
we O
evaluate O
the O
text O
window O
size O
from O
5 O
seconds O
to O
9 O
seconds O
, O
and O
got O
the O
following O
accuracies:32.1 O
% O
, O
29.6 O
% O
, O
41.5 O
% O
, O
28.2 O
% O
, O
34.4 O
% O
. O

We O
achieved O
best O
results O
with O
text O
window O
size O
as O
7 O
seconds O
, O
and O
used O
this O
in O
rest O
of O
the O
experiments O
. O

Joint O
lv O
- O
LSTM O
Model O
Our O
final O
lv O
- O
LSTM O
model O
combines O
the O
best O
vision O
and O
language O
models O
: O
V O
- O
CNN O
- O
LSTM O
and O
L O
- O
Char O
- O
LSTM O
. O

For O
the O
vision O
and O
language O
models O
, O
we O
can O
extract O
features O
Fv O
and O
Fl O
from O
V O
- O
CNN O
- O
LSTN O
and O
LChar O
- O
LSTM O
, O
respectively O
. O

Then O
we O
concatenate O
Fv O
and O
Fl O
, O
and O
feed O
it O
into O
a O
2-layer O
MLP O
. O

The O
completed O
model O
is O
shown O
in O
Figure O
3d O
. O

We O
expect O
there O
is O
room O
to O
improve O
this O
approach O
, O
by O
using O
more O
involved O
representations O
, O
e.g. O
, O
Bilinear O
Pooling O
( O
Fukui O
et O
al O
. O

, O
2016 O
) O
, O
Memory O
Networks O
( O
Xiong O
et O
al O
. O

, O
2016 O
) O
, O
and O
Attention O
Models O
( O
Lu O
et O
al O
. O

, O
2016 O
) O
; O
this O
is O
future O
work O
. O

7The O
number O
of O
these O
stop O
characters B
is O
then O
an O
encoding O
of O
the O
number O
of O
chats B
in O
the O
window O
. O

Therefore O
, O
the O
L O
- O
Char O
- O
LSTM O
could O
learn O
to O
use O
this O
# O
chats B
information O
, O
if O
it O
is O
a O
useful O
feature O
. O

Also O
, O
some O
content O
has O
been O
deleted O
by O
Twitch.tv O
or O
the O
channel O
itself O
due O
to O
the O
usage O
of O
improper O
words O
. O

We O
use O
symbol O
” O
\n O
” O
to O
replace O
such O
cases O
. O

Method O
Data O
UF O
P O
R O
F O
L O
- O
Char O
- O
LSTM O
C O
100 O
% O
0.11 O
0.99 O
19.6 O
L O
- O
Char O
- O
LSTM O
C O
last O
25 O
% O
0.35 O
0.51 O
41.5 O
L O
- O
Word O
- O
LSTM O
C O
last O
25 O
% O
0.10 O
0.99 O
19.2 O
V O
- O
CNN O
V O
100 O
% O
0.40 O
0.93 O
56.2 O
V O
- O
CNN O
V O
last O
25 O
% O
0.57 O
0.74 O
64.0 O
V O
- O
CNN O
- O
LSTM O
V O
last O
25 O
% O
0.58 O
0.82 O
68.3 O
lv O
- O
LSTM O
C+V O
last O
25 O
% O
0.77 O
0.72 O
74.8 O
Table O
2 O
: O
Ablation O
Study O
: O
Effects B
of O
various O
models O
. O

C O
: O
Chat O
, O
V O
: O
Video O
, O
UF O
: O
% O
of O
frames B
Used O
in O
highlight B
clips O
as O
positive O
training O
examples O
; O
P O
: O
Precision O
, O
R O
: O
Recall O
, O
F O
: O
F O
- O
score O
. O

5 O
Experiments O
and O
Results O
Training O
Details O
In O
development O
and O
ablation O
studies O
, O
we O
use O
train B
and O
val O
splits B
of O
the O
data O
from O
NALCS O
to O
evaluate O
models O
in O
Section O
3 O
. O

For O
the O
final O
results O
, O
models O
are O
retrained O
on O
the O
combination O
of O
train B
and O
val O
data O
( O
following O
major O
vision O
benchmarks O
e.g. O
PASCAL O
- O
VOC O
and O
COCO O
) O
, O
and O
performance O
is O
measured O
on O
the O
test O
set O
. O

We O
separate O
the O
highlight B
prediction O
to O
three O
different O
tasks O
based B
on O
using O
different O
input B
data O
: O
videos O
, O
chats B
, O
and O
videos+chats O
. O

The O
details O
of O
dataset O
split B
are O
in O
Section O
3 O
. O

Our O
code O
is O
implemented O
in O
PyTorch O
. O

To O
deal O
with O
the O
large O
number O
of O
frames B
total O
, O
we O
sample O
only O
5k O
positive O
and O
5k O
negative O
examples O
in O
each O
epoch O
. O

We O
use O
batch O
size O
of O
32 O
and O
run O
60 O
epochs O
in O
all O
experiments O
. O

Weight O
decay O
is O
10−4 O
and O
learning O
rate O
is O
set O
as O
10−2 O
in O
the O
first O
20 O
epochs O
and O
10−3 O
after O
that O
. O

Cross O
entropy O
loss O
is O
used O
. O

Highlights B
are O
generated O
by O
fans O
and O
consist O
of O
clips O
. O

We O
match B
each O
clip O
to O
when O
it O
happened O
in O
the O
full B
match B
and O
call B
this O
the O
highlight B
clip O
( O
non O
- O
overlapping O
) O
. O

The O
action O
of O
interest O
( O
kill B
, O
objective B
control B
, O
etc O
. O

) O
often O
happens O
in O
the O
later O
part O
of O
a O
highlight B
clip O
, O
while O
the O
clip O
contains O
some O
additional O
context O
before O
that O
action O
that O
may O
help O
set O
the O
stage B
. O

For O
some O
of O
our O
experimental O
settings O
( O
Table O
2 O
) O
, O
we O
used O
a O
heuristic O
of O
only O
including O
the O
last O
25 O
% O
frames B
in O
every O
highlight B
clip O
as O
positive O
training O
examples O
. O

During O
evaluation O
, O
we O
used O
all O
frames B
in O
the O
highlight B
clip O
. O

Ablation O
Study O
Table O
2 O
shows O
the O
performance O
of O
each O
module O
separately O
on O
the O
dev O
set O
. O

For O
the O
basic O
L O
- O
Char O
- O
LSTM O
and O
V O
- O
CNN O
models O
, O
using O
only O
the O
last O
25 O
% O
of O
frames B
in O
highlight B
clips O
in O
training O
works O
best O
. O

In O
order O
to O
evaluate O
the O
performance O
of O
L O
- O
Char O
- O
LSTM O
model O
, O
we O
also O
train B
a O
Word O
- O
LSTM O
model O
by O
tokenizing O
all O
the O
chats B
and O
Method O
Data O
NALCS O
LMS O
L O
- O
Char O
- O
LSTM O
chat B
43.2 O
39.7 O
V O
- O
CNN O
- O
LSTM O
video O
72.2 O
69.2 O
lv O
- O
LSTM O
chat+video O
74.7 O
70.0 O
Table O
3 O
: O
Test O
Results O
on O
the O
NALCS O
( O
English O
) O
and O
LMS O
( O
Traditional O
Chinese O
) O
datasets O
. O

only O
considering O
the O
words O
that O
appeared O
more O
than O
10 O
times B
, O
which O
results O
in O
10019 O
words O
. O

We O
use O
this O
vocabulary O
to O
encode O
the O
words O
to O
1-hot O
vectors O
. O

The O
L O
- O
Char O
- O
LSTM O
outperforms O
L O
- O
WordLSTM O
by O
22.3 O
% O
. O

Test O
Results O
Test O
results O
are O
shown O
in O
Table O
3 O
. O

Somewhat O
surprisingly O
, O
the O
vision O
only O
model O
is O
more O
accurate O
than O
the O
language O
only O
model O
, O
despite O
the O
real B
- I
time I
nature O
of O
the O
comment O
stream O
. O

This O
is O
perhaps O
due O
to O
the O
visual O
form O
of O
the O
game B
, O
where O
highlight B
events O
may O
have O
similar O
animations B
. O

However O
, O
including O
language O
with O
vision O
in O
the O
lv O
- O
LSTM O
model O
significantly O
improves O
over O
vision O
alone O
, O
as O
the O
comments O
may O
exhibit O
additional O
contextual O
information O
. O

Comparing O
results O
between O
ablation O
and O
the O
final O
test O
, O
it O
seems O
more O
data O
contributes O
to O
higher O
accuracy O
. O

This O
effect B
is O
more O
apparent O
in O
the O
vision O
models O
, O
perhaps O
due O
to O
complexity O
. O

Moreover O
, O
L O
- O
Char O
- O
LSTM O
performs O
better O
in O
English O
compared O
to O
traditional O
Chinese O
. O

From O
the O
numbers O
given O
in O
Section O
3 O
, O
variation O
in O
the O
number O
of O
chats B
in O
NALCS O
was O
much O
higher O
than O
LMS O
, O
which O
one O
may O
expect O
to O
have O
a O
critical O
effect B
in O
the O
language O
model O
. O

However O
, O
our O
results O
seem O
to O
suggest O
that O
the O
L O
- O
Char O
- O
LSTM O
model O
can O
pickup O
other O
factors O
of O
the O
chat B
data O
( O
e.g. O
content O
) O
instead O
of O
just O
counting O
the O
number O
of O
chats B
. O

We O
expect O
a O
different O
language O
model O
more O
suitable O
for O
the O
traditional O
Chinese O
language O
should O
be O
able O
to O
improve O
the O
results O
for O
the O
LMS O
data O
. O

6 O
Conclusion O
We O
presented O
a O
new O
dataset O
and O
multimodal O
methods O
for O
highlight B
prediction O
, O
based B
on O
visual O
cues O
and O
textual O
audience O
chat B
reactions O
in O
multiple O
languages O
. O

We O
hope O
our O
new O
dataset O
can O
encourage O
further O
multilingual O
, O
multimodal O
research O
. O

Acknowledgments O
We O
thank O
Tamara O
Berg O
, O
Phil O
Ammirato O
, O
and O
the O
reviewers O
for O
their O
helpful O
suggestions O
, O
and O
we O
acknowledge O
support B
from O
NSF O
1533771 O
. O

References O
Stanislaw O
Antol O
, O
Aishwarya O
Agrawal O
, O
Jiasen O
Lu O
, O
Margaret O
Mitchell O
, O
Dhruv O
Batra O
, O
C. O
Lawrence O
Zitnick O
, O
and O
Devi O
Parikh O
. O

2015 O
. O

VQA O
: O
Visual O
Question O
Answering O
. O

In O
ICCV O
. O

Sandra O
E. O
F. O
de O
Avila O
, O
Ana O
P. O
B. O
Lopes O
, O
Antonio O
da O
Luz O
Jr. O
, O
and O
Arnaldo O
de O
A. O
Arajo O
. O

2011 O
. O

Vsumm O
: O
A O
mechanism B
designed O
to O
produce O
static O
video O
summaries O
and O
a O
novel O
evaluation O
method O
. O

In O
Pattern O
Recognition O
Letters O
. O

M. O
Bertini O
, O
A. O
Del O
Bimbo O
, O
and O
W. O
Nunziati O
. O

2005 O
. O

Soccer O
videos B
highlight I
prediction O
and O
annotation O
in O
real O
time B
. O

ICIAP O
. O

David O
L. O
Chen O
and O
William O
B. O
Dolan O
. O

2011 O
. O

Collecting O
highly O
parallel O
data O
for O
paraphrase O
evaluation O
. O

In O
ACL O
. O

Chih O
- O
Chieh O
Cheng O
and O
Chiou O
- O
Ting O
Hsu O
. O

2006 O
. O

Fusion O
of O
audio O
and O
motion O
information O
on O
hmmbased O
highlight B
extraction O
for O
baseball O
games B
. O

IEEE O
Trans O
. O

Multimedia O
. O

Sumit O
Chopra O
, O
Michael O
Auli O
, O
and O
Alexander O
M. O
Rush O
. O

2016 O
. O

Abstractive O
sentence O
summarization O
with O
attentive O
recurrent O
neural O
networks O
. O

In O
NAACL O
. O

Katja O
Filippova O
, O
Enrique O
Alfonseca O
, O
Carlos O
A O
Colmenares O
, O
Lukasz O
Kaiser O
, O
and O
Oriol O
Vinyals O
. O

2015 O
. O

Sentence O
compression O
by O
deletion O
with O
lstms O
. O

EMNLP O
. O

Katja O
Filippova O
and O
Yasemin O
Altun O
. O

2013 O
. O

The O
lack O
of O
parallel O
data O
in O
sentence O
compression O
. O

EMNLP O
. O

Akira O
Fukui O
, O
Dong O
Huk O
Park O
, O
Daylen O
Yang O
, O
Anna O
Rohrbach O
, O
Trevor O
Darrell O
, O
and O
Marcus O
Rohrbach O
. O

2016 O
. O

Multimodal O
compact O
bilinear O
pooling O
for O
visual O
question O
answering O
and O
visual O
grounding O
. O

In O
EMNLP O
. O

Alex O
Graves O
. O

2013 O
. O

Generating O
sequences O
with O
recurrent O
neural O
networks O
. O

Neural O
computation O
. O

Michael O
Gygli O
, O
Helmut O
Grabner O
, O
Hayko O
Riemenschneider O
, O
and O
Luc O
Van O
Gool O
. O

2014 O
. O

Creating O
summaries O
from O
user B
videos O
. O

In O
ECCV O
. O

Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O

2016 O
. O

Deep O
residual O
learning O
for O
image O
recognition O
. O

In O
CVPR O
. O

Liang O
- O
Chi O
Hsieh O
, O
Ching O
- O
Wei O
Lee O
, O
Tzu O
- O
Hsuan O
Chiu O
, O
and O
Winston O
Hsu O
. O

2012 O
. O

Live O
semantic O
sport O
highlight B
detection O
based B
on O
analyzing O
tweets O
of O
twitter O
. O

ICME O
. O

Ting O
- O
Hao O
K. O
Huang O
, O
Francis O
Ferraro O
, O
Nasrin O
Mostafazadeh O
, O
Ishan O
Misra O
, O
Jacob O
Devlin O
, O
Aishwarya O
Agrawal O
, O
Ross O
Girshick O
, O
Xiaodong O
He O
, O
Pushmeet O
Kohli O
, O
Dhruv O
Batra O
, O
et O
al O
. O

2016 O
. O

Visual O
storytelling O
. O

In O
NAACL O
. O

Sahar O
Kazemzadeh O
, O
Vicente O
Ordonez O
, O
Mark O
Matten O
, O
and O
Tamara O
Berg O
. O

2014 O
. O

Referitgame O
: O
Referring O
to O
objects B
in O
photographs O
of O
natural O
scenes O
. O

In O
EMNLP O
. O

Ranjay O
Krishna O
, O
Yuke O
Zhu O
, O
Oliver O
Groth O
, O
Justin O
Johnson O
, O
Kenji O
Hata O
, O
Joshua O
Kravitz O
, O
Stephanie O
Chen O
, O
Yannis O
Kalantidis O
, O
Li O
- O
Jia O
Li O
, O
David O
A O
Shamma O
, O
Michael O
Bernstein O
, O
and O
Li O
Fei O
- O
Fei O
. O

2016 O
. O

Visual O
genome O
: O
Connecting O
language O
and O
vision O
using O
crowdsourced O
dense O
image O
annotations O
. O

In O
arXiv:1602.07332 O
. O

Tsung O
- O
Yi O
Lin O
, O
Michael O
Maire O
, O
Serge O
Belongie O
, O
James O
Hays O
, O
Pietro O
Perona O
, O
Deva O
Ramanan O
, O
Piotr O
Dollr O
, O
and O
C. O
Lawrence O
Zitnick O
. O

2014 O
. O

Microsoft O
coco O
: O
Common O
objects B
in O
context O
. O

In O
ECCV O
. O

Jiasen O
Lu O
, O
Jianwei O
Yang O
, O
Dhruv O
Batra O
, O
and O
Devi O
Parikh O
. O

2016 O
. O

Hierarchical O
question O
- O
image O
coattention O
for O
visual O
question O
answering O
. O

In O
NIPS O
. O

Hongyuan O
Mei O
, O
Mohit O
Bansal O
, O
and O
Matthew O
R. O
Walter O
. O

2016 O
. O

What O
to O
talk O
about O
and O
how O
? O
selective O
generation O
using O
lstms O
with O
coarse O
- O
to O
- O
fine O
alignment O
. O

NAACL O
. O

Ramesh O
Nallapati O
, O
Bowen O
Zhou O
, O
Caglar O
Gulcehre O
, O
Bing O
Xiang O
, O
et O
al O
. O

2016 O
. O

Abstractive O
text O
summarization O
using O
sequence O
- O
to O
- O
sequence O
rnns O
and O
beyond O
. O

In O
CoNLL O
. O

Vicente O
Ordonez O
, O
Girish O
Kulkarni O
, O
and O
Tamara O
L O
Berg O
. O

2011 O
. O

Im2text O
: O
Describing O
images O
using O
1 O
million O
captioned O
photographs O
. O

In O
NIPS O
. O

Cyrus O
Rashtchian O
, O
Peter O
Young O
, O
Micah O
Hodosh O
, O
and O
Julia O
Hockenmaier O
. O

2010 O
. O

Collecting O
image O
annotations O
using O
amazon O
’s O
mechanical O
turk O
. O

NAACL O
HLT O
workshop O
. O

Anna O
Rohrbach O
, O
Marcus O
Rohrbach O
, O
Niket O
Tandon O
, O
and O
Bernt O
Schiele O
. O

2015 O
. O

A O
dataset O
for O
movie O
description O
. O

In O
CVPR O
. O

Olga O
Russakovsky O
, O
Jia O
Deng O
, O
Hao O
Su O
, O
Jonathan O
Krause O
, O
Sanjeev O
Satheesh O
, O
Sean O
Ma O
, O
Zhiheng O
Huang O
, O
Andrej O
Karpathy O
, O
Aditya O
Khosla O
, O
Michael O
Bernstein O
, O
Alexander O
C. O
Berg O
, O
and O
Li O
Fei O
- O
Fei O
. O

2015 O
. O

Imagenet O
large O
scale O
visual O
recognition O
challenge O
. O

In O
IJCV O
. O

Abigail O
See O
, O
Peter O
J O
Liu O
, O
and O
Christopher O
D O
Manning O
. O

2017 O
. O

Get O
to O
the O
point B
: O
Summarization O
with O
pointergenerator O
networks O
. O

In O
ACL O
. O

Yale O
Song O
. O

2016 O
. O

Real B
- I
time I
video B
highlights I
for O
yahoo O
esports O
. O

In O
arXiv:1611.08780 O
. O

Yale O
Song O
, O
Jordi O
Vallmitjana O
, O
Amanda O
Stent O
, O
and O
Alejandro O
Jaimes O
. O

2015 O
. O

Tvsum O
: O
Summarizing O
web O
videos O
using O
titles O
. O

In O
CVPR O
. O

Ilya O
Sutskever O
, O
Oriol O
Vinyals O
, O
and O
Quoc O
V. O
Le O
. O

2014 O
. O

Sequence O
to O
sequence O
learning O
with O
neural O
networks O
. O

In O
NIPS O
. O

Makarand O
Tapaswi O
, O
Yukun O
Zhu O
, O
Rainer O
Stiefelhagen O
, O
Antonio O
Torralba O
, O
Raquel O
Urtasun O
, O
and O
Sanja O
Fidler O
. O

2016 O
. O

Movieqa O
: O
Understanding O
stories O
in O
movies O
through O
question O
- O
answering O
. O

In O
CVPR O
. O

Atousa O
Torabi O
, O
Christopher O
Pal O
, O
Hugo O
Larochelle O
, O
and O
Aaron O
Courville O
. O

2015 O
. O

Using O
descriptive O
video O
services O
to O
create O
a O
large O
data O
source O
for O
video O
annotation O
research O
. O

In O
arXiv:1503.01070v1 O
. O

Jinjun O
Wang O
, O
Changsheng O
Xu O
, O
Engsiong O
Chng O
, O
and O
Qi O
Tian O
. O

2004 O
. O

Sports O
highlight B
detection O
from O
keyword O
sequences O
using O
hmm O
. O

ICME O
. O

Caiming O
Xiong O
, O
Stephen O
Merity O
, O
and O
Richard O
Socher O
. O

2016 O
. O

Dynamic O
memory O
networks O
for O
visual O
and O
textual O
question O
answering O
. O

In O
ICML O
. O

Licheng O
Yu O
, O
Eunbyung O
Park O
, O
Alexander O
C. O
Berg O
, O
and O
Tamara O
L. O
Berg O
. O

2015 O
. O

Visual O
madlibs O
: O
Fill O
- O
in O
- O
theblank O
image O
description O
and O
question O
answering O
. O

In O
ICCV O
. O

Performance O
Dynamics O
and O
Success O
in O
Online O
Games O
Anna O
Sapienza O
USC O
Information O
Sciences O
Institute O
Marina O
del O
Rey O
, O
California O
, O
90292 O
Hao O
Peng O
USC O
Information O
Sciences O
Institute O
Marina O
del O
Rey O
, O
California O
, O
90292 O
Emilio O
Ferrara O
USC O
Information O
Sciences O
Institute O
Marina O
del O
Rey O
, O
California O
, O
90292 O
Abstract O
— O
Online O
data O
provide O
a O
way O
to O
monitor O
how O
users B
behave O
in O
social O
systems O
like O
social O
networks O
and O
online B
games I
, O
and O
understand O
which O
features O
turn B
an O
ordinary O
individual O
into O
a O
successful O
one O
. O

Here O
, O
we O
propose O
to O
study O
individual O
performance O
and O
success O
in O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
. O

Our O
purpose O
is O
to O
identify O
those O
behaviors O
and O
playing B
styles I
that O
are O
characteristic O
of O
players B
with O
high B
skill O
level I
and I
that O
distinguish O
them O
from O
other O
players B
. O

To O
this O
aim O
, O
we O
study O
Defense O
of O
the O
ancient O
2 O
( O
Dota O
2 O
) O
, O
a O
popular O
MOBA O
game B
. O

Our O
findings O
highlight B
three O
main O
aspects O
to O
be O
successful O
in O
the O
game B
: O
( O
i O
) O
players B
need O
to O
have O
a O
warm B
- I
up I
period I
to O
enhance O
their O
performance O
in O
the O
game B
; O
( O
ii O
) O
having O
a O
long O
in B
- I
game I
experience B
does O
not O
necessarily O
translate O
in O
achieving O
better O
skills B
; O
but O
rather O
, O
( O
iii O
) O
players B
that O
reach O
high B
skill O
levels I
differentiate I
from O
others O
because O
of O
their O
aggressive O
playing O
strategy B
, O
which O
implies O
to O
kill B
opponents B
more O
often O
than O
cooperating O
with O
teammates B
, O
and O
trying O
to O
give O
an O
early B
end O
to O
the O
match B
. O

I. O
INTRODUCTION O
The O
increasing B
availability O
of O
online O
data O
provides O
a O
key O
way O
to O
monitor O
and O
study O
how O
users B
behave O
in O
online O
social O
systems O
and O
to O
understand O
which O
are O
the O
characteristics O
that O
drive O
users B
to O
success O
, O
e.g. O
, O
having O
a O
popular O
account O
on O
Twitter O
, O
being O
an O
experienced O
user B
on O
StackExchange O
( O
collecting O
badges O
) O
, O
or O
becoming O
an O
advanced O
player B
in O
online B
games I
. O

A O
key O
research O
question O
is O
to O
understand O
how O
these O
behavioral O
patterns O
and O
users B
’ O
performance O
are O
influenced O
: O
which O
are O
the O
major O
characteristics O
that O
bring O
users B
to O
perform O
better O
( O
reaching O
success O
) O
? O
Which O
characteristics O
have O
a O
bad O
influence O
on O
the O
user B
behavior O
( O
performance O
depletion O
) O
? O
Individual O
performance O
and O
success O
were O
previously O
studied O
in O
different O
contexts O
: O
individual O
impact B
in O
scientific O
research O
[ O
1 O
] O
, O
[ O
2 O
] O
, O
successful O
learning O
strategies B
in O
education O
[ O
3 O
] O
, O
[ O
4 O
] O
, O
online O
popularity O
[ O
5]–[7 O
] O
, O
sport O
performance O
[ O
8 O
] O
, O
etc O
. O

Recently O
, O
online O
platforms O
have O
attracted O
much O
attention O
from O
our O
research O
community B
, O
as O
they O
provide O
a O
way O
to O
study O
new O
aspects O
related O
to O
human O
behavior O
. O

This O
is O
the O
case O
for O
social O
media O
like O
Twitter O
, O
whose O
data O
can O
be O
studied O
to O
identify O
the O
mechanisms B
behind O
user B
influence O
and O
popularity O
[ O
5 O
] O
, O
[ O
9]–[11 O
] O
, O
Q&A O
networks O
as O
StackExchange O
[ O
7 O
] O
, O
[ O
12 O
] O
, O
[ O
13 O
] O
, O
in O
which O
users B
acquire O
experience B
by O
collecting O
badges O
when O
fulfilling O
a O
specific O
task O
, O
and O
online B
games I
[ O
14]–[16 O
] O
, O
where O
players B
increase B
their O
skills B
by O
playing B
individually O
or O
by O
working O
as O
a O
part O
of O
a O
team B
. O

Here O
, O
we O
focus B
on O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
, O
a O
popular O
game B
genre O
in O
which O
users B
fight B
with O
their O
teammates B
to O
conquer O
the O
opponent B
base I
[ O
17 O
] O
, O
[ O
18 O
] O
. O

In O
particular O
, O
we O
analyze O
a O
well O
- O
known O
MOBA O
game B
: O
Defense B
of O
the O
ancient O
2 O
( O
Dota O
2 O
) O
. O

This O
type O
of O
game B
provides O
indeed O
key O
insights O
on O
how O
users B
behave O
and O
improve O
their O
skills B
over O
time B
to O
reach O
success O
[ O
19]–[22 O
] O
. O

By O
studying O
individual O
matches B
over O
time B
, O
we O
not O
only O
explore O
how O
players B
improve O
while O
playing B
consecutive O
matches B
, O
but O
also O
monitor O
how O
their O
performance O
changes O
based B
on O
the O
role B
they O
are O
impersonating O
[ O
23 O
] O
. O

In O
particular O
, O
we O
are O
interested O
in O
deepening O
our O
understanding O
of O
the O
way O
players B
either O
enhance O
or O
worsen O
their O
skills B
in O
the O
game B
. O

To O
this O
aim O
, O
we O
study O
actions O
of O
individual B
players I
in O
consecutive O
matches B
( O
i.e. O
sessions B
) O
to O
identify O
how O
and O
why O
their O
performance O
changes O
. O

We O
are O
indeed O
interested O
in O
addressing O
some O
of O
the O
following O
questions O
: O
how O
can O
we O
identify O
successful O
players B
? O
Are O
high B

 O
skill I
levels I
only O
reached O
by O
long O
- O
time B
experienced O
players B
? O
What O
are O
the O
key O
features O
defining O
a O
successful O
player B
? O
What O
are O
the O
differences O
between O
high O
and O
low O
experience B
/ O
skill B
players B
? O
Giving O
an O
answer O
to O
these O
questions O
could O
help O
to O
design O
new O
incentives O
( O
badges O
, O
rewards B
) O
with O
the O
aim O
of O
increasing B
player B
engagement O
in O
the O
game B
as O
well O
as O
providing O
useful O
recommendations O
to O
lower O
performer B
players I
to O
encourage O
them O
to O
change O
their O
attitude O
and O
achieve O
better O
results O
[ O
24 O
] O
. O

The O
present O
paper O
is O
organized O
as O
follows O
: O
in O
§ O
II O
we O
describe O
the O
data O
and O
the O
collection O
process O
used O
in O
the O
study O
; O
in O
§ O
III O
we O
introduce O
the O
methods O
used O
to O
perform O
the O
analysis O
; O
in O
§ O
IV O
we O
report B
the O
results O
of O
our O
study O
and O
compare O
low O
/ O
high O
experience B
players B
and O
low O
/ O
high B
skill I
players I
to O
understand O
which O
are O
the O
main B
characteristics I
of O
successful O
strategies B
in O
the O
game B
; O
in O
§ O
V O
we O
comment O
the O
related O
work O
and O
provide O
a O
comparison O
with O
our O
results O
; O
finally O
, O
in O
§ O
VI O
we O
draw O
the O
conclusions O
of O
our O
study O
, O
and O
describe O
the O
main O
findings O
and O
future O
work O
. O

II O
. O

DATA O
DESCRIPTION O
AND O
COLLECTION O
A. O
Dota O
2 O
Dota O
2 O
is O
one O
of O
the O
most O
popular O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
video B
games I
providing O
a O
combination O
of O
action O
, O
strategy B
and O
role B
play B
. O

It O
is O
played B
by O
millions O
of O
both O
professional O
and O
casual O
fans O
every O
day O
worldwide.12 O
A O
single O
match B
involves O
a O
dynamic O
and O
fast O
- O
paced O
battle B
between O
two O
teams B
of O
5 O
players B
competing O
to O
collectively O
destroy O
the O
base B
1https://www.opendota.com/ O
2https://www.dotabuff.com/ O
arXiv:1801.09783v1 O
[ O
cs O
. O

SI O
] O
29 O
Jan O
2018 O
structure O
, O
i.e. O
the O
Ancient O
defended O
by O
the O
opposing B
team I
. O

Each O
player B
in O
a O
match B
controls B
one O
of O
the O
113 O
characters B
, O
i.e. O
heroes O
available O
in O
the O
game B
. O

These O
heroes B
are O
an O
essential O
element O
of O
the O
game B
as O
having O
different O
combinations O
of O
heroes B
in O
the O
two O
opposing B
teams I
makes O
each O
match B
unique O
. O

Each O
hero B
is O
categorized O
, O
depending O
on O
its O
skills B
, O
in O
three O
separated O
groups B
, O
namely O
Strength O
, O
Agility O
and O
Intelligence O
, O
which O
in O
this O
paper O
we O
refer O
to O
as O
hero B
types O
. O

Strength B
heroes B
are O
strong O
warriors B
that O
can O
survive O
receiving O
more O
damage B
than O
others O
and O
can O
withstand O
longer O
battles B
. O

Agility B
heroes B
are O
known O
for O
their O
quick O
response O
and O
attack B
speed O
. O

Intelligence O
heroes B
often O
utilize O
their O
magical O
powers B
to O
enhance O
the O
attack B
against O
others O
and O
can O
also O
support B
their O
allies B
, O
e.g. O
, O
via O
healing B
or O
buffing O
skills B
, O
in O
different O
tasks O
. O

By O
design O
, O
a O
single B
hero I
can O
not O
be O
perfect O
in O
all O
three O
dimensions O
, O
but O
the O
interplay O
between O
different O
types O
of O
heroes B
impacts B
the O
overall O
strategy B
and O
outcome O
of O
each O
match B
. O

On O
the O
battlefield B
, O
players B
accumulate O
experience B
and O
gold B
through O
a O
variety O
of O
actions O
( O
kill B
, O
assist B
, O
death B
, O
etc O
. O

) O
that O
can O
be O
later O
spent O
to O
acquire O
new O
skills B
to O
improve O
one O
’s O
hero B
and O
overtake O
the O
opponents B
. O

Players B
usually O
play B
several O
matches B
consecutively O
before O
taking O
an O
extended O
break O
. O

On O
average O
, O
each O
match B
lasts O
for O
40 O
minutes O
. O

According O
to O
different O
ways O
of O
assembling O
players B
for O
a O
competition,3 O
all O
matches B
can O
be O
grouped B
into O
8 O
different O
types O
, O
namely O
Public B
matchmaking I
, O
Practice O
, O
Tournament O
, O
Tutorial O
, O
Co O
- O
op O
with O
AI O
, O
Team O
match B
, O
Solo B
queue I
, O
Ranked B
matchmaking B
, O
and O
Solo O
Mid O
1 O
vs O
1 O
. O

In O
this O
paper O
, O
we O
only O
focus B
on O
matches B
with O
Public O
or O
Ranked B
matchmaking B
to O
ensure O
the O
presence O
of O
10 O
human O
players B
in O
each O
match B
. O

B. O
Data O
Collection O
and O
Preprocessing O
We O
collected O
3,566,804 O
matches B
spanning O
from O
July O
17 O
, O
2013 O
to O
December O
13 O
, O
2015 O
using O
the O
Dota O
2 O
official O
API.4 O
Due O
to O
data O
sparsity O
, O
we O
perform O
our O
analysis O
on O
3,300,146 O
matches B
played B
since O
May O
2015 O
. O

Each O
entry O
in O
our O
dataset O
contains O
match B
meta B
information O
, O
including O
start B
time B
, O
duration O
, O
lobby O
type O
( O
Ranked B
matchmaking B
, O
Public B
matchmaking I
, O
etc O
. O

) O
and O
winning O
status B
, O
and O
it O
also O
contains O
information O
about O
each O
player B
’s O
performance O
in O
that O
match B
, O
such O
as O
number O
of O
kills B
, O
assists B
, O
deaths B
, O
gold B
earned O
, O
etc O
. O

We O
preprocessed O
the O
collected O
data O
and O
selected O
all O
those O
matches B
for O
which O
we O
have O
access O
to O
complete O
information O
. O

In O
particular O
, O
we O
discarded O
all O
the O
matches B
characterized O
by O
one O
of O
the O
following O
cases O
: O
• O
Connection B
errors O
: O
some O
matches B
end O
earlier O
due O
to O
connection B
errors O
. O

In O
this O
case O
, O
the O
winning O
status B
related O
to O
the O
match B
corresponds O
to O
a O
null O
value O
, O
and O
the O
actions O
performed O
by O
the O
players B
will O
be O
only O
partially O
recorded O
. O

• O
Default O
players B
: O
not O
all O
the O
players B
allow O
to O
make O
their O
personal O
information O
public O
. O

In O
this O
case O
, O
a O
unique O
identifier O
is O
assigned O
to O
all O
these O
players B
, O
thus O
preventing O
their O
identification O
. O

• O
Leaver B
status B
: O
some O
players B
quit O
a O
match B
at O
the O
beginning O
, O
thus O
leaving O
the O
two O
opposing B
team I
unbalanced O
. O

In O
3https://dota2api.readthedocs.io/en/latest/responses.html#lobby-type O
4http://dota2api.readthedocs.io/en/latest/ O
this O
case O
, O
the O
player B
’s O
feature O
fields B
( O
kills B
, O
assists B
, O
deaths B
, O
etc O
) O
will O
be O
0 O
. O

After O
this O
filtering O
, O
we O
collect O
all O
players B
that O
appear O
in O
the O
selected B
matches I
, O
which O
correspond O
to O
a O
total O
of O
1,805,225 O
players B
. O

Finally O
, O
we O
compute O
the O
distribution O
of O
the O
number O
of O
matches B
per O
user B
among O
the O
1,805,225 O
selected B
players I
. O

The O
distribution O
, O
reported B
in O
Fig O
. O

1 O
, O
shows O
that O
almost O
75 O
% O
of O
the O
players B
participated O
to O
less O
than O
10 O
matches B
. O

For O
our O
analysis O
, O
we O
require O
that O
each O
selected B
player I
played B
at O
least O
10 O
matches B
. O

The O
final O
number O
of O
player B
selected O
after O
applying O
this O
threshold O
is O
460,026 O
over O
the O
1.8 O
million O
initial O
ones O
. O

0 O
200 O
400 O
600 O
800 O
1000 O
1200 O
1400 O
# O
of O
matches B
played B
per O
user B
100 O
101 O
102 O
103 O
104 O
105 O
106 O
# O
of O
players B
Fig O
. O

1 O
. O

Distribution O
of O
the O
number O
of O
matches B
played B
by O
each O
user B
over O
the O
total O
1,805,225 O
players B
selected O
after O
the O
data O
preprocessing O
step O
. O

III O
. O

METHODS O
With O
the O
aim O
of O
investigating O
how O
player B
’s O
performance O
evolves O
over O
time B
, O
we O
focus B
on O
two O
different O
aspects O
. O

First O
of O
all O
, O
we O
look O
at O
their O
performance O
in O
separated O
sessions B
: O
to O
this O
aim O
, O
we O
first O
grouped B
together O
matches B
that O
are O
played B
in O
sequence O
without O
extended O
breaks O
. O

Second O
, O
we O
are O
interested O
in O
understanding O
how O
players B
’ O
performance O
changes O
accordingly O
to O
the O
type O
of O
hero B
they O
select O
and O
if O
there O
is O
any O
preference O
in O
the O
hero B
type O
selection I
. O

Thus O
, O
we O
study O
player B
’s O
behaviors I
over O
time B
by O
dividing O
them O
in O
game B
sessions B
and O
further O
explore O
player B
’s O
performance O
based B
on O
the O
selected O
hero B
by O
looking O
at O
sessions B
characterized O
by O
the O
same O
hero B
type O
. O

A. O
Game O
Sessions O
We O
divide O
our O
dataset O
in O
time B
series O
of O
matches B
played B
by O
those O
players B
having O
more O
than O
10 O
matches B
. O

The O
long O
sequence O
of O
each O
player B
’s O
ordered O
historical O
matches B
can O
be O
divided O
into O
short O
time B
game B
sessions B
, O
namely O
short O
periods O
of O
playing B
behavior O
without O
an O
extended O
break O
. O

In O
this O
work O
, O
we O
identify O
game B
sessions B
by O
examining O
the O
time B
intervals O
between O
consecutive O
matches B
against O
a O
predefined O
threshold O
. O

We O
indeed O
define O
a O
game B
session B
to O
be O
composed O
by O
several O
consecutive O
matches B
with O
no O
more O
than O
a O
15-minute O
break O
between O
them O
. O

This O
threshold O
corresponds O
to O
the O
peak O
of O
the O
TABLE O
I O
NUMBER O
OF O
SESSIONS B
OF O
LENGTH O
BETWEEN O
1 O
AND O
5 O
, O
DIVIDED O
BY O
HERO O
TYPES O
. O

Hero O
type B
Sessions O
of I
length I
1 O
Length O
2 O
Length O
3 O
Length O
4 O
Length O
5 O
Intelligence O
4,310,247 O
288,211 O
34,375 O
5,860 O
1,233 O
Agility O
3,503,961 O
200,129 O
21,508 O
3,435 O
712 O
Strength O
3,375,104 O
184,659 O
18,845 O
2,920 O
577 O
distribution O
of O
the O
time B
breaks O
between O
all O
the O
matches B
in O
our O
dataset O
. O

By O
dividing O
matches B
into O
sessions B
, O
we O
expect O
that O
users B
’ O
playing B
behavior O
, O
e.g. O
adopted O
strategies B
, O
may O
be O
different O
for O
matches B
in O
the O
same O
session B
. O

We O
report B
the O
distribution O
of O
the O
number O
of O
sessions B
with O
different O
lengths O
in O
Fig O
. O

2 O
. O

In O
the O
following O
, O
we O
will O
focus B
on O
the O
analysis B

 O
of I
sessions I
whose I
length I
is O
between O
1 O
and O
5 O
matches B
. O

This O
amounts O
for O
over O
90 O
% O
of O
the O
total O
sessions B
. O

0 O
5 O
10 O
15 O
20 O
25 O
30 O
session B
length O
100 O
101 O
102 O
103 O
104 O
105 O
106 O
107 O
108 O
# O
of O
sessions B
Fig O
. O

2 O
. O

Distribution O
of O
the O
number O
of O
sessions B
of O
different O
lengths O
. O

B. O
Game O
Sessions O
for O
Hero O
Types O
A O
player B
must O
select O
a O
specific O
hero B
before O
starting B
a O
match B
. O

The O
decision O
is O
based B
upon O
compound O
factors O
: O
overall O
team B

 O
formation I
and O
role B
balance O
, O
as O
well O
as O
player B
’s O
preferences O
. O

Although O
different O
heroes B
have O
different O
skill B
sets I
and O
abilities B
, O
heroes B
of O
the O
same O
type O
often O
have O
similar O
strengths B
and O
weaknesses B
. O

Thus O
, O
with O
the O
expectation O
that O
a O
player B
controlling B
heroes B
of O
different O
types O
in O
a O
session B
may O
have O
different O
performance O
, O
we O
further O
identify O
game B
sessions B
in O
which O
a O
player B
selected O
the O
same O
hero B
type O
in O
all O
matches B
of O
a O
single O
session B
. O

We O
report B
the O
number O
of O
sessions B
divided O
by O
hero B
type O
in O
Tab O
. O

I. O
We O
can O
observe O
that O
in O
general O
there O
is O
the O
tendency O
to O
use O
more O
Intelligence O
heroes B
which O
have O
a O
tactic B
and O
supporting B
role B
, O
followed O
by O
Agility O
and O
Strength O
heroes B
. O

C. O
TrueSkill O
Players O
in O
Dota O
2 O
are O
ranked B
by O
using O
a O
Matchmaking O
Rating O
( O
MMR),5 O
whose O
value O
characterizes O
the O
skill B
level I
of I
a O
player I
. O

5https://dota2.gamepedia.com/Matchmaking O
Rating O
This O
value O
is O
used O
to O
generate O
the O
team B
in O
each O
game B
and O
it O
increases B
/ O
decreases O
if O
a O
player B
wins O
/ O
loses O
the O
game B
. O

In O
the O
available O
data O
, O
we O
do O
not O
have O
access O
to O
the O
information O
related O
to O
the O
official O
MMR O
of O
each O
player B
. O

For O
this O
reason O
, O
we O
choose O
to O
define O
the O
skill B
level I
of I
each O
player I
in O
our O
dataset O
by O
computing O
the O
player B
TrueSkill O
after O
each O
match B
. O

This O
computation O
allows O
us O
to O
characterize O
players B
by O
different O
skill B
levels I
. O

The O
TrueSkill O
is O
a O
rating B
system I
designed O
by O
Microsoft,6 O
whose O
aim O
is O
to O
rank B
players B
in O
online B
games I
according O
to O
their O
skill B
level I
. O

It O
was O
developed O
to O
be O
used O
for O
video B
game I
matchmaking B
on O
Xbox O
Live O
and O
it O
is O
based B
on O
the O
popular O
Elo O
rating B
system I
, O
used O
in O
professional O
chess.7 O
Contrary O
to O
the O
Elo O
rating B
system I
, O
the O
TrueSkill O
is O
specifically O
created O
for O
games B
with O
more O
than O
two O
players B
, O
like O
MOBA O
games B
, O
as O
opposed O
to O
1 O
vs O
1 O
games B
. O

This O
method O
represents O
a O
player B
’s O
skill I
as O
normal O
distribution O
, O
characterized O
by O
two O
parameters O
: O
µ O
, O
i.e. O
the O
average B
skill I

 O
of I
a I
player B
, O
and O
σ O
, O
i.e. O
the O
level B
of O
uncertainty O
in O
the O
player B
’s O
skill B
. O

To O
compute O
the O
TrueSkill O
of O
each O
player B
we O
rely O
on O
the O
open B
Python O
implementation O
of O
the O
trueskill O
library,8 O
in O
which O
we O
use O
as O
starting B
values O
for O
each O
player B
the O
default O
values O
of O
the O
library O
: O
µ O
= O
25 O
and O
σ O
= O
25 O
3 O
. O

Then O
, O
for O
each O
player B
, O
the O
results O
of O
all O
matches B
they O
played B
, O
as O
well O
as O
the O
TrueSkill O
scores O
of O
their O
opponents B
at O
the O
time B
of O
that O
match B
, O
are O
used O
to O
determine O
each O
player B
’s O
current O
TrueSkill O
score O
. O

These O
scores O
are O
finally O
used O
to O
group B
users B
and O
identify O
high O
/ O
low O
skill B
ones O
. O

D. O
Experienced O
players B
vs O
performers O
With O
the O
aim O
of O
identifying O
the O
characteristics O
that O
make O
a O
player B
successful O
in O
the O
game B
, O
we O
divide O
players B
in O
two O
categories O
, O
i.e. O
high O
experience B
/ O
skill B
players B
and O
low O
experience B
/ O
skill B
players B
, O
according O
to O
two O
different O
metrics O
. O

In O
the O
first O
case O
, O
we O
divide O
the O
two O
groups B
of O
players B
by O
taking O
into O
account O
the O
total O
number O
of O
matches B
they O
played B
. O

We O
compute O
the O
distribution O
of O
the O
number O
of O
matches B
for O
all O
players B
having O
more O
than O
10 O
matches B
and O
we O
respectively O
label O
as O
low O
and O
high O
experience B
players B
those O
falling B
in O
the O
5th O
and O
the O
95th O
percentile O
of O
the O
distribution O
. O

In O
the O
second O
case O
, O
we O
divide O
the O
two O
groups B
by O
taking O
into O
account O
their O
experience B
in O
terms O
of O
skill B
level I
in O
the O
game B
. O

Thus O
, O
we O
select O
the O
TrueSkill O
of O
each O
player B
having O
more O
than O
10 O
matches B
, O
as O
defined O
in O
the O
previous O
section O
, O
and O
we O
compute O
the O
TrueSkill O
distribution O
. O

We O
respectively O
define O
, O
accordingly O
to O
the O
other O
group B
division O
, O
as O
low O
and O
6https://www.microsoft.com/en-us/research/project/ O
trueskill O
- O
ranking B
- O
system/ O
7https://en.wikipedia.org/wiki/Elo O
rating B
system I
8https://pypi.python.org/pypi/trueskill O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Intelligence O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Agility O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Strength O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
Fig O
. O

3 O
. O

Distribution O
of O
performed O
actions O
in O
a O
match B
divided O
by O
hero B
type O
. O

The O
three O
axes O
represent O
the O
percentages O
of O
kills B
, O
assists B
and O
deaths B
performed O
, O
while O
the O
color O
intensity O
represents O
the O
normalized O
number O
of O
matches B
for O
which O
a O
certain O
triplet O
is O
performed O
. O

1 O
2 O
3 O
4 O
5 O
5.5 O
6.0 O
6.5 O
7.0 O
7.5 O
8.0 O
# O
of O
kills B
Strength O
1 O
2 O
3 O
4 O
5 O
Agility O
1 O
2 O
3 O
4 O
5 O
Intelligence O
1 O
2 O
3 O
4 O
5 O
11.0 O
11.5 O
12.0 O
12.5 O
13.0 O
13.5 O
14.0 O
# O
of O
assists B
1 O
2 O
3 O
4 O
5 O
1 O
2 O
3 O
4 O
5 O
1 O
2 O
3 O
4 O
5 O
match B
position O
6.6 O
6.8 O
7.0 O
7.2 O
7.4 O
# O
of O
deaths B
1 O
2 O
3 O
4 O
5 O
match B
position O
1 O
2 O
3 O
4 O
5 O
match B
position O
1 O
matches B
2 O
matches B
3 O
matches B
4 O
matches B
5 O
matches B
Fig O
. O

4 O
. O

Mean O
and O
standard O
deviation O
of O
the O
number O
of O
kills B
, O
assists B
and O
deaths B
along O
sessions B
of O
different O
lengths O
( O
from O
1 O
to O
5 O
) O
divided O
by O
hero B
type O
. O

high B
skill I
players I
those O
falling B
in O
the O
5th O
and O
95th O
percentile O
of O
the O
TrueSkill O
distribution O
. O

In O
the O
following O
section O
, O
we O
will O
report B
the O
results O
obtained O
by O
analyzing O
our O
dataset O
and O
we O
will O
compare O
the O
defined O
categorizations O
of O
players B
, O
to O
detect O
which O
characteristics O
make O
a O
player B
successful O
. O

IV O
. O

RESULTS O
A. O
Hero O
type O
characterization O
We O
start B
our O
analysis O
by O
characterizing O
the O
three O
hero B
types O
, O
Intelligence O
, O
Agility O
, O
and O
Strength O
and O
by O
studying O
how O
the O
players B
perform O
when O
using O
one O
of O
these O
heroes B
. O

We O
take O
into O
account O
both O
the O
hero B
type O
that O
is O
used O
by O
a O
certain O
player B
and O
the O
overall O
actions O
he O
/ O
she O
performs O
in O
the O
match B
: O
number O
of O
kills B
, O
number O
of O
assists B
, O
and O
number O
of O
deaths B
. O

Fig O
. O

3 O
shows O
the O
distribution O
of O
the O
number O
of O
actions O
in O
each O
match B
, O
in O
which O
we O
computed O
the O
percentages O
of O
kills B
, O
assists B
and O
deaths B
for O
each O
hero B
type O
. O

By O
comparing O
these O
ternary O
plots O
, O
we O
can O
characterize O
the O
different O
hero B
types O
, O
depending O
on O
the O
actions O
performed O
by O
players B
. O

The O
three O
distributions O
1 O
2 O
3 O
4 O
5 O
5.5 O
6.0 O
6.5 O
7.0 O
7.5 O
# O
of O
kills B
Strength O
1 O
2 O
3 O
4 O
5 O
Agility O
1 O
2 O
3 O
4 O
5 O
Intelligence O
1 O
2 O
3 O
4 O
5 O
11.0 O
11.5 O
12.0 O
12.5 O
13.0 O
13.5 O
# O
of O
assists B
1 O
2 O
3 O
4 O
5 O
1 O
2 O
3 O
4 O
5 O
1 O
2 O
3 O
4 O
5 O
match B
position O
6.6 O
6.8 O
7.0 O
7.2 O
7.4 O
# O
of O
deaths B
1 O
2 O
3 O
4 O
5 O
match B
position O
1 O
2 O
3 O
4 O
5 O
match B
position O
1 O
matches B
2 O
matches B
3 O
matches B
4 O
matches B
5 O
matches B
Fig O
. O

5 O
. O

Player O
’s O
performance O
in O
terms O
of O
number O
of O
kills B
, O
assists B
, O
and O
deaths B
over O
randomized O
sessions B
of O
length O
from O
1 O
to O
5 O
. O

Each O
session B
is O
given O
by O
the O
random O
shuffle O
of O
its O
matches B
. O

indeed O
highlight B
the O
different O
roles B
of O
the O
hero B
types O
. O

The O
Agility O
hero B
’s O
distribution O
is O
more O
centered O
than O
the O
others O
. O

This O
is O
due O
to O
the O
fact O
that O
the O
kills B
performed O
by O
this O
hero B
type O
are O
more O
than O
in O
the O
other O
cases O
. O

This O
type O
of O
hero B
is O
indeed O
the O
one O
having O
the O
greatest O
offensive O
power B
. O

However O
, O
they O
also O
need O
support B
as O
they O
suffer O
a O
higher O
death B
rate O
in O
the O
first O
part O
of O
the O
game B
. O

On O
the O
contrary O
, O
the O
distributions O
related O
to O
both O
Intelligence O
and O
Strength O
heroes B
are O
similar O
. O

These O
distributions O
display O
higher O
number O
of O
assists B
and O
lower B

 O
number I
of I
kills I
than O
the O
Agility O
heroes B
. O

This O
is O
due O
to O
the O
fact O
that O
Intelligence O
heroes B
usually O
have O
a O
support B
role B
, O
i.e. O
, O
assist B
the O
team B
fight I
and O
keep O
allies B
alive O
. O

However O
, O
Strength O
heroes O
are O
characterized O
by O
lower B
death I
rates I
than O
Intelligence O
ones O
. O

They O
are O
indeed O
hard B
to O
kill B
and O
often O
start B
a O
team B
fight I
. O

B. O
The O
warm B
- I
up I
effect I
We O
are O
now O
interested O
in O
understanding O
how O
players B
perform O
over O
time B
, O
and O
if O
the O
use O
of O
a O
certain O
type O
of O
hero B
leads B
to O
better O
performance O
across O
consecutive O
matches B
. O

Thus O
, O
we O
divide O
player B
’s O
histories O
in O
separate O
sessions B
, O
as O
defined O
in O
§ O
III O
, O
and O
we O
study O
sessions B
of O
different O
lengths O
: O
from O
1 O
to O
5 O
. O

For O
each O
session B
of O
a O
given O
length O
, O
we O
compute O
the O
mean O
and O
the O
standard O
deviation O
of O
the O
number O
of O
kills B
, O
assists B
, O
and O
deaths B
, O
divided O
by O
hero B
type O
. O

The O
results O
are O
reported B
in O
Fig O
. O

4 O
, O
which O
provides O
a O
characterization O
of O
the O
hero B
type O
consistent O
with O
what O
we O
observed O
by O
looking O
at O
the O
ternary O
distributions O
over O
all O
the O
matches B
. O

On O
the O
one O
hand O
, O
players B
using O
Agility O
heroes B
perform O
a O
greater O
number O
of O
kills B
and O
lower O
number O
of O
assists B
and O
deaths B
if O
compared O
with O
the O
other O
two O
hero B
types O
. O

On O
the O
other O
hand O
, O
players B
using O
Strength O
and O
Intelligence O
hero B
types O
are O
more O
inclined O
in O
assisting B
during O
a O
team B
fight I
and O
sacrifice O
for O
the O
teammates B
( O
shown O
by O
the O
greater O
number O
of O
deaths B
) O
. O

Moreover O
, O
these O
results O
show O
an O
interesting O
trend O
among O
the O
consecutive O
matches B
in O
a O
session B
: O
a O
warm B
- I
up I
effect I
. O

During O
the O
same O
session B
indeed O
, O
the O
performance O
of O
players B
increases B
and O
this O
is O
particularly O
evident O
in O
the O
last O
match B
of O
each O
session B
. O

This O
rise O
in O
player B
’s O
performance O
indicates O
that O
players B
warmup O
along O
the O
matches B
and O
increment O
the O
final O
number O
of O
actions O
of O
almost O
10 O
% O
. O

Another O
interesting O
feature O
is O
that O
the O
major O
increment O
occurs O
in O
the O
last O
match B
of O
the O
session B
. O

This O
could O
be O
interpreted O
by O
the O
fact O
that O
players B
, O
after O
the O
warm B
- I
up I
period I
, O
reach O
a O
satisfactory O
performance O
and O
thus O
decide O
to O
stop O
their O
playing O
session B
, O
i.e. O
, O
“ O
leaving O
the O
game B
as O
winners B
. O

” O
Finally O
, O
to O
verify O
the O
robustness O
of O
our O
results O
, O
we O
carry B
out O
our O
analysis O
on O
randomized O
sessions B
, O
which O
are O
computed O
by O
fixing O
the O
sessions B
of O
each O
player B
and O
shuffling O
at O
random O
the O
matches B
in O
the O
session B
[ O
7 O
] O
, O
[ O
13 O
] O
. O

The O
results O
on O
the O
randomized O
data O
are O
shown O
in O
Fig O
. O

5 O
, O
where O
the O
characteristic O
warm B
- I
up I

 O
effect I
is O
not O
anymore O
present O
. O

Therefore O
, O
the O
flat O
trend O
across O
matches B
in O
the O
random O
sessions B
suggests O
that O
the O
improvement O
we O
observed O
during O
sessions B
is O
a O
significant O
result O
. O

0.298 O
0.300 O
0.302 O
0.304 O
0.306 O
0.308 O
Percentage O
Strength B
0.306 O
0.308 O
0.310 O
0.312 O
0.314 O
0.316 O
0.318 O
Percentage O
Agility O
High O
Exp O
. O

Low O
Exp O
. O

High O
TS O
Low O
TS O
0.378 O
0.380 O
0.382 O
0.384 O
0.386 O
0.388 O
0.390 O
0.392 O
Percentage O
Intelligence O
Fig O
. O

6 O
. O

Comparison O
between O
the O
percentage O
of O
hero B
types O
used O
by O
low O
/ O
high O
experience B
players B
and O
low O
/ O
high O
TrueSkill O
( O
TS O
) O
players B
. O

Each O
category O
, O
divided O
in O
the O
two O
groups B
( O
low O
/ O
high O
) O
, O
shows O
different O
preferences O
in O
the O
choice O
of O
the O
hero B
type O
. O

C. O
Experience O
players B
vs O
performer B
players I
In O
the O
last O
part O
of O
our O
analysis O
, O
we O
aim O
at O
identifying O
the O
strategies B
and O
features O
that O
make O
a O
player B
successful O
. O

For O
this O
purpose O
, O
we O
study O
players B
from O
two O
different O
angles O
by O
comparing O
experienced O
players B
with O
skilled B
players I
. O

As O
introduced O
in O
§ O
III O
, O
we O
divide O
players B
into O
four O
groups B
: O
first O
we O
select O
low O
/ O
high O
experienced O
players B
based B
on O
their O
seniority O
in O
terms O
of O
number O
of O
matches B
; O
then O
we O
select O
low O
/ O
high B
skill I

 O
players I
based B
on O
their O
TrueSkill O
scores O
, O
i.e. O
skill B
level I
. O

We O
compare O
these O
two O
groups B
of O
players B
( O
experienced B
vs O
skilled B
) O
by O
looking O
at O
their O
preferences O
when O
selecting O
an O
hero B
type O
. O

We O
compute O
the O
percentage O
of O
hero B
types O
selected O
by O
each O
player B
in O
the O
different O
categories O
and O
report B
in O
Fig O
. O

6 O
the O
corresponding O
mean O
and O
standard O
error O
. O

The O
figure O
shows O
that O
the O
more O
experience B
a O
player B
has O
, O
the O
more O
he O
/ O
she O
tends O
to O
prefer O
Intelligence O
and O
Agility O
roles B
, O
and O
thus O
both O
supporter O
and O
fighter B
heroes B
. O

However O
, O
this O
scenario O
is O
different O
when O
looking O
at O
the O
skill B
level I
of I
players I
. O

Players B
with O
high B
skill I

 O
level I
play B
less O
supporter O
roles B
in O
proportion O
and O
prefer O
fighter B
hero B
types O
such O
as O
Agility O
and O
Strength O
heroes B
. O

This O
result O
also O
suggests O
that O
there O
is O
no O
direct O
correlation O
between O
successful O
( O
high B
skill I
) O
players I
and O
longtime O
( O
experienced B
) O
players B
. O

This O
discrepancy O
between O
experienced O
players B
and O
skilled B

 O
players I
is O
also O
reflected O
in O
their O
strategies B
and O
actions O
during O
matches B
. O

In O
the O
first O
case O
, O
both O
low O
and O
high O
experience B
players B
show O
a O
distribution O
of O
performed O
actions O
which O
is O
consistent O
with O
the O
one O
of O
Fig O
. O

3 O
computed O
over O
all O
the O
players B
in O
the O
dataset O
. O

There O
is O
no O
evident O
difference O
in O
the O
collective O
strategies B
of O
low O
and O
high O
experience B
players B
, O
thus O
supporting B
the O
conclusion O
that O
playing B
longer O
does O
not O
necessarily O
lead B
to O
be O
successful O
in O
the O
game B
. O

On O
the O
contrary O
, O
the O
comparison O
between O
performed O
actions O
during O
the O
matches B
played B
by O
low O
and O
high B
skill I
players I
highlights B
their O
discrepancies O
and O
characteristic O
features O
. O

As O
reported B
in O
Fig O
. O

7a O
, O
low O
skill B
players B
are O
characterized O
by O
distributions O
that O
are O
skewed O
to O
the O
left O
side O
of O
the O
ternary O
plots O
, O
thus O
indicating O
that O
this O
group B
of O
players B
generally O
performs O
a O
low B
number I
of I
kills I
while O
prefers O
to O
assists B
teammates B
in O
combat B
. O

High B
skill I
players I
are O
instead O
characterized O
by O
distributions O
that O
are O
more O
centered O
in O
the O
ternary O
plots O
of O
Intelligence O
and O
Strength O
heroes B
, O
while O
in O
the O
case O
of O
Agility O
heroes O
the O
distribution O
tends O
to O
be O
skewed O
on O
the O
right O
part O
of O
the O
plot O
( O
cf O
. O

Fig O
. O

7b O
) O
. O

These O
features O
indicate O
that O
in O
general O
high B
skill I
players I
make O
more O
kills B
than O
other O
players B
, O
even O
in O
the O
case O
in O
which O
their O
role B
is O
to O
support B
the O
teammates B
. O

Moreover O
, O
when O
using O
Intelligence O
or O
Strength O
heroes B
they O
sacrifice O
( O
die O
) O
themselves O
less O
than O
low O
skill B
players B
. O

Finally O
, O
we O
investigate O
if O
the O
two O
categorizations O
of O
players B
( O
experience B
/ O
skill B
) O
provide O
any O
other O
insight O
that O
could O
help O
in O
characterizing O
a O
successful O
player B
. O
We I
look I
at O
the O
total O
duration O
of O
the O
matches B
played B
by O
low O
/ O
high O
experience B
players B
and O
low O
/ O
high B
skill I
players I
, O
to O
highlight B
groups B
differences O
. O

TABLE O
II O
MEAN O
AND O
STANDARD O
ERROR O
OF O
THE O
MATCH B
DURATIONS O
DIVIDED O
BY O
LOW O
/ O
HIGH O
EXPERIENCE B
PLAYERS B
AND O
LOW O
/ O
HIGH O
TRUESKILL O
PLAYERS B
. O

Player O
group B
mean O
duration O
SE O
Low O
exp O
. O

players B
2,532 O
s O
±0.34 O
s O
High O
exp O
. O

players B
2,507 O
s O
±1.16 O
s O
Low O
TS O
players B
2,532 O
s O
±0.87 O
s O
High O
TS O
players B
2,447 O
s O
±1.05 O
s O
We O
computed O
mean O
and O
standard O
error O
over O
the O
four O
groups B
of O
players B
, O
which O
are O
reported B
in O
Tab O
. O

II O
. O

According O
to O
previous O
results O
on O
low O
/ O
high O
experience B
players B
strategies B
, O
the O
difference O
displayed O
in O
the O
average O
duration O
of O
the O
matches B
between O
these O
two O
groups B
is O
marginal O
( O
' O
25s O
) O
. O

On O
the O
contrary O
, O
high B
skill I
players I
display O
a O
bigger O
difference O
if O
compared O
with O
low O
skill B
players B
( O
' O
85s O
) O
. O

In O
particular O
, O
players B
with O
higher B

 O
skill I
level I
play B
shorter O
matches B
than O
those O
with O
low B
skill I

 O
level I
, O
whose O
average O
duration O
is O
closer B
to O
the O
one O
related O
to O
low O
/ O
high O
experienced O
players B
. O

A O
possible O
explanation O
for O
this O
phenomenon O
could O
rely O
on O
the O
more O
aggressive O
strategy B
that O
high B
skill I
players I
use O
. O

They O
indeed O
tend O
to O
perform O
more O
kills B
than O
others O
, O
even O
when O
they O
are O
interpreting O
support B
roles B
, O
and O
this O
playing B
style I
likely O
causes O
an O
early B
end O
to O
the O
matches B
. O

In O
conclusion O
, O
our O
findings O
delineate O
as O
successful O
players B
in O
Dota O
2 O
the O
more O
individualistic O
players B
, O
i.e. O
, O
those O
that O
prevail O
over O
others O
by O
killing B
more O
even O
when O
they O
should O
interpret O
a O
support B
role B
and O
try O
to O
end O
the O
match B
as O
soon O
as O
possible O
by O
conquering O
the O
opponent B
base I
thanks O
to O
power B
plays I
. O

0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Intelligence O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Agility O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Strength O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
( O
a O
) O
Low O
TrueSkill O
players B
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Intelligence O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Agility O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
100 O
90 O
80 O
70 O
60 O
50 O
40 O
30 O
20 O
10 O
0 O
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
100 O
← O
assists B
kills B
→ O
← O
deaths B
Strength O
0.000 O
0.008 O
0.016 O
0.024 O
0.032 O
0.040 O
0.048 O
0.056 O
0.064 O
0.072 O
0.080 O
( O
b O
) O
High O
TrueSkill O
players B
Fig O
. O

7 O
. O

Distribution O
of O
performed O
actions O
in O
a O
match B
divided O
by O
hero B
type O
and O
player B
’s O
skill I
level B
. O

The O
three O
axes O
represent O
the O
percentages O
of O
kills B
, O
assists B
and O
deaths B
, O
while O
the O
color O
intensity O
represents O
the O
normalized O
number O
of O
matches B
for O
which O
a O
certain O
triplet O
is O
performed O
. O

V. O
RELATED O
WORK O
A O
great O
amount O
of O
work O
has O
been O
devoted O
to O
analyze O
several O
facets O
in O
MOBA O
games B
to O
identify O
common O
behavioral O
patterns O
, O
their O
evolution B
as O
well O
as O
characteristics O
that O
bring O
a O
team B
in O
a O
match B
to O
be O
successful O
and O
win O
the O
game B
. O

Drachen O
and O
collaborators O
[ O
19 O
] O
explored O
how O
game B
behaviors O
change O
and O
affect O
team B
skill I
levels I
. O

They O
analyzed O
the O
spatio O
- O
temporal O
behaviors O
of O
teams B
during O
a O
match B
. O

They O
looked O
at O
the O
temporal O
distribution O
of O
the O
distances O
between O
players B
in O
the O
same O
team B
and O
how O
this O
distribution O
varies O
between O
high O
/ O
low O
skill B
teams B
. O

Eggert O
et O
al O
. O

[ O
23 O
] O
were O
interested O
in O
classifying O
player B
’s O
behaviors I
via O
machine O
learning O
. O

These O
studies O
are O
focused B
on O
quantifying O
collective O
behaviors O
of O
teams B
in O
Dota O
2 O
and O
relate O
these O
behaviors O
to O
player B
’s O
roles I
that O
are O
not O
the O
official O
hero B
types O
we O
used O
in O
our O
analysis O
. O

Other O
techniques B
were O
developed O
to O
detect O
and O
rank B
the O
features O
in O
a O
team B
fight I
that O
help O
to O
predict O
whether O
a O
team B
will O
win O
a O
match B
or O
not O
[ O
20]–[22 O
] O
, O
[ O
25 O
] O
. O

In O
these O
studies O
, O
authors O
analyzed O
snapshots O
of O
a O
combat B
and O
described O
the O
action O
as O
a O
network O
, O
to O
extract O
common O
patterns O
leading B
to O
winning O
the O
match B
; O
the O
authors O
ranked B
factors O
that O
can O
be O
extrinsic O
to O
the O
game B
itself O
, O
by O
studying O
social O
ties O
of O
players B
in O
a O
team B
. O

We O
differentiate O
from O
these O
research O
directions O
as O
in O
the O
present O
paper O
we O
are O
mainly O
interested O
in O
providing O
an O
individual O
characterizations O
of O
players B
through O
both O
the O
performed O
actions O
and O
the O
interpreted O
role B
. O

Moreover O
, O
we O
define O
a O
player B
to O
be O
successful O
if O
his O
/ O
her O
skill B
level I
( O
TrueSkill O
) O
is O
high O
, O
while O
in O
the O
existing O
literature O
the O
overall O
team B
level B
is O
analyzed O
and O
defined O
by O
the O
collective O
rate O
of O
victories B
/ O
losses O
. O

VI O
. O

CONCLUSION O
Online O
platforms O
, O
such O
as O
social O
networks O
, O
Q&A O
sites O
, O
and O
online B
games I
, O
provide O
a O
wealth O
of O
information O
, O
whose O
details O
can O
help O
identifying O
how O
people O
behave O
in O
different O
contexts O
. O

In O
particular O
, O
they O
are O
a O
useful O
means O
to O
detect O
the O
major O
characteristics O
that O
turn B
an O
ordinary O
user B
in O
a O
successful O
one O
. O

Here O
, O
we O
analyzed O
multiplayer O
online O
battle B
arena O
( O
MOBA O
) O
games B
, O
and O
in O
particular O
Dota O
2 O
, O
with O
the O
main O
purpose O
of O
extracting O
the O
underlying O
characteristics O
that O
distinguish O
players B
with O
a O
low B
skill O
level I
from I
those O
that O
in O
the O
course O
of O
their O
gaming O
experience B
reached O
the O
higher B
skill O
level I
. I
We O
decided O
to O
focus B
on O
one O
important O
aspect O
of O
the O
game B
: O
the O
characters B
players B
impersonate O
, O
namely O
heroes B
. O

By O
following O
the O
official O
hero B
categorization O
of O
Dota O
2 O
, O
we O
divided O
the O
113 O
heroes B
of O
the O
game B
in O
three O
main O
types O
: O
Intelligence O
, O
Agility O
, O
and O
Strength O
. O

These O
three O
hero B
types O
define O
a O
character B
on O
the O
basis O
of O
their O
ability B
and O
role B
, O
from O
support B
roles B
( O
as O
heroes B
in O
the O
Intelligence O
group B
) O
to O
fighter B
roles B
( O
as O
Agility O
heroes B
) O
. O

As O
a O
first O
step O
in O
our O
evaluation O
, O
we O
characterized O
these O
three O
hero B
types O
by O
looking O
at O
the O
way O
in O
which O
they O
are O
used O
by O
Dota O
2 O
players B
. O
We I
observed I
that O
overall O
, O
Intelligence O
heroes B
are O
used O
more O
than O
the O
other O
two O
types O
, O
probably O
because O
this O
supporting B
role B
is O
seen O
as O
more O
central O
to O
the O
game B
dynamics O
, O
providing O
more O
flexible O
heroes B
that O
can O
be O
more O
enticing O
to O
play B
. O

We O
also O
showed O
that O
players B
use O
heroes B
according O
to O
their O
ability B
and O
strength B
: O
Intelligence O
heroes B
to O
assist B
, O
Strength O
heroes B
to O
start B
team B
fights I
as O
well O
as O
assisting B
teammates B
, O
and O
Agility O
heroes B
to O
kill B
enemies B
. O

One O
interesting O
result O
we O
discussed O
is O
that O
players B
need O
some O
time B
to O
warm O
up O
and O
increase B
their O
performance O
across O
matches B
. O

By O
inspecting O
players B
’ O
performance O
in O
consecutive O
matches B
, O
we O
found O
that O
the O
last O
match B
in O
a O
session B
corresponds O
to O
the O
one O
having O
higher O
performance O
in O
terms O
of O
number O
of O
actions O
( O
kills B
, O
assists B
, O
deaths B
) O
. O

The O
robustness O
of O
this O
result O
is O
proved O
by O
randomizing O
the O
matches B
within O
sessions B
: O
the O
resulting O
null O
model O
does O
not O
display O
a O
significant O
ascending O
/ O
descending O
trend O
— O
on O
the O
contrary O
it O
remains O
constant O
from O
one O
match B
to O
the O
next O
— O
corroborating O
our O
finding O
. O

Finally O
, O
we O
divided O
players B
by O
their I
experience I
( O
total O
number O
of O
matches B
played B
) O
in O
the O
game B
and O
by O
their O
skill B
level I
, O
by O
computing O
the O
so O
called B
TrueSkill O
. O

Within O
these O
two O
groups B
, O
we O
identified O
low O
/ O
high O
experience B
players B
and O
low O
/ O
high B
skill I

 O
players I
. O

In O
- O
depth O
comparisons O
led B
to O
the O
conclusion O
that O
playing B
longer O
and O
thus O
having O
higher O
experience B
does O
not O
necessarily O
imply O
that O
one O
’s O
skill B
level I
will O
increase B
. O

This O
finding O
is O
supported B
by O
the O
analysis O
of O
the O
distributions O
of O
actions O
performed O
by O
players B
with O
different O
hero B
types O
. O

Low O
/ O
high O
experience B
players B
exhibit O
distributions O
which O
are O
consistent O
with O
those O
computed O
over O
all O
the O
players B
in O
the O
dataset O
, O
and O
thus O
do O
not O
show O
a O
specific O
playing O
style O
. O

On O
the O
contrary O
, O
the O
investigation O
of O
low O
/ O
high B
skill I
players I
led B
to O
the O
characterization O
of O
successful O
play B
styles I
. O

Successful O
players B
, O
which O
here O
correspond O
to O
those O
having O
high B
skill O
level I

 I
( O
i.e. O
, O
high O
TrueSkill O
) O
, O
are O
aggressive O
players B
who O
prefer O
to O
kill B
enemies B
more O
than O
assisting B
teammates B
in O
a O
fight B
even O
if O
this O
is O
the O
role B
they O
are O
interpreting O
. O

This O
behavior O
leads B
these O
players B
to O
prefer O
the O
use O
of O
Agility O
and O
Strength O
hero B
types O
and O
to O
try O
to O
trigger B
an O
early B
end O
in O
the O
game B
with O
power B
plays I
. O

Future O
work O
will O
be O
devoted O
to O
further O
inspect O
the O
characteristics O
that O
make O
players B
successful O
. O

We O
will O
look O
for O
possible O
differences O
in O
the O
playing O
style O
of O
high B
skill I
players I
between O
Ranked O
and O
Public O
matches B
. O

This O
will O
help O
us O
understand O
if O
being O
part O
of O
a O
team B
of O
players B
of O
different O
levels B
could O
be O
an O
incentive O
for O
low O
skill B
players B
to O
perform O
better O
by O
learning O
from O
high B
skill I
players I
. O

Another O
possible O
direction O
will O
be O
to O
investigate O
how O
the O
TrueSkill O
of O
players B
evolves O
in O
time B
and O
if O
the O
playing B
styles I
of O
successful O
players B
we O
uncovered O
in O
the O
present O
work O
are O
constant O
or O
learned O
over O
time B
. O

ACKNOWLEDGMENT O
The O
authors O
are O
grateful O
to O
DARPA O
for O
support B
( O
grant O
# O
D16AP00115 O
) O
. O

This O
project O
does O
not O
necessarily O
reflect O
the O
position O
/ O
policy O
of O
the O
Government O
; O
no O
official O
endorsement O
should O
be O
inferred O
. O

Approved O
for O
public O
release O
; O
unlimited O
distribution O
. O

REFERENCES O
[ O
1 O
] O
F. O
Radicchi O
, O
S. O
Fortunato O
, O
B. O
Markines O
, O
and O
A. O
Vespignani O
, O
“ O
Diffusion O
of O
scientific O
credits O
and O
the O
ranking O
of O
scientists O
, O
” O
Physical O
Review O
E O
, O
vol O
. O

80 O
, O
no O
. O

5 O
, O
p. O
056103 O
, O
2009 O
. O

[ O
2 O
] O
R. O
Sinatra O
, O
D. O
Wang O
, O
P. O
Deville O
, O
C. O
Song O
, O
and O
A.-L. O
Barabasi O
, O
“ O
Quan- O
´ O
tifying O
the O
evolution B
of O
individual O
scientific O
impact B
, O
” O
Science O
, O
vol O
. O

354 O
, O
no O
. O

6312 O
, O
p. O
aaf5239 O
, O
2016 O
. O

[ O
3 O
] O
C. O
Romero O
and O
S. O
Ventura O
, O
“ O
Data O
mining O
in O
education O
, O
” O
Wiley O
Interdisciplinary O
Reviews O
: O
Data O
Mining O
and O
Knowledge O
Discovery O
, O
vol O
. O

3 O
, O
no O
. O

1 O
, O
pp O
. O

12–27 O
, O
2013 O
. O

[ O
4 O
] O
G. O
C. O
Rodi O
, O
V. O
Loreto O
, O
V. O
D. O
Servedio O
, O
and O
F. O
Tria O
, O
“ O
Optimal O
learning O
paths B
in O
information O
networks O
, O
” O
Scientific O
reports B
, O
vol O
. O

5 O
, O
2015 O
. O

[ O
5 O
] O
S. O
Aral O
and O
D. O
Walker O
, O
“ O
Identifying O
influential O
and O
susceptible O
members O
of O
social O
networks O
, O
” O
Science O
, O
vol O
. O

337 O
, O
no O
. O

6092 O
, O
pp O
. O

337–341 O
, O
2012 O
. O

[ O
6 O
] O
E. O
Ferrara O
, O
R. O
Interdonato O
, O
and O
A. O
Tagarelli O
, O
“ O
Online O
popularity O
and O
topical O
interests O
through O
the O
lens O
of O
instagram O
, O
” O
in O
Proceedings O
of O
the O
25th O
conference O
on O
Hypertext O
and O
social O
media O
. O

ACM O
, O
2014 O
, O
pp O
. O

24–34 O
. O

[ O
7 O
] O
P. O
Singer O
, O
E. O
Ferrara O
, O
F. O
Kooti O
, O
M. O
Strohmaier O
, O
and O
K. O
Lerman O
, O
“ O
Evidence O
of O
online O
performance O
deterioration O
in O
user B
sessions B
on O
reddit O
, O
” O
PloS O
one O
, O
vol O
. O

11 O
, O
no O
. O

8 O
, O
p. O
e0161636 O
, O
2016 O
. O

[ O
8 O
] O
D. O
Memmert O
, O
K. O
A. O
Lemmink O
, O
and O
J. O
Sampaio O
, O
“ O
Current O
approaches O
to O
tactical B
performance O
analyses I
in O
soccer O
using O
position O
data O
, O
” O
Sports O
Medicine O
, O
vol O
. O

47 O
, O
no O
. O

1 O
, O
pp O
. O

1–10 O
, O
2017 O
. O

[ O
9 O
] O
M. O
Cha O
, O
H. O
Haddadi O
, O
F. O
Benevenuto O
, O
and O
P. O
K. O
Gummadi O
, O
“ O
Measuring O
user B
influence O
in O
twitter O
: O
The O
million O
follower O
fallacy O
. O

” O
Icwsm O
, O
vol O
. O

10 O
, O
no O
. O

10 O
- O
17 O
, O
p. O
30 O
, O
2010 O
. O

[ O
10 O
] O
L. O
Hong O
, O
O. O
Dan O
, O
and O
B. O
D. O
Davison O
, O
“ O
Predicting O
popular O
messages B
in O
twitter O
, O
” O
in O
Proceedings O
of O
the O
20th O
international O
conference O
companion O
on O
World O
wide O
web O
. O

ACM O
, O
2011 O
, O
pp O
. O

57–58 O
. O

[ O
11 O
] O
F. O
Kooti O
, O
E. O
Moro O
, O
and O
K. O
Lerman O
, O
“ O
Twitter O
session B
analytics O
: O
Profiling O
users B
short O
- O
term O
behavioral O
changes O
, O
” O
in O
International O
Conference O
on O
Social O
Informatics O
. O

Springer O
, O
2016 O
, O
pp O
. O

71–86 O
. O

[ O
12 O
] O
D. O
Movshovitz O
- O
Attias O
, O
Y. O
Movshovitz O
- O
Attias O
, O
P. O
Steenkiste O
, O
and O
C. O
Faloutsos O
, O
“ O
Analysis O
of O
the O
reputation O
system O
and O
user B
contributions O
on O
a O
question O
answering O
website O
: O
Stackoverflow O
, O
” O
in O
Proceedings O
of O
the O
2013 O
IEEE O
/ O
ACM O
International O
Conference O
on O
Advances O
in O
Social O
Networks O
Analysis O
and O
Mining O
. O

ACM O
, O
2013 O
, O
pp O
. O

886–893 O
. O

[ O
13 O
] O
E. O
Ferrara O
, O
N. O
Alipourfard O
, O
K. O
Burghardt O
, O
C. O
Gopal O
, O
and O
K. O
Lerman O
, O
“ O
Dynamics O
of O
content O
quality O
in O
collaborative O
knowledge O
production O
, O
” O
in O
International O
AAAI O
Conference O
on O
Web O
and O
Social O
Media O
, O
2017 O
. O

[ O
14 O
] O
J. O
Huang O
, O
T. O
Zimmermann O
, O
N. O
Nagapan O
, O
C. O
Harrison O
, O
and O
B. O
C. O
Phillips O
, O
“ O
Mastering B
the O
art O
of O
war O
: O
how O
patterns O
of O
gameplay B
influence O
skill B
in O
halo O
, O
” O
in O
Proceedings O
of O
the O
SIGCHI O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
. O

ACM O
, O
2013 O
, O
pp O
. O

695–704 O
. O

[ O
15 O
] O
S. O
Bardzell O
, O
J. O
Bardzell O
, O
T. O
Pace O
, O
and O
K. O
Reed O
, O
“ O
Blissfully O
productive O
: O
grouping B
and O
cooperation B
in O
world B
of O
warcraft O
instance O
runs O
, O
” O
in O
Proceedings O
of O
the O
2008 O
ACM O
conference O
on O
Computer O
supported B
cooperative O
work O
. O

ACM O
, O
2008 O
, O
pp O
. O

357–360 O
. O

[ O
16 O
] O
G. O
A. O
Benefield O
, O
C. O
Shen O
, O
and O
A. O
Leavitt O
, O
“ O
Virtual O
team B
networks O
: O
How O
group B
social O
capital O
affects O
team B
success O
in O
a O
massively O
multiplayer O
online B
game I
, O
” O
in O
Proc O
. O

of O
the O
19th O
Conference O
on O
Computer O
- O
Supported B
Cooperative O
Work O
& O
Social O
Computing O
. O

ACM O
, O
2016 O
, O
pp O
. O

679–690 O
. O

[ O
17 O
] O
N. O
Ducheneaut O
and O
R. O
J. O
Moore O
, O
“ O
The O
social O
side O
of O
gaming O
: O
a O
study O
of O
interaction O
patterns O
in O
a O
massively O
multiplayer O
online B
game I
, O
” O
in O
Proceedings O
of O
the O
2004 O
ACM O
conference O
on O
Computer O
supported B
cooperative O
work O
. O

ACM O
, O
2004 O
, O
pp O
. O

360–369 O
. O

[ O
18 O
] O
S. O
Ferrari O
, O
“ O
From O
generative O
to O
conventional O
play B
: O
Moba O
and O
league B
of I

 O
legends I
, O
” O
in O
Proceedings O
of O
DiGRA O
, O
2013 O
, O
pp O
. O

1–17 O
. O

[ O
19 O
] O
A. O
Drachen O
, O
M. O
Yancey O
, O
J. O
Maguire O
, O
D. O
Chu O
, O
I. O
Y. O
Wang O
, O
T. O
Mahlmann O
, O
M. O
Schubert O
, O
and O
D. O
Klabajan O
, O
“ O
Skill O
- O
based B
differences O
in O
spatiotemporal O
team B
behaviour O
in O
defence O
of O
the O
ancients O
2 O
( O
dota B
2 O
) O
, O
” O
in O
Games O
media O
entertainment O
( O
GEM O
) O
, O
2014 O
IEEE O
. O

IEEE O
, O
2014 O
, O
pp O
. O

1–8 O
. O

[ O
20 O
] O
P. O
Yang O
, O
B. O
E. O
Harrison O
, O
and O
D. O
L. O
Roberts O
, O
“ O
Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
moba B
games I
. O

” O
in O
FDG O
, O
2014 O
. O

[ O
21 O
] O
N. O
Pobiedina O
, O
J. O
Neidhardt O
, O
M. O
d. O
C. O
C. O
Moreno O
, O
L. O
Grad O
- O
Gyenge O
, O
and O
H. O
Werthner O
, O
“ O
On O
successful O
team B
formation I
: O
Statistical O
analysis O
of O
a O
multiplayer O
online B
game I
, O
” O
in O
Business O
Informatics O
( O
CBI O
) O
, O
2013 O
IEEE O
15th O
Conference O
on O
. O

IEEE O
, O
2013 O
, O
pp O
. O

55–62 O
. O

[ O
22 O
] O
N. O
Pobiedina O
, O
J. O
Neidhardt O
, O
M. O
d. O
C. O
Calatrava O
Moreno O
, O
and O
H. O
Werthner O
, O
“ O
Ranking B
factors O
of O
team B
success O
, O
” O
in O
Proceedings O
of O
the O
22nd O
International O
Conference O
on O
World O
Wide O
Web O
. O

ACM O
, O
2013 O
, O
pp O
. O

1185–1194 O
. O

[ O
23 O
] O
C. O
Eggert O
, O
M. O
Herrlich O
, O
J. O
Smeddinck O
, O
and O
R. O
Malaka O
, O
“ O
Classification O
of O
player B
roles I
in O
the O
team B
- O
based I
multi O
- O
player B
game B
dota B
2 O
, O
” O
in O
International O
Conference O
on O
Entertainment O
Computing O
. O

Springer O
, O
2015 O
, O
pp O
. O

112–125 O
. O

[ O
24 O
] O
S. O
Deterding O
, O
D. O
Dixon O
, O
R. O
Khaled O
, O
and O
L. O
Nacke O
, O
“ O
From O
game B
design O
elements O
to O
gamefulness O
: O
defining O
gamification O
, O
” O
in O
Proceedings O
of O
the O
15th O
international O
academic O
MindTrek O
conference O
: O
Envisioning O
future O
media O
environments O
. O

ACM O
, O
2011 O
, O
pp O
. O

9–15 O
. O

[ O
25 O
] O
A. O
Sapienza O
, O
A. O
Bessi O
, O
and O
E. O
Ferrara O
, O
“ O
Non O
- O
negative O
tensor O
factorization O
for O
human O
behavioral O
pattern O
mining O
in O
online B
games I
, O
” O
arXiv O
preprint O
arXiv:1702.05695 O
, O
2017 O
. O

Modeling O
Game O
Avatar O
Synergy O
and O
Opposition O
through O
Embedding O
in O
Multiplayer O
Online O
Battle O
Arena O
Games O
Zhengxing O
Chen1 O
, O
Yuyu O
Xu1 O
, O
Truong O
- O
Huy O
D. O
Nguyen2 O
, O
Yizhou O
Sun3 O
and O
Magy O
Seif O
El O
- O
Nasr1 O
1College O
of O
Information O
and O
Computer O
Science O
, O
Northeastern O
University O
2Department O
of O
Computer O
and O
Information O
Science O
, O
Fordham O
University O
3Department O
of O
Computer O
Science O
, O
University O
of O
California O
, O
Log O
Angeles O
1 O
czxttkl@gmail.com O
, O
yuyuxu@ccs.neu.edu O
, O
m.seifel-nasr@neu.edu O
2 O
tnguyen88@fordham.edu O
3 O
yzsun@cs.ucla.edu O
Abstract O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
have O
received O
increasing B
worldwide O
popularity O
recently O
. O

In O
such O
games B
, O
players B
compete O
in O
teams B
against O
each O
other O
by O
controlling B
selected O
game B
avatars I
, O
each O
of O
which O
is O
designed O
with O
different O
strengths B
and O
weaknesses B
. O

Intuitively O
, O
putting O
together O
game B
avatars I
that O
complement O
each O
other O
( O
synergy O
) O
and O
suppress O
those O
of O
opponents B
( O
opposition O
) O
would O
result O
in O
a O
stronger O
team B
. O

In O
- O
depth O
understanding O
of O
synergy O
and O
opposition O
relationships O
among O
game B
avatars I
benefits O
player B
in O
making O
decisions O
in O
game B
avatar I
drafting B
and O
gaining O
better O
prediction O
of O
match B
events O
. O

However O
, O
due O
to O
intricate O
design O
and O
complex O
interactions O
between O
game B
avatars I
, O
thorough O
understanding O
of O
their O
relationships O
is O
not O
a O
trivial O
task O
. O

In O
this O
paper O
, O
we O
propose O
a O
latent O
variable O
model O
, O
namely O
Game O
Avatar O
Embedding O
( O
GAE O
) O
, O
to O
learn O
avatars B
’ O
numerical O
representations O
which O
encode O
synergy O
and O
opposition O
relationships O
between O
pairs O
of O
avatars B
. O

The O
merits O
of O
our O
model O
are O
twofold O
: O
( O
1 O
) O
the O
captured B
synergy O
and O
opposition O
relationships O
are O
sensible O
to O
experienced B
human O
players B
’ O
perception O
; O
( O
2 O
) O
the O
learned O
numerical O
representations O
of O
game B
avatars I
allow O
many O
important O
downstream O
tasks O
, O
such O
as O
similar O
avatar B
search O
, O
match B
outcome O
prediction O
, O
and O
avatar B
pick B
recommender O
. O

To O
our O
best O
knowledge O
, O
no O
previous O
model O
is O
able O
to O
simultaneously O
support B
both O
features O
. O

Our O
quantitative O
and O
qualitative O
evaluations O
on O
real O
match B
data O
from O
three O
commercial O
MOBA O
games B
illustrate O
the O
benefits O
of O
our O
model O
. O

Introduction O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
has O
been O
one O
of O
the O
most O
popular O
e O
- O
sports O
game B
types O
. O

For O
example O
, O
League O
of O
Legends O
( O
Riot O
Games O
) O
, O
one O
of O
the O
most O
popular O
e O
- O
Sports O
, O
reportedly O
has O
90 O
million O
accounts O
registered O
, O
27 O
million O
unique O
daily O
players B
, O
and O
7.5 O
million O
concurrent O
users B
at O
peak O
( O
Minotti O
2016 O
; O
Tassi O
2016 O
) O
. O

In O
a O
MOBA O
game B
, O
2 O
teams B
, O
composed O
of O
5 O
player B
each O
, O
combat B
in O
a O
virtual O
environment O
. O

The O
goal B
is O
to O
beat O
the O
opposite B
team I
and O
destroy O
their O
base B
. O

Game O
avatars B
are O
often O
designed O
with O
a O
variety O
of O
attributes O
, O
skills B
, O
roles B
, O
etc O
. O

, O
which O
is O
intended O
to O
provide O
players B
with O
choices O
and O
options O
so O
that O
every O
player B
can O
find O
a O
character B
that O
fits O
their O
preferences O
. O

Moreover O
, O
it O
is O
customary O
for O
avatars B
in O
such O
games B
to O
possess O
strength B
in O
one O
aspect O
, O
but O
weakness B
in O
others O
. O

As O
such O
, O
in O
order O
to O
win O
a O
match B
, O
it O
is O
Copyright O
c O
2017 O
, O
Association O
for O
the O
Advancement O
of O
Artificial O
Intelligence O
( O
www.aaai.org O
) O
. O

All O
rights O
reserved O
. O

well O
- O
known O
that O
players B
need O
to O
not O
only O
control B
their O
own O
game B
avatars I
well O
, O
but O
also O
need O
to O
select O
a O
game B
avatar I
that O
, O
together O
with O
other O
team B
members I
’ O
picks B
, O
forms O
a O
team B
that O
enhances O
skills B
and O
complements O
weaknesses B
of O
each O
other O
( O
synergy O
) O
, O
while O
posing O
suppressing O
strengths B
over O
those O
in O
the O
opponent B
team I
( O
opposition O
) O
1 O
2 O
. O

For O
example O
, O
in O
DOTA O
2 O
( O
Valve O
Corporation O
) O
, O
avatar B
Clockwerk O
has O
high O
synergy O
with O
Naix O
because O
Clockwerk O
can O
transport O
Naix O
to O
target B
enemy B
directly O
, O
increasing B
the O
limited O
mobility B
of O
Naix O
to O
hit O
enemy B
. O

Naix O
also O
delivers O
large B
damage I
to O
complement O
the O
limited B
attack I
by O
Clockwerk O
, O
making O
them O
an O
efficient O
fighting B
duo O
. O

In O
another O
example O
, O
avatar B
Anti O
- O
Mage O
’s O
mana B

 O
burn I
skill I
reduces O
an O
opponent B
’s O
mana B
resource I
, O
making O
him O
a O
natural O
opposition O
to O
Medusa O
whose O
durable O
capability O
is O
completely O
relying O
on O
how O
much O
mana B
it O
has O
. O

Comprehensive O
understanding O
of O
synergy O
and O
opposition O
relationships O
between O
game B
avatar I
enhances O
player B
awareness O
and O
experience B
in O
games B
. O

First O
, O
it O
allows O
players B
to O
make O
good B
decisions I
in O
drafting B
their O
team B
’s O
game B
avatars I
to O
maximize O
the O
chance B
of O
winning O
. O

Second O
, O
it O
improves O
the O
prediction O
of O
the O
match B
’s O
progress O
and O
final O
outcomes O
, O
which O
helps O
players B
in O
preparing O
for O
strategies B
in O
advance O
. O

Lastly O
, O
it O
helps O
players B
discover O
other O
game B
avatars I
that O
match B
their O
personal O
expertise O
or O
preferences O
. O

However O
, O
due O
to O
the O
intricate O
design O
and O
complex O
interactions O
among O
game B
avatars I
, O
thorough O
understanding O
of O
game B
avatars I
’ O
pros O
and O
cons O
and O
their O
relationships O
is O
not O
a O
trivial O
task O
to O
human O
players B
. O

In O
order O
to O
model O
game B
avatars I
’ O
synergy I
and O
opposition O
relationships O
, O
we O
propose O
a O
latent O
variable O
model O
, O
called B
Game O
Avatar O
Embedding O
( O
GAE O
) O
. O

GAE O
models O
game B
avatars O
as O
vectors O
in O
a O
low O
- O
dimensional O
space O
learned O
. O

We O
hypothesized O
that O
the O
probability O
function O
of O
a O
match B
outcome O
constitutes O
of O
pairwise O
synergy O
and O
opposition O
interactions O
formulated O
by O
game B
avatar I
vectors O
. O

Game B
avatar I
vectors O
and O
other O
model O
parameters O
are O
learned O
by O
gradient O
descent O
through O
maximizing O
the O
likelihood O
function O
of O
all O
observed O
match B
outcomes O
. O

Latent O
variable O
models O
( O
LVM O
) O
and O
embedding O
techniques B
have O
been O
shown O
to O
successfully O
capture B
characteristics O
of O
entities B
in O
texts O
( O
Mikolov O
et O
al O
. O

2013 O
) O
, O
1http://www.weskimo.com/a-guide-todrafting.html O
2https://www.reddit.com/r/learndota2/ O
comments/3f9szo O
/ O
how O
to O
counter O
pick B
heroes/ O
arXiv:1803.10402v1 O
[ O
cs O
. O

SI O
] O
28 O
Mar O
2018 O
graphs O
( O
Maaten O
and O
Hinton O
2008 O
) O
, O
and O
recommendation O
systems O
( O
Koren O
, O
Bell O
, O
and O
Volinsky O
2009 O
) O
. O

The O
advantages B
of O
these O
techniques B
include O
: O
1 O
. O

less O
manual O
feature O
engineering O
required O
, O
2 O
. O

robust O
learning O
even O
when O
the O
data O
sparsity O
problem O
is O
present O
, O
and O
3 O
. O

better O
reusability O
for O
downstream O
tasks O
than O
pure O
predictive O
algorithms O
such O
as O
Gradient O
Boosting O
Decision O
Tree O
( O
Friedman O
2001 O
) O
and O
Factorization O
Machine O
( O
Rendle O
2010 O
) O
. O

Inheriting O
the O
advantages B
from O
LVM O
, O
GAE O
is O
the O
first O
model O
to O
our O
best O
knowledge O
that O
not O
only O
captures B
synergy O
and O
opposition O
relationships O
robustly O
, O
but O
also O
facilitates O
many O
important O
downstream O
tasks O
which O
consume O
game B
avatar I
vectors O
as O
input B
, O
e.g. O
, O
similarity O
search O
on O
game B
avatars I
, O
team B
composition I
analysis I
( O
Kim O
et O
al O
. O

2016 O
; O
Agarwala O
and O
Pearce O
2014 O
) O
and O
match B
outcome O
prediction O
( O
Yang O
, O
Qin O
, O
and O
Lei O
2016 O
) O
. O

In O
sum O
, O
the O
contributions O
of O
our O
paper O
are O
multi O
- O
fold O
: O
1 O
. O

we O
describe O
a O
novel O
Game O
Avatar O
Embedding O
( O
GAE O
) O
model O
which O
characterizes O
game B
avatars I
in O
vectors O
in O
terms O
of O
synergy O
and O
opposition O
; O
2 O
. O

we O
demonstrate O
the O
effectiveness O
of O
the O
model O
via O
quantitative O
experiments O
on O
real O
data O
from O
three O
commercial O
MOBA O
games B
; O
3 O
. O

we O
showcase O
how O
our O
model O
facilitates O
downstream O
tasks O
such O
as O
similar O
avatar B
search O
and O
avatar B
pick B
recommendation O
, O
which O
off O
- O
the O
- O
shelf O
Machine O
Learning O
models O
can O
not O
accomplish O
. O

Our O
paper O
is O
structured O
as O
follows O
. O

We O
first O
present O
related O
work O
, O
then O
introduce O
GAE O
models O
with O
details O
on O
model O
specification O
and O
learning O
process O
. O

Next O
, O
we O
report B
quantitative O
evaluation O
results O
of O
GAE O
, O
followed O
by O
two O
case O
studies O
. O

Finally O
, O
we O
conclude O
our O
paper O
and O
discuss O
limitations O
as O
well O
as O
future O
directions O
. O

Related O
Works O
As O
we O
adopt O
an O
embedding O
model O
to O
uncover O
synergy O
and O
opposition O
relationships O
among O
game B
avatars I
from O
teambased O
outcomes O
for O
MOBA O
games B
, O
our O
work O
aligns O
with O
recent O
research O
in O
team B
formation I
analysis O
, O
MOBA O
game B
research O
, O
and O
embedding O
- O
based B
modeling O
methods O
. O

Team O
Formation O
Analysis O
Team O
formation O
analysis O
is O
the O
research O
topic O
which O
aims O
to O
uncover O
factors O
that O
impact B
team B
performance I
. O

Team B
formation I
has O
already O
been O
studied O
in O
MOBA O
games B
( O
Pobiedina O
et O
al O
. O

2013a O
; O
Semenov O
et O
al O
. O

2016 O
) O
. O

( O
Pobiedina O
et O
al O
. O

2013a O
) O
verified O
that O
team B
success O
depends O
on O
a O
successful O
selected O
combination O
of O
game B
avatars I
. O

( O
Semenov O
et O
al O
. O

2016 O
) O
predicts O
MOBA O
outcomes O
by O
using O
2-way O
factorization O
machine O
( O
Rendle O
2010 O
) O
, O
which O
can O
reliably O
estimate O
the O
levels B

 O
of I
pairwise I
relationships O
through O
factorized O
parameterization O
. O

However O
, O
their O
method O
does O
not O
naturally O
derive O
meaningful O
numerical O
representations O
of O
game B
avatars I
that O
can O
be O
analyzed O
and O
utilized O
for O
many O
other O
downstream O
applications O
. O

Although O
we O
focus B
on O
video B
game I
data O
in O
this O
paper O
, O
team B
formation I
analysis O
could O
help O
advance O
many O
other O
domains O
, O
such O
as O
social O
networks O
( O
Lappas O
, O
Liu O
, O
and O
Terzi O
2009 O
; O
Anagnostopoulos O
et O
al O
. O

2012 O
) O
, O
crowdsourcing O
( O
Rahman O
et O
al O
. O

2015 O
; O
Kittur O
2010 O
; O
Roy O
et O
al O
. O

2015 O
) O
, O
and O
robotics O
( O
Liemhetcharat O
and O
Veloso O
2012 O
) O
. O

Existing O
works O
that O
also O
use O
machine O
learning O
to O
learn O
chacacterization O
of O
team B
members I
( O
Liemhetcharat O
and O
Veloso O
2012 O
; O
Rahman O
et O
al O
. O

2015 O
) O
are O
different O
than O
our O
work O
in O
that O
: O
( O
1 O
) O
the O
dimensions O
of O
team B
member I
characterization O
are O
often O
pre O
- O
defined O
and O
fixed O
, O
such O
as O
a O
fixed O
set B
of I
skills I
, O
which O
requires O
manual O
efforts O
and O
domain O
knowledge O
; O
( O
2 O
) O
no O
opposition O
relationship O
has O
been O
modeled O
. O

MOBA O
Game O
Research O
The O
rich O
design O
of O
MOBA O
games B
has O
attracted O
variety O
of O
research O
to O
be O
conducted O
upon O
them O
. O

For O
example O
, O
team B

 O
formation I
analysis O
( O
Pobiedina O
et O
al O
. O

2013a O
; O
Pobiedina O
et O
al O
. O

2013b O
; O
Neidhardt O
, O
Huang O
, O
and O
Contractor O
2015 O
; O
Kim O
et O
al O
. O

2016 O
; O
Agarwala O
and O
Pearce O
2014 O
) O
, O
skill B
decomposition O
( O
Chen O
et O
al O
. O

2016 O
) O
, O
match B
outcome O
prediction O
and O
avatar B
pick B
recommendation O
systems O
( O
Bhattacharya O
and O
Sabik O
) O
. O

They O
shed O
lights O
on O
real B
- I
world I
problems O
or O
facilitate O
building O
adaptive O
player B
experience B
( O
Nguyen O
, O
Chen O
, O
and O
El O
- O
Nasr O
2015 O
) O
. O

Many O
of O
these O
tasks O
rely O
on O
processing O
vectors O
which O
encode O
characteristics O
of O
game B
avatars I
. O

For O
example O
, O
in O
team B

 O
formation I
analysis O
, O
the O
calculation O
of O
team B
diversity O
is O
averaged O
pairwise O
cosine O
distances O
between O
game B
avatars I
’ O
attribute O
vectors O
. O

Principal O
Component O
Analysis O
( O
Jolliffe O
2002 O
) O
and O
t O
- O
SNE O
( O
Maaten O
and O
Hinton O
2008 O
) O
, O
two O
frequently O
used O
dimension O
reduction O
techniques B
in O
clustering O
and O
visualization O
, O
are O
also O
based B
on O
entities B
’ O
vectors O
. O

Our O
GAE O
model O
induces O
the O
vectors O
of O
game B
avatars I
encoding O
their O
synergy O
and O
opposition O
relationships O
, O
which O
can O
facilitate O
many O
downstream O
tasks O
that O
perform O
upon O
vectors O
. O

LVMs O
and O
Embedding O
Models O
LVMs O
/ O
embedding O
models O
have O
been O
long O
studied O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
( O
Mikolov O
et O
al O
. O

2013 O
) O
, O
graph O
( O
Maaten O
and O
Hinton O
2008 O
) O
, O
and O
recommendation O
system O
( O
Koren O
, O
Bell O
, O
and O
Volinsky O
2009 O
) O
. O

In O
this O
family O
of O
models O
, O
entities B
are O
associated O
with O
vectors O
in O
a O
shared O
, O
continuous O
low O
- O
dimensional O
space O
which O
encode O
entities O
’ O
characteristics O
efficiently O
and O
effectively O
. O

We O
will O
use O
vectors O
and O
embedding O
interchangeably O
to O
refer O
to O
the O
numerical O
representations O
of O
entities B
. O

Some O
salient O
advantages B
of O
embedding O
models O
/ O
LVMs O
are O
as O
follows O
: O
1 O
. O

it O
requires O
little O
human O
labor O
for O
feature O
engineering O
because O
entity B
vectors O
can O
be O
learned O
based B
on O
labels O
or O
what O
are O
observed O
explicitly O
( O
e.g. O
, O
links O
between O
nodes O
in O
a O
graph O
, O
user B
- O
item B
matrix O
, O
word O
sequences O
) O
. O

2 O
. O

information O
between O
entities B
can O
be O
shared O
more O
effectively O
during O
the O
learning O
phase O
. O

For O
example O
, O
similar O
embeddings O
of O
two O
similar O
words O
can O
be O
learned O
if O
they O
are O
often O
used O
and O
occur O
in O
the O
similar O
contexts O
, O
even O
though O
they O
do O
not O
appear O
together O
. O

3 O
. O

learned O
vector O
representation O
can O
be O
reused O
by O
many O
kinds O
of O
applications O
, O
such O
as O
sentiment O
analysis O
( O
Maas O
et O
al O
. O

2011 O
) O
and O
data O
visualization O
( O
Maaten O
and O
Hinton O
2008 O
) O
. O

In O
our O
paper O
, O
game B
avatars I
are O
embedded O
as O
lowdimensional O
vectors O
. O

Their O
values O
are O
learned O
( O
supervised O
) O
through O
maximizing O
the O
winning O
probabilities O
( O
defined O
in O
Section O
3 O
) O
of O
all O
observed O
match B
outcomes O
. O

We O
will O
show O
in O
Performance O
Evaluation O
and O
Case O
Study O
that O
the O
learned O
game B
avatar I
embeddings O
indeed O
capture B
sensible O
teamrelated O
characteristics O
and O
allow O
for O
other O
downstream O
applications O
, O
such O
as O
similar O
avatar B
search O
and O
avatar B
pick B
recommendation O
. O

This O
can O
not O
be O
achieved O
by O
previous O
methods O
such O
as O
Logistic O
Regression O
, O
Factorization O
Machine O
( O
Semenov O
et O
al O
. O

2016 O
) O
and O
Gradient O
Boosting O
Decision O
Tree O
( O
Friedman O
2001 O
) O
which O
simply O
predict O
match B
outcomes O
without O
a O
means O
to O
derive O
game B
avatar I
embeddings O
to O
be O
reused O
in O
other O
tasks O
. O

Preliminary O
and O
Problem O
Definition O
Suppose O
the O
training O
data O
is O
a O
match B
set O
M O
= O
{ O
M1 O
, O
M2 O
, O
· O
· O
· O
, O
MZ O
} O
with O
Z O
matches B
. O

There O
are O
N O
unique O
game B
avatars I
appearing O
in O
total O
, O
denoted O
by O
A O
= O
{ O
A1 O
, O
A2 O
, O
· O
· O
· O
, O
AN O
} O
. O

We O
assume O
each O
match B
is O
competed O
between O
two O
teams B
, O
the O
red B
and I
the O
blue I
team I
. O

We O
use O
Tz O
, O
r O
= O
{ O
Ai O
} O
and O
Tz O
, O
b O
= O
{ O
Aj O
} O
to O
denote O
the O
sets O
of O
game B
avatars I
in O
the O
red B
team O
and I
the I
blue I
team B
in O
Mz O
, O
respectively O
. O

Since O
we O
are O
studying O
5-vs-5 O
MOBA O
games B
, O
we O
have O
for O
∀z O
, O
|Tz O
, O
r| O
= O
5 O
and O
|Tz O
, O
b| O
= O
5 O
. O

We O
use O
T O
= O
{ O
( O
Tz O
, O
r O
, O
Tz O
, O
b)|z O
= O
1 O
, O
· O
· O
· O
, O
Z O
} O
to O
denote O
all O
game B
avatar I
line O
- O
ups O
of O
M. O
Match O
outcomes O
are O
marked B
as O
O O
= O
{ O
o1 O
, O
o2 O
, O
· O
· O
· O
, O
oZ O
} O
. O

oz O
= O
1 O
means O
the O
red B
team I
wins I
over O
the O
blue B
team I
in O
Mz O
otherwise O
oz O
= O
0 O
. O

We O
use O
p(oz O
= O
1 O
) O
and O
p(oz O
= O
0 O
) O
to O
denote O
the O
winning O
probability O
from O
the O
view B
of I
the O
red I
team I
and O
the O
blue B
team I
, O
respectively O
. O

Hence O
, O
p(oz O
= O
0 O
) O
= O
1 O
− O
p(oz O
= O
1 O
) O
. O

Game O
Avatar O
Embedding O
Model O
In O
this O
section O
, O
we O
will O
describe O
the O
proposed O
model O
, O
the O
learning O
process O
, O
as O
well O
as O
discuss O
its O
relationships O
with O
Factorization O
Machines O
, O
a O
related O
model O
. O

Model O
Synergy O
and O
Opposition O
Inspired O
by O
embedding O
methods O
which O
have O
managed O
to O
learn O
low O
- O
dimensional O
vectors O
to O
capture B
abundant O
attributes O
of O
entities B
, O
we O
propose O
to O
map O
characteristics O
of O
game B

 O
avatars I
into O
a O
low O
- O
dimensional O
latent O
space O
. O

For O
a O
game B

 O
avatar I
Ai O
, O
its O
latent O
feature O
vector O
is O
denoted O
as O
ai O
∈ O
R O
K. O
A O
∈ O
R O
N×K O
is O
the O
latent O
feature O
matrix O
such O
that O
A O
= O
{ O
ai O
} O
. O

We O
choose O
to O
use O
a O
bilinear O
model O
to O
model O
synergy O
and O
opposition O
relationships O
between O
pairs O
of O
avatars B
. O

The O
bilinear O
model O
allows O
us O
to O
separately O
learn O
game B
avatar I
embeddings O
, O
as O
well O
as O
the O
matrices O
that O
determine O
the O
extents O
of O
synergy O
and O
opposition O
across O
different O
dimensions O
of O
game B

 O
avatar I
embeddings O
. O

First O
, O
we O
introduce O
the O
intra O
- O
team B
synergy I
score O
function O
S(i O
, O
j O
) O
, O
which O
calculates O
the O
level B
of I
synergy I
to O
which O
Ai O
exerts O
on O
Aj O
in O
the O
same O
team B
: O
S(i O
, O
j O
) O
= O
a O
T O
i O
· O
P O
· O
aj O
= O
X O
K O
m=1 O
X O
K O
n=1 O
aim O
· O
pmn O
· O
ajn O
( O
1 O
) O
P O
∈ O
R O
K×K O
is O
named O
synergy O
matrix O
. O

There O
are O
two O
ways O
to O
understand O
P O
intuitively O
: O
1 O
. O

one O
can O
think O
of O
a O
T O
i O
· O
P O
= O
a O
0 O
i O
as O
converting O
Ai O
’s O
embedding O
into O
the O
dimensions O
that O
Aj O
looks O
for O
as O
a O
helpful O
teammate B
. O

Then O
, O
the O
higher O
the O
dot B
product O
is O
between O
a O
0 O
i O
and O
aj O
, O
the O
higher O
synergy O
the O
two O
game B
avatars I
can O
build O
. O

2 O
. O

alternatively O
, O
one O
can O
think O
that O
pmn O
measures O
how O
much O
m O
- O
th O
dimension O
of O
ai O
fits O
n O
- O
th O
dimension O
of O
aj O
in O
terms O
of O
intra O
- O
team B
interaction O
. O

Second O
, O
we O
define O
the O
inter O
- O
team B
opposition O
score O
function O
C(i O
, O
j O
) O
, O
which O
quantifies O
the O
extent O
to O
which O
Ai O
counters O
Aj O
in O
the O
opposite B
team I
: O
C(i O
, O
j O
) O
= O
a O
T O
i O
· O
Q O
· O
aj O
= O
X O
K O
m=1 O
X O
K O
n=1 O
aim O
· O
qmn O
· O
ajn O
( O
2 O
) O
Q O
∈ O
R O
K×K O
is O
named O
opposition O
matrix O
. O

In O
a O
similar O
way O
to O
understand O
P O
, O
qmn O
measures O
the O
influence O
on O
Ai O
countering O
Aj O
, O
given O
their O
embeddings O
’ O
interaction O
on O
m O
- O
th O
and O
n O
- O
th O
dimension O
respectively O
. O

Note O
that O
P O
and O
Q O
are O
not O
necessarily O
symmetric O
, O
as O
the O
level B
of O
opposition O
in O
which O
Ai O
suppress O
Aj O
could O
be O
different O
from O
that O
of O
Aj O
on O
Ai O
. O

In O
this O
model O
, O
we O
only O
capture B
pairwise O
relationships O
because O
they O
are O
much O
more O
prevalent O
. O

We O
also O
find O
advanced O
models O
such O
as O
Gradient O
Boosting O
Decision O
Trees O
( O
Friedman O
2001 O
) O
potentially O
considering O
more O
intricate O
relationships O
do O
not O
improve O
the O
match B
outcome O
prediction O
task O
on O
all O
the O
three O
datasets O
we O
study O
( O
See O
Section O
Performance O
Evaluation O
) O
. O

Still O
, O
it O
is O
possible O
to O
extend O
GAE O
for O
higher O
order O
interactions O
by O
modeling O
them O
using O
tensors O
and O
tensor O
operations O
( O
Kolda O
and O
Bader O
2009 O
) O
. O

We O
will O
explore O
this O
aspect O
in O
the O
future O
. O

Model O
Winning O
Probability O
Next O
, O
we O
propose O
to O
model O
a O
winning O
outcome O
as O
the O
linear O
breakdown O
of O
the O
individual O
biases O
, O
as O
well O
as O
their O
intrateam O
and O
inter O
- O
team B
interactions O
, O
of O
game B
avatars I
from O
the O
two O
teams B
involved O
. O

Individual O
biases O
represent O
game B
avatars O
’ O
intrinsic O
control B
difficulty O
that O
affects O
match B
outcomes O
, O
denoted O
as O
b O
= O
{ O
b1 O
, O
b2 O
, O
· O
· O
· O
, O
bN O
} O
. O

Hence O
, O
the O
winning O
outcome O
p(oz O
= O
1 O
) O
of O
a O
match B
Mz O
is O
defined O
as O
follows O
: O
p(oz O
= O
1 O
) O
= O
σ O

  O
X O
i∈Tz O
, O
r O
bi O
− O
X O
j∈Tz O
, O
b O
bj O
+ O
X O
i O
, O
j∈Tz O
, O
r O
i6 O
= O
j O
S(i O
, O
j O
) O
− O
X O
i O
, O
j∈Tz O
, O
b O
i6 O
= O
j O
S(i O
, O
j O
) O
+ O
X O
i∈Tz O
, O
r O
X O
j∈Tz O
, O
b O
C(i O
, O
j O
) O
− O
C(j O
, O
i O
) O
 O
( O
3 O
) O
where O
σ O
( O
· O
) O
is O
sigmoid O
function O
1 O
1+exp(−x O
) O
. O

The O
input B
of O
the O
sigmoid O
function O
is O
the O
sum O
of O
the O
differences O
of O
: O
( O
1 O
) O
individual O
biases O
towards O
winning O
, O
( O
2 O
) O
synergy O
strength B
inside O
the O
team B
, O
and O
( O
3 O
) O
opposition O
intensity O
against O
the O
opponent B
team I
. O

The O
latter O
two O
differences O
depend O
on O
traversing O
all O
valid O
pairs O
of O
game B
avatars I
within O
the O
same O
team B
or O
across O
the O
two O
teams B
. O

The O
larger O
the O
differences O
are O
the O
closer B
p(oz O
= O
1 O
) O
is O
to O
1 O
meaning O
the O
advantageous O
team B
is O
more O
likely O
to O
win O
. O

Note O
that O
in O
our O
formulation O
, O
players B
’ O
individual B
skill I
levels I
are O
not O
accounted O
for O
in O
the O
winning O
outcome O
’s O
probability O
. O

This O
is O
reasonable O
, O
since O
the O
data O
we O
collected O
is O
from O
highly O
selective O
ranked B
matches B
. O

Most O
commercial O
MOBA O
games B
have O
proprietary O
matchmaking B
systems I
to O
ensure O
that O
only O
sufficiently O
experienced B
players B
with I
similar I
skill I
levels B
are O
allowed O
to O
compete O
in O
ranked B
matches B
( O
the O
type B
of I

 O
matches I
we O
study O
) O
. O

Therefore O
, O
the O
chance B
of O
results O
being O
skewed O
by O
data O
from O
incompetent B
players I
is O
low O
. O

Objective O
Function O
and O
Learning O
Assuming O
that O
each O
match B
is O
independent O
, O
the O
overall O
likelihood O
function O
is O
: O
p(O O
, O
T O
|A O
, O
P O
, O
Q O
, O
b O
) O
= O
Y O
Z O
z=1 O
p(oz O
= O
1)oz O
p(oz O
= O
0)1−oz O
( O
4 O
) O
The O
objective B
function O
is O
to O
minimize O
the O
negative O
log O
likelihood O
w.r.t O
Θ O
= O
{ O
A O
, O
P O
, O
Q O
, O
b O
} O
: O
J(Θ O
) O
= O
− O
1 O
Z O
X O
Z O
z=1 O


 O
oz O
log O
p(oz O
= O
1 O
) O
+ O
( O
1 O
− O
oz O
) O
log O
p(oz O
= O
0) O
( O
5 O
) O
For O
parameter O
learning O
, O
we O
use O
AdaGrad O
( O
Duchi O
, O
Hazan O
, O
and O
Singer O
2011 O
) O
to O
update B
parameters O
based B
on O
a O
small O
batch O
of O
matches B
in O
each O
iteration O
. O

Relation O
to O
Factorization O
Machine O
Model O
GAE O
has O
a O
close B
relationship O
with O
2-way O
factorization O
machine O
( O
FM O
) O
( O
Rendle O
2010 O
) O
, O
which O
has O
been O
applied O
in O
( O
Semenov O
et O
al O
. O

2016 O
) O
to O
predict O
match B
outcomes O
of O
the O
same O
kind O
of O
games B
. O

In O
( O
Semenov O
et O
al O
. O

2016 O
) O
, O
for O
a O
match B
Mz O
, O
the O
feature O
vector O
xz O
∈ O
{ O
0 O
, O
1 O
} O
2N O
is O
a O
binary O
vector O
indicating O
which O
five O
avatars B
appear O
in O
the O
red B
and I
blue I
team I
respectively O
: O
xzi O
= O
 O
 O
 O
1 O
, O
if O
i O
≤ O
N O
and O
avatar B
i O
was O
in O
the O
red B
team I
or O
if O
i O
> O
N O
and O
avatar B
i O
− O
N O
was O
in O
the O
blue B
team I
0 O
, O
otherwise O
( O
6 O
) O
and O
FM O
models O
a O
winning O
probability O
by O
additionally O
exploring O
pairwise O
interactions O
between O
non O
- O
zero O
features O
: O
p(oz O
= O
1 O
) O
= O
σ O

  O
X O
i∈Tz O
, O
r O
ci O
+ O
X O
j∈Tz O
, O
b O
cj+N O
+ O
X O
i O
, O
j∈Tz O
, O
r O
i O
< O
j O
< O
vi O
, O
vj O
> O
+ O
X O
i O
, O
j∈Tz O
, O
b O
i O
< O
j O
< O
vi+N O
, O
vj+N O
> O
+ O
X O
i∈Tz O
, O
r O
X O
j∈Tz O
, O
b O
< O
vi O
, O
vj+N O
> O
 O
, O
( O
7 O
) O
where O
ci O
∈ O
R O
and O
vi O
∈ O
R O
K O
for O
∀i O
= O
1 O
, O
· O
· O
· O
, O
2N O
are O
firstorder O
and O
second O
- O
order O
parameters O
, O
and O
< O
· O
, O
· O
> O
is O
dot B
product O
operation O
. O

Therefore O
, O
each O
avatar B
Ai O
is O
associated O
with O
a O
quartet O
of O
parameters O
( O
ci O
, O
ci+N O
, O
vi O
, O
vi+N O
) O
. O

Dot O
productions O
in O
Eqn O
. O

7 O
can O
be O
re O
- O
written O
in O
the O
vectormatrix O
- O
vector O
product O
form O
that O
is O
similar O
to O
Eqn O
. O

1 O
and O
Eqn O
. O

2 O
. O

For O
each O
Ai O
, O
we O
can O
set O
vi O
= O
Uiui O
and O
vi+N O
= O
Viui O
where O
Ui O
and O
Vi O
are O
two O
matrices O
that O
linearly O
transform O
the O
same O
base B
ui O
. O

ui O
can O
be O
seen O
as O
the O
equivalence O
of O
ai O
in O
GAE O
, O
a O
vector O
capturing B
characteristics O
of O
Ai O
. O

Therefore O
, O
each O
pairwise O
term O
in O
Eqn O
. O

7 O
can O
be O
written O
as O
: O
< O
vi O
, O
vj O
> O
= O
u O
T O
i O
( O
U O
T O
i O
Uj O
) O
uj O
( O
8) O
< O
vi+N O
, O
vj+N O
> O
= O
u O
T O
i O
( O
V O
T O
i O
Vj O
) O
uj O
( O
9 O
) O
< O
vi O
, O
vj+N O
> O
= O
u O
T O
i O
( O
U O
T O
i O
Vj O
) O
uj O
( O
10 O
) O
FM O
and O
GAE O
both O
attempt O
to O
estimate O
second O
- O
order O
interactions O
through O
the O
embedding O
/ O
factorization O
technique B
. O

Therefore O
, O
they O
both O
inherit O
the O
advantages B
of O
the O
factorization O
technique B
that O
all O
pairs O
of O
co O
- O
occurrences O
can O
help O
the O
learning O
of O
any O
particular O
pair O
of O
interaction O
. O

Since O
the O
hierarchy O
of O
both O
models O
is O
the O
linear O
summation O
of O
first O
- O
order O
biases O
and O
second O
- O
order O
interactions O
, O
FM O
and O
GAE O
are O
expected O
to O
have O
similar O
classification O
performance O
in O
match B
outcome O
prediction O
. O

However O
, O
for O
FM O
, O
it O
is O
unclear O
how O
to O
determine O
Ui O
and O
Vi O
for O
∀i O
. O

On O
the O
contrast O
, O
GAE O
by O
design O
can O
learn O
game B
avatar I
embedding O
and O
synergy O
and O
opposition O
matrices O
at O
the O
same O
time B
, O
which O
enables O
practitioners O
to O
reuse O
game B
avatar I
embedding O
for O
other O
downstream O
tasks O
. O

Performance O
Evaluation O
We O
evaluate O
the O
utility B
of O
GAE O
using O
datasets O
collected O
from O
three O
commercial O
MOBA O
games B
, O
namely O
Defense O
of O
the O
Ancients O
( O
DOTA2 O
) O
, O
Heroes O
of O
Newerth O
( O
HoN O
) O
, O
and O
Heroes O
of O
the O
Storms O
( O
HotS O
) O
. O

All O
data O
is O
from O
5-vs-5 O
matches B
that O
pit O
ten O
random O
players B
in O
two O
teams B
against O
each O
other O
. O

No O
major B
game I
update I
affecting O
the O
mechanics B
of I
the O
games I
occurred O
during O
the O
data O
collection O
phase O
. O

All O
three O
games B
employ O
matchmaking B
systems I
that O
select B
only O
players I
of O
similar B
skill I
levels I
when O
assembling O
a O
match B
. O

All O
the O
three O
datasets O
have O
roughly O
balanced O
winning O
outcomes O
for O
both O
the O
red B
and I
blue I
teams I
. O

Statistics O
of O
the O
three O
datasets O
is O
shown O
in O
Table O
1 O
. O

The O
HotS O
match B
data O
was O
downloaded O
from O
a O
third O
- O
party O
game B
log O
website3 O
; O
all O
the O
matches B
happened O
during O
the O
last O
month O
of O
2016 O
. O

The O
HoN O
dataset O
was O
collected O
by O
( O
Suznjevic O
, O
Matijasevic O
, O
and O
Konfic O
2015 O
) O
, O
which O
contains O
matches B
played B
between O
December O
20 O
, O
2014 O
from O
April O
29 O
, O
2015 O
. O

Finally O
, O
for O
DOTA O
2 O
, O
we O
use O
the O
original O
data O
set O
collected O
between O
February O
11 O
, O
2016 O
to O
March O
2 O
, O
2016 O
by O
Semenov O
et O
al O
. O

( O
Semenov O
et O
al O
. O

2016 O
) O
, O
and O
extract O
a O
subset O
of O
matches B
played B
by O
gamers B
with I
similar I
skill I
levels B
( O
i.e. O
, O
normal O
level B
) O
. O

Experiment O
Setup O
and O
Results O
There O
are O
two O
experiments O
designed O
to O
assess O
the O
effectiveness O
of O
GAE O
. O

The O
first O
is O
conducted O
as O
a O
numerical O
evaluation O
in O
terms O
of O
outcome O
prediction O
, O
while O
the O
second O
evaluates O
GAE O
’s O
interpretability O
using O
human O
experts O
, O
as O
compared O
to O
other O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

3https://www.hotslogs.com/Info/A O
Table O
1 O
: O
Statistics O
of O
datasets O
HotS O
HoN O
DOTA2 O
# O
of O
Matches O
1,814,066 O
1,101,046 O
3,056,596 O
# O
of O
Avatars O
58 O
126 O
111 O
Table O
2 O
: O
Outcome O
prediction O
AUC O
on O
test O
datasets O
; O
( O
* O
) O
indicate O
where O
GAE O
outperforms O
with O
p O
- O
values O
< O
0.001 O
. O

Models O
HotS B
HoN O
DOTA2 O
LR O
0.6095 O
* O
0.6115 O
* O
0.6875 O
* O
GBDT O
0.6375 O
* O
0.6144 O
* O
0.7014 O
* O
FM O
0.6440 O
0.6154 O
* O
0.7143 O
GAE O
0.6437 O
0.6220 O
0.7143 O
Outcome O
Prediction O
Results O
First O
, O
we O
evaluate O
the O
match B
outcome O
prediction O
performance O
of O
GAE O
against O
well O
- O
known O
baselines O
, O
including O
Logistic O
Regression O
( O
LR O
) O
4 O
, O
Gradient O
Boosting O
Decision O
Trees O
( O
GBDT O
) O
5 O
and O
2-way O
Factorization O
Machine O
( O
FM O
) O
6 O
. O

For O
each O
game B
dataset O
, O
we O
adopt O
10-fold O
cross O
- O
validation O
procedure O
with O
train B
: O
validate O
: O
test O
ratio O
set O
to O
be O
8:1:1 O
. O

In O
each O
fold O
, O
a O
model O
with O
different O
configurations O
of O
hyperparameters O
( O
e.g. O
, O
regularization O
penalty O
, O
the O
number O
of O
trees O
, O
the O
dimension O
of O
latent O
space O
, O
etc O
. O

) O
is O
trained B
on O
the O
train B
dataset O
and O
the O
best O
hyperparameters O
is O
determined O
according O
to O
the O
classification O
performance O
on O
the O
validation O
dataset O
. O

The O
classification O
performance O
of O
the O
model O
with O
the O
best O
hyperparameters O
on O
the O
test O
dataset O
will O
be O
recorded O
as O
the O
final O
measurement O
of O
its O
classification O
strength B
. O

For O
GAE O
, O
we O
use O
Eqn O
. O

3 O
to O
predict O
outcomes O
on O
test O
datasets O
. O

For O
baseline O
models O
, O
Eqn O
. O

6 O
is O
used O
to O
construct O
feature O
vectors O
, O
similar O
to O
how O
it O
is O
done O
in O
previous O
works O
. O

The O
area B
under O
ROC O
Curve O
( O
AUC O
) O
is O
used O
as O
the O
classification O
performance O
measurement O
. O

Ten O
test O
AUC O
are O
recorded O
during O
the O
10-fold O
cross O
- O
validation O
for O
each O
model O
( O
LR O
, O
GBDT O
, O
FM O
and O
GAE O
) O
such O
that O
classification O
performance O
can O
be O
compared O
using O
paired O
t O
- O
test O
( O
with O
confidence O
level B
0.001 O
) O
. O

Table O
2 O
reports B
the O
classification O
performances O
of O
all O
models O
in O
match B
outcome O
prediction O
. O

The O
paired O
t O
- O
tests O
showed O
that O
GAE O
has O
significantly O
higher O
test O
AUC O
than O
other O
models O
except O
GAE O
vs. O
FM O
in O
Hots O
and O
DOTA2 O
. O

We O
observe O
that O
LR O
has O
the O
worst O
classification O
AUC O
in O
all O
three O
games B
. O

That O
is O
not O
surprising O
because O
LR O
does O
not O
model O
interactions O
between O
avatars B
. O

This O
verifies O
that O
there O
do O
exist O
team B
synergy I
and O
opposition O
between O
game B
avatars I
. O

GBDT O
is O
a O
tree O
- O
based B
model O
that O
could O
handle O
interactions O
among O
more O
than O
two O
game B
avatars I
. O

However O
, O
it O
achieves O
statistically O
worse O
results O
than O
GAE O
. O

This O
demonstrates O
: O
( O
1 O
) O
the O
strength B
of O
embedding O
methods O
in O
effectively O
encoding O
meaningful O
information O
of O
pairwise O
synergy O
and O
opposition O
relationships O
in O
a O
low O
- O
dimensional O
space O
; O
( O
2 O
) O
much O
more O
4http://scikit-learn.org/ O
5https://github.com/dmlc/xgboost O
6https://github.com/ibayer/fastFM O
Table O
3 O
: O
Pearson O
’s O
r O
between O
human O
ratings O
and O
GAE O
/ O
baseline O
scores O
on O
3 O
evaluation O
sets O
of O
pairs O
( O
boldface O
indicates O
p O
- O
values O
< O
0.001 O
) O
Similarity O
Synergy O
Opposition O
Win O
- O
Ratio O
Matrix O
0.5258 O
- O
- O
FM O
- O
0.7841 O
0.7669 O
GAE O
0.8080 O
0.8488 O
0.7384 O
data O
might O
be O
needed O
for O
GBDT O
to O
fully O
capture B
more O
complicated O
relationships O
. O

When O
GAE O
and O
FM O
are O
tuned O
with O
a O
proper O
number O
of O
latent O
space O
dimensions O
K O
, O
they O
achieve O
comparable O
AUC O
in O
HotS O
and O
DOTA2 O
. O

This O
verified O
our O
expectation O
in O
Section O
Relation O
to O
Factorization O
Machine O
Model O
that O
GAE O
and O
FM O
should O
have O
similar O
outcome O
prediction O
performance O
because O
they O
both O
rely O
on O
factorization O
techniques B
to O
quantify O
pairwise O
interactions O
. O

However O
, O
the O
exception O
is O
HoN O
where O
GAE O
is O
statistically O
significantly O
better O
than O
FM O
in O
HoN O
and O
GAE O
appears O
to O
have O
smaller O
improvement O
over O
LR O
than O
in O
other O
games B
. O

We O
will O
investigate O
the O
characteristics O
of O
HoN O
compared O
to O
other O
MOBA O
games B
in O
the O
future O
. O

Overall O
, O
GAE O
predicted O
match B
outcomes O
well O
and O
robustly O
. O

Human O
Evaluation O
Second O
, O
we O
would O
like O
to O
validate O
how O
sensible O
GAE O
results O
are O
as O
compared O
to O
the O
experts O
’ O
, O
i.e. O
, O
human O
players B
’ O
, O
judgment B
. O

We O
ask O
human O
players B
to O
rate O
pairs O
of O
game B
avatars I
in O
terms O
of O
similarity O
, O
synergy O
and O
opposition O
. O

Intuitively O
, O
if O
a O
model O
’s O
scores O
are O
highly O
correlated O
with O
human O
ratings O
, O
we O
conclude O
that O
such O
model O
generates O
sensible O
results O
. O

Since O
recruitment O
of O
knowledgeable O
players B
is O
a O
relatively O
expensive O
task O
, O
we O
only O
evaluate O
on O
the O
DOTA2 O
dataset O
. O

Based B
on O
a O
pilot O
test O
of O
three O
DOTA2 O
players B
, O
60 O
pairs O
are O
selected O
which O
have O
clear B
similarity O
, O
synergy O
and O
opposition O
relationships O
( O
20 O
pairs O
for O
each O
kind O
of O
relationship O
) O
. O

For O
example O
, O
the O
20 O
similarity O
evaluation O
pairs O
include O
both O
very O
similar O
as O
well O
as O
very O
different O
pairs O
of O
game B
avatars I
because O
either O
kind O
is O
expected O
to O
be O
evaluated O
consistently O
by O
subjects O
. O

When O
using O
GAE O
to O
evaluate O
the O
pairs O
, O
the O
similarity O
is O
determined O
by O
cosine O
similarity O
between O
the O
learned O
game B

 O
avatar I
embeddings O
. O

The O
synergy O
is O
determined O
by O
S(i O
, O
j O
) O
+ O
S(j O
, O
i O
) O
and O
the O
opposition O
by O
the O
absolute O
value O
of O
C(i O
, O
j O
) O
− O
C(j O
, O
i O
) O
for O
any O
pair O
of O
game B
avatars I
Ai O
and O
Aj O
. O

Note O
that O
besides O
GAE O
, O
we O
are O
not O
aware O
of O
any O
approach O
that O
can O
handle O
similarity O
, O
synergy O
, O
and O
opposition O
queries O
all O
in O
a O
single O
model O
. O

FMs O
can O
naturally O
answer O
synergy O
and O
opposition O
queries O
; O
more O
specifically O
, O
two O
avatars B
’ O
synergy O
and O
opposition O
levels B
can O
be O
obtained O
using O
the O
left O
hand O
side O
of O
Eqn O
. O

8 O
and O
Eqn O
. O

10 O
respectively O
. O

However O
, O
they O
are O
not O
designed O
for O
similarity O
search O
, O
so O
we O
created O
an O
ad O
- O
hoc O
baseline O
method O
to O
compute O
avatars B
’ O
similarity O
based B
on O
the O
cosine O
similarity O
between O
the O
respective O
rows O
of O
a O
win O
- O
ratio O
matrix O
W O
∈ O
R O
N×2N O
, O
constructed O
as O
: O
Wi O
, O
j O
= O
# O
of O
matches B
( O
Ai O
, O
Aj O
) O
win O
# O
of O
matches B
( O
Ai O
, O
Aj O
) O
from O
the O
same O
team B
( O
11 O
) O
Wi O
, O
j+N O
= O
# O
of O
matches B
Ai O
wins O
over O
Aj O
# O
of O
matches B
Ai O
, O
Aj O
from O
the O
opposite B
teams I
( O
12 O
) O
To O
collect O
human O
ratings O
, O
we O
created O
a O
survey O
asking O
subjects O
to O
rate O
on O
a O
5-point O
Likert O
scale O
the O
level B
of O
similarity O
, O
synergy O
or O
opposition O
on O
the O
60 O
pairs O
, O
with O
1 O
as O
“ O
not O
at O
all O
” O
and O
5 O
as O
“ O
very O
much O
” O
, O
and O
asked O
ten O
similarly O
skillful B
DOTA2 O
players B
to O
provide O
their O
ratings O
. O

We O
produce O
Pearson O
’s O
r O
between O
human O
ratings O
and O
GAE O
/ O
baseline O
scores O
on O
the O
20 O
pairs O
in O
each O
kind O
of O
relationship O
. O

We O
compared O
the O
correlations O
( O
using O
Pearson O
’s O
r O
) O
between O
human O
ratings O
and O
those O
by O
GAE O
and O
the O
baseline O
. O

Better O
correlation O
corresponds O
to O
more O
sensible O
results O
from O
the O
players B
’ O
perspective O
. O

As O
shown O
in O
Table O
3 O
, O
for O
similarity O
queries O
, O
GAE O
’s O
results O
better O
correlate O
with O
human O
ratings O
than O
those O
by O
the O
baseline O
, O
suggesting O
that O
similarity O
search O
based B
on O
the O
learned O
embeddings O
by O
GAE O
are O
more O
sensible O
. O

For O
synergy O
and O
opposition O
queries O
, O
both O
GAE O
and O
the O
baseline O
correlate O
with O
human O
ratings O
with O
high O
Pearson O
’s O
r O
( O
≥ O
0.7 O
) O
with O
p O
- O
value O
< O
0.001 O
, O
which O
indicates O
both O
methods O
are O
sensible O
to O
human O
players B
. O

This O
can O
be O
explained O
by O
the O
similarity O
of O
FM O
and O
GAE O
’s O
approach O
in O
using O
the O
factorization O
/ O
embedding O
technique B
to O
model O
pairwise O
interactions O
. O

Case O
Study O
In O
this O
section O
, O
we O
would O
like O
to O
qualitatively O
demonstrate O
the O
quality O
and O
utility B
of O
GAE O
within O
the O
context O
of O
practical O
applications O
. O

All O
analyses O
are O
done O
with O
the O
help O
of O
three O
seasoned O
DOTA2 O
players B
, O
conducted O
on O
GAE O
’s O
results O
( O
K O
= O
75 O
) O
as O
learned O
from O
DOTA2 O
data O
. O

Game B
avatars I
are O
called B
heroes B
in O
DOTA O
2 O
. O

Application O
- O
Similarity O
Search O
One O
direct O
downstream O
application O
utilizing O
GAE O
’s O
game B

 O
avatar I
vectors O
is O
similarity O
search O
. O

It O
can O
help O
players B
, O
both O
starters B
or O
pros O
, O
to O
expand O
their O
hero B
pools O
by O
recommending O
heroes B
similar O
to O
what O
they O
are O
already O
familiar O
with O
or O
good O
at O
. O

For O
example O
, O
given O
input B
hero B
Clinkz O
, O
the O
top O
three O
heroes B
GAE O
returns O
are O
Weaver O
, O
Riki O
and O
Mirana O
. O

After O
examining O
the O
results O
, O
the O
three O
seasoned O
DOTA2 O
players B
all O
agreed O
that O
the O
top O
three O
heroes B
are O
very O
similar O
to O
Clinkz O
, O
as O
they O
are O
all O
Agility O
Carry O
heroes B
with O
low B
hit I
points I
, O
great B

 O
escape I
capability I
, O
and O
sharing O
a O
stealthy B
play I
style I
. O

Application O
- O
Personalized O
Recommendation O
Kim O
et O
al O
. O

( O
Kim O
et O
al O
. O

2016 O
) O
suggested O
that O
the O
ideal O
game B

 O
avatar I
to O
maximize O
the O
winning B
chance I
fit O
players B
’ O
personal O
expertise O
and O
team B
congruency O
in O
parallel O
, O
guided O
by O
which O
GAE O
could O
be O
used O
for O
a O
personalized O
avatar B
pick B
recommendation O
system O
. O

We O
select O
a O
real O
match B
played B
by O
one O
of O
our O
DOTA2 O
players B
for O
illustration O
although O
the O
real O
implementation O
and O
verification O
of O
this O
idea O
requires O
more O
work O
in O
the O
future O
. O

In O
a O
ranked B
match B
, O
the O
player B
is O
the O
last O
to O
pick B
a O
hero B
, O
when O
his O
team B
have O
picked B
Puck O
, O
Ember O
Spirit O
, O
Lion O
, O
Necrophos O
, O
and O
the O
opposite B
team I
picked B
Silencer O
, O
Pudge O
, O
Sand O
King O
, O
Juggernaut O
, O
Anti O
- O
Mage O
. O

Given O
30 O
seconds O
to O
make O
the O
pick B
, O
he O
wants O
to O
prioritize O
the O
hero B
selection I
that O
synergizes O
with O
his O
team B
and O
opposes O
the O
other O
team B
. O

Using O
Eqn O
. O

3 O
to O
search O
the O
hero B
that O
maximizes O
the O
winning O
probability O
, O
GAE O
returns O
the O
top O
recommendation O
, O
Ursa O
. O

However O
, O
the O
player B
has O
not O
played B
Ursa O
before O
, O
thus O
are O
less O
confident O
about O
playing B
it O
. O

Based B
on O
the O
similarity O
search O
on O
the O
learned O
game B
avatar I
embeddings O
, O
GAE O
returns O
a O
list B
of I

 O
heroes I
similar O
to O
Ursa O
, O
top O
3 O
being O
Troll O
Warlord O
, O
Sven O
and O
Juggernaut O
. O

Finally O
, O
the O
player B
decides O
to O
go O
for O
Sven O
since O
that O
is O
the O
hero B
he O
is O
experienced B
with O
and O
Sven O
is O
also O
one O
of O
the O
top O
5 O
heroes B
identified O
with O
best B
overall I
synergy I
and O
opposition O
besides O
Ursa O
. O

Analyzing O
the O
above O
example O
, O
all O
the O
three O
DOTA2 O
players B
strongly O
agreed O
that O
Ursa O
is O
a O
very O
suitable O
choice O
given O
that O
this O
player B
’s O
team B
lacks O
burst B
physical I
damage I
. O

In O
addition O
, O
they O
had O
different O
extra O
interpretations O
on O
the O
Ursa O
pick B
. O

For O
example O
, O
one O
player B
identified O
that O
Ursa O
could O
help O
the O
player B
finish O
the O
game B
early B
, O
disallowing O
the O
opposing B

 O
team I
to O
elongate O
the O
game B
when O
Anti B
- I
mage I
will O
show O
his O
max O
advantage B
as O
a O
late B
game I
Carry O
. O

Another O
player B
recognized O
that O
Ursa O
could O
increase B
team B
fights I
capability O
since O
Ursa O
is O
a O
Tank O
Carry O
hero B
who O
is O
durable O
in O
fights B
. O

All O
the O
three O
players B
agreed O
that O
Sven O
is O
similar O
to O
Ursa O
, O
as O
both O
heroes B
output O
high B
burst I
physical I
damage I
. O

They O
also O
proposed O
that O
GAE O
recommendation O
can O
be O
used O
differently O
according O
to O
personal O
prioritization O
. O

For O
example O
, O
some O
players B
who O
strongly O
prefer O
skill B
familiarity O
can O
first O
use O
GAE O
to O
list O
their O
familiar B
heroes I
then O
run O
synergy O
and O
opposition O
search O
. O

As O
a O
summary O
, O
GAE O
provides O
an O
interface B
to O
perform O
queries O
of O
similar O
, O
synergy O
and O
opposition O
simultaneously O
. O

These O
capabilities O
can O
then O
be O
incorporated O
into O
downstream O
applications O
, O
giving O
users B
a O
white O
- O
box O
tool O
to O
help O
them O
better O
understand O
the O
game B
and O
make O
better O
in B
- I
game I
choices O
that O
maximize O
the O
winning B
chance I
. O

Conclusions O
, O
Limitations O
and O
Future O
Works O
Modeling O
synergy O
and O
opposition O
relationships O
between O
game B
avatars I
is O
an O
important O
task O
that O
helps O
players B
understand O
the O
game B
and O
make O
better O
decisions O
in O
forming O
effective O
teams B
. O

To O
tackle O
this O
task O
, O
our O
proposed O
embeddingbased O
method O
models O
synergy O
and O
opposition O
relationships O
with O
game B
avatars I
encoded O
as O
vector O
representation O
. O

Our O
quantitative O
and O
qualitative O
analyses O
show O
that O
GAE O
is O
able O
to O
capture B
pairwise O
synergy O
and O
opposition O
relationships O
between O
game B
avatars I
that O
are O
sensible O
to O
human O
players B
. O

Moreover O
, O
the O
learned O
game B
avatar I
embeddings O
effectively O
capture B
important O
characteristics O
of O
game B
avatars I
because O
similarity O
search O
based B
on O
the O
game B
avatar I
embeddings O
also O
highly O
correlate O
with O
human O
ratings O
. O

Our O
model O
opens B
new O
doors O
to O
many O
downstream O
tasks O
, O
such O
as O
similarity O
search O
on O
game B
avatars I
and O
personalized O
avatar B
recommendation O
. O

There O
are O
some O
future O
directions O
that O
we O
will O
pursue O
next O
. O

First O
, O
we O
want O
to O
study O
the O
extension O
of O
GAE O
in O
capturing B
higher O
- O
order O
relationships O
that O
involve O
more O
than O
two O
avatars B
, O
as O
well O
as O
study O
the O
trade O
- O
offs O
between O
its O
performance O
improvement O
and O
computation O
overhead O
incurred O
. O

Second O
, O
we O
are O
currently O
limited O
to O
access O
to O
sensitive O
human O
player B
information O
. O

In O
the O
future O
, O
we O
hope O
to O
collect O
match B
data O
with O
richer O
player B
information O
and O
model O
player B
and O
game B
avatar I
embeddings O
within O
the O
same O
model O
. O

References O
[ O
Agarwala O
and O
Pearce O
2014 O
] O
Agarwala O
, O
A. O
, O
and O
Pearce O
, O
M. O
2014 O
. O

Learning O
dota B
2 O
team B
compositions I
. O

Technical O
report B
, O
tech O
. O

rep O
. O

, O
Stanford O
University O
. O

[ O
Anagnostopoulos O
et O
al O
. O

2012 O
] O
Anagnostopoulos O
, O
A. O
; O
Becchetti O
, O
L. O
; O
Castillo O
, O
C. O
; O
Gionis O
, O
A. O
; O
and O
Leonardi O
, O
S. O
2012 O
. O

Online B
team I
formation I
in O
social O
networks O
. O

In O
Proceedings O
of O
the O
21st O
international O
conference O
on O
World O
Wide O
Web O
, O
839 O
– O
848 O
. O

ACM O
. O

[ O
Bhattacharya O
and O
Sabik O
] O
Bhattacharya O
, O
R. O
, O
and O
Sabik O
, O
A. O
Data O
- O
driven O
recommendation O
systems O
for O
multiplayer O
online O
battle B
arenas O
. O

[ O
Chen O
et O
al O
. O

2016 O
] O
Chen O
, O
Z. O
; O
Sun O
, O
Y. O
; O
Seif O
El O
- O
Nasr O
, O
M. O
; O
and O
Nguyen O
, O
T.-H. O
D. O
2016 O
. O

Player B
skill I
decomposition I
in O
multiplayer O
online O
battle B
arenas O
. O

In O
Meaningful O
Play O
. O

[ O
Duchi O
, O
Hazan O
, O
and O
Singer O
2011 O
] O
Duchi O
, O
J. O
; O
Hazan O
, O
E. O
; O
and O
Singer O
, O
Y. O
2011 O
. O

Adaptive O
subgradient O
methods O
for O
online O
learning O
and O
stochastic O
optimization O
. O

Journal O
of O
Machine O
Learning O
Research O
12(Jul):2121–2159 O
. O

[ O
Friedman O
2001 O
] O
Friedman O
, O
J. O
H. O
2001 O
. O

Greedy O
function O
approximation O
: O
a O
gradient O
boosting B
machine O
. O

Annals O
of O
statistics O
1189–1232 O
. O

[ O
Jolliffe O
2002 O
] O
Jolliffe O
, O
I. O
2002 O
. O

Principal O
component O
analysis O
. O

Wiley O
Online O
Library O
. O

[ O
Kim O
et O
al O
. O

2016 O
] O
Kim O
, O
J. O
; O
Keegan O
, O
B. O
C. O
; O
Park O
, O
S. O
; O
and O
Oh O
, O
A. O
2016 O
. O

The O
proficiency O
- O
congruency O
dilemma O
: O
Virtual O
team B
design O
and O
performance O
in O
multiplayer O
online B
games I
. O

In O
Proceedings O
of O
the O
2016 O
CHI O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
, O
4351–4365 O
. O

ACM O
. O

[ O
Kittur O
2010 O
] O
Kittur O
, O
A. O
2010 O
. O

Crowdsourcing O
, O
collaboration O
and O
creativity O
. O

ACM O
Crossroads O
17(2):22–26 O
. O

[ O
Kolda O
and O
Bader O
2009 O
] O
Kolda O
, O
T. O
G. O
, O
and O
Bader O
, O
B. O
W. O
2009 O
. O

Tensor O
decompositions O
and O
applications O
. O

SIAM O
review O
51(3):455–500 O
. O

[ O
Koren O
, O
Bell O
, O
and O
Volinsky O
2009 O
] O
Koren O
, O
Y. O
; O
Bell O
, O
R. O
; O
and O
Volinsky O
, O
C. O
2009 O
. O

Matrix O
factorization O
techniques B
for O
recommender O
systems O
. O

Computer O
( O
8):30–37 O
. O

[ O
Lappas O
, O
Liu O
, O
and O
Terzi O
2009 O
] O
Lappas O
, O
T. O
; O
Liu O
, O
K. O
; O
and O
Terzi O
, O
E. O
2009 O
. O

Finding O
a O
team B
of O
experts O
in O
social O
networks O
. O

In O
Proceedings O
of O
the O
15th O
ACM O
SIGKDD O
international O
conference O
on O
Knowledge O
discovery O
and O
data O
mining O
, O
467–476 O
. O

ACM O
. O

[ O
Liemhetcharat O
and O
Veloso O
2012 O
] O
Liemhetcharat O
, O
S. O
, O
and O
Veloso O
, O
M. O
2012 O
. O

Modeling O
and O
learning O
synergy O
for O
team B

 O
formation I
with O
heterogeneous O
agents O
. O

In O
Proceedings O
of O
the O
11th O
International O
Conference O
on O
Autonomous O
Agents O
and O
Multiagent O
Systems O
- O
Volume O
1 O
, O
365–374 O
. O

International O
Foundation O
for O
Autonomous O
Agents O
and O
Multiagent O
Systems O
. O

[ O
Maas O
et O
al O
. O

2011 O
] O
Maas O
, O
A. O
L. O
; O
Daly O
, O
R. O
E. O
; O
Pham O
, O
P. O
T. O
; O
Huang O
, O
D. O
; O
Ng O
, O
A. O
Y. O
; O
and O
Potts O
, O
C. O
2011 O
. O

Learning O
word O
vectors O
for O
sentiment O
analysis O
. O

In O
Proceedings O
of O
the O
49th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
- O
Volume O
1 O
, O
142 O
– O
150 O
. O

Association O
for O
Computational O
Linguistics O
. O

[ O
Maaten O
and O
Hinton O
2008 O
] O
Maaten O
, O
L. O
v. O
d. O
, O
and O
Hinton O
, O
G. O
2008 O
. O

Visualizing O
data O
using O
t O
- O
sne O
. O

Journal O
of O
Machine O
Learning O
Research O
9(Nov):2579–2605 O
. O

[ O
Mikolov O
et O
al O
. O

2013 O
] O
Mikolov O
, O
T. O
; O
Sutskever O
, O
I. O
; O
Chen O
, O
K. O
; O
Corrado O
, O
G. O
S. O
; O
and O
Dean O
, O
J. O
2013 O
. O

Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O

In O
Advances O
in O
neural O
information O
processing O
systems O
, O
3111 O
– O
3119 O
. O

[ O
Minotti O
2016 O
] O
Minotti O
, O
M. O
2016 O
. O

Comparing O
MOBAs B
: O
League O
of O
Legends O
vs. O
Dota O
2 O
vs. O
Smite O
vs. O
Heroes O
of O
the O
Storm O
. O

http://venturebeat.com/2015/07/ O
15 O
/ O
comparing O
- O
mobas O
- O
league B
- O
of I
- I
legendsvs O
- O
dota-2-vs O
- O
smite O
- O
vs O
- O
heroes B
- O
of O
- O
thestorm/. O
Online O
; O
accessed O
May O
, O
2016 O
. O

[ O
Neidhardt O
, O
Huang O
, O
and O
Contractor O
2015 O
] O
Neidhardt O
, O
J. O
; O
Huang O
, O
Y. O
; O
and O
Contractor O
, O
N. O
2015 O
. O

Team B
vs. O
team B
: O
Success O
factors O
in O
a O
multiplayer O
online O
battle B
arena O
game B
. O

In O
Academy O
of O
Management O
Proceedings O
, O
volume O
2015 O
, O
18725 O
. O

Academy O
of O
Management O
. O

[ O
Nguyen O
, O
Chen O
, O
and O
El O
- O
Nasr O
2015 O
] O
Nguyen O
, O
T.-H. O
D. O
; O
Chen O
, O
Z. O
; O
and O
El O
- O
Nasr O
, O
M. O
S. O
2015 O
. O

Analytics O
- O
based B
AI O
Techniques O
for O
Better O
Gaming O
Experience B
, O
volume O
2 O
of O
Game O
AI O
Pro O
. O

Boca O
Raton O
, O
Florida O
: O
CRC O
Press O
. O

[ O
Pobiedina O
et O
al O
. O

2013a O
] O
Pobiedina O
, O
N. O
; O
Neidhardt O
, O
J. O
; O
Calatrava O
Moreno O
, O
M. O
d. O
C. O
; O
Grad O
- O
Gyenge O
, O
L. O
; O
and O
Werthner O
, O
H. O
2013a O
. O

On O
Successful O
Team O
Formation O
: O
Statistical O
Analysis O
of O
a O
Multiplayer O
Online O
Game O
. O

In O
2013 O
IEEE O
15th O
Conference O
on O
Business O
Informatics O
, O
55–62 O
. O

IEEE O
. O

[ O
Pobiedina O
et O
al O
. O

2013b O
] O
Pobiedina O
, O
N. O
; O
Neidhardt O
, O
J. O
; O
Calatrava O
Moreno O
, O
M. O
d. O
C. O
; O
and O
Werthner O
, O
H. O
2013b O
. O

Ranking B
factors O
of O
team B
success O
. O

In O
Proceedings O
of O
the O
22nd O
international O
conference O
on O
World O
Wide O
Web O
companion O
, O
1185 O
– O
1194 O
. O

International O
World O
Wide O
Web O
Conferences O
Steering O
Committee O
. O

[ O
Rahman O
et O
al O
. O

2015 O
] O
Rahman O
, O
H. O
; O
Thirumuruganathan O
, O
S. O
; O
Roy O
, O
S. O
B. O
; O
Amer O
- O
Yahia O
, O
S. O
; O
and O
Das O
, O
G. O
2015 O
. O

Worker O
skill B
estimation O
in B
team O
- I
based I
tasks O
. O

Proceedings O
of O
the O
VLDB O
Endowment O
8(11):1142–1153 O
. O

[ O
Rendle O
2010 O
] O
Rendle O
, O
S. O
2010 O
. O

Factorization O
machines O
. O

In O
2010 O
IEEE O
International O
Conference O
on O
Data O
Mining O
, O
995 O
– O
1000 O
. O

IEEE O
. O

[ O
Roy O
et O
al O
. O

2015 O
] O
Roy O
, O
S. O
B. O
; O
Lykourentzou O
, O
I. O
; O
Thirumuruganathan O
, O
S. O
; O
Amer O
- O
Yahia O
, O
S. O
; O
and O
Das O
, O
G. O
2015 O
. O

Task O
assignment O
optimization O
in B
knowledge O
- I
intensive I
crowdsourcing O
. O

The O
VLDB O
Journal O
24(4):467–491 O
. O

[ O
Semenov O
et O
al O
. O

2016 O
] O
Semenov O
, O
A. O
; O
Romov O
, O
P. O
; O
Korolev O
, O
S. O
; O
Yashkov O
, O
D. O
; O
and O
Neklyudov O
, O
K. O
2016 O
. O

Performance O
of O
Machine O
Learning O
Algorithms O
in O
Predicting O
Game O
Outcome O
from O
Drafts O
in O
Dota O
2 O
. O

In O
Analysis O
of O
Images O
, O
Social O
Networks O
and O
Texts O
. O

Springer O
. O

26–37 O
. O

[ O
Suznjevic O
, O
Matijasevic O
, O
and O
Konfic O
2015 O
] O
Suznjevic O
, O
M. O
; O
Matijasevic O
, O
M. O
; O
and O
Konfic O
, O
J. O
2015 O
. O

Application O
context O
based B
algorithm O
for O
player B
skill I
evaluation I
in O
MOBA O
games B
. O

In O
Network O
and O
Systems O
Support O
for O
Games O
( O
NetGames O
) O
, O
2015 O
International O
Workshop O
on O
, O
1–6 O
. O

IEEE O
. O

[ O
Tassi O
2016 O
] O
Tassi O
, O
P. O
2016 O
. O

Riot B
’s O
’ O
League O
of O
Legends O
’ O
Reveals O
Astonishing O
27 O
Million O
Daily O
Players O
, O
67 O
Million O
Monthly O
. O

http://www.forbes.com/ O
sites O
/ O
insertcoin/2014/01/27 O
/ O
riotsleague O
- O
of O
- O
legends O
- O
reveals O
- O
astonishing27-million O
- O
daily O
- O
players-67-millionmonthly/#26ff8e543511 O
. O

Online O
; O
accessed O
May O
, O
2016 O
. O

[ O
Yang O
, O
Qin O
, O
and O
Lei O
2016 O
] O
Yang O
, O
Y. O
; O
Qin O
, O
T. O
; O
and O
Lei O
, O
Y.-H. O
2016 O
. O

Real B
- I
time I
esports O
match B
result O
prediction O
. O

arXiv O
preprint O
arXiv:1701.03162 O
. O

Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
Daniel O
R. O
Jiang O
1 O
Emmanuel O
Ekwedike O
2 O
3 O
Han O
Liu O
2 O
4 O
Abstract O
Inspired O
by O
recent O
successes O
of O
Monte O
- O
Carlo O
tree O
search O
( O
MCTS O
) O
in O
a O
number O
of O
artificial O
intelligence O
( O
AI O
) O
application O
domains O
, O
we O
propose O
a O
model O
- O
based B
reinforcement O
learning O
( O
RL O
) O
technique B
that O
iteratively O
applies O
MCTS O
on O
batches O
of O
small O
, O
finite O
- O
horizon O
versions O
of O
the O
original O
infinite O
- O
horizon O
Markov O
decision O
process O
. O

The O
terminal O
condition O
of O
the O
finite O
- O
horizon O
problems O
, O
or O
the O
leaf O
- O
node O
evaluator O
of O
the O
decision O
tree O
generated O
by O
MCTS O
, O
is O
specified O
using O
a O
combination O
of O
an O
estimated O
value O
function O
and O
an O
estimated O
policy O
function O
. O

The O
recommendations O
generated O
by O
the O
MCTS O
procedure O
are O
then O
provided O
as O
feedback O
in O
order O
to O
refine O
, O
through O
classification O
and O
regression O
, O
the O
leaf O
- O
node O
evaluator O
for O
the O
next O
iteration O
. O

We O
provide O
the O
first O
sample O
complexity O
bounds O
for O
a O
tree O
search O
- O
based B
RL O
algorithm O
. O

In O
addition O
, O
we O
show O
that O
a O
deep O
neural O
network O
implementation O
of O
the O
technique B
can O
create O
a O
competitive B
AI O
agent O
for O
the O
popular O
multi O
- O
player B
online O
battle B
arena O
( O
MOBA O
) O
game B
King O
of O
Glory O
. O

1 O
. O

Introduction O
Monte O
- O
Carlo O
tree O
search O
( O
MCTS O
) O
, O
introduced O
in O
Coulom O
( O
2006 O
) O
and O
surveyed O
in O
detail O
by O
Browne O
et O
al O
. O

( O
2012 O
) O
, O
has O
received O
attention O
in O
recent O
years O
for O
its O
successes O
in O
gameplay B
artificial O
intelligence O
( O
AI O
) O
, O
culminating O
in O
the O
Go O
- O
playing B
AI O
AlphaGo O
( O
Silver O
et O
al O
. O

, O
2016 O
) O
. O

MCTS O
seeks O
to O
iteratively O
build O
the O
decision O
tree O
associated O
with O
a O
given O
Markov O
decision O
process O
( O
MDP O
) O
so O
that O
attention O
is O
focused B
on O
“ O
important O
” O
areas B
of O
the O
state O
space O
, O
assuming O
a O
given O
initial O
state O
( O
or O
root O
node O
of O
the O
decision O
tree O
) O
. O

The O
intuition O
behind O
MCTS O
is O
that O
if O
rough O
estimates O
of O
state O
or O
action O
values O
are O
given O
, O
then O
it O
is O
only O
necessary O
to O
expand O
the O
decision O
tree O
in O
the O
direction O
of O
states O
and O
actions O
with O
high O
estimated O
value O
. O

To O
accomplish O
this O
, O
MCTS O
utilizes O
the O
guidance O
of O
1University O
of O
Pittsburgh O
2Tencent O
AI O
Lab O
3 O
Princeton O
University O
4Northwestern O
University O
. O

Correspondence O
to O
: O
Daniel O
R. O
Jiang O
< O
drjiang@pitt.edu O
> O
. O

Proceedings O
of O
the O
35 O
th O
International O
Conference O
on O
Machine O
Learning O
, O
Stockholm O
, O
Sweden O
, O
PMLR O
80 O
, O
2018 O
. O

Copyright O
2018 O
by O
the O
author(s O
) O
. O

leaf O
- O
node O
evaluators O
( O
either O
a O
policy O
function O
( O
Chaslot O
et O
al O
. O

, O
2006 O
) O
rollout O
, O
a O
value O
function O
evaluation O
( O
Campbell O
et O
al O
. O

, O
2002 O
; O
Enzenberger O
, O
2004 O
) O
, O
or O
a O
mixture O
of O
both O
( O
Silver O
et O
al O
. O

, O
2016 O
) O
) O
to O
produce O
estimates O
of O
downstream O
values O
once O
the O
tree O
has O
reached O
a O
certain O
depth O
( O
Browne O
et O
al O
. O

, O
2012 O
) O
. O

The O
information O
from O
the O
leaf O
- O
nodes O
are O
then O
backpropagated O
up O
the O
tree O
. O

The O
performance O
of O
MCTS O
depends O
heavily O
on O
the O
quality O
of O
the O
policy O
/ O
value O
approximations O
( O
Gelly O
& O
Silver O
, O
2007 O
) O
, O
and O
at O
the O
same O
time B
, O
the O
successes O
of O
MCTS O
in O
Go O
show O
that O
MCTS O
improves O
upon O
a O
given O
policy O
when O
the O
policy O
is O
used O
for O
leaf O
evaluation O
, O
and O
in O
fact O
, O
it O
can O
be O
viewed O
as O
a O
policy O
improvement O
operator O
( O
Silver O
et O
al O
. O

, O
2017 O
) O
. O

In O
this O
paper O
, O
we O
study O
a O
new O
feedback O
- O
based B
framework O
, O
wherein O
MCTS O
updates B
its O
own O
leaf O
- O
node O
evaluators O
using O
observations O
generated O
at O
the O
root O
node O
. O

MCTS O
is O
typically O
viewed O
as O
an O
online O
planner O
, O
where O
a O
decision O
tree O
is O
built O
starting B
from O
the O
current O
state O
as O
the O
root O
node O
( O
Chaslot O
et O
al O
. O

, O
2006 O
; O
2008 O
; O
Hingston O
& O
Masek O
, O
2007 O
; O
Maˆıtrepierre O
et O
al O
. O

, O
2008 O
; O
Cazenave O
, O
2009 O
; O
Mehat O
& O
´ O
Cazenave O
, O
2010 O
; O
Gelly O
& O
Silver O
, O
2011 O
; O
Gelly O
et O
al O
. O

, O
2012 O
; O
Silver O
et O
al O
. O

, O
2016 O
) O
. O

The O
standard O
goal B
of O
MCTS O
is O
to O
recommend O
an O
action O
for O
the O
root O
node O
only O
. O

After O
the O
action O
is O
taken O
, O
the O
system O
moves B
forward O
and O
a O
new O
tree O
is O
created O
from O
the O
next O
state O
( O
statistics O
from O
the O
old O
tree O
may O
be O
partially O
saved O
or O
completely O
discarded O
) O
. O

MCTS O
is O
thus O
a O
“ O
local O
” O
procedure O
( O
in O
that O
it O
only O
returns O
an O
action O
for O
a O
given O
state O
) O
and O
is O
inherently O
different O
from O
value O
function O
approximation O
or O
policy O
function O
approximation O
approaches O
where O
a O
“ O
global O
” O
policy O
( O
one O
that O
contains O
policy O
information O
about O
all O
states O
) O
is O
built O
. O

In B
real O
- I
time I
decision O
- O
making O
applications O
, O
it O
is O
more O
difficult O
to O
build O
an O
adequate O
“ O
on O
- O
the O
- O
fly O
” O
local O
approximation O
than O
it O
is O
to O
use O
pre O
- O
trained B
global O
policy O
in O
the O
short O
amount O
of O
time B
available O
for O
decision B
- I
making I
. O

For O
games B
like O
Chess O
or O
Go O
, O
online O
planning O
using O
MCTS O
may O
be O
appropriate O
, O
but O
in O
games B
where O
fast O
decisions O
are O
necessary O
( O
e.g. O
, O
Atari O
or O
MOBA O
video B
games I
) O
, O
tree O
search O
methods O
are O
too O
slow B
( O
Guo O
et O
al O
. O

, O
2014 O
) O
. O

The O
proposed O
algorithm O
is O
intended O
to O
be O
used O
in O
an O
off O
- O
policy O
fashion O
during O
the O
reinforcement O
learning O
( O
RL O
) O
training O
phase O
. O

Once O
the O
training O
is O
complete O
, O
the O
policies O
associated O
with O
leafnode O
evaluation O
can O
be O
implemented O
to O
make O
fast O
, O
real B
- I
time I
decisions O
without O
any O
further O
need O
for O
tree O
search O
. O

Main O
Contributions O
. O

These O
characteristics O
of O
MCTS O
motivate O
our O
proposed O
method O
, O
which O
attempts O
to O
leverage O
the O
arXiv:1805.05935v1 O
[ O
cs O
. O

AI O
] O
15 O
May O
2018 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
local O
properties O
of O
MCTS O
into O
a O
training O
procedure O
to O
iteratively O
build O
global O
policy O
across O
all O
states O
. O

The O
idea O
is O
to O
apply O
MCTS O
on O
batches O
of O
small O
, O
finite O
- O
horizon O
versions O
of O
the O
original O
infinite O
- O
horizon O
Markov O
decision O
process O
( O
MDP O
) O
. O

A O
rough O
summary O
is O
as O
follows O
: O
( O
1 O
) O
initialize O
an O
arbitrary O
value O
function O
and O
a O
policy O
function O
; O
( O
2 O
) O
start B
( O
possibly O
in O
parallel O
) O
a O
batch O
of O
MCTS O
instances O
, O
limited O
in O
search O
- O
depth O
, O
initialized O
from O
a O
set O
of O
sampled O
states O
, O
while O
incorporating O
a O
combination O
of O
the O
value O
and O
policy O
function O
as O
leaf O
- O
node O
evaluators O
; O
( O
3 O
) O
update B
both O
the O
value O
and O
policy O
functions O
using O
the O
latest O
MCTS O
root O
node O
observations O
; O
( O
4 O
) O
Repeat O
starting B
from O
step O
( O
2 O
) O
. O

This O
method O
exploits O
the O
idea O
that O
an O
MCTS O
policy O
is O
better O
than O
either O
of O
the O
leaf O
- O
node O
evaluator O
policies O
alone O
( O
Silver O
et O
al O
. O

, O
2016 O
) O
, O
yet O
improved O
leaf O
- O
node O
evaluators O
also O
improve O
the O
quality O
of O
MCTS O
( O
Gelly O
& O
Silver O
, O
2007 O
) O
. O

The O
primary O
contributions O
of O
this O
paper O
are O
summarized O
below O
. O

1 O
. O

We O
propose O
a O
batch O
, O
MCTS O
- O
based B
RL O
method O
that O
operates O
on O
continuous O
state O
, O
finite O
action O
MDPs O
and O
exploits O
the O
idea O
that O
leaf O
- O
evaluators O
can O
be O
updated B
to O
produce O
a O
stronger O
tree O
search O
using O
previous O
tree O
search O
results O
. O

Function O
approximators O
are O
used O
to O
track O
policy O
and O
value O
function O
approximations O
, O
where O
the O
latter O
is O
used O
to O
reduce O
the O
length O
of O
the O
tree O
search O
rollout O
( O
oftentimes O
, O
the O
rollout O
of O
the O
policy O
becomes O
a O
computational O
bottle O
- O
neck O
in O
complex O
environments O
) O
. O

2 O
. O

We O
provide O
a O
full B
sample O
complexity O
analysis O
of O
the O
method O
and O
show O
that O
with O
large O
enough O
sample O
sizes O
and O
sufficiently O
large O
tree O
search O
effort O
, O
the O
performance O
of O
the O
estimated O
policies O
can O
be O
made O
close B
to O
optimal O
, O
up O
to O
some O
unavoidable O
approximation O
error O
. O

To O
our O
knowledge O
, O
batch O
MCTS O
- O
based B
RL O
methods O
have O
not O
been O
theoretically O
analyzed O
. O

3 O
. O

An O
implementation O
of O
the O
feedback O
- O
based B
tree O
search O
algorithm O
using O
deep O
neural O
networks O
is O
tested O
on O
the O
recently O
popular O
MOBA O
game B
King O
of O
Glory O
( O
a O
North O
American O
version O
of O
the O
same O
game B
is O
titled O
Arena O
of O
Valor O
) O
. O

The O
result O
is O
a O
competitive B
AI O
agent O
for O
the O
1v1 O
mode O
of O
the O
game B
. O

2 O
. O

Related O
Work O
The O
idea O
of O
leveraging O
tree O
search O
during O
training O
was O
first O
explored O
by O
Guo O
et O
al O
. O

( O
2014 O
) O
in O
the O
context O
of O
Atari O
games B
, O
where O
MCTS O
was O
used O
to O
generate O
offline B
training O
data O
for O
a O
supervised O
learning O
( O
classification O
) O
procedure O
. O

The O
authors O
showed O
that O
by O
using O
the O
power B
of O
tree O
search O
offline B
, O
the O
resulting O
policy O
was O
able O
to O
outperform O
the O
deep O
Q O
- O
network O
( O
DQN O
) O
approach O
of O
( O
Mnih O
et O
al O
. O

, O
2013 O
) O
. O

A O
natural O
next O
step O
is O
to O
repeatedly O
apply O
the O
procedure O
of O
Guo O
et O
al O
. O

( O
2014 O
) O
. O

In O
building O
AlphaGo O
Zero O
, O
Silver O
et O
al O
. O

( O
2017 O
) O
extends O
the O
ideas O
of O
Guo O
et O
al O
. O

( O
2014 O
) O
into O
an O
iterative O
procedure O
, O
where O
the O
neural O
network O
policy O
is O
updated B
after O
every O
episode O
and O
then O
reincorporated O
into O
tree O
search O
. O

The O
technique B
was O
able O
to O
produce O
a O
superhuman O
Go O
- O
playing B
AI O
( O
and O
improves O
upon O
the O
previous O
AlphaGo O
versions O
) O
without O
any O
human O
replay O
data O
. O

Our O
proposed O
algorithm O
is O
a O
provably O
near O
- O
optimal O
variant O
( O
and O
in O
some O
respects O
, O
generalization O
) O
of O
the O
AlphaGo O
Zero O
algorithm O
. O

The O
key O
differences O
are O
the O
following O
: O
( O
1 O
) O
our O
theoretical O
results O
cover O
a O
continuous O
, O
rather O
than O
finite O
, O
state O
space O
setting O
, O
( O
2 O
) O
the O
environment O
is O
a O
stochastic O
MDP O
rather O
than O
a O
sequential O
deterministic O
two O
player B
game B
, O
( O
3 O
) O
we O
use O
batch O
updates B
, O
( O
4 O
) O
the O
feedback O
of O
previous O
results O
to O
the O
leaf O
- O
evaluator O
manifests O
as O
both O
policy O
and O
value O
updates B
rather O
than O
just O
the O
value O
( O
as O
Silver O
et O
al O
. O

( O
2017 O
) O
does O
not O
use O
policy O
rollouts O
) O
. O

Anthony O
et O
al O
. O

( O
2017 O
) O
proposes O
a O
general O
framework O
called B
expert O
iteration O
that O
combines O
supervised O
learning O
with O
tree O
search O
- O
based B
planning O
. O

The O
methods O
described O
in O
Guo O
et O
al O
. O

( O
2014 O
) O
, O
Silver O
et O
al O
. O

( O
2017 O
) O
, O
and O
the O
current O
paper O
can O
all O
be O
( O
at O
least O
loosely O
) O
expressed O
under O
the O
expert O
iteration O
framework O
. O

However O
, O
no O
theoretical O
insights O
were O
given O
in O
any O
of O
these O
previous O
works O
and O
our O
paper O
intends O
to O
fill O
this O
gap O
by O
providing O
a O
full B
theoretical O
analysis O
of O
an O
iterative O
, O
MCTS O
- O
based B
RL O
algorithm O
. O

Our O
analysis O
relies O
on O
the O
concentrability O
coefficient O
idea O
of O
Munos O
( O
2007 O
) O
for O
approximate O
value O
iteration O
and O
builds O
upon O
the O
work O
on O
classification O
based B
policy O
iteration O
( O
Lazaric O
et O
al O
. O

, O
2016 O
) O
, O
approximate O
modified O
policy O
iteration O
( O
Scherrer O
et O
al O
. O

, O
2015 O
) O
, O
and O
fitted O
value O
iteration O
( O
Munos O
& O
Szepesvari O
´ O
, O
2008 O
) O
. O

Sample O
complexity O
results O
for O
MCTS O
are O
relatively O
sparse O
. O

Teraoka O
et O
al O
. O

( O
2014 O
) O
gives O
a O
high O
probability O
upper O
bound O
on O
the O
number O
of O
playouts O
needed O
to O
achieve O
-accuracy O
at O
the O
root O
node O
for O
a O
stylized O
version O
of O
MCTS O
called B
FindTopWinner O
. O

More O
recently O
, O
Kaufmann O
& O
Koolen O
( O
2017 O
) O
provided O
high O
probability O
bounds O
on O
the O
sample O
complexity O
of O
two O
other O
variants O
of O
MCTS O
called B
UGapEMCTS O
and O
LUCB O
- O
MCTS O
. O

In O
this O
paper O
, O
we O
do O
not O
require O
any O
particular O
implementation O
of O
MCTS O
, O
but O
make O
a O
generic O
assumption O
on O
its O
accuracy O
that O
is O
inspired O
by O
these O
results O
. O

3 O
. O

Problem O
Formulation O
Consider O
a O
discounted O
, O
infinite O
- O
horizon O
MDP O
with O
a O
continuous O
state O
space O
S O
and O
finite O
action O
space O
A. O
For O
all O
( O
s O
, O
a O
) O
∈ O
S O
×A O
, O
the O
reward B
function O
r O
: O
S O
×A O
→ O
R O
satisfies O
r(s O
, O
a O
) O
∈ O
[ O
0 O
, O
Rmax O
] O
. O

The O
transition O
kernel O
, O
which O
describes O
transitions O
to O
the O
next O
state O
given O
current O
state O
s O
and O
action O
a O
, O
is O
written O
p(·|s O
, O
a O
) O
— O
a O
probability O
measure O
over O
S. O
Given O
a O
discount O
factor O
γ O
∈ O
[ O
0 O
, O
1 O
) O
, O
the O
value O
function O
V O
π O
of O
a O
policy O
π O
: O
S O
→ O
A O
starting O
in O
s O
= O
s0 O
∈ O
S O
is O
given O
by O
V O
π O
( O
s O
) O
= O
E O
" O
X∞ O
t=0 O
γ O
t O
r(st O
, O
πt(st O
) O
) O
# O
, O
( O
1 O
) O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
where O
st O
is O
the O
state O
visited O
at O
time B
t. O
Let O
Π O
be O
the O
set O
of O
all O
stationary O
, O
deterministic O
policies O
( O
i.e. O
, O
mappings O
from O
state O
to O
action O
) O
. O

The O
optimal O
value O
function O
is O
obtained O
by O
maximizing O
over O
all O
policies O
: O
V O
∗ O
( O
s O
) O
= O
supπ∈Π O
V O
π O
( O
s O
) O
. O

Both O
V O
π O
and O
V O
∗ O
are O
bounded O
by O
Vmax O
= O
Rmax/(1−γ O
) O
. O

We O
let O
F O
be O
the O
set O
of O
bounded O
, O
real O
- O
valued O
functions O
mapping O
S O
to O
[ O
0 O
, O
Vmax O
] O
. O

We O
frequently O
make O
use O
of O
the O
shorthand O
operator O
Tπ O
: O
F O
→ O
F O
, O
where O
the O
quantity O
( O
TπV O
) O
( O
s O
) O
is O
be O
interpreted O
as O
the O
reward B
gained O
by O
taking O
an O
action O
according O
to O
π O
, O
receiving O
the O
reward B
r(s O
, O
π(s O
) O
) O
, O
and O
then O
receiving O
an O
expected O
terminal O
reward B
according O
to O
the O
argument O
V O
: O
( O
TπV O
) O
( O
s O
) O
= O
r(s O
, O
π(s O
) O
) O
+ O
γ O
Z O
S O
V O
( O
˜s O
) O
p(ds˜|s O
, O
π(s O
) O
) O
. O

It O
is O
well O
- O
known O
that O
V O
π O
is O
the O
unique O
fixed O
- O
point B
of O
Tπ O
, O
meaning O
TπV O
π O
= O
V O
π O
( O
Puterman O
, O
2014 O
) O
. O

The O
Bellman O
operator O
T O
: O
F O
→ O
F O
is O
similarly O
defined O
using O
the O
maximizing O
action O
: O
( O
T O
V O
) O
( O
s O
) O
= O
max O
a∈A O
h O
r(s O
, O
a O
) O
+ O
γ O
Z O
S O
V O
( O
˜s O
) O
p(ds˜|s O
, O
a O
) O
i O
. O

It O
is O
also O
known O
that O
V O
∗ O
is O
the O
unique O
fixed O
- O
point B
of O
T O
( O
Puterman O
, O
2014 O
) O
and O
that O
acting O
greedily O
with O
respect O
to O
the O
optimal O
value O
function O
V O
∗ O
produces O
an O
optimal O
policy O
: O
π O
∗ O
( O
s O
) O
∈ O
arg O
max O
a∈A O
h O
r(s O
, O
a O
) O
+ O
γ O
Z O
S O
V O
∗ O
( O
˜s O
) O
p(ds˜|s O
, O
a O
) O
i O
. O

We O
use O
the O
notation O
T O
d O
to O
mean O
the O
d O
compositions O
of O
the O
mapping O
T O
, O
e.g. O
, O
T O
2V O
= O
T(T O
V O
) O
. O

Lastly O
, O
let O
V O
∈ O
F O
and O
let O
ν O
be O
a O
distribution O
over O
S. O
We O
define O
left O
and O
right O
versions O
of O
an O
operator O
Pπ O
: O
( O
PπV O
) O
( O
s O
) O
= O
Z O
S O
V O
( O
˜s O
) O
p(ds˜|s O
, O
π(s O
) O
) O
, O
( O
νPπ)(ds˜ O
) O
= O
Z O
S O
p(ds˜|s O
, O
π(s O
) O
) O
ν(ds O
) O
. O

Note O
that O
PπV O
∈ O
F O
and O
µPπ O
is O
another O
distribution O
over O
S. O
4 O
. O

Feedback O
- O
Based B
Tree O
Search O
Algorithm O
We O
now O
formally O
describe O
the O
proposed O
algorithm O
. O

The O
parameters O
are O
as O
follows O
. O

Let O
Π¯ O
⊆ O
Π O
be O
a O
space O
of O
approximate O
policies O
and O
F O
⊆ O
F O
¯ O
be O
a O
space O
of O
approximate O
value O
functions O
( O
e.g. O
, O
classes B
of O
neural O
network O
architectures O
) O
. O

We O
let O
πk O
∈ O
Π¯ O
be O
the O
policy O
function O
approximation O
( O
PFA O
) O
and O
Vk O
∈ O
F¯ O
be O
the O
value O
function O
approximation O
( O
VFA O
) O
at O
iteration O
k O
of O
the O
algorithm O
. O

Parameters O
subscripted O
with O
‘ O
0 O
’ O
are O
used O
in O
the O
value O
function O
approximation O
( O
regression O
) O
phase O
and O
parameters O
subscripted O
with O
‘ O
1 O
’ O
are O
used O
in O
the O
tree O
search O
phase O
. O

The O
full B
description O
of O
the O
procedure O
is O
given O
in O
Figure O
1 O
, O
using O
the O
notation O
Ta O
= O
Tπa O
, O
where O
πa O
maps O
all O
states O
to O
the O
action O
a O
∈ O
A. O
We O
now O
summarize O
the O
two O
phases O
, O
VFA O
( O
Steps O
2 O
and O
3 O
) O
and O
MCTS O
( O
Steps O
4 O
, O
5 O
, O
and O
6 O
) O
. O

VFA O
Phase O
. O

Given O
a O
policy O
πk O
, O
we O
wish O
to O
approximate O
its O
value O
by O
fitting O
a O
function O
using O
subroutine O
Regress O
on O
N0 O
states O
sampled O
from O
a O
distribution O
ρ0 O
. O

Each O
call B
to O
MCTS O
requires O
repeatedly O
performing O
rollouts O
that O
are O
initiated O
from O
leaf O
- O
nodes O
of O
the O
decision O
tree O
. O

Because O
repeating O
full B
rollouts O
during O
tree O
search O
is O
expensive O
, O
the O
idea O
is O
that O
a O
VFA O
obtained O
from O
a O
one O
- O
time B
regression O
on O
a O
single O
set B

 O
of I
rollouts I
can O
drastically O
reduce O
the O
computation O
needed O
for O
MCTS O
. O

For O
each O
sampled O
state O
s O
, O
we O
estimate O
its O
value O
using O
M0 O
full B
rollouts O
, O
which O
can O
be O
obtained O
using O
the O
absorption O
time B
formulation O
of O
an O
infinite O
horizon O
MDP O
( O
Puterman O
, O
2014 O
, O
Proposition O
5.3.1 O
) O
. O

MCTS O
Phase O
. O

On O
every O
iteration O
k O
, O
we O
sample O
a O
set O
of O
N1 O
i.i.d O
. O

states O
from O
a O
distribution O
ρ1 O
over O
S. O
From O
each O
state O
, O
a O
tree O
search O
algorithm O
, O
denoted O
MCTS O
, O
is O
executed O
for O
M1 O
iterations O
on O
a O
search O
tree O
of O
maximum O
depth O
d. O
We O
assume O
here O
that O
the O
leaf O
evaluator O
is O
a O
general O
function O
of O
the O
PFA O
and O
VFA O
from O
the O
previous O
iteration O
, O
πk O
and O
Vk O
, O
and O
it O
is O
denoted O
as O
a O
“ O
subroutine O
” O
LeafEval O
. O

The O
results O
of O
the O
MCTS O
procedure O
are O
piped O
into O
a O
subroutine O
Classify O
, O
which O
fits O
a O
new O
policy O
πk+1 O
using O
classification O
( O
from O
continuous O
states O
to O
discrete O
actions O
) O
on O
the O
new O
data O
. O

As O
discussed O
more O
in O
Assumption O
4 O
, O
Classify O
uses O
L1 O
observations O
( O
one O
- O
step O
rollouts O
) O
to O
compute O
a O
loss O
function O
. O

1 O
. O

Sample O
a O
set O
of O
N0 O
i.i.d O
. O

states O
S0,k O
from O
ρ0 O
and O
N1 O
i.i.d O
. O

states O
S1,k O
from O
ρ1 O
. O

2 O
. O

Compute O
a O
sample O
average O
Yˆk(s O
) O
of O
M0 O
independent O
rollouts O
of O
πk O
for O
each O
s O
∈ O
S0,k O
. O

See O
Assumption O
1 O
. O

3 O
. O

Use O
Regress O
on O
the O
set O
{ O
Yˆk(s O
) O
: O
s O
∈ O
S0,k O
} O
to O
obtain O
a O
value O
function O
Vk O
∈ O
F¯. O
See O
Assumption O
1 O
. O

4 O
. O

From O
each O
s O
∈ O
S1,k O
, O
run O
MCTS O
with O
parameters O
M1 O
, O
d O
, O
and O
evaluator O
LeafEval O
. O

Return O
estimated O
value O
of O
each O
s O
, O
denoted O
Uˆk(s O
) O
. O

See O
Assumptions O
2 O
and O
3 O
. O

5 O
. O

For O
each O
s O
∈ O
S1,k O
and O
a O
∈ O
A O
, O
create O
estimate O
Qˆk(s O
, O
a O
) O
≈ O
( O
Ta O
Vk)(s O
) O
by O
averaging O
L1 O
transitions O
from O
p(·|s O
, O
a O
) O
. O

See O
Assumption O
4 O
. O

6 O
. O

Use O
Classify O
to O
solve O
a O
cost B
- O
sensitive O
classification O
problem O
and O
obtain O
the O
next O
policy O
πk+1 O
∈ O
Π¯ O
. O

Costs B
are O
measured O
using O
{ O
Uˆk(s O
) O
: O
s O
∈ O
S1,k O
} O
and O
{ O
Qˆk(s O
, O
πk+1(s O
) O
) O
: O
s O
∈ O
S1,k O
} O
. O

See O
Assumption O
4 O
. O

Increment O
k O
and O
return O
to O
Step O
1 O
. O

Figure O
1 O
. O

Feedback O
- O
Based B
Tree O
Search O
Algorithm O
The O
illustration O
given O
in O
Figure O
2 O
shows O
the O
interactions O
( O
and O
feedback O
loop O
) O
of O
the O
basic O
components O
of O
the O
algorithm O
: O
( O
1 O
) O
a O
set O
of O
tree O
search O
runs O
initiated O
from O
a O
batch O
of O
sampled O
states O
( O
triangles O
) O
, O
( O
2 O
) O
leaf O
evaluation O
using O
πk O
and O
Vk O
is O
used O
during O
tree O
search O
, O
and O
( O
3 O
) O
updated B
PFA O
and O
VFA O
πk+1 O
and O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
S O
s O
1 O
s O
2 O
s O
N1 O
s O
· O
· O
· O
3 O
leaf O
evaluation O
update B
πk O
and O
Vk O
πk+1 O
and O
Vk+1 O
tree O
search O
Figure O
2 O
. O

Illustration O
of O
the O
Feedback O
Loop O
Vk+1 O
using O
tree O
search O
results O
. O

5 O
. O

Assumptions O
Figure O
1 O
shows O
the O
algorithm O
written O
with O
general O
subroutines O
Regress O
, O
MCTS O
, O
LeafEval O
, O
and O
Classify O
, O
allowing O
for O
variations O
in O
implementation O
suited O
for O
different O
problems O
. O

However O
, O
our O
analysis O
assumes O
specific O
choices O
and O
properties O
of O
these O
subroutines O
, O
which O
we O
describe O
now O
. O

The O
regression O
step O
solves O
a O
least O
absolute O
deviation O
problem O
to O
minimize O
an O
empirical O
version O
of O
kf O
− O
V O
πk O
k1 O
, O
ρ0 O
= O
Z O
S O
|f(s O
) O
− O
V O
πk O
( O
s)|ρ0(ds O
) O
, O
as O
described O
in O
the O
first O
assumption O
. O

Assumption O
1 O
( O
Regress O
Subroutine O
) O
. O

For O
each O
s O
i O
∈ O
S0,k O
, O
define O
s O
i O
= O
s O
ij O
0 O
for O
all O
j O
and O
for O
each O
t O
, O
the O
state O
s O
ij O
t+1 O
is O
drawn O
from O
p(·|s O
ij O
t O
, O
πk(s O
ij O
t O
) O
) O
. O

Let O
Yˆ O
k(s O
i O
) O
be O
an O
estimate O
of O
V O
πk O
( O
s O
i O
) O
using O
M0 O
rollouts O
and O
Vk O
, O
the O
VFA O
resulting O
from O
Regress O
, O
obtained O
via O
least O
absolute O
deviation O
regression O
: O
Yˆ O
k(s O
i O
0 O
) O
= O
1 O
M0 O
X O
M0 O
j=1 O
X∞ O
t=0 O
γ O
t O
r(s O
ij O
t O
, O
πk(s O
ij O
t O
) O
) O
, O
( O
2 O
) O
Vk O
∈ O
arg O
min O
f∈F¯ O
1 O
N0 O
X O
N0 O
i=1 O


 O
f(s O
i O
) O
− O
Yˆ O
k(s O
i O
) O



 O
. O

( O
3 O
) O
There O
are O
many O
ways O
that O
LeafEval O
may O
be O
defined O
. O

The O
standard O
leaf O
evaluator O
for O
MCTS O
is O
to O
simulate O
a O
default O
or O
“ O
rollout O
” O
policy O
( O
Browne O
et O
al O
. O

, O
2012 O
) O
until O
the O
end O
of O
the O
game B
, O
though O
in O
related O
tree O
search O
techniques B
, O
authors O
have O
also O
opted O
for O
a O
value O
function O
approximation O
( O
Campbell O
et O
al O
. O

, O
2002 O
; O
Enzenberger O
, O
2004 O
) O
. O

It O
is O
also O
possible O
to O
combine O
the O
two O
approximations O
: O
Silver O
et O
al O
. O

( O
2016 O
) O
uses O
a O
weighted O
combination O
of O
a O
full B
rollout O
from O
a O
pre O
- O
trained O
policy O
and O
a O
pre O
- O
trained B
value O
function O
approximation O
. O

Assumption O
2 O
( O
LeafEval O
Subroutine O
) O
. O

Our O
approach O
uses O
a O
partial O
rollout O
of O
length O
h O
≥ O
0 O
and O
a O
value O
estimation O
at O
the O
end O
. O

LeafEval O
produces O
unbiased O
observations O
of O
Jk(s O
) O
= O
E O
" O
h O
X−1 O
t=0 O
γ O
t O
r(˜st O
, O
πk(˜st O
) O
) O
+ O
γ O
h O
Vk(˜sh O
) O
# O
, O
( O
4 O
) O
where O
s˜0 O
= O
s. O
Assumption O
2 O
is O
motivated O
by O
our O
MOBA O
game B
, O
on O
which O
we O
observed O
that O
even O
short O
rollouts O
( O
as O
opposed O
to O
simply O
using O
a O
VFA O
) O
are O
immensely O
helpful O
in O
determining O
local O
outcomes O
( O
e.g. O
, O
dodging O
attacks B
, O
eliminating O
minions B
, O
health O
regeneration O
) O
. O

At O
the O
same O
time B
, O
we O
found O
that O
numerous O
full B
rollouts O
simulated O
using O
the O
relatively O
slow B
and O
complex O
game B
engine O
is O
far O
too O
time B
- O
consuming O
within O
tree O
search O
. O

We O
also O
need O
to O
make O
an O
assumption O
on O
the O
sample O
complexity O
of O
MCTS O
, O
of O
which O
there O
are O
many O
possible O
variations O
( O
Chaslot O
et O
al O
. O

, O
2006 O
; O
Coulom O
, O
2006 O
; O
Kocsis O
& O
Szepesvari O
´ O
, O
2006 O
; O
Gelly O
& O
Silver O
, O
2007 O
; O
Couetoux O
et O
al O
. O

¨ O
, O
2011a;b O
; O
AlKanj O
et O
al O
. O

, O
2016 O
; O
Jiang O
et O
al O
. O

, O
2017 O
) O
. O

Particularly O
relevant O
to O
our O
continuous O
- O
state O
setting O
are O
tree O
expansion O
techniques B
called B
progressive O
widening O
and O
double O
progressive O
widening O
, O
proposed O
in O
Couetoux O
et O
al O
. O

¨ O
( O
2011a O
) O
, O
which O
have O
proven O
successful O
in O
problems O
with O
continuous O
state O
/ O
action O
spaces O
. O

To O
our O
knowledge O
, O
analysis O
of O
the O
sample O
complexity O
is O
only O
available O
for O
stylized O
versions O
of O
MCTS O
on O
finite O
problems O
, O
like O
Teraoka O
et O
al O
. O

( O
2014 O
) O
and O
Kaufmann O
& O
Koolen O
( O
2017 O
) O
. O

Theorems O
from O
these O
papers O
show O
upper O
bounds O
on O
the O
number O
of O
iterations O
needed O
so O
that O
with O
high O
probability O
( O
greater O
than O
1 O
− O
δ O
) O
, O
the O
value O
at O
the O
root O
node O
is O
accurate O
within O
a O
tolerance O
of O
. O
Fortunately O
, O
there O
are O
ways O
to O
discretize O
continuous O
state O
MDPs O
that O
enjoy O
error O
guarantees O
, O
such O
as O
Bertsekas O
( O
1975 O
) O
, O
Dufour O
& O
Prieto O
- O
Rumeau O
( O
2012 O
) O
, O
or O
Saldi O
et O
al O
. O

( O
2017 O
) O
. O

These O
error O
bounds O
can O
be O
combined O
with O
the O
MCTS O
guarantees O
of O
Teraoka O
et O
al O
. O

( O
2014 O
) O
and O
Kaufmann O
& O
Koolen O
( O
2017 O
) O
to O
produce O
a O
sample O
complexity O
bound O
for O
MCTS O
on O
continuous O
problems O
. O

The O
next O
assumption O
captures B
the O
essence O
of O
these O
results O
( O
and O
if O
desired O
, O
can O
be O
made O
precise O
for O
specific O
implementations O
through O
the O
references O
above O
) O
. O

Assumption O
3 O
( O
MCTS O
Subroutine O
) O
. O

Consider O
a O
d O
- O
stage B
, O
finite O
- O
horizon O
subproblem O
of O
( O
1 O
) O
with O
terminal O
value O
function O
J O
and O
initial O
state O
is O
s. O
Let O
the O
result O
of O
MCTS O
be O
denoted O
Uˆ(s O
) O
. O

We O
assume O
that O
there O
exists O
a O
function O
m( O
, O
δ O
) O
, O
such O
that O
if O
m( O
, O
δ O
) O
iterations O
of O
MCTS O
are O
used O
, O
the O
inequality O
|Uˆ(s)−(T O
d O
J)(s)| O
≤ O
 O
holds O
with O
probability O
at O
least O
1−δ O
. O

Now O
, O
we O
are O
ready O
to O
discuss O
the O
Classify O
subroutine O
. O

Our O
goal B
is O
to O
select O
a O
policy O
π O
∈ O
Π¯ O
that O
closely O
mimics O
the O
performance O
of O
the O
MCTS O
result O
, O
similar O
to O
practical O
implementations O
in O
existing O
work O
( O
Guo O
et O
al O
. O

, O
2014 O
; O
Silver O
et O
al O
. O

, O
2017 O
; O
Anthony O
et O
al O
. O

, O
2017 O
) O
. O

The O
question O
is O
: O
given O
a O
candidate O
π O
, O
how O
do O
we O
measure O
“ O
closeness O
” O
to O
the O
MCTS O
policy O
? O
We O
take O
inspiration O
from O
previous O
work O
in O
classificationbased O
RL O
and O
use O
a O
cost B
- O
based B
penalization O
of O
classification O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
errors O
( O
Langford O
& O
Zadrozny O
, O
2005 O
; O
Li O
et O
al O
. O

, O
2007 O
; O
Lazaric O
et O
al O
. O

, O
2016 O
) O
. O

Since O
Uˆ(s O
i O
) O
is O
an O
approximation O
of O
the O
performance O
of O
the O
MCTS O
policy O
, O
we O
should O
try O
to O
select O
a O
policy O
π O
with O
similar O
performance O
. O

To O
estimate O
the O
performance O
of O
some O
candidate O
policy O
π O
, O
we O
use O
a O
one O
- O
step O
rollout O
and O
evaluate O
the O
downstream O
cost B
using O
Vk O
. O

Assumption O
4 O
( O
Classify O
Subroutine O
) O
. O

For O
each O
s O
i O
∈ O
S1,k O
and O
a O
∈ O
A O
, O
let O
Qˆ O
k(s O
i O
, O
a O
) O
be O
an O
estimate O
of O
the O
value O
of O
stateaction O
pair O
( O
s O
i O
, O
a O
) O
using O
L1 O
samples O
. O

Qˆ O
k(s O
i O
, O
a O
) O
= O
1 O
L1 O
X O
L1 O
j=1 O


 O
r(s O
i O
, O
a O
) O
+ O
γ O
Vk(˜s O
j O
( O
a O
) O
) O
. O

Let O
πk+1 O
, O
the O
result O
of O
Classify O
, O
be O
obtained O
by O
minimizing O
the O
discrepancy O
between O
the O
MCTS O
result O
Uˆ O
k O
and O
the O
estimated O
value O
of O
the O
policy O
under O
approximations O
Qˆ O
k O
: O
πk+1 O
∈ O
arg O
min O
π∈Π¯ O
1 O
N1 O
X O
N1 O
i=1 O


 O
Uˆ O
k(s O
i O
) O
− O
Qˆ O
k(s O
i O
, O
π(s O
i O
) O
) O



 O
, O
where O
s˜ O
j O
( O
a O
) O
are O
i.i.d O
. O

samples O
from O
p O
( O
· O
| O
s O
i O
, O
a O
) O
. O

An O
issue O
that O
arises O
during O
the O
analysis O
is O
that O
even O
though O
we O
can O
control B
the O
distribution O
from O
which O
states O
are O
sampled O
, O
this O
distribution O
is O
transformed O
by O
the O
transition O
kernel O
of O
the O
policies O
used O
for O
rollout O
/ O
lookahead O
. O

Let O
us O
now O
introduce O
the O
concentrability O
coefficient O
idea O
of O
Munos O
( O
2007 O
) O
( O
and O
used O
subsequently O
by O
many O
authors O
, O
including O
Munos O
& O
Szepesvari O
´ O
( O
2008 O
) O
, O
Lazaric O
et O
al O
. O

( O
2016 O
) O
, O
Scherrer O
et O
al O
. O

( O
2015 O
) O
, O
and O
Haskell O
et O
al O
. O

( O
2016 O
) O
) O
. O

Assumption O
5 O
( O
Concentrability O
) O
. O

Consider O
any O
sequence O
of O
m O
policies O
µ1 O
, O
µ2 O
, O
. O

. O

. O

, O
µm O
∈ O
Π. O
Suppose O
we O
start B
in O
distribution O
ν O
and O
that O
the O
state O
distribution O
attained O
after O
applying O
the O
m O
policies O
in O
succession O
, O
ν O
Pµ1 O
Pµ2 O
· O
· O
· O
Pµm O
, O
is O
absolutely O
continuous O
with O
respect O
to O
ρ1 O
. O

We O
define O
an O
m O
- O
step O
concentrability O
coefficient O
Am O
= O
sup O
µ1 O
, O
... O
, O
µm O









 O
dνPµ1 O
Pµ2 O
· O
· O
· O
Pµm O
dρ1 O









 O
∞ O
, O
and O
assume O
that O
P∞ O
i O
, O
j=0 O
γ O
i+j O
Ai+j O
< O
∞. O
Similarly O
, O
we O
assume O
ρ1Pµ1 O
Pµ2 O
· O
· O
· O
Pµm O
, O
is O
absolutely O
continuous O
with O
respect O
to O
ρ0 O
and O
assume O
that O
A O
0 O
m O
= O
sup O
µ1 O
, O
... O
, O
µm O









 O
dρ1Pµ1 O
Pµ2 O
· O
· O
· O
Pµm O
dρ0 O









 O
∞ O
is O
finite O
for O
any O
m. O
The O
concentrability O
coefficient O
describes O
how O
the O
state O
distribution O
changes O
after O
m O
steps O
of O
arbitrary O
policies O
and O
how O
it O
relates O
to O
a O
given O
reference O
distribution O
. O

Assumptions O
1 O
- O
5 O
are O
used O
for O
the O
remainder O
of O
the O
paper O
. O

6 O
. O

Sample O
Complexity O
Analysis O
Before O
presenting O
the O
sample O
complexity O
analysis O
, O
let O
us O
consider O
an O
algorithm O
that O
generates O
a O
sequence O
of O
policies O
{ O
π0 O
, O
π1 O
, O
π2 O
, O
. O

. O

. O

} O
satisfying O
Tπk+1 O
T O
d−1V O
πk O
= O
T O
d O
V O
πk O
with O
no O
error O
. O

It O
is O
proved O
in O
Bertsekas O
& O
Tsitsiklis O
( O
1996 O
, O
pp O
. O

30 O
- O
31 O
) O
that O
πk O
→ O
π O
∗ O
in O
the O
finite O
state O
and O
action O
setting O
. O

Our O
proposed O
algorithm O
in O
Figure O
1 O
can O
be O
viewed O
as O
approximately O
satisfying O
this O
iteration O
in O
a O
continuous O
state O
space O
setting O
, O
where O
MCTS O
plays B
the O
role B
of O
T O
d O
and O
evaluation O
of O
πk O
uses O
a O
combination O
of O
accurate O
rollouts O
( O
due O
to O
Classify O
) O
and O
fast O
VFA O
evaluations O
( O
due O
to O
Regress O
) O
. O

The O
sample O
complexity O
analysis O
requires O
the O
effects B
of O
all O
errors O
to O
be O
systematically O
analyzed O
. O

For O
some O
K O
≥ O
0 O
, O
our O
goal B
is O
to O
develop O
a O
high O
probability O
upper O
bound O
on O
the O
expected O
suboptimality O
, O
over O
an O
initial O
state O
distribution O
ν O
, O
of O
the O
performance O
of O
policy O
πK O
, O
written O
as O
kV O
∗ O
− O
V O
πK O
k1,ν O
. O

Because O
there O
is O
no O
requirement O
to O
control B
errors O
with O
probability O
one O
, O
bounds O
in O
k O
· O
k1,ν O
tend O
to O
be O
much O
more O
useful O
in O
practice O
than O
ones O
in O
the O
traditional O
k O
· O
k∞. O
Notice O
that O
: O
1 O
N1 O
X O
N1 O
i=1 O


 O
Uˆ O
k(s O
i O
) O
− O
Qˆ O
k(s O
i O
, O
πk+1(s O
i O
) O
) O



 O
≈ O




 O
T O
d O
V O
πk O
− O
Tπk+1 O
V O
πk O





 O
1,ρ1 O
, O
( O
5 O
) O
where O
the O
left O
- O
hand O
- O
side O
is O
the O
loss O
function O
used O
in O
the O
classification O
step O
from O
Assumption O
4 O
. O

It O
turns B
out O
that O
we O
can O
relate O
the O
right O
- O
hand O
- O
side O
( O
albeit O
under O
a O
different O
distribution O
) O
to O
the O
expected O
suboptimality O
after O
K O
iterations O
kV O
∗ O
− O
V O
πK O
k1,ν O
, O
as O
shown O
in O
the O
following O
lemma O
. O

Full B
proofs O
of O
all O
results O
are O
given O
in O
the O
supplementary O
material O
. O

Lemma O
1 O
( O
Loss O
to O
Performance O
Relationship O
) O
. O

The O
expected O
suboptimality O
of O
πK O
can O
be O
bounded O
as O
follows O
: O
kV O
∗−V O
πK O
k1,ν O
≤ O
γ O
K O
d O
kV O
∗ O
− O
V O
π0 O
k∞ O
+ O
X O
K O
k=1 O
γ O
( O
K−k)d O




 O
T O
d O
V O
πk−1 O
− O
Tπk O
V O
πk−1 O





 O
1,Λν O
, O
k O
where O
Λν O
, O
k O
= O
ν O
( O
Pπ∗ O
) O
( O
K−k)d O


 O
I O
− O
( O
γPπk O
) O
−1 O
. O

From O
Lemma O
1 O
, O
we O
see O
that O
the O
expected O
suboptimality O
at O
iteration O
K O
can O
be O
upper O
bounded O
by O
the O
suboptimality O
of O
the O
initial O
policy O
π0 O
( O
in O
maximum O
norm O
) O
plus O
a O
discounted O
and O
re O
- O
weighted O
version O
of O
kT O
d O
V O
πk−1 O
− O
Tπk O
V O
πk−1 O
k1,ρ1 O
accumulated O
over O
prior O
iterations O
. O

Hypothetically O
, O
if O
( O
T O
d O
V O
πk−1 O
) O
( O
s O
) O
− O
( O
Tπk O
V O
πk−1 O
) O
( O
s O
) O
were O
small O
for O
all O
iterations O
k O
and O
all O
states O
s O
, O
then O
the O
suboptimality O
of O
πK O
converges O
linearly O
to O
zero O
. O

Hence O
, O
we O
may O
refer O
to O
kT O
d O
V O
πk−1 O
− O
Tπk O
V O
πk−1 O
k1,ρ1 O
as O
the O
“ O
true O
loss O
, O
” O
the O
target B
term O
to O
be O
minimized O
at O
iteration O
k. O
We O
now O
have O
a O
starting O
point B
for O
the O
analysis O
: O
if O
( O
5 O
) O
can O
be O
made O
precise O
, O
then O
the O
result O
can O
be O
combined O
with O
Lemma O
1 O
to O
provide O
an O
explicit O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
! O
! O
T O
d O
V O
πk O
− O
Tπk+1 O
V O
πk O
! O
! O
1 O
, O
ρ1 O
! O
! O
V O
k O
− O
V O
πk O
! O
! O
1 O
, O
ρ0 O
! O
! O
T O
dJk O
− O
Tπk+1 O
Vk O
! O
! O
1 O
, O
ρ1 O
state O
space O
sampling O
approximation O
over O
F¯ O
B O
′ O
γ O
min O
f∈F¯ O
! O
f O
− O
V O
πk O
! O
1 O
, O
ρ0 O
additional O
error O
ǫ O
min O
π∈Π¯ O
! O
T O
d O
V O
πk O
− O
Tπ O
V O
πk O
! O
1 O
, O
ρ1 O
state O
/ O
rollout O
sampling O
approximation O
over O
Π¯ O
“ O
true O
loss O
of O
πk+1 O
” O
tree O
search O
error O
Figure O
3 O
. O

Various O
Errors O
Analyzed O
in O
Lemma O
3 O
bound O
on O
kV O
∗ O
− O
V O
πK O
k1,ν O
. O

The O
various O
errors O
that O
we O
incur O
when O
relating O
the O
objective B
of O
Classify O
to O
the O
true O
loss O
include O
the O
error O
due O
to O
regression O
using O
functions O
in O
F¯ O
; O
the O
error O
due O
to O
sampling O
the O
state O
space O
according O
to O
ρ1 O
; O
the O
error O
of O
estimating O
( O
Tπ O
Vk)(s O
) O
using O
the O
sample O
average O
of O
one O
- O
step O
rollouts O
Qˆ O
k(s O
, O
π(s O
) O
) O
; O
and O
of O
course O
, O
the O
error O
due O
to O
MCTS O
. O

We O
now O
give O
a O
series O
of O
lemmas O
that O
help O
us O
carry B
out O
the O
analysis O
. O

In O
the O
algorithmic O
setting O
, O
the O
policy O
πk O
is O
a O
random O
quantity O
that O
depends O
on O
the O
samples O
collected O
in O
previous O
iterations O
; O
however O
, O
for O
simplicity O
, O
the O
lemmas O
that O
follow O
are O
stated O
from O
the O
perspective O
of O
a O
fixed O
policy O
µ O
or O
fixed O
value O
function O
approximation O
V O
rather O
than O
πk O
or O
Vk O
. O

Conditioning O
arguments O
will O
be O
used O
when O
invoking O
these O
lemmas O
( O
see O
supplementary O
material O
) O
. O

Lemma O
2 O
( O
Propagation O
of O
VFA O
Error O
) O
. O

Consider O
a O
policy O
µ O
∈ O
Π O
and O
value O
function O
V O
∈ O
F. O
Analogous O
to O
( O
4 O
) O
, O
let O
J O
= O
T O
h O
µ O
V O
. O

Then O
, O
under O
Assumption O
5 O
, O
we O
have O
the O
bounds O
: O
( O
a O
) O
supπ∈Π¯ O
kTπ O
V O
− O
Tπ O
V O
µk1,ρ1 O
≤ O
γ O
A0 O
1 O
kV O
− O
V O
µk1,ρ0 O
, O
( O
b O
) O
kT O
d O
J O
− O
T O
d O
V O
µk1,ρ1 O
≤ O
γ O
d+hA0 O
d+h O
kV O
− O
V O
µk1,ρ0 O
. O

The O
lemma O
above O
addresses O
the O
fact O
that O
instead O
of O
using O
V O
πk O
directly O
, O
Classify O
and O
MCTS O
only O
have O
access O
to O
the O
estimates O
Vk O
and O
Jk O
= O
T O
h O
πk O
Vk O
( O
h O
steps O
of O
rollout O
with O
an O
evaluation O
of O
Vk O
at O
the O
end O
) O
, O
respectively O
. O

Note O
that O
propagation O
of O
the O
error O
in O
Vk O
is O
discounted O
by O
γ O
or O
γ O
d+h O
and O
since O
the O
lemma O
converts O
between O
k O
· O
k1,ρ1 O
and O
k O
· O
k1,ρ0 O
, O
it O
is O
also O
impacted B
by O
the O
concentrability O
coefficients O
A0 O
1 O
and O
A0 O
d+h O
. O

Let O
dΠ¯ O
be O
the O
VC O
- O
dimension O
of O
the O
class B
of O
binary O
classifiers O
Π¯ O
and O
let O
dF¯ O
be O
the O
pseudo O
- O
dimension O
of O
the O
function O
class B
F¯. O
The O
VC O
- O
dimension O
is O
a O
measure O
of O
the O
capacity O
of O
Π¯ O
and O
the O
notion O
of O
a O
pseudo O
- O
dimension O
is O
a O
generalization O
of O
the O
VC O
- O
dimension O
to O
real O
- O
valued O
functions O
( O
see O
, O
e.g. O
, O
Pollard O
( O
1990 O
) O
, O
Haussler O
( O
1992 O
) O
, O
Mohri O
et O
al O
. O

( O
2012 O
) O
for O
definitions O
of O
both O
) O
. O

Similar O
to O
Lazaric O
et O
al O
. O

( O
2016 O
) O
and O
Scherrer O
et O
al O
. O

( O
2015 O
) O
, O
we O
will O
present O
results O
for O
the O
case O
of O
two O
actions O
, O
i.e. O
, O
|A| O
= O
2 O
. O

The O
extension O
to O
multiple O
actions O
is O
possible O
by O
performing O
an O
analysis O
along O
the O
lines O
of O
Lazaric O
et O
al O
. O

( O
2016 O
, O
Section O
6 O
) O
. O

We O
now O
quantify O
the O
error O
illustrated O
in O
Figure O
3 O
. O

Define O
the O
quantity O
B0 O
γ O
= O
γ O
A0 O
1 O
+ O
γ O
d+hA0 O
d+h O
, O
the O
sum O
of O
the O
coefficients O
from O
Lemma O
2 O
. O

Lemma O
3 O
. O

Suppose O
the O
regression O
sample O
size O
N0 O
is O
O O


 O
( O
VmaxB O
0 O
γ O
) O
2 O
 O
−2 O


 O
log(1 O
/ O
δ O
) O
+ O
dF¯ O
log(VmaxB O
0 O
γ/ O
) O
 O
and O
the O
sample O
size O
M0 O
, O
for O
estimating O
the O
regression O
targets B
, O
is O
O O


 O
( O
VmaxB O
0 O
γ O
) O
2 O
 O
−2 O


 O
log(N0 O
/ O
δ O
) O
 O
. O

Furthermore O
, O
there O
exist O
constants O
C1 O
, O
C2 O
, O
C3 O
, O
and O
C4 O
, O
such O
that O
if O
N1 O
and O
L1 O
are O
large O
enough O
to O
satisfy O
N1 O
≥ O
C1V O
2 O
max O
 O
−2 O


 O
log(C2 O
/ O
δ O
) O
+ O
dΠ¯ O
log(eN1 O
/ O
dΠ¯ O
) O


 O
, O
L1 O
≥ O
C1V O
2 O
max O
 O
−2 O


 O
log(C2N1 O
/ O
δ O
) O
+ O
dΠ¯ O
log(eL1 O
/ O
dΠ¯ O
) O


 O
, O
and O
if O
M1 O
≥ O
m(C3 O
 O
, O
C4 O
δ O
/ O
N1 O
) O
, O
then O
kT O
d O
V O
πk O
− O
Tπk+1 O
V O
πk O
k1,ρ1 O
≤ O
B O
0 O
γ O
min O
f∈F¯ O
kf O
− O
V O
πk O
k1,ρ0 O
+ O
min O
π∈Π¯ O
kT O
d O
V O
πk O
− O
Tπ O
V O
πk O
k1,ρ1 O
+ O
 O
with O
probability O
at O
least O
1 O
− O
δ O
. O

Sketch O
of O
Proof O
. O

By O
adding O
and O
subtracting O
terms O
, O
applying O
the O
triangle O
inequality O
, O
and O
invoking O
Lemma O
2 O
, O
we O
see O
that O
: O
kT O
d O
V O
πk O
− O
Tπk+1 O
V O
πk O
k1,ρ1 O
≤ O
B O
0 O
γ O
kVk O
− O
V O
πk O
k1,ρ0 O
+ O
kT O
dJk O
− O
Tπk+1 O
Vkk1,ρ1 O
, O
Here O
, O
the O
error O
is O
split B
into O
two O
terms O
. O

The O
first O
depends O
on O
the O
sample O
S0,k O
and O
the O
history O
through O
πk O
while O
the O
second O
term O
depends O
on O
the O
sample O
S1,k O
and O
the O
history O
through O
Vk O
. O

We O
can O
thus O
view O
πk O
as O
fixed O
when O
analyzing O
the O
first O
term O
and O
Vk O
as O
fixed O
when O
analyzing O
the O
second O
term O
( O
details O
in O
the O
supplementary O
material O
) O
. O

The O
first O
term O
kVk O
−V O
πk O
k1,ρ0 O
contributes O
the O
quantity O
minf∈F¯ O
kf O
− O
V O
πk O
k1,ρ0 O
in O
the O
final O
bound O
with O
additional O
estimation O
error O
contained O
within O
. O
The O
second O
term O
kT O
dJk−Tπk+1 O
Vkk1,ρ1 O
contributes O
the O
rest O
. O

See O
Figure O
3 O
for O
an O
illustration O
of O
the O
main O
proof O
steps O
. O

The O
first O
two O
terms O
on O
the O
right O
- O
hand O
- O
side O
are O
related O
to O
the O
approximation O
power B
of O
F¯ O
and O
Π¯ O
and O
can O
be O
considered O
unavoidable O
. O

We O
upper O
- O
bound O
these O
terms O
by O
maximizing O
over O
Π¯ O
, O
in O
effect B
removing O
the O
dependence O
on O
the O
random O
process O
πk O
in O
the O
analysis O
of O
the O
next O
theorem O
. O

We O
define O
: O
D0(Π¯ O
, O
F¯ O
) O
= O
max O
π∈Π¯ O
min O
f∈F¯ O
kf O
− O
V O
π O
k1,ρ0 O
, O
D O
d O
1 O
( O
Π O
) O
= O
max O
¯ O
π∈Π¯ O
min O
π0∈Π¯ O
kT O
d O
V O
π O
− O
Tπ0 O
V O
π O
k1,ρ1 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
Figure O
4 O
. O

Screenshot B
from O
1v1 O
King O
of O
Glory O
two O
terms O
that O
are O
closely O
related O
to O
the O
notion O
of O
inherent O
Bellman O
error O
( O
Antos O
et O
al O
. O

, O
2008 O
; O
Munos O
& O
Szepesvari O
´ O
, O
2008 O
; O
Lazaric O
et O
al O
. O

, O
2016 O
; O
Scherrer O
et O
al O
. O

, O
2015 O
; O
Haskell O
et O
al O
. O

, O
2017 O
) O
. O

Also O
, O
let O
Bγ O
= O
P∞ O
i O
, O
j=0 O
γ O
i+j O
Ai+j O
, O
which O
was O
assumed O
to O
be O
finite O
in O
Assumption O
5 O
. O

Theorem O
1 O
. O

Suppose O
the O
sample O
size O
requirements O
of O
Lemma O
3 O
are O
satisfied O
with O
/Bγ O
and O
δ O
/ O
K O
replacing O
 O
and O
δ O
, O
respectively O
. O

Then O
, O
the O
suboptimality O
of O
the O
policy O
πK O
can O
be O
bounded O
as O
follows O
: O
kV O
∗ O
− O
V O
πK O
k1,ν O
≤ O
Bγ O
[ O
B O
0 O
γ O
D0(Π¯ O
, O
F¯ O
) O
+ O
D O
d O
1 O
( O
Π O
) O
] O
¯ O
+ O
γ O
Kd O
kV O
∗ O
− O
V O
π0 O
k∞ O
+ O
 O
, O
with O
probability O
at O
least O
1 O
− O
δ O
. O

Search O
Depth O
. O

How O
should O
the O
search O
depth O
d O
be O
chosen O
? O
Theorem O
1 O
shows O
that O
as O
d O
increases B
, O
fewer O
iterations O
K O
are O
needed O
to O
achieve O
a O
given O
accuracy O
; O
however O
, O
the O
effort O
required O
of O
tree O
search O
( O
i.e. O
, O
the O
function O
m( O
, O
δ O
) O
) O
grows O
exponentially O
in O
d. O
At O
the O
other O
extreme O
( O
d O
= O
1 O
) O
, O
more O
iterations O
K O
are O
needed O
and O
the O
“ O
fixed O
cost B
” O
of O
each O
iteration O
of O
the O
algorithm O
( O
i.e. O
, O
sampling O
, O
regression O
, O
and O
classification O
— O
all O
of O
the O
steps O
that O
do O
not O
depend O
on O
d O
) O
becomes O
more O
prominent O
. O

For O
a O
given O
problem O
and O
algorithm O
parameters O
, O
these O
computational O
costs B
can O
each O
be O
estimated O
and O
Theorem O
1 O
can O
serve O
as O
a O
guide O
to O
selecting O
an O
optimal O
d. O
7 O
. O

Case O
Study O
: O
King O
of O
Glory O
MOBA O
AI O
We O
implemented O
Feedback O
- O
Based O
Tree O
Search O
within O
a O
new O
and O
challenging O
environment O
, O
the O
recently O
popular O
MOBA O
game B
King O
of O
Glory O
by O
Tencent O
( O
the O
game B
is O
also O
known O
as O
Honor O
of O
Kings O
and O
a O
North O
American O
release O
of O
the O
game B
is O
titled O
Arena O
of O
Valor O
) O
. O

Our O
implementation O
of O
the O
algorithm O
is O
one O
of O
the O
first O
attempts O
to O
design O
an O
AI O
for O
the O
1v1 O
version O
of O
this O
game B
. O

Game O
Description O
. O

In O
the O
King O
of O
Glory O
, O
players B
are O
divided O
into O
two O
opposing B
teams I
and O
each O
team B
has O
a O
base B
located O
on O
the O
opposite O
corners O
of O
the O
game B
map I
( O
similar O
to O
other O
MOBA O
games B
, O
like O
League O
of O
Legends O
or O
Dota O
2 O
) O
. O

The O
bases O
are O
guarded O
by O
towers B
, O
which O
can O
attack B
the O
enemies B
when O
they O
are O
within O
a O
certain O
attack B
range B
. O

The O
goal B
of O
each O
team B
is O
to O
overcome O
the O
towers B
and O
eventually O
destroy O
the O
opposing B
team I
’s O
“ O
crystal B
, O
” O
located O
at O
the O
enemy B
’s O
base B
. O

For O
this O
paper O
, O
we O
only O
consider O
the O
1v1 O
mode O
, O
where O
each O
player B
controls B
a O
primary O
“ O
hero B
” O
alongside O
less O
powerful B
game B
- O
controlled B
characters B
called B
“ O
minions B
. O

” O
These O
units B
guard O
the O
path B
to O
the O
crystal B
and O
will O
automatically O
fire O
( O
weak B
) O
attacks B
at O
enemies B
within O
range B
. O

Figure O
4 O
shows O
the O
two O
heroes B
and O
their O
minions B
; O
the O
upper O
- O
left O
corner O
shows O
the O
map O
, O
with O
the O
blue B
and O
red O
markers O
pinpointing O
the O
towers B
and O
crystals B
. O

Experimental O
Setup O
. O

The O
state O
variable O
of O
the O
system O
is O
taken O
to O
be O
a O
41-dimensional O
vector O
containing O
information O
obtained O
directly O
from O
the O
game B
engine O
, O
including O
hero B
locations B
, O
hero B
health O
, O
minion B
health O
, O
hero B
skill B
states O
, O
and O
relative O
locations B
to O
various O
structures O
. O

There O
are O
22 O
actions O
, O
including O
move B
, O
attack B
, O
heal O
, O
and O
special O
skill B
actions O
, O
some O
of O
which O
are O
associated O
with O
( O
discretized O
) O
directions O
. O

The O
reward B
function O
is O
designed O
to O
mimic O
reward B
shaping O
( O
Ng O
et O
al O
. O

, O
1999 O
) O
and O
uses O
a O
combination O
of O
signals O
including O
health O
, O
kills B
, O
damage B
dealt O
, O
and O
proximity O
to O
crystal B
. O

We O
trained B
five O
King O
of O
Glory O
agents O
, O
using O
the O
hero B
DiRenJie O
: O
1 O
. O

The O
“ O
FBTS O
” O
agent O
is O
trained B
using O
our O
feedback O
- O
based B
tree O
search O
algorithm O
for O
K O
= O
7 O
iterations O
of O
50 O
games B
each O
. O

The O
search O
depth O
is O
d O
= O
7 O
and O
rollout O
length O
is O
h O
= O
5 O
. O

Each O
call B
to O
MCTS O
ran O
for O
400 O
iterations O
. O

2 O
. O

The O
second O
agent O
is O
labeled O
“ O
NR O
” O
for O
no O
rollouts O
. O

It O
uses O
the O
same O
parameters O
as O
the O
FBTS O
agent O
except O
no O
rollouts O
are O
used O
. O

At O
a O
high B
level I
, O
this O
bears O
some O
similarity O
to O
the O
AlphaGo O
Zero O
algorithm O
( O
Silver O
et O
al O
. O

, O
2017 O
) O
in O
a O
batch O
setting O
. O

3 O
. O

The O
“ O
DPI O
” O
agent O
uses O
the O
direct O
policy O
iteration O
technique B
of O
( O
Lazaric O
et O
al O
. O

, O
2016 O
) O
for O
K O
= O
10 O
iterations O
. O

There O
is O
no O
value O
function O
and O
no O
tree O
search O
( O
due O
to O
computational O
limitations O
, O
more O
iterations O
are O
possible O
when O
tree O
search O
is O
not O
used O
) O
. O

4 O
. O

We O
then O
have O
the O
“ O
AVI O
” O
agent O
, O
which O
implements O
approximate O
value O
iteration O
( O
De O
Farias O
& O
Van O
Roy O
, O
2000 O
; O
Van O
Roy O
, O
2006 O
; O
Munos O
, O
2007 O
; O
Munos O
& O
Szepesvari O
´ O
, O
2008 O
) O
for O
K O
= O
10 O
iterations O
. O

This O
algorithm O
can O
be O
considered O
a O
batch O
version O
of O
DQN O
( O
Mnih O
et O
al O
. O

, O
2013 O
) O
. O

5 O
. O

Lastly O
, O
we O
consider O
an O
“ O
SL O
” O
agent O
trained B
via O
supervised O
learning O
on O
a O
dataset O
of O
approximately O
100,000 O
state O
/ O
action O
pairs O
of O
human O
gameplay B
data O
. O

Notably O
, O
the O
policy O
architecture O
used O
here O
is O
consistent O
with O
the O
previous O
agents O
. O

In O
fact O
, O
both O
the O
policy O
and O
value O
function O
approximations O
are O
consistent O
across O
all O
agents O
; O
they O
use O
fully O
- O
connected O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
neural O
networks O
with O
five O
and O
two O
hidden O
layers O
, O
respectively O
, O
and O
SELU O
( O
scaled O
exponential O
linear O
unit B
) O
activation O
( O
Klambauer O
et O
al O
. O

, O
2017 O
) O
. O

The O
initial O
policy O
π0 O
takes O
random O
actions O
: O
move B
( O
w.p O
. O

0.5 O
) O
, O
directional O
attack B
( O
w.p O
. O

0.2 O
) O
, O
or O
a O
special O
skill B
( O
w.p O
. O

0.3 O
) O
. O

Besides O
biasing O
the O
move B
direction O
toward O
the O
forward O
direction O
, O
no O
other O
heuristic O
information O
is O
used O
by O
π0 O
. O

MCTS O
was O
chosen O
to O
be O
a O
variant O
of O
UCT O
( O
Kocsis O
& O
Szepesvari O
´ O
, O
2006 O
) O
that O
is O
more O
amenable O
toward O
parallel O
simulations O
: O
instead O
of O
using O
the O
argmax O
of O
the O
UCB O
scores O
, O
we O
sample O
actions O
according O
to O
the O
distribution O
obtained O
by O
applying O
softmax O
to O
the O
UCB O
scores O
. O

In O
the O
practical O
implementation O
of O
the O
algorithm O
, O
Regress O
uses O
a O
cosine O
proximity O
loss O
while O
Classify O
uses O
a O
negative O
log O
- O
likelihood O
loss O
, O
differing O
from O
the O
theoretical O
specifications O
. O

Due O
to O
the O
inability O
to O
“ O
rewind O
” O
or O
“ O
fast O
- O
forward O
” O
the O
game B
environment O
to O
arbitrary O
states O
, O
the O
sampling O
distribution O
ρ0 O
is O
implemented O
by O
first O
taking O
random O
actions O
( O
for O
a O
random O
number O
of O
steps O
) O
to O
arrive O
at O
an O
initial O
state O
and O
then O
following O
πk O
until O
the O
end O
of O
the O
game B
. O

To O
reduce O
correlation O
during O
value O
approximation O
, O
we O
discard O
2/3 O
of O
the O
states O
encountered O
in O
these O
trajectories O
. O

For O
ρ1 O
, O
we O
follow O
the O
MCTS O
policy O
while O
occasionally O
injecting O
noise O
( O
in O
the O
form O
of O
random O
actions O
and O
random O
switches O
to O
the O
default O
policy O
) O
to O
reduce O
correlation O
. O

During O
rollouts O
, O
we O
use O
the O
internal O
AI O
for O
the O
hero B
DiRenJie O
as O
the O
opponent B
. O

Results O
. O

As O
the O
game B
is O
nearly O
deterministic O
, O
our O
primary O
methodology O
for O
testing O
to O
compare O
the O
agents O
’ O
effectiveness O
against O
a O
common O
set O
of O
opponents B
chosen O
from O
the O
internal O
AIs O
. O

We O
also O
added O
the O
internal O
DiRenJie O
AI O
as O
a O
“ O
sanity O
check O
” O
baseline O
agent O
. O

To O
select O
the O
test O
opponents B
, O
we O
played B
the O
internal O
DiRenJie O
AI O
against O
other O
internal O
AIs O
( O
i.e. O
, O
other O
heroes B
) O
and O
selected O
six O
heroes B
of O
the O
marksman O
type O
that O
the O
internal O
DiRenJie O
AI O
is O
able O
to O
defeat B
. O

Each O
of O
our O
agents O
, O
including O
the O
internal O
DiRenJie O
AI O
, O
was O
then O
played B
against O
every O
test O
opponent B
. O

Figure O
5 O
shows O
the O
length O
of O
time B
, O
measured O
in O
frames B
, O
for O
each O
agent O
to O
defeat B
the O
test O
opponents B
( O
a O
value O
of O
20,000 O
frames B
is O
assigned O
if O
the O
opponent B
won O
) O
. O

Against O
the O
set O
of O
common O
opponents B
, O
FBTS O
significantly O
outperforms O
DPI O
, O
AVI O
, O
SL O
, O
and O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.0 O
Fraction O
of O
Game O
0.5 O
1.0 O
1.5 O
2.0 O
Gold O
Ratio O
vs. O
NR O
vs. O
DPI O
vs. O
AVI O
vs. O
SL O
Figure O
6 O
. O

In B
- I
game I
Behavior O
the O
internal O
AI O
. O

However O
, O
FBTS O
only O
slightly O
outperforms O
NR O
on O
average O
( O
which O
is O
perhaps O
not O
surprising O
as O
NR O
is O
the O
only O
other O
agent O
that O
also O
uses O
MCTS O
) O
. O

Our O
second O
set B

 O
of I
results I
help O
to O
visualize O
head O
- O
to O
- O
head O
battles B
played B
between O
FBTS O
and O
the O
four O
baselines O
( O
all O
of O
which O
are O
won O
by O
FBTS O
) O
: O
Figure O
6 O
shows O
the O
ratio O
of O
the O
FBTS O
agent O
’s O
gold B
to O
its O
opponent B
’s O
gold B
as O
a O
function O
of O
time B
. O

Gold B
is O
collected O
throughout O
the O
game B
as O
heroes B
deal B
damage I
and O
defeat B
enemies B
, O
so O
a O
ratio O
above O
1.0 O
( O
above O
the O
red O
region O
) O
indicates O
good O
relative O
performance O
by O
FBTS O
. O

As O
the O
figure O
shows O
, O
each O
game B
ends O
with O
FBTS O
achieving O
a O
gold B
ratio O
in O
the O
range B
of O
[ O
1.25 O
, O
1.75 O
] O
. O

8 O
. O

Conclusion O
& O
Future O
Work O
In O
this O
paper O
, O
we O
provide O
a O
sample O
complexity O
analysis O
for O
feedback O
- O
based B
tree O
search O
, O
an O
RL O
algorithm O
based B
on O
repeatedly O
solving O
finite O
- O
horizon O
subproblems O
using O
MCTS O
. O

Our O
primary O
methodological O
avenues O
for O
future O
work O
are O
( O
1 O
) O
to O
analyze O
a O
self O
- O
play B
variant O
of O
the O
algorithm O
and O
( O
2 O
) O
to O
consider O
related O
techniques B
in B
multi O
- I
agent I
domains O
( O
see O
, O
e.g. O
, O
Hu O
& O
Wellman O
( O
2003 O
) O
) O
. O

The O
implementation O
of O
the O
algorithm O
in O
the O
1v1 O
MOBA O
game B
King O
of O
Glory O
provided O
us O
encouraging O
results O
against O
several O
related O
algorithms O
; O
however O
, O
significant O
work O
remains O
for O
the O
agent O
to O
become O
competitive B
with O
humans O
. O

vs. O
Hero O
1 O
vs. O
Hero O
2 O
vs. O
Hero O
3 O
vs. O
Hero O
4 O
vs. O
Hero O
5 O
vs. O
Hero O
6 O
Average O
0 O
5 O
10 O
15 O
Loss O
Frames B
until O
Win O
( O
x O
1000 O
) O
FBTS O
NR O
DPI O
AVI O
SL O
Internal O
AI O
Figure O
5 O
. O

Number O
of O
Frames B
to O
Defeat B
Marksman O
Heroes O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
Acknowledgements O
We O
sincerely O
appreciate O
the O
helpful O
feedback O
from O
four O
anonymous O
reviewers O
, O
which O
helped O
to O
significantly O
improve O
the O
paper O
. O

We O
also O
wish O
to O
thank O
our O
colleagues O
at O
Tencent O
AI O
Lab O
, O
particularly O
Carson O
Eisenach O
and O
Xiangru O
Lian O
, O
for O
assistance O
with O
the O
test O
environment O
and O
for O
providing O
the O
SL O
agent O
. O

The O
first O
author O
is O
very O
grateful O
for O
the O
support B
from O
Tencent O
AI O
Lab O
through O
a O
faculty O
award O
. O

References O
Al O
- O
Kanj O
, O
Lina O
, O
Powell O
, O
Warren O
B O
, O
and O
Bouzaiene O
- O
Ayari O
, O
Belgacem O
. O

The O
information O
- O
collecting O
vehicle O
routing O
problem O
: O
Stochastic O
optimization O
for O
emergency O
storm O
response O
. O

arXiv O
preprint O
arXiv:1605.05711 O
, O
2016 O
. O

Anthony O
, O
Thomas O
, O
Tian O
, O
Zheng O
, O
and O
Barber O
, O
David O
. O

Thinking O
fast O
and O
slow B
with O
deep O
learning O
and O
tree O
search O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pp O
. O

5366–5376 O
, O
2017 O
. O

Antos O
, O
Andras O
, O
Szepesv O
´ O
ari O
, O
Csaba O
, O
and O
Munos O
, O
R O
´ O
emi O
. O

Learn- O
´ O
ing O
near O
- O
optimal O
policies O
with O
bellman O
- O
residual O
minimization O
based B
fitted O
policy O
iteration O
and O
a O
single O
sample O
path B
. O

Machine O
Learning O
, O
71(1):89–129 O
, O
2008 O
. O

Bertsekas O
, O
Dimitri O
P. O
Convergence O
of O
discretization O
procedures O
in O
dynamic O
programming O
. O

IEEE O
Transactions O
on O
Automatic O
Control O
, O
20(3):415–419 O
, O
1975 O
. O

Bertsekas O
, O
Dimitri O
P O
and O
Tsitsiklis O
, O
John O
N. O
Neuro O
- O
dynamic O
Programming O
. O

Athena O
Scientific O
, O
Belmont O
, O
MA O
, O
1996 O
. O

Browne O
, O
Cameron O
B O
, O
Powley O
, O
Edward O
, O
Whitehouse O
, O
Daniel O
, O
Lucas O
, O
Simon O
M O
, O
Cowling O
, O
Peter O
I O
, O
Rohlfshagen O
, O
Philipp O
, O
Tavener O
, O
Stephen O
, O
Perez O
, O
Diego O
, O
Samothrakis O
, O
Spyridon O
, O
and O
Colton O
, O
Simon O
. O

A O
survey O
of O
monte O
carlo O
tree O
search O
methods O
. O

IEEE O
Transactions O
on O
Computational O
Intelligence O
and O
AI O
in O
games B
, O
4(1):1–43 O
, O
2012 O
. O

Campbell O
, O
Murray O
, O
Hoane O
Jr O
, O
A O
Joseph O
, O
and O
Hsu O
, O
Fenghsiung O
. O

Deep O
blue B
. O

Artificial O
Intelligence O
, O
134(1 O
- O
2):57 O
– O
83 O
, O
2002 O
. O

Cazenave O
, O
Tristan O
. O

Nested O
Monte O
- O
Carlo O
search O
. O

In O
International O
Joint O
Conference O
on O
Artificial O
Intelligence O
, O
pp O
. O

456–461 O
, O
2009 O
. O

Chaslot O
, O
Guillaume O
, O
Saito O
, O
Jahn O
- O
Takeshi O
, O
Uiterwijk O
, O
Jos O
WHM O
, O
Bouzy O
, O
Bruno O
, O
and O
van O
den O
Herik O
, O
H O
Jaap O
. O

Monte O
- O
Carlo O
strategies B
for O
computer O
Go O
. O

In O
18th O
BelgianDutch O
Conference O
on O
Artificial O
Intelligence O
, O
pp O
. O

83–90 O
, O
2006 O
. O

Chaslot O
, O
Guillaume O
, O
Bakkes O
, O
Sander O
, O
Szita O
, O
Istvan O
, O
and O
Spronck O
, O
Pieter O
. O

Monte O
- O
carlo O
tree O
search O
: O
A O
new O
framework O
for O
game B
AI O
. O

In O
AAAI O
Conference O
on O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
, O
2008 O
. O

Couetoux O
, O
Adrien O
, O
Hoock O
, O
Jean O
- O
Baptiste O
, O
Sokolovska O
, O
Na- O
¨ O
taliya O
, O
Teytaud O
, O
Olivier O
, O
and O
Bonnard O
, O
Nicolas O
. O

Continuous O
upper O
confidence O
trees O
. O

In O
International O
Conference O
on O
Learning O
and O
Intelligent O
Optimization O
, O
pp O
. O

433–445 O
. O

Springer O
, O
2011a O
. O

Couetoux O
, O
Adrien O
, O
Milone O
, O
Mario O
, O
Brendel O
, O
M O
¨ O
aty O
´ O
as O
, O
Dogh- O
´ O
men O
, O
Hassan O
, O
Sebag O
, O
Michele O
, O
and O
Teytaud O
, O
Olivier O
. O

Continuous O
rapid O
action O
value O
estimates O
. O

In O
Asian O
Conference O
on O
Machine O
Learning O
, O
pp O
. O

19–31 O
, O
2011b O
. O

Coulom O
, O
Remi O
. O

Efficient O
selectivity O
and O
backup O
operators O
´ O
in B
Monte O
- I
Carlo I
tree O
search O
. O

In O
International O
Conference O
on O
Computers O
and O
Games O
, O
pp O
. O

72–83 O
, O
2006 O
. O

De O
Farias O
, O
D O
Pucci O
and O
Van O
Roy O
, O
Benjamin O
. O

On O
the O
existence O
of O
fixed O
points B
for O
approximate O
value O
iteration O
and O
temporal O
- O
difference O
learning O
. O

Journal O
of O
Optimization O
theory O
and O
Applications O
, O
105(3):589–608 O
, O
2000 O
. O

Dufour O
, O
Franc¸ois O
and O
Prieto O
- O
Rumeau O
, O
Tomas O
. O

Approxi- O
´ O
mation O
of O
markov O
decision O
processes O
with O
general O
state O
space O
. O

Journal O
of O
Mathematical O
Analysis O
and O
Applications O
, O
388(2):1254–1267 O
, O
2012 O
. O

Enzenberger O
, O
Markus O
. O

Evaluation O
in O
go O
by O
a O
neural O
network O
using O
soft O
segmentation O
. O

In O
Advances O
in O
Computer O
Games O
, O
pp O
. O

97–108 O
. O

Springer O
, O
2004 O
. O

Gelly O
, O
Sylvain O
and O
Silver O
, O
David O
. O

Combining O
online O
and O
offline B
knowledge O
in O
UCT O
. O

In O
Proceedings O
of O
the O
24th O
International O
Conference O
on O
Machine O
learning O
, O
pp O
. O

273 O
– O
280 O
, O
2007 O
. O

Gelly O
, O
Sylvain O
and O
Silver O
, O
David O
. O

Monte O
- O
carlo O
tree O
search O
and O
rapid O
action O
value O
estimation O
in O
computer O
Go O
. O

Artificial O
Intelligence O
, O
175(11):1856–1875 O
, O
2011 O
. O

Gelly O
, O
Sylvain O
, O
Kocsis O
, O
Levente O
, O
Schoenauer O
, O
Marc O
, O
Sebag O
, O
Michele O
, O
Silver O
, O
David O
, O
Szepesvari O
, O
Csaba O
, O
and O
Teytaud O
, O
´ O
Olivier O
. O

The O
grand O
challenge O
of O
computer O
Go O
: O
Monte O
Carlo O
tree O
search O
and O
extensions O
. O

Communications B
of O
the O
ACM O
, O
55(3):106–113 O
, O
2012 O
. O

Guo O
, O
Xiaoxiao O
, O
Singh O
, O
Satinder O
, O
Lee O
, O
Honglak O
, O
Lewis O
, O
Richard O
L O
, O
and O
Wang O
, O
Xiaoshi O
. O

Deep O
learning O
for O
realtime O
Atari O
game B
play B
using O
offline B
Monte O
- O
Carlo O
tree O
search O
planning O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pp O
. O

3338–3346 O
, O
2014 O
. O

Haskell O
, O
William O
B O
, O
Jain O
, O
Rahul O
, O
and O
Kalathil O
, O
Dileep O
. O

Empirical O
dynamic O
programming O
. O

Mathematics O
of O
Operations O
Research O
, O
41(2):402–429 O
, O
2016 O
. O

Haskell O
, O
William O
B O
, O
Jain O
, O
Rahul O
, O
Sharma O
, O
Hiteshi O
, O
and O
Yu O
, O
Pengqian O
. O

An O
empirical O
dynamic O
programming O
algorithm O
for O
continuous O
MDPs O
. O

arXiv O
preprint O
arXiv:1709.07506 O
, O
2017 O
. O

Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
Haussler O
, O
David O
. O

Decision O
theoretic O
generalizations O
of O
the O
PAC O
model O
for O
neural O
net O
and O
other O
learning O
applications O
. O

Information O
and O
Computation O
, O
100(1):78–150 O
, O
1992 O
. O

Hingston O
, O
Philip O
and O
Masek O
, O
Martin O
. O

Experiments O
with O
Monte O
Carlo O
Othello O
. O

In O
IEEE O
Congress O
on O
Evolutionary O
Computation O
, O
pp O
. O

4059–4064 O
. O

IEEE O
, O
2007 O
. O

Hu O
, O
Junling O
and O
Wellman O
, O
Michael O
P. O
Nash O
Q O
- O
learning O
for O
general O
- O
sum O
stochastic O
games B
. O

Journal O
of O
Machine O
Learning O
Research O
, O
4(Nov):1039–1069 O
, O
2003 O
. O

Jiang O
, O
Daniel O
R O
, O
Al O
- O
Kanj O
, O
Lina O
, O
and O
Powell O
, O
Warren O
B. O
Monte O
carlo O
tree O
search O
with O
sampled O
information O
relaxation O
dual O
bounds O
. O

arXiv O
preprint O
arXiv:1704.05963 O
, O
2017 O
. O

Kaufmann O
, O
Emilie O
and O
Koolen O
, O
Wouter O
. O

Monte O
- O
Carlo O
tree O
search O
by O
best O
arm O
identification O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pp O
. O

4904–4913 O
, O
2017 O
. O

Klambauer O
, O
Gunter O
, O
Unterthiner O
, O
Thomas O
, O
Mayr O
, O
Andreas O
, O
¨ O
and O
Hochreiter O
, O
Sepp O
. O

Self O
- O
normalizing O
neural O
networks O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pp O
. O

972–981 O
, O
2017 O
. O

Kocsis O
, O
Levente O
and O
Szepesvari O
, O
Csaba O
. O

Bandit O
based B
´ O
Monte O
- O
Carlo O
planning O
. O

In O
European O
Conference O
on O
Machine O
Learning O
, O
pp O
. O

282–293 O
, O
2006 O
. O

Langford O
, O
John O
and O
Zadrozny O
, O
Bianca O
. O

Relating O
reinforcement O
learning O
performance O
to O
classification O
performance O
. O

In O
Proceedings O
of O
the O
22nd O
International O
Conference O
on O
Machine O
Learning O
, O
pp O
. O

473–480 O
, O
2005 O
. O

Lazaric O
, O
Alessandro O
, O
Ghavamzadeh O
, O
Mohammad O
, O
and O
Munos O
, O
Remi O
. O

Analysis O
of O
classification O
- O
based B
policy O
´ O
iteration O
algorithms O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
17(19):1–30 O
, O
2016 O
. O

Li O
, O
Lihong O
, O
Bulitko O
, O
Vadim O
, O
and O
Greiner O
, O
Russell O
. O

Focus O
of O
attention O
in O
reinforcement O
learning O
. O

Journal O
of O
Universal O
Computer O
Science O
, O
13(9):1246–1269 O
, O
2007 O
. O

Maˆıtrepierre O
, O
Raphael O
, O
Mary O
, O
J O
¨ O
er O
´ O
emie O
, O
and O
Munos O
, O
R O
´ O
emi O
. O

´ O
Adaptative O
play B
in O
Texas O
hold’em O
poker O
. O

In O
European O
Conference O
on O
Artificial O
Intelligence O
, O
2008 O
. O

Mehat O
, O
Jean O
and O
Cazenave O
, O
Tristan O
. O

Combining O
UCT O
and O
´ O
nested O
Monte O
Carlo O
search O
for O
single O
- O
player B
general O
game B
playing O
. O

IEEE O
Transactions O
on O
Computational O
Intelligence O
and O
AI O
in O
Games O
, O
2(4):271–277 O
, O
2010 O
. O

Mnih O
, O
Volodymyr O
, O
Kavukcuoglu O
, O
Koray O
, O
Silver O
, O
David O
, O
Graves O
, O
Alex O
, O
Antonoglou O
, O
Ioannis O
, O
Wierstra O
, O
Daan O
, O
and O
Riedmiller O
, O
Martin O
. O

Playing B
Atari O
with O
deep O
reinforcement O
learning O
. O

arXiv O
preprint O
arXiv:1312.5602 O
, O
2013 O
. O

Mohri O
, O
Mehryar O
, O
Rostamizadeh O
, O
Afshin O
, O
and O
Talwalkar O
, O
Ameet O
. O

Foundations O
of O
Machine O
Learning O
. O

MIT O
Press O
, O
2012 O
. O

Munos O
, O
Remi O
. O

Performance O
bounds O
in O
l O
´ O
p O
- O
norm O
for O
approximate O
value O
iteration O
. O

SIAM O
Journal O
on O
Control O
and O
Optimization O
, O
46(2):541–561 O
, O
2007 O
. O

Munos O
, O
Remi O
and O
Szepesv O
´ O
ari O
, O
Csaba O
. O

Finite O
- O
time B
bounds O
´ O
for O
fitted O
value O
iteration O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
9(May):815–857 O
, O
2008 O
. O

Ng O
, O
Andrew O
Y O
, O
Harada O
, O
Daishi O
, O
and O
Russell O
, O
Stuart O
. O

Policy O
invariance O
under O
reward B
transformations O
: O
Theory O
and O
application O
to O
reward B
shaping O
. O

In O
Proceedings O
of O
the O
16th O
International O
Conference O
on O
Machine O
Learning O
, O
pp O
. O

278–287 O
, O
1999 O
. O

Pollard O
, O
David O
. O

Empirical O
processes O
: O
Theory O
and O
applications O
. O

In B
NSF O
- I
CBMS I
Regional O
Conference O
Series O
in O
Probability O
and O
Statistics O
, O
pp O
. O

i–86 O
. O

JSTOR O
, O
1990 O
. O

Puterman O
, O
Martin O
L. O
Markov O
Decision O
Processes O
: O
Discrete O
Stochastic O
Dynamic O
Programming O
. O

John O
Wiley O
& O
Sons O
, O
2014 O
. O

Saldi O
, O
Naci O
, O
Yuksel O
, O
Serdar O
, O
and O
Linder O
, O
Tam O
¨ O
as O
. O

On O
the O
´ O
asymptotic O
optimality O
of O
finite O
approximations O
to O
markov O
decision O
processes O
with O
Borel O
spaces O
. O

Mathematics O
of O
Operations O
Research O
, O
42(4):945–978 O
, O
2017 O
. O

Scherrer O
, O
Bruno O
, O
Ghavamzadeh O
, O
Mohammad O
, O
Gabillon O
, O
Victor O
, O
Lesner O
, O
Boris O
, O
and O
Geist O
, O
Matthieu O
. O

Approximate O
modified O
policy O
iteration O
and O
its O
application O
to O
the O
game B
of O
tetris O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
16 O
( O
Aug):1629–1676 O
, O
2015 O
. O

Silver O
, O
D. O
, O
Huang O
, O
A. O
, O
Maddison O
, O
C. O
J. O
, O
Guez O
, O
A. O
, O
Sifre O
, O
L. O
, O
van O
den O
Driessche O
, O
G. O
, O
Schrittwieser O
, O
J. O
, O
Antonoglou O
, O
I. O
, O
Panneershelvam O
, O
V. O
, O
Lanctot O
, O
M. O
, O
Dieleman O
, O
S. O
, O
Grewe O
, O
D. O
, O
Nham O
, O
J. O
, O
Kalchbrenner O
, O
N. O
, O
Sutskever O
, O
I. O
, O
Lillicrap O
, O
T. O
, O
Leach O
, O
M. O
, O
Kavukcuoglu O
, O
K. O
, O
Graepel O
, O
T. O
, O
and O
Hassabis O
, O
D. O
Mastering B
the O
game B
of O
Go O
with O
deep O
neural O
networks O
and O
tree O
search O
. O

Nature O
, O
529(7587):484–489 O
, O
2016 O
. O

Silver O
, O
David O
, O
Schrittwieser O
, O
Julian O
, O
Simonyan O
, O
Karen O
, O
Antonoglou O
, O
Ioannis O
, O
Huang O
, O
Aja O
, O
Guez O
, O
Arthur O
, O
Hubert O
, O
Thomas O
, O
Baker O
, O
Lucas O
, O
Lai O
, O
Matthew O
, O
Bolton O
, O
Adrian O
, O
et O
al O
. O

Mastering B
the O
game B
of O
go O
without O
human O
knowledge O
. O

Nature O
, O
550(7676):354 O
, O
2017 O
. O

Teraoka O
, O
Kazuki O
, O
Hatano O
, O
Kohei O
, O
and O
Takimoto O
, O
Eiji O
. O

Efficient O
sampling O
method O
for O
Monte O
Carlo O
tree O
search O
problem O
. O

IEICE O
Transactions O
on O
Information O
and O
Systems O
, O
97 O
( O
3):392–398 O
, O
2014 O
. O

Van O
Roy O
, O
Benjamin O
. O

Performance O
loss O
bounds O
for O
approximate O
value O
iteration O
with O
state O
aggregation O
. O

Mathematics O
of O
Operations O
Research O
, O
31(2):234–244 O
, O
2006 O
. O

Supplementary O
Material O
for O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
Daniel O
R. O
Jiang O
, O
Emmanuel O
Ekwedike O
, O
Han O
Liu O
A O
Outline O
Section O
B O
contains O
the O
proofs O
for O
the O
results O
stated O
in O
the O
main O
paper O
. O

• O
The O
proofs O
of O
Lemma O
1 O
and O
Lemma O
2 O
from O
the O
main O
paper O
are O
given O
in O
Sections O
B.1 O
and O
B.2 O
. O

These O
two O
lemmas O
are O
important O
in O
that O
they O
provide O
the O
main O
structure O
for O
the O
sample O
complexity O
analysis O
. O

The O
bounds O
hold O
pointwise O
. O

• O
In O
Section O
B.3 O
, O
we O
provide O
some O
additional O
lemmas O
that O
are O
omitted O
from O
the O
main O
paper O
. O

• O
Section O
B.4 O
gives O
the O
proof O
of O
Lemma O
3 O
from O
the O
main O
paper O
, O
which O
makes O
use O
of O
Lemma O
2 O
and O
the O
results O
from O
Section O
B.3 O
. O

• O
We O
prove O
the O
main O
result O
, O
Theorem O
1 O
, O
in O
Section O
B.5 O
. O

Lastly O
, O
in O
Section O
C O
, O
we O
provide O
additional O
implementation O
details O
regarding O
the O
neural O
network O
architecture O
, O
state O
features O
, O
and O
computation O
. O

1 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
B O
Proofs O
B.1 O
Proof O
of O
Lemma O
1 O
This O
proof O
is O
a O
modification O
of O
arguments O
used O
in O
[ O
Lazaric O
et O
al O
. O

, O
2016 O
, O
Equation O
8 O
and O
Theorem O
7 O
] O
. O

By O
the O
fixed O
point B
property O
of O
Tπk+1 O
and O
the O
definition O
of O
the O
Bellman O
operator O
T O
, O
we O
have O
V O
πk O
− O
V O
πk+1 O
≤ O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk+1 O
. O

Subtracting O
and O
adding O
Tπk+1 O
V O
πk O
: O
V O
πk O
− O
V O
πk+1 O
≤ O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
+ O
Tπk+1 O
V O
πk O
− O
Tπk+1 O
V O
πk+1 O
≤ O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
+ O
( O
γPπk+1 O
) O


 O
V O
πk O
− O
V O
πk+1 O
 O
. O

( O
B.1 O
) O
Similarly O
, O
we O
will O
bound O
the O
difference O
between O
V O
∗ O
and O
V O
πk+1 O
in O
terms O
of O
the O
distances O
between O
V O
∗ O
− O
V O
πk O
and O
V O
πk O
− O
V O
πk+1 O
: O
V O
∗ O
− O
V O
πk+1 O
≤ O
( O
γPπ∗ O
) O
d O


 O
V O
∗ O
− O
V O
πk O
 O
+ O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
+ O
( O
γPπk+1 O
) O


 O
V O
πk O
− O
V O
πk+1 O
 O
. O

( O
B.2 O
) O
Using O
the O
bound O
V O
πk O
− O
V O
πk+1 O
≤ O
[ O
I O
− O
( O
γPπk+1 O
) O
] O
−1 O
( O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
) O
from O
( O
B.1 O
) O
on O
the O
last O
term O
of O
the O
right O
side O
of O
( O
B.2 O
) O
along O
with O
a O
power B
series O
expansion O
on O
the O
inverse O
, O
we O
obtain O
: O
V O
∗ O
− O
V O
πk+1 O
≤ O
( O
γPπ∗ O
) O
d O
( O
V O
∗ O
− O
V O
πk O
) O
+ O
 O
I O
+ O
( O
γPπk+1 O
) O
X∞ O
j=0 O
( O
γPπk+1 O
) O
j O
 O


 O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
 O
= O
( O
γPπ∗ O
) O
d O
( O
V O
∗ O
− O
V O
πk O
) O
+ O
I O
− O
( O
γPπk+1 O
) O
−1 O


 O
T O
dV O
πk O
− O
Tπk+1 O
V O
πk O
 O
, O
which O
can O
be O
iterated O
to O
show O
: O
V O
∗ O
−V O
πK O
≤ O
( O
γPπ∗ O
) O
Kd O
( O
V O
∗ O
−V O
π0 O
) O
+ O
X O
K O
k=1 O
( O
γPπ∗ O
) O
( O
K−k O
) O
d O


 O
I O
−(γPπk O
) O
−1 O


 O
T O
dV O
πk−1 O
−Tπk O
V O
πk−1 O
 O
. O

The O
statement O
from O
the O
lemma O
follows O
from O
taking O
absolute O
value O
, O
bounding O
by O
the O
maximum O
norm O
, O
and O
integrating O
. O

B.2 O
Proof O
of O
Lemma O
2 O
For O
part O
( O
a O
) O
, O
we O
note O
the O
following O
: O
kTπ O
V O
− O
Tπ O
V O
µ O
k1,ρ1 O
= O
γ O
Z O
S O


 O
( O
Pπ O
V O
) O
( O
s O
) O
− O
( O
Pπ O
V O
µ O
) O
( O
s O
) O


  O
ρ1(ds O
) O
≤ O
γ O
Z O
S O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O



 O
d(ρ1Pπ O
) O
dρ0 O
ρ0(ds O
) O
≤ O
γ O









 O
d(ρ1Pπ O
) O
dρ0 O









 O
∞ O
kV O
− O
V O
µ O
k1,ρ0 O
. O

By O
the O
concentrability O
conditions O
of O
Assumption O
5 O
, O
the O
right O
- O
hand O
- O
side O
can O
be O
bounded O
by O
γ O
A0 O
1 O
kV O
− O
V O
µk1,ρ0 O
. O

Now O
, O
we O
can O
apply O
the O
same O
steps O
with O
the O
roles B
of O
Tπ O
V O
and O
Tπ O
V O
µ O
reversed O
to O
see O
that O
the O
same O
inequality O
holds O
for O
kTπ O
V O
−Tπ O
V O
µk1,ρ1 O
and O
part O
( O
a O
) O
is O
complete O
. O

2 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Le O
For O
part O
( O
b O
) O
, O
we O
partition O
the O
state O
space O
S O
into O
two O
sets O
: O
S O
+ O
= O
 O
s O
∈ O
S O
: O
( O
T O
dJ)(s O
) O
≥ O
( O
T O
dV O
µ O
) O
( O
s O
) O

	
 O
and O
S O
- O
= O
 O
s O
∈ O
S O
: O
( O
T O
dJ)(s O
) O
< O
( O
T O
dV O
µ O
) O
( O
s O
) O

	
 O
. O

We O
start B
with O
S O
+ O
. O

Consider O
the O
finite O
- O
horizon O
d O
- O
stage B
MDP O
with O
terminal O
value O
J O
and O
the O
same O
dynamics O
as O
our O
infinite O
- O
horizon O
MDP O
of O
interest O
. O

Let O
π O
J O
1 O
, O
πJ O
2 O
, O
. O

. O

. O

, O
πJ O
d O
be O
the O
timedependent O
optimal O
policy O
for O
this O
MDP O
. O

Thus O
, O
we O
have O
Tπ O
J O
1 O
Tπ O
J O
2 O
· O
· O
· O
Tπ O
J O
d O
J O
= O
T O
dJ O
and O
Tπ O
J O
1 O
Tπ O
J O
2 O
· O
· O
· O
Tπ O
J O
d O
V O
µ O
≤ O
T O
dV O
µ O
. O

Using O
similar O
steps O
as O
for O
part O
( O
a O
) O
, O
the O
following O
hold O
: O
Z O
S+ O


 O
( O
T O
dJ)(s O
) O
− O
( O
T O
dV O
µ O
) O
( O
s O
) O


 O
ρ1(ds O
) O
≤ O
Z O
S+ O


 O
( O
T O
dT O
h O
µ O
V O
) O
( O
s O
) O
− O
( O
Tπ O
J O
1 O
Tπ O
J O
2 O
· O
· O
· O
Tπ O
J O
d O
T O
h O
µ O
V O
µ O
) O
( O
s O
) O


 O
ρ1(ds O
) O
≤ O
γ O
d+h O
Z O
S+ O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O



 O
d(ρ1Pπ O
J O
1 O
Pπ O
J O
2 O
· O
· O
· O
Pπ O
J O
d O
P O
h O
µ O
) O
dρ0 O
ρ0(ds O
) O
≤ O
γ O
d+h O
A O
0 O
d+h O
Z O
S+ O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O


  O
ρ0(ds O
) O
. O

Now O
, O
using O
the O
optimal O
policy O
with O
respect O
to O
the O
d O
- O
stage B
MDP O
with O
terminal O
condition O
V O
µ O
, O
we O
can O
repeat O
these O
steps O
to O
show O
that O
Z O
S- O


 O
( O
T O
dV O
µ O
) O
( O
s O
) O
− O
( O
T O
dJ)(s O
) O


 O
ρ1(ds O
) O
≤ O
γ O
d+h O
A O
0 O
d+h O
Z O
S- O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O


  O
ρ0(ds O
) O
. O

Summing O
the O
two O
inequalities O
, O
we O
obtain O
: O
kT O
dJ O
− O
T O
dV O
µ O
k1,ρ1 O
≤ O
γ O
d+h O
A O
0 O
d+h O
Z O
S+ O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O


  O
ρ0(ds O
) O
+ O
Z O
S- O


 O
V O
( O
s O
) O
− O
V O
µ O
( O
s O
) O


  O
ρ0(ds O
) O
 O
= O
γ O
d+h O
A O
0 O
d+h O
kV O
− O
V O
µ O
k1,ρ0 O
, O
which O
completes O
the O
proof O
. O

B.3 O
Additional O
Technical O
Lemmas O
Lemma O
B.1 O
( O
Section O
4 O
, O
Corollary O
2 O
of O
Haussler O
[ O
1992 O
] O
) O
. O

Let O
G O
be O
a O
set O
of O
functions O
from O
X O
to O
[ O
0 O
, O
B O
] O
with O
pseudo O
- O
dimension O
dG O
< O
∞. O
Then O
for O
all O
0 O
< O
 O
≤ O
B O
, O
it O
holds O
that O
P O


 O
sup O
g∈G O





 O
1 O
m O
Xm O
i=1 O
g(X(i O
) O
) O
− O
E O


 O
g(X O
) O






 O
> O
 O
! O
≤ O
8 O
 O
32eB O
 O
log O
32eB O
 O
dG O
exp O
 O
− O
 O
2 O
m O
64B2 O
 O
, O
( O
B.3 O
) O
where O
X(i O
) O
are O
i.i.d O
. O

draws O
from O
the O
distribution O
of O
the O
random O
variable O
X. O
Lemma O
B.2 O
. O

Consider O
a O
policy O
µ O
∈ O
Π O
and O
suppose O
each O
s O
i O
is O
sampled O
i.i.d O
. O

from O
ρ0 O
. O

Define O
initial O
states O
s O
ij O
0 O
= O
s O
i O
for O
all O
j. O
Analogous O
to O
Step O
5 O
of O
the O
algorithm O
and O
Assumption O
1 O
, O
let O
: O
Yˆ O
( O
s O
i O
) O
= O
1 O
M0 O
X O
M0 O
j=1 O
X∞ O
t=0 O
γ O
t O
r(s O
ij O
t O
, O
µ(s O
ij O
t O
) O
) O
and O
V O
∈ O
arg O
min O
f∈F¯ O
1 O
N0 O
X O
N0 O
i=1 O


 O
f(s O
i O
) O
− O
Yˆ O
( O
s O
i O
) O



 O
. O

3 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
For O
δ O
∈ O
( O
0 O
, O
1 O
) O
and O
 O
∈ O
( O
0 O
, O
Vmax O
) O
, O
if O
the O
number O
of O
sampled O
states O
N0 O
satisfies O
the O
condition O
: O
N0 O
≥ O
 O
32Vmax O
 O
2 O
 O
log O
32 O
δ O
+ O
2dF¯ O
log O
64eVmax O
 O
 O
= O
: O
Γa( O
, O
δ O
) O
, O
and O
the O
number O
of O
rollouts O
performed O
from O
each O
state O
M0 O
satisfies O
: O
M0 O
≥ O
8 O
 O
Vmax O
 O
2 O
log O
8N0 O
δ O
= O
: O
Γb( O
, O
δ O
) O
, O
then O
we O
have O
the O
following O
bound O
on O
the O
error O
of O
the O
value O
function O
approximation O
: O
kV O
− O
V O
µ O
k1,ρ0 O
≤ O
min O
f∈F¯ O
kf O
− O
V O
µ O
k1,ρ0 O
+ O
 O
, O
with O
probabilty O
at O
least O
1 O
− O
δ O
. O

Proof O
. O

Recall O
that O
the O
estimated O
value O
function O
V O
satisfies O
V O
∈ O
arg O
min O
f∈Fb O
1 O
N0 O
X O
N0 O
i=1 O






 O
f(s O
i O
) O
− O
1 O
M0 O
X O
M0 O
j=1 O
h O
V O
π O
( O
s O
i O
0 O
) O
+ O
ξ O
j O
( O
s O
i O
0 O
) O
i O






 O
, O
where O
for O
each O
i O
, O
the O
terms O
ξ O
j O
( O
s O
i O
0 O
) O
are O
i.i.d O
. O

mean O
zero O
error O
. O

The O
inner O
summation O
over O
j O
is O
an O
equivalent O
way O
to O
write O
Yˆ O
( O
s O
i O
0 O
) O
. O

Noting O
that O
the O
rollout O
results O
V O
µ O
( O
s O
i O
0 O
) O
+ O
ξ O
j O
( O
s O
i O
0 O
) O
∈ O
[ O
0 O
, O
Vmax O
] O
, O
we O
have O
by O
Hoeffding O
’s O
inequality O
followed O
by O
a O
union O
bound O
: O
P O
 O
max O
i O


 O
Yˆ O
( O
s O
i O
0 O
) O
− O
V O
µ O
( O
s O
i O
0 O
) O


  O
> O
 O
≤ O
N0 O
∆1( O
, O
M0 O
) O
, O
( O
B.4 O
) O
where O
∆1( O
, O
M0 O
) O
= O
2 O
exp O
−2M0 O
 O
2 O
/ O
V O
2 O
max O
. O

Define O
the O
function O
∆2( O
, O
N0 O
) O
= O
8 O
 O
32eVmax O
 O
log O
32eVmax O
 O
dF¯ O
exp O
 O
− O
 O
2N0 O
64V O
2 O
max O
 O
, O
representing O
the O
right O
- O
hand O
- O
side O
of O
the O
bound O
in O
Lemma O
B.1 O
with O
B O
= O
Vmax O
and O
m O
= O
N0 O
. O

Next O
, O
we O
define O
the O
loss O
minimizing O
function O
f O
∗ O
∈ O
arg O
minf∈F¯ O
kf O
−V O
µk1,ρ0 O
. O

By O
Lemma O
B.1 O
, O
the O
probabilities O
of O
the O
events O
 O




 O
kV O
− O
V O
µ O
k1,ρ0 O
− O
1 O
N0 O
X O
N0 O
i=1 O


 O
V O
( O
s O
i O
) O
− O
V O
µ O
( O
s O
i O
) O







 O
> O
 O
4 O
 O
and O
 O




 O
kf O
∗ O
− O
V O
µ O
k1,ρ0 O
− O
1 O
N0 O
X O
N0 O
i=1 O


 O
f O
∗ O
( O
s O
i O
) O
− O
V O
µ O
( O
s O
i O
) O







 O
> O
 O
4 O
 O
( O
B.5 O
) O
are O
each O
bounded O
by O
∆2(/4 O
, O
N0 O
) O
. O

Also O
, O
it O
follows O
by O
the O
definition O
of O
V O
that O
1 O
N0 O
X O
N0 O
i=1 O


 O
V O
( O
s O
i O
) O
− O
Yˆ O
( O
s O
i O
) O


  O
≤ O
1 O
N0 O
X O
N0 O
i=1 O


 O
f O
∗ O
( O
s O
i O
) O
− O
Yˆ O
( O
s O
i O
) O



 O
. O

Therefore O
, O
using O
( O
B.4 O
) O
twice O
and O
( O
B.5 O
) O
once O
, O
we O
have O
by O
a O
union O
bound O
that O
the O
inequality O
kV O
− O
V O
µ O
k1,ρ0 O
≤ O
minf∈F¯ O
kf O
− O
V O
µk1,ρ0 O
+ O
 O
4 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learnin O
happens O
with O
probability O
greater O
than O
1 O
− O
2 O
N0 O
∆1(/4 O
, O
M0 O
) O
− O
2 O
∆2(/4 O
, O
N0 O
) O
. O

We O
then O
choose O
N0 O
so O
that O
∆2(/4 O
, O
N0 O
) O
= O
δ/4 O
( O
following O
Haussler O
[ O
1992 O
] O
, O
we O
utilize O
the O
inequality O
log(a O
log O
a O
) O
< O
2 O
log(a/2 O
) O
for O
a O
≥ O
5 O
) O
. O

To O
conclude O
, O
we O
choose O
M0 O
so O
that O
∆1(/4 O
, O
M0 O
) O
= O
δ/(4N0 O
) O
. O

Lemma O
B.3 O
( O
Sampling O
Error O
) O
. O

Suppose O
|A| O
= O
2 O
and O
let O
dΠ¯ O
be O
the O
VC O
- O
dimension O
of O
Π¯ O
. O

Consider O
Z O
, O
V O
∈ O
F O
and O
suppose O
each O
s O
i O
is O
sampled O
i.i.d O
. O

from O
ρ1 O
. O

Also O
, O
let O
w O
j O
be O
i.i.d O
. O

samples O
from O
the O
standard O
uniform O
distribution O
and O
g O
: O
S O
× O
A O
× O
[ O
0 O
, O
1 O
] O
→ O
S O
be O
a O
transition O
function O
such O
that O
g(s O
, O
a O
, O
w O
) O
has O
the O
same O
distribution O
as O
p(·|s O
, O
a O
) O
. O

For O
δ O
∈ O
( O
0 O
, O
1 O
) O
and O
 O
∈ O
( O
0 O
, O
Vmax O
) O
, O
if O
the O
number O
of O
sampled O
states O
N1 O
satisfies O
the O
condition O
: O
N1 O
≥ O
128 O
 O
Vmax O
 O
2 O
 O
log O
8 O
δ O
+ O
dΠ¯ O
log O
eN1 O
dΠ¯ O
 O
= O
: O
Γc( O
, O
δ O
, O
N1 O
) O
, O
and O
the O
number O
of O
sampled O
transitions O
L1 O
satisfies O
: O
L1 O
≥ O
128 O
 O
Vmax O
 O
2 O
 O
log O
8 O
δ O
+ O
dΠ¯ O
log O
eL1 O
dΠ¯ O
 O
= O
: O
Γd( O
, O
δ O
, O
L1 O
) O
, O
then O
we O
have O
the O
bounds O
: O
( O
a O
) O
sup O
π∈Π¯ O





 O
1 O
N1 O
X O
N1 O
i=1 O
|Z(s O
i O
) O
− O
( O
Tπ O
V O
) O
( O
s O
i O
) O
| O
− O
kZ O
− O
Tπ O
V O
k1,ρ1 O





 O
≤ O
 O
w.p O
. O

at O
least O
1 O
− O
δ O
. O

( O
b O
) O
sup O
π∈Π¯ O





 O
1 O
L1 O
X O
L1 O
j=1 O
h O
r(s O
i O
, O
π(s O
i O
) O
) O
+ O
γ O
V O
g(s O
i O
, O
π(s O
i O
) O
, O
wj O
) O
 O
i O
−(Tπ O
V O
) O
( O
s O
i O
) O





 O
≤ O
 O
w.p O
. O

at O
least O
1−δ O
. O

Proof O
. O

We O
remark O
that O
in O
both O
( O
a O
) O
and O
( O
b O
) O
, O
the O
term O
within O
the O
absolute O
value O
is O
bounded O
between O
0 O
and O
Vmax O
. O

A O
second O
remark O
is O
that O
we O
reformulated O
the O
problem O
using O
w O
j O
to O
take O
advantage B
of O
the O
fact O
that O
these O
random O
samples O
do O
not O
depend O
on O
the O
policy O
π O
. O

Such O
a O
property O
is O
required O
to O
invoke O
[ O
Gy¨orfi O
et O
al O
. O

, O
2006 O
, O
Theorem O
9.1 O
] O
, O
a O
result O
that O
[ O
Lazaric O
et O
al O
. O

, O
2016 O
, O
Lemma O
3 O
] O
depends O
on O
. O

With O
these O
two O
issues O
in O
mind O
, O
an O
argument O
similar O
to O
the O
proof O
of O
[ O
Lazaric O
et O
al O
. O

, O
2016 O
, O
Lemma O
3 O
] O
gives O
the O
conclusion O
for O
both O
( O
a O
) O
and O
( O
b O
) O
. O

B.4 O
Proof O
of O
Lemma O
3 O
On O
each O
iteration O
of O
the O
the O
algorithm O
, O
two O
random O
samples O
are O
used O
: O
S0,k O
and O
S1,k O
. O

From O
S0,k O
, O
we O
obtain O
Vk O
and O
from O
S1,k O
we O
obtain O
πk+1 O
. O

Let O
Sk O
= O
( O
S0,k O
, O
S1,k O
) O
represent O
both O
of O
the O
samples O
at O
iteration O
k. O
We O
define O
: O
Gk−1 O
= O
σ{S1 O
, O
S2 O
, O
. O

. O

. O

, O
Sk−1 O
} O
and O
G O
0 O
k−1 O
= O
σ{S1 O
, O
S2 O
, O
. O

. O

. O

, O
Sk−1 O
, O
S0,k O
} O
. O

Due O
to O
the O
progression B
of O
the O
algorithm O
with O
two O
random O
samples O
per O
iteration O
, O
we O
will O
analyze O
each O
iteration O
in O
two O
steps O
. O

We O
first O
separate O
the O
two O
random O
samples O
by O
noting O
5 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learnin O
that O
: O
kT O
dV O
πk O
− O
Tπk+1 O
V O
πk O
k1,ρ1 O
≤ O
kT O
dV O
πk O
− O
T O
dJkk1,ρ1 O
+ O
kTπk+1 O
Vk O
− O
Tπk+1 O
V O
πk O
k1,ρ1 O
+ O
kT O
dJk O
− O
Tπk+1 O
Vkk1,ρ1 O
≤ O
( O
γ O
A0 O
1 O
+ O
γ O
d+hA O
0 O
d+h O
) O
kVk O
− O
V O
πk O
k1,ρ0 O
+ O
kT O
dJk O
− O
Tπk+1 O
Vkk1,ρ1 O
, O
( O
B.6 O
) O
where O
the O
first O
inequality O
follows O
by O
adding O
and O
subtracting O
terms O
and O
the O
triangle O
inequality O
while O
the O
second O
inequality O
follows O
by O
Lemma O
2 O
. O

Now O
, O
we O
may O
analyze O
the O
first O
term O
on O
the O
right O
- O
hand O
- O
side O
conditional O
on O
Gk−1 O
and O
the O
second O
term O
conditional O
on O
G O
0 O
k−1 O
. O

As O
it O
is O
currently O
stated O
, O
Lemma O
B.2 O
gives O
an O
unconditional O
probability O
for O
a O
fixed O
policy O
µ. O
However O
, O
since O
S0,k O
is O
independent O
from O
Gk−1 O
and O
πk O
is O
Gk−1-measurable O
, O
we O
can O
utilize O
Lemma O
B.2 O
in O
a O
conditional O
setting O
using O
a O
well O
- O
known O
property O
of O
conditional O
expectations O
[ O
Resnick O
, O
2013 O
, O
Property O
12 O
, O
Section O
10.3 O
] O
. O

This O
property O
will O
be O
repeatedly O
used O
in O
this O
proof O
( O
without O
further O
mention O
) O
. O

We O
obtain O
that O
for O
a O
sample O
size O
N0 O
≥ O
Γa( O
0/(γ O
A0 O
1 O
+ O
γ O
d+hA0 O
d+h O
) O
, O
δ0 O
) O
, O
P O
 O
kVk O
− O
V O
πk O
k1,ρ0 O
> O
min O
f∈F¯ O
kf O
− O
V O
πk O
k1,ρ0 O
+ O
 O
0 O
/(γ O
A0 O
1 O
+ O
γ O
d+hA O
0 O
d+h O
) O


  O
Gk−1 O
 O
≤ O
δ O
0 O
. O

( O
B.7 O
) O
It O
remains O
for O
us O
to O
analyze O
the O
error O
of O
the O
second O
term O
kT O
dJk O
− O
Tπk+1 O
Vkk1,ρ1 O
. O

By O
part O
( O
a O
) O
of O
Lemma O
B.3 O
with O
Z O
= O
T O
dJk O
and O
V O
= O
Vk O
, O
if O
N1 O
≥ O
Γc( O
0 O
, O
δ0 O
, O
N1 O
) O
and O
s O
i O
are O
sampled O
i.i.d O
. O

from O
ρ1 O
, O
we O
have O
P O

 



 O
1 O
N1 O
X O
N1 O
i=1 O


 O
( O
T O
dJk)(s O
i O
) O
− O
( O
Tπk+1 O
Vk)(s O
i O
) O


  O
− O




 O
T O
dJk O
− O
Tπk+1 O
Vk O





 O
1,ρ1 O





 O
> O
0 O



  O
G O
0 O
k−1 O
! O
≤ O
δ O
0 O
. O

( O
B.8 O
) O
The O
term O
( O
Tπk+1 O
Vk)(s O
i O
) O
is O
approximated O
using O
L1 O
samples O
. O

Part O
( O
b O
) O
of O
Lemma O
B.3 O
along O
with O
a O
union O
bound O
shows O
that O
if O
L1 O
≥ O
Γd( O
0 O
, O
δ0 O
/ O
N1 O
, O
L1 O
) O
, O
then O
P O
 O
max O
i O


 O
Qˆ O
k(s O
i O
, O
πk+1(s O
i O
) O
) O
− O
( O
Tπk+1 O
Vk)(s O
i O
) O


  O
> O
0 O


  O
G O
0 O
k−1 O
 O
≤ O
δ O
0 O
. O

( O
B.9 O
) O
Similarly O
, O
by O
Assumption O
3 O
, O
if O
the O
number O
of O
iterations O
of O
MCTS O
M1 O
exceeds O
m( O
0 O
, O
δ0 O
/ O
N1 O
) O
, O
we O
can O
take O
a O
union O
bound O
to O
arrive O
at O
P O
 O
max O
i O


 O
Uˆ O
k(s O
i O
) O
− O
( O
T O
dJk)(s O
i O
) O


  O
> O
0 O



  O
G O
0 O
k−1 O
 O
≤ O
δ O
0 O
. O

( O
B.10 O
) O
The O
maximum O
over O
i O
can O
be O
replaced O
with O
an O
average O
over O
the O
N1 O
samples O
and O
the O
conclusion O
of O
the O
last O
two O
bounds O
would O
remain O
unchanged O
. O

Since O
πk+1 O
is O
assumed O
to O
optimize O
a O
quantity O
involving O
Uˆ O
k O
and O
Qˆ O
k O
, O
we O
want O
to O
relate O
this O
back O
to O
kT O
dJk O
−Tπk+1 O
Vkk1,ρ1 O
. O

Indeed O
, O
taking O
expectation O
of O
both O
sides O
of O
inequalities O
( O
B.8)–(B.10 O
) O
and O
then O
combining O
, O
we O
obtain O
that O
with O
probability O
at O
least O
1 O
− O
3δ O
0 O
, O




 O
T O
dJk O
− O
Tπk+1 O
Vk O





 O
1,ρ1 O
≤ O
1 O
N1 O
X O
N1 O
i=1 O


 O
Uˆ O
k(s O
i O
) O
− O
Qˆ O
k(s O
i O
, O
πk+1(s O
i O
) O
) O


  O
+ O
3 O
0 O
6 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
≤ O
1 O
N1 O
X O
N1 O
i=1 O


 O
Uˆ O
k(s O
i O
) O
− O
Qˆ O
k(s O
i O
, O
π˜(s O
i O
) O
) O


  O
+ O
3 O
0 O
where O
˜π O
∈ O
arg O
minπ∈Π¯ O
kT O
dV O
πk O
−Tπ O
V O
πk O
k1,ρ1 O
. O

Following O
the O
same O
steps O
in O
reverse O
, O
we O
have O
: O




 O
T O
dJk O
− O
Tπk+1 O
Vk O





 O
1,ρ1 O
≤ O
min O
π∈Π¯ O
kT O
dV O
πk O
− O
Tπ O
V O
πk O
k1,ρ1 O
+ O
6 O
0 O
, O
( O
B.11 O
) O
with O
probability O
at O
least O
1 O
− O
6 O
δ O
0 O
. O

Finally O
, O
we O
take O
expectation O
of O
both O
sides O
of O
( O
B.7 O
) O
and O
then O
combine O
with O
( O
B.6 O
) O
and O
( O
B.11 O
) O
while O
setting O
 O
0 O
= O
/7 O
and O
δ O
0 O
= O
δ/7 O
to O
obtain O
kT O
dV O
πk O
− O
Tπk+1 O
V O
πk O
k1,ρ1 O
≤ O
( O
γ O
A0 O
1 O
+ O
γ O
d+hA O
0 O
d+h O
) O
min O
f∈F¯ O
kf O
− O
V O
πk O
k1,ρ0 O
+ O
min O
π∈Π¯ O
kT O
dV O
πk O
− O
Tπ O
V O
πk O
k1,ρ1 O
+ O
 O
with O
probability O
at O
least O
1 O
− O
δ O
. O

B.5 O
Proof O
of O
Theorem O
1 O
This O
proof O
synthesizes O
the O
previous O
lemmas O
. O

From O
the O
definition O
of O
D0(Π¯ O
, O
F¯ O
) O
and O
D O
d O
1 O
( O
Π O
) O
¯ O
from O
the O
main O
paper O
, O
we O
note O
that O
if O
the O
sample O
size O
assumptions O
of O
Lemma O
3 O
are O
satisfied O
, O
kT O
dV O
πk− O
Tπk+1 O
V O
πk O
k1,ρ1 O
≤ O
B O
0 O
γ O
D0(Π¯ O
, O
F¯ O
) O
+ O
D O
d O
1 O
( O
Π O
) O
+ O
¯ O
 O
, O
( O
B.12 O
) O
with O
probability O
at O
least O
1 O
− O
δ O
. O

This O
removes O
any O
dependence O
on O
the O
iteration O
k O
from O
the O
right O
- O
hand O
- O
side O
. O

We O
now O
integrate O
all O
results O
with O
Lemma O
1 O
in O
order O
to O
find O
a O
bound O
on O
the O
suboptimality O
kV O
∗ O
− O
V O
πK O
k1,ν O
. O

Consider O
the O
distribution O
Λν O
, O
k O
, O
as O
defined O
in O
Lemma O
1 O
, O
which O
needs O
to O
be O
related O
to O
ν O
. O

We O
can O
use O
the O
power B
series O
expansion O
to O
write O
: O
Λν O
, O
k O
= O
ν O
( O
Pπ∗ O
) O
K−k O
X∞ O
i=0 O
( O
γPπk O
) O
i O
. O

For O
a O
fixed O
i O
, O
the O
measure O
ν O
is O
transformed O
by O
applying O
π O
∗ O
a O
total O
of O
K O
− O
k O
times B
and O
then O
πk O
a O
total O
of O
i O
times B
. O

We O
see O
that O
the O
summation O
term O
on O
the O
right O
- O
hand O
- O
side O
of O
Lemma O
1 O
can O
be O
upper O
- O
bounded O
in O
the O
following O
way O
: O
X O
K O
k=1 O
γ O
K−k O




 O
T O
dV O
πk−1 O
− O
Tπk O
V O
πk−1 O





 O
1,Λν O
, O
k O
≤ O
K O
X−1 O
j=0 O
X∞ O
i=0 O
γ O
j+i O
Aj+i O
 O
max O
k≤K O




 O
T O
dV O
πk−1 O
− O
Tπk O
V O
πk−1 O





 O
1,ρ1 O
, O
where O
we O
use O
Assumption O
5 O
with O
m O
= O
K O
− O
k O
+ O
i O
, O
maximize O
over O
k O
for O
the O
loss O
term O
, O
and O
then O
re O
- O
index O
with O
j O
= O
K O
− O
k. O
The O
coefficient O
in O
parentheses O
can O
be O
upper O
- O
bounded O
by O
Bγ O
( O
since O
all O
Aj+i O
are O
nonnegative O
) O
. O

Finally O
, O
we O
use O
( O
B.12 O
) O
and O
then O
a O
union O
bound O
over O
the O
K O
iterations O
to O
conclude O
the O
statement O
of O
the O
theorem O
. O

7 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
C O
Implementation O
Details O
C.1 O
Neural O
Network O
Architecture O
The O
policy O
and O
value O
function O
approximations O
use O
fully O
- O
connected O
neural O
networks O
with O
five O
and O
two O
hidden O
layers O
, O
respectively O
, O
and O
SELU O
( O
scaled O
exponential O
linear O
unit B
) O
activation O
[ O
Klambauer O
et O
al O
. O

, O
2017 O
] O
. O

The O
policy O
network O
contains O
two O
sets O
of O
outputs O
: O
( O
1 O
) O
one O
of O
seven O
actions O
( O
no O
action O
, O
normal O
attack B
, O
move B
, O
skill B
1 O
, O
skill B
2 O
, O
skill B
3 O
, O
and O
heal O
) O
and O
( O
2 O
) O
a O
twodimensional O
direction O
parameter O
used O
for O
the O
action O
. O

The O
first O
two O
hidden O
layers O
are O
shared O
and O
have O
120 O
and O
100 O
hidden B
units B
, O
while O
each O
of O
the O
two O
outputs O
corresponds O
to O
a O
set O
of O
three O
hidden O
layers O
with O
80 O
, O
70 O
, O
and O
50 O
hidden B
units B
. O

The O
value O
function O
approximation O
uses O
a O
fully O
- O
connected O
network O
with O
128 O
hidden B
units B
in O
the O
first O
layer O
and O
96 O
hidden B
units B
in O
the O
second O
layer O
. O

As O
mentioned O
in O
the O
main O
paper O
, O
this O
architecture O
is O
consistent O
across O
all O
agents O
whenever O
policy O
and/or O
value O
networks O
are O
needed O
. O

C.2 O
Features O
of O
the O
State O
As O
shown O
in O
Table O
1 O
, O
the O
state O
of O
the O
game B
is O
represented O
by O
41-dimensional O
feature O
vector O
, O
which O
was O
constructed O
using O
the O
output O
from O
the O
game B
engine O
and O
API O
. O

The O
features O
consists O
of O
basic O
attributes O
of O
the O
two O
heroes B
, O
the O
computer O
- O
controlled B
units B
, O
and O
structures O
. O

The O
feature O
lists O
also O
have O
information O
on O
the O
relative O
positions O
of O
the O
other O
units B
and O
structures O
with O
respect O
to O
the O
hero B
controlled B
by O
algorithm O
. O

Table O
1 O
: O
State O
Feature O
List O
No O
. O

Feature O
Dimensions O
1 O
Location O
of O
Hero O
1 O
2 O
2 O
Location O
of O
Hero O
2 O
2 O
3 O
HP O
of O
Hero O
1 O
1 O
4 O
HP O
of O
Hero O
2 O
1 O
5 O
Hero O
1 O
skill B
cooldowns O
5 O
6 O
Hero O
2 O
skill B
cooldowns O
5 O
7 O
Direction O
to O
enemy B
hero I
3 O
8 O
Direction O
to O
enemy B
tower I
4 O
9 O
Direction O
to O
enemy B
minion B
3 O
10 O
Enemy O
tower B
HP O
1 O
11 O
Enemy O
minion B
HP O
1 O
12 O
Direction O
to O
the O
spring O
3 O
13 O
Total O
HP O
of O
allied O
minions B
1 O
14 O
Enemy O
’s O
tower B
attacking B
Hero O
1 O
3 O
15 O
Hero O
1 O
in O
range B
of O
enemy B
towers I
3 O
16 O
Hero O
2 O
in O
range B
of O
enemy B
towers I
3 O
8 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
C.3 O
Tree O
Search O
Details O
We O
provide O
some O
more O
information O
regarding O
the O
implementation O
of O
feedback O
- O
based B
tree O
search O
. O

A O
major O
challenge O
in O
implementing O
in O
King O
of O
Glory O
is O
that O
the O
game B
engine O
can O
only O
move B
forward O
, O
meaning O
that O
our O
sampled O
states O
are O
not O
i.i.d O
. O

and O
instead O
follow O
the O
trajectory O
of O
the O
policy O
induced O
by O
MCTS O
. O

However O
, O
to O
decrease O
the O
correlation O
between O
visited O
states O
, O
we O
inject O
random O
movements B
and O
random O
switches O
to O
the O
internal O
AI O
policy O
in O
order O
to O
move B
to O
a O
“ O
more O
random O
” O
next O
state O
. O

Rollouts O
are O
performed O
on O
separate O
processors O
to O
enable O
tree O
search O
in O
a O
game B
engine O
that O
can O
not O
rewind O
. O

All O
experiments O
use O
the O
c4.2xlarge O
instances O
on O
Amazon O
Web O
Services O
, O
and O
we O
utilized O
parallelization O
across O
four O
cores O
within O
each O
call B
to O
MCTS O
. O

References O
L. O
Gy¨orfi O
, O
M. O
Kohler O
, O
A. O
Krzyzak O
, O
and O
H. O
Walk O
. O

A O
Distribution O
- O
free B
Theory O
of O
Nonparametric O
Regression O
. O

Springer O
Science O
& O
Business O
Media O
, O
2006 O
. O

D. O
Haussler O
. O

Decision O
theoretic O
generalizations O
of O
the O
PAC O
model O
for O
neural O
net O
and O
other O
learning O
applications O
. O

Information O
and O
Computation O
, O
100(1):78–150 O
, O
1992 O
. O

G. O
Klambauer O
, O
T. O
Unterthiner O
, O
A. O
Mayr O
, O
and O
S. O
Hochreiter O
. O

Self O
- O
normalizing O
neural O
networks O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
972–981 O
, O
2017 O
. O

A. O
Lazaric O
, O
M. O
Ghavamzadeh O
, O
and O
R. O
Munos O
. O

Analysis O
of O
classification O
- O
based B
policy O
iteration O
algorithms O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
17(19):1–30 O
, O
2016 O
. O

S. O
I. O
Resnick O
. O

A O
Probability O
Path O
. O

Springer O
Science O
& O
Business O
Media O
, O
2013 O
. O

9 O
Feedback O
- O
Based B
Tree O
Search O
for O
Reinforcement O
Learning O
The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
Games O
Zhengxing O
Chen O
Northeastern O
University O
Boston O
, O
MA O
, O
USA O
czxttkl@gmail.com O
Truong O
- O
Huy O
D O
Nguyen O
Fordham O
University O
Bronx O
, O
NY O
, O
USA O
tnguyen88@fordham.edu O
Yuyu O
Xu O
Northeastern O
University O
Boston O
, O
MA O
, O
USA O
yuyuxu@ccs.neu.edu O
Christopher O
Amato O
Northeastern O
University O
Boston O
, O
MA O
, O
USA O
c.amato@northeastern.edu O
Seth O
Cooper O
Northeastern O
University O
Boston O
, O
MA O
, O
USA O
scooper@ccs.neu.edu O
Yizhou O
Sun O
University O
of O
California O
, O
Los O
Angeles O
Los O
Angeles O
, O
CA O
, O
USA O
yzsun@cs.ucla.edu O
Magy O
Seif O
El O
- O
Nasr O
Northeastern O
University O
Boston O
, O
MA O
, O
USA O
m.seifel-nasr@northeastern.edu O
ABSTRACT O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
have O
received O
increasing B
popularity O
recently O
. O

In O
a O
match B
of O
such O
games B
, O
players B
compete O
in O
two O
teams B
of O
five O
, O
each O
controlling B
an O
in B
- I
game I
avatar I
, O
known O
as O
heroes B
, O
selected O
from O
a O
roster O
of O
more O
than O
100 O
. O

The O
selection O
of O
heroes B
, O
also O
known O
as O
pick B
or O
draft B
, O
takes O
place O
before O
the O
match B
starts B
and O
alternates O
between O
the O
two O
teams B
until O
each O
player B
has O
selected O
one O
hero B
. O

Heroes B
are O
designed O
with O
different O
strengths B
and O
weaknesses B
to O
promote O
team B
cooperation I
in O
a O
game B
. O

Intuitively O
, O
heroes B
in O
a O
strong O
team B
should O
complement O
each O
other O
’s O
strengths B
and O
suppress O
those O
of O
opponents B
. O

Hero B
drafting B
is O
therefore O
a O
challenging O
problem O
due O
to O
the O
complex O
hero B
- O
to O
- O
hero B
relationships O
to O
consider O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
hero B
recommendation O
system O
that O
suggests O
heroes B
to O
add O
to O
an O
existing O
team B
while O
maximizing O
the O
team B
’s O
prospect O
for O
victory B
. O

To O
that O
end O
, O
we O
model O
the O
drafting O
between O
two O
teams B
as O
a O
combinatorial O
game B
and O
use O
Monte O
Carlo O
Tree O
Search O
( O
MCTS O
) O
for O
estimating O
the O
values O
of O
hero B
combinations O
. O

Our O
empirical O
evaluation O
shows O
that O
hero B
teams B
drafted B
by O
our O
recommendation O
algorithm O
have O
significantly O
a O
higher O
win B
rate I
against O
teams B
constructed O
by O
other O
baseline O
and O
state O
- O
of O
- O
the O
- O
art O
strategies B
. O

KEYWORDS O
Monte O
Carlo O
Tree O
Search O
, O
Multiplayer O
Online O
Battle O
Arena O
, O
Hero O
Pick O
Permission O
to O
make O
digital O
or O
hard B
copies O
of O
all O
or O
part O
of O
this O
work O
for O
personal O
or O
classroom O
use O
is O
granted O
without O
fee O
provided O
that O
copies O
are O
not O
made O
or O
distributed O
for O
profit O
or O
commercial O
advantage B
and O
that O
copies O
bear O
this O
notice O
and O
the O
full B
citation O
on O
the O
first O
page O
. O

Copyrights O
for O
components O
of O
this O
work O
owned O
by O
others O
than O
ACM O
must O
be O
honored O
. O

Abstracting O
with O
credit O
is O
permitted O
. O

To O
copy O
otherwise O
, O
or O
republish O
, O
to O
post O
on O
servers B
or O
to O
redistribute O
to O
lists O
, O
requires O
prior O
specific O
permission O
and/or O
a O
fee O
. O

Request O
permissions O
from O
permissions@acm.org O
. O

Conference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USA O
© O
2018 O
Association O
for O
Computing O
Machinery O
. O

ACM O
ISBN O
978-x O
- O
xxxx O
- O
xxxx O
- O
x O
/ O
YY O
/ O
MM O
. O

. O

. O

$ O
15.00 O
https://doi.org/10.1145/nnnnnnn.nnnnnnn O
ACM O
Reference O
Format O
: O
Zhengxing O
Chen O
, O
Truong O
- O
Huy O
D O
Nguyen O
, O
Yuyu O
Xu O
, O
Christopher O
Amato O
, O
Seth O
Cooper O
, O
Yizhou O
Sun O
, O
and O
Magy O
Seif O
El O
- O
Nasr O
. O

2018 O
. O

The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
Games O
. O

In O
Proceedings O
of O
ACM O
Conference O
( O
Conference’17 O
) O
. O

ACM O
, O
New O
York O
, O
NY O
, O
USA O
, O
9 O
pages O
. O

https://doi.org/10.1145/nnnnnnn.nnnnnnn O
1 O
INTRODUCTION O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
is O
one O
of O
the O
most O
popular O
contemporary O
e O
- O
sports O
. O

Games B
such O
as O
League O
of O
Legends O
( O
Riot B
Games O
) O
and O
DOTA O
2 O
( O
Valve O
Corporation O
) O
have O
attracted O
millions O
of O
players B
to O
play B
and O
watch O
[ O
21 O
, O
36 O
] O
. O

In O
a O
classic O
match B
of O
such O
games B
, O
two O
teams B
, O
each O
composed O
of O
five O
players B
, O
combat B
in O
a O
virtual O
game B

 O
map I
, O
the O
goal B
of O
which O
is O
to O
beat O
the O
opposite B
team I
by O
destroying O
their O
base B
. O

Each O
player B
controls B
an O
in B
- I
game I
avatar I
, O
known O
as O
heroes B
, O
to O
co O
- O
operate O
with O
other O
teammates B
in O
attacking B
opponents B
’ O
heroes B
, O
armies O
, O
defensive O
structures O
, O
and O
ultimately O
base B
, O
while O
defending O
their O
own O
in B
- I
game I
properties O
. O

The O
selection O
of O
heroes B
, O
also O
known O
as O
pick B
or O
draft B
, O
takes O
place O
before O
each O
match B
starts B
and O
alternates O
between O
two O
teams B
until O
each O
player B
has O
selected O
one O
hero B
. O

The O
alternating O
order O
of O
drafting B
is O
“ O
1 O
- O
2 O
- O
2 O
- O
2 O
- O
2 O
- O
1 O
” O
, O
meaning O
that O
the O
first O
team B
picks B
one O
hero B
, O
followed O
by O
the O
second O
team B
picking B
two O
heroes B
, O
then O
the O
first O
team B
picking B
two O
heroes B
, O
and O
so O
on O
. O

The O
process O
ends O
with O
the O
second O
team B
picking B
their O
last O
hero B
. O

We O
refer O
to O
the O
10 O
heroes B
in O
a O
completed O
draft B
as O
a O
hero B
line O
- O
up O
. O

Heroes B
are O
often O
designed O
with O
a O
variety O
of O
physical O
attributes O
and O
skills B
, O
which O
together O
add O
to O
a O
team B
’s O
overall O
power B
. O

Therefore O
, O
players B
need O
to O
draft B
heroes B
that O
can O
enhance O
the O
strengths B
and O
compensate O
for O
weaknesses B
of O
teammates B
’ O
heroes B
( O
i.e. O
, O
synergy O
) O
, O
while O
posing O
suppressing O
strengths B
over O
those O
in O
the O
opponent B
team I
( O
i.e. O
, O
opposition O
) O
. O

For O
example O
, O
in O
DOTA O
2 O
, O
hero B
Clockwerk O
has O
high O
synergy O
with O
Naix O
because O
Clockwerk O
can O
transport O
Naix O
to O
target B
enemies B
directly O
, O
making O
up O
for O
Naix O
’s O
limited O
mobility B
in O
fighting O
. O

In O
another O
example O
, O
hero B
Anti O
- O
Mage O
’s O
mana B
burn I
skill I
reduces O
an O
opponent B
’s O
mana B
resource I
, O
making O
him O
a O
natural O
opposition O
to O
arXiv:1806.10130v1 O
[ O
cs O
. O

AI O
] O
26 O
Jun O
2018 O
Conference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USAZhengxing O
Chen O
, O
Truong O
- O
Huy O
D O
Nguyen O
, O
Yuyu O
Xu O
, O
Christopher O
Amato O
, O
Seth O
Cooper O
, O
Yizhou O
Sun O
, O
and O
Magy O
Seif O
El O
- O
Nasr O
Medusa O
, O
the O
durability O
of O
whom O
is O
completely O
reliant O
on O
how O
much O
mana B
she O
has O
. O

In O
games B
like O
League O
of O
Legends O
and O
DOTA O
2 O
, O
there O
are O
possibly O
more O
than O
100 O
heroes B
that O
can O
be O
picked B
by O
a O
player B
at O
the O
time B
of O
drafting B
. O

As O
estimated O
by O
[ O
15 O
] O
, O
the O
number O
of O
possible O
hero B
lineups O
in O
DOTA O
2 O
is O
approximately O
1.56 O
× O
1016 O
. O

Due O
to O
the O
complex O
synergistic O
and O
opposing O
relationships O
among O
heroes B
, O
as O
described O
above O
, O
and O
large O
numbers O
of O
follow O
- O
up O
pick B
possibilities O
by O
other O
players B
, O
selecting O
a O
suitable O
hero B
that O
can O
synergize O
teammates B
and O
counter O
opponents B
is O
a O
challenging O
task O
to O
human O
players B
. O

In O
terms O
of O
the O
gaming O
experience B
, O
failing O
to O
pick B
heroes B
that O
fit O
teammates B
’ O
currently O
selected O
heroes B
and O
counter O
opponent B
heroes B
can O
lead B
to O
negative O
feelings O
, O
reducing O
the O
joy O
of O
playing B
an O
otherwise O
entertaining O
game B
and O
igniting O
even O
toxic O
behaviors O
. O

Specifically O
, O
with O
poorly O
picked B
heroes B
, O
players B
tend O
to O
experience B
repeated O
losses O
, O
which O
brings O
frustration O
and O
leads B
to O
disengagement O
in O
the O
game B
[ O
23 O
, O
28 O
, O
30 O
, O
38 O
] O
. O

Furthermore O
, O
it O
may O
lower O
players B
’ O
confidence O
and O
gives O
rise O
to O
quitting O
matches B
early B
[ O
31 O
] O
, O
blaming O
[ O
19 O
] O
, O
and O
cyberbullying O
[ O
20 O
] O
, O
all O
of O
which O
negatively O
impact B
the O
experience B
of O
not O
only O
one O
player B
but O
potentially O
the O
whole O
player B
community B
. O

Since O
MOBA O
games B
are O
team B
- O
based I
, O
we O
propose O
a O
hero B
pick B
recommendation O
system O
for O
suggesting O
the O
approximated O
“ O
optimal O
” O
hero B
pick B
for O
team B
victory B
. O

To O
this O
end O
, O
we O
view O
the O
drafting O
between O
two O
teams B
as O
a O
combinatorial O
game B
, O
characterized O
as O
a O
two O
- O
person O
zero O
- O
sum O
game B
with O
perfect O
information O
, O
discrete O
and O
sequential O
actions O
and O
deterministic O
rewards B
[ O
5 O
] O
. O

Under O
this O
problem O
formulation O
, O
the O
goal B
is O
to O
decide O
at O
each O
step O
an O
optimal O
hero B
pick B
that O
leads B
to O
the O
hero B
line O
- O
up O
with O
the O
largest O
predicted O
win B
rate I
on O
its O
team B
, O
assuming O
that O
both O
teams B
behave O
optimally O
in O
the O
remaining O
picks B
. O

This O
problem O
, O
known O
as O
sequential O
decision O
making O
, O
can O
be O
solved O
by O
search O
algorithms O
. O

However O
, O
in O
the O
early B
or O
mid O
stage B
of O
the O
draft B
, O
exhaustive O
search O
might O
be O
prohibitive O
due O
to O
the O
large O
branching O
factor O
of O
more O
than O
100 O
candidate O
heroes B
per O
pick B
. O

To O
tackle O
the O
large O
search O
space O
, O
we O
propose O
to O
use O
Monte O
Carlo O
Tree O
Search O
( O
MCTS O
) O
, O
a O
heuristic O
search O
algorithm O
that O
efficiently O
estimates O
the O
long O
- O
term O
value O
of O
each O
pick B
by O
simulating O
possible O
following O
picks B
until O
completion O
. O

Each O
hero B
- O
lineup O
is O
associated O
with O
a O
reward B
, O
defined O
as O
a O
predicted O
team B
win I
rate O
representing O
the O
estimated O
strength B
of O
the O
hero B
line O
- O
up O
. O

MCTS O
then O
back O
- O
propagates O
the O
reward B
to O
update B
the O
values O
of O
simulated O
picks B
. O

As O
more O
simulations O
are O
executed O
, O
the O
estimated O
values O
become O
more O
accurate O
, O
allowing O
MCTS O
to O
identify O
the O
next O
pick B
optimal O
for O
team B
victory B
. O

The O
specific O
version O
of O
MCTS O
[ O
18 O
] O
we O
use O
, O
namely O
Upper O
Confidence O
Bound O
applied O
to O
Trees O
, O
or O
UCT O
, O
is O
an O
anytime O
algorithm O
, O
i.e. O
, O
it O
has O
the O
theoretical O
guarantee O
to O
converge O
to O
the O
optimal O
pick B
given O
sufficient O
time B
and O
memory O
, O
while O
it O
can O
be O
stopped O
at O
any O
time B
to O
return O
an O
approximate O
solution O
. O

In O
contrast O
, O
previous O
works O
either O
predict O
player B
tendency O
to O
pick B
heroes B
[ O
35 O
] O
, O
or O
leverage O
association O
rules B
mined O
from O
historical O
hero B
line O
- O
ups O
as O
heuristics O
[ O
15 O
] O
. O

They O
are O
not O
guaranteed O
to O
converge O
to O
the O
optimal O
hero B
pick B
for O
team B
victory B
. O

In O
sum O
, O
the O
contributions O
of O
our O
paper O
are O
three O
- O
fold O
. O

( O
1 O
) O
we O
provide O
a O
formal O
formulation O
of O
the O
drafting B
process O
in O
MOBA O
games B
as O
a O
combinatorial O
game B
; O
( O
2 O
) O
we O
detail O
a O
new O
hero B
recommendation O
approach O
to O
solve O
the O
above O
problem O
using O
an O
MCTS O
algorithm O
; O
( O
3 O
) O
we O
report B
the O
results O
of O
our O
empirical O
simulation O
experiments O
, O
demonstrating O
the O
superiority O
of O
our O
algorithm O
over O
other O
baseline O
and O
state O
- O
of O
- O
the O
- O
arts O
strategies B
. O

Our O
paper O
is O
structured O
as O
follows O
. O

We O
first O
present O
background O
and O
related O
works O
, O
then O
introduce O
the O
problem O
formulation O
and O
our O
algorithm O
. O

Next O
, O
we O
detail O
the O
procedure O
of O
our O
experiments O
as O
well O
as O
evaluation O
results O
. O

Finally O
, O
we O
conclude O
our O
paper O
and O
discuss O
limitations O
as O
well O
as O
future O
directions O
. O

2 O
DRAFTING O
IN O
MOBA O
GAMES O
MOBA O
games B
have O
attracted O
a O
variety O
of O
research O
thanks O
to O
their O
complexity O
and O
design O
richness O
. O

Some O
research O
problems O
of O
interest O
include O
team B
formation I
analysis O
[ O
16 O
, O
22 O
, O
25 O
, O
26 O
] O
, O
skill B
analysis O
[ O
7 O
, O
9 O
] O
, O
and O
hero B
pick B
recommendation O
systems O
[ O
15 O
, O
35 O
] O
. O

A O
typical O
MOBA O
match B
is O
played B
by O
two O
teams B
of O
five O
players B
, O
each O
of O
whom O
controls B
one O
hero B
selected O
from O
a O
pool O
of O
more O
than O
100 O
candidates O
before O
the O
match B
begins O
. O

Each O
hero B
has O
different O
abilities B
, O
making O
it O
suitable O
for O
different O
tactical O
roles B
or O
play B

 O
styles I
in O
the O
game B
, O
e.g. O
dealing O
long O
- O
distance O
damage B
, O
healing O
teammates B
, O
or O
spearheading O
with O
strong O
shields B
. O

Moreover O
, O
there O
exist O
sophisticated O
synergistic O
and O
oppositional O
relationships O
between O
heroes B
, O
which O
are O
learned O
over O
time B
by O
experienced O
players B
. O

Due O
to O
such O
sophistication O
, O
the O
hero B
drafting B
phase O
also O
becomes O
a O
critical O
component O
contributing O
to O
match B
outcomes O
, O
even O
though O
it O
happens O
before O
the O
real O
match B
starts B
. O

Previous O
research O
has O
highlighted B
that O
interactions O
between O
heroes B
greatly O
influence O
match B
outcomes O
[ O
16 O
, O
25 O
, O
29 O
] O
. O

There O
also O
exist O
many O
online O
discussions O
recommending O
hero B
picks B
within O
the O
player B
communities O
1 O
2 O
. O

In O
the O
drafting O
process O
, O
two O
teams B
alternate O
to O
pick B
heroes B
in O
a O
“ O
1 O
- O
2 O
- O
2 O
- O
2 O
- O
2 O
- O
1 O
” O
order O
, O
during O
which O
heroes B
already O
selected O
are O
visible O
to O
both O
teams B
. O

Heroes B
can O
only O
be O
selected O
from O
a O
fixed O
pool O
and O
no O
duplication O
is O
allowed O
in O
the O
same O
match B
. O

Note O
that O
the O
drafting B
rules B
described O
here O
are O
based B
on O
“ O
Ranked B
All O
Pick O
” O
, O
a O
popular O
match B
mode O
in O
DOTA O
2 O
. O

For O
simplicity O
of O
illustration O
, O
we O
will O
first O
focus B
on O
this O
kind O
of O
match B
and O
its O
drafting O
process O
. O

There O
are O
other O
more O
sophisticated O
mechanics B
deployed O
in O
the O
drafting O
process O
that O
may O
affect O
match B
outcomes O
, O
such O
as O
banning O
( O
i.e. O
, O
certain O
heroes B
can O
be O
prohibited O
from O
selection O
by O
either O
team B
) O
, O
which O
we O
will O
study O
in O
the O
end O
of O
our O
performance O
evaluation O
( O
Section O
5.6 O
) O
. O

3 O
RELATED O
WORKS O
Previous O
works O
on O
hero B
pick B
recommendation O
can O
be O
categorized O
into O
two O
main O
approaches O
, O
based B
on O
( O
1 O
) O
historical O
selection O
frequency O
, O
and O
( O
2 O
) O
previous O
win B
rate I
. O

In O
the O
first O
category O
, O
researchers O
proposed O
to O
recommend O
heroes B
that O
players B
tend O
to O
select O
[ O
35 O
] O
. O

Essentially O
, O
hero B
pick B
recommendation O
is O
modelled O
as O
a O
sequence O
prediction O
problem O
, O
in O
which O
the O
next O
hero B
to O
pick B
is O
predicted O
from O
the O
sequence O
of O
picks B
made O
already O
in O
the O
draft B
. O

Since O
the O
sequence O
prediction O
model O
is O
trained B
from O
pick B
sequences O
in O
historical O
matches B
, O
the O
predicted O
hero B
is O
“ O
what O
is O
likely O
to O
be O
picked B
, O
not O
what O
is O
necessarily O
best O
” O
[ O
35 O
] O
. O

Therefore O
, O
hero B
recommendation O
based B
on O
such O
a O
method O
may O
not O
be O
optimal O
for O
team B
victory B
. O

1http://www.weskimo.com/a-guide-to-drafting.html O
2https://www.reddit.com/r/learndota2/comments/3f9szo/how_to_counter_pick_he O
roes/ O
The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
GamesConference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USA O
In O
the O
second O
category O
, O
heuristics O
are O
learned O
and O
used O
to O
seek O
for O
heroes B
that O
can O
improve O
the O
team B
expected O
win I
rate O
. O

Hanke O
and O
Chaimowicz O
proposed O
to O
mine O
association O
rules B
[ O
1 O
] O
from O
historical O
hero B
line O
- O
up O
data O
and O
use O
them O
as O
the O
heuristic O
to O
recommend O
heroes B
[ O
15 O
] O
. O

Here O
, O
association O
rules B
are O
hero B
subsets O
that O
appear O
frequently O
together O
either O
in O
the O
winning O
team B
or O
in O
opposite B
teams I
. O

Any O
hero B
contained O
in O
the O
discovered O
association O
rules B
together O
with O
the O
heroes B
picked B
already O
is O
suggested O
to O
be O
a O
good O
candidate O
to O
pick B
next O
. O

However O
, O
such O
method O
does O
not O
consider O
what O
heroes B
will O
be O
picked B
in O
the O
rest O
of O
the O
drafting O
process O
, O
hence O
this O
is O
essentially O
a O
myopic O
, O
greedy O
- O
based B
approach O
. O

Although O
there O
are O
other O
works O
using O
machine O
learning O
models O
to O
predict O
match B
outcomes O
[ O
29 O
, O
37 O
] O
, O
they O
do O
not O
focus B
on O
how O
to O
utilize O
these O
models O
for O
hero B
recommendation O
. O

4 O
METHODOLOGIES O
In O
this O
section O
we O
will O
formally O
describe O
how O
a O
draft B
between O
two O
teams B
can O
be O
cast O
as O
a O
combinatorial O
game B
. O

We O
will O
then O
proceed O
to O
presenting O
how O
Monte O
Carlo O
Tree O
Search O
( O
MCTS O
) O
is O
applied O
to O
compute O
the O
optimal O
hero B
pick B
. O

4.1 O
Problem O
Formulation O
As O
we O
introduced O
previously O
, O
a O
draft B
takes O
place O
with O
players B
from O
the O
two O
teams B
picking B
heroes B
in O
an O
alternating O
order O
. O

We O
assume O
all O
players B
from O
the O
same O
team B
share O
the O
same O
goal B
: O
build O
a O
hero B
line O
- O
up O
with O
the O
highest O
predicted O
win B
rate I
as O
a O
team B
prior O
to O
the O
real O
match B
starts B
. O

Therefore O
, O
although O
in O
real O
life O
the O
five O
players B
on O
a O
team B
are O
different O
individuals O
, O
we O
regard O
the O
whole O
team B
as O
one O
collective O
player B
. O

As O
such O
, O
the O
draft B
can O
be O
considered O
as O
a O
game B
played B
by O
two O
players B
, O
each O
representing O
a O
team B
. O

Following O
the O
terminology O
in O
DOTA O
2 O
, O
the O
two O
players B
are O
called B
Radiant O
and O
Dire O
, O
respectively O
. O

Without O
loss O
of O
generality O
, O
we O
assume O
that O
the O
problem O
is O
formulated O
with O
Radiant O
being O
the O
team B
to O
pick B
heroes B
first O
. O

More O
specifically O
, O
the O
draft B
can O
be O
defined O
as O
a O
combinatorial O
game B
[ O
5 O
] O
with O
the O
following O
elements O
: O
• O
the O
number O
of O
players B
: O
n O
= O
2 O
. O

• O
game B
states I
: O
S O
⊂ O
Z O
N O
. O

A O
game B
state I
s O
∈ O
S O
is O
an O
N O
- O
dimensional O
vector O
encoding O
which O
heroes B
have O
been O
picked B
by O
both O
players B
, O
where O
N O
is O
the O
number O
of O
total O
distinct O
heroes B
. O

The O
components O
of O
s O
can O
take O
value O
as O
one O
, O
negative O
one O
, O
and O
zero O
, O
corresponding O
to O
a O
hero B
being O
picked B
by O
Radiant O
, O
Dire O
, O
or O
neither O
of O
them O
, O
respectively O
: O
si O
= O
 O
 O
 O
1 O
, O
hero B
i O
picked B
by O
Radiant O
−1 O
, O
hero B
i O
picked B
by O
Dire O
0 O
, O
otherwise O
( O
1 O
) O
The O
number O
of O
components O
equal O
to O
one O
( O
or O
negative O
one O
) O
can O
not O
exceed O
five O
, O
because O
each O
player B
will O
eventually O
select O
five O
heroes B
. O

As O
per O
the O
rule B
of O
drafting O
, O
states O
are O
fully O
observable O
to O
both O
players B
. O

• O
the O
initial O
game B
state I
s0 O
= O
0 O
N O
. O

s0 O
∈ O
S O
is O
a O
blank O
draft B
with O
no O
hero B
picked B
yet O
, O
so O
it O
is O
a O
zero O
vector O
. O

• O
the O
terminal O
states O
ST O
⊆ O
S O
, O
whereby O
the O
draft B
is O
complete O
. O

ST O
include O
those O
states O
with O
exactly O
five O
components O
being O
one O
and O
five O
components O
being O
negative O
one O
, O
denoting O
a O
completed O
draft B
. O

• O
the O
turn B
function O
ρ O
: O
S O
→ O
( O
Radiant O
, O
Dire O
) O
. O

It O
decides O
which O
player B
is O
about O
to O
pick B
in O
each O
state O
. O

ρ O
is O
based B
on O
the O
pick B
order O
“ O
1 O
- O
2 O
- O
2- O
2 O
- O
2 O
- O
1 O
” O
between O
the O
two O
players B
. O

• O
the O
set B
of I
actions I
A. O
Each O
action O
represents O
a O
hero B
pick B
by O
the O
current O
player B
, O
i.e. O
, O
( O
deterministically O
) O
changing O
a O
zero O
component O
of O
a O
non O
- O
terminal O
state O
s O
∈ O
S O
\ O
ST O
to O
one O
( O
if O
the O
picking B
player B
is O
Radiant O
) O
or O
negative O
one O
( O
if O
the O
picking B
player B
is O
Dire O
) O
. O

There O
are O
a O
finite O
number O
of O
actions O
and O
they O
are O
applied O
sequentially O
according O
to O
ρ O
. O

• O
the O
reward B
function O
R O
: O
S O
→ O
R O
2 O
. O

R(s O
) O
outputs O
a O
two O
- O
dimension O
reward B
, O
with O
the O
first O
component O
R O
1 O
( O
s O
) O
being O
the O
reward B
assigned O
to O
Radiant O
and O
the O
second O
component O
R O
2 O
( O
s O
) O
being O
the O
reward B
assigned O
to O
Dire O
. O

Since O
both O
teams B
strive O
to O
maximize O
the O
predicted O
win B
rate I
of O
their O
team B
based I
on O
the O
completed O
draft B
, O
R O
is O
only O
defined O
for O
terminal O
states O
s O
∈ O
ST O
, O
whereby O
R O
1 O
( O
s O
) O
= O
−R O
2 O
( O
s O
) O
= O
w(s O
) O
with O
w(s O
) O
denoting O
the O
predicted O
team B
win I
rate O
of O
Radiant O
. O

Designed O
with O
the O
elements O
above O
, O
the O
draft B
is O
regarded O
as O
a O
combinatorial O
game B
characterizing O
the O
two O
- O
person O
, O
zero O
- O
sum O
property O
, O
perfect O
information O
, O
deterministic O
rewards B
, O
and O
discrete O
and O
sequential O
actions O
. O

4.2 O
Apply O
Monte O
Carlo O
Tree O
Search O
to O
Find O
Optimal O
Pick B
Many O
classic O
games B
like O
Go O
, O
Chess O
, O
and O
Tic O
Tac O
Toe O
are O
also O
combinatorial O
games B
. O

A O
popular O
approach O
to O
solve O
combinatorial O
games B
is O
the O
minimax O
algorithm O
[ O
17 O
] O
, O
which O
finds O
the O
optimal O
action O
for O
a O
player B
by O
constructing O
a O
complete O
search O
tree O
comprising O
of O
all O
possible O
actions O
alternated O
by O
each O
player B
. O

Minimax O
assumes O
that O
both O
parties O
are O
playing B
rationally O
, O
i.e. O
, O
each O
selecting O
actions O
that O
maximizes O
their O
accumulated O
rewards B
or O
minimizing O
accumulated O
costs B
. O

Since O
minimax O
is O
exhaustive O
in O
nature O
, O
it O
does O
not O
scale O
well O
for O
games B
with O
a O
large O
search O
space O
such O
as O
Go O
. O

Therefore O
, O
heuristic O
functions O
to O
approximate O
the O
actions O
’ O
values O
after O
limited O
search O
depth O
and/or O
techniques B
for O
pruning O
search O
trees O
[ O
17 O
] O
are O
needed O
. O

Even O
with O
these O
techniques B
, O
minimax O
may O
still O
be O
infeasible O
or O
not O
perform O
well O
for O
complex O
search O
problems O
. O

As O
an O
alternative O
to O
minimax O
, O
Monte O
Carlo O
Tree O
Search O
( O
MCTS O
) O
[ O
8 O
, O
18 O
, O
24 O
] O
is O
a O
class B
of O
heuristic O
search O
algorithms O
for O
identifying O
nearoptimal O
actions O
without O
having O
to O
construct O
the O
complete O
search O
tree O
. O

It O
has O
attracted O
much O
attention O
in O
recent O
years O
due O
to O
successful O
application O
to O
Go O
- O
playing B
programs O
[ O
32 O
, O
33 O
] O
, O
General O
Game O
Playing B
[ O
10 O
] O
, O
and O
Real O
- O
Time O
Strategy O
games B
[ O
3 O
] O
. O

In O
essence O
, O
MCTS O
involves O
iteratively O
building O
a O
search O
tree O
at O
each O
decision O
state O
to O
estimate O
the O
values O
of O
the O
state O
and O
available O
actions O
at O
that O
state O
. O

The O
main O
idea O
of O
MCTS O
in O
improving O
the O
efficiency O
of O
search O
is O
to O
prioritize O
expanding O
the O
search O
trees O
in O
the O
direction O
of O
the O
most O
promising O
actions O
. O

A O
draft B
in O
a O
normal O
MOBA O
is O
regarded O
as O
a O
combinatorial O
game B
with O
a O
branching O
factor O
at O
least O
100 O
and O
depth O
10 O
, O
since O
popular O
MOBA O
games B
such O
as O
League O
of O
Legends O
and O
DOTA O
2 O
sport O
more O
than O
100 O
heroes B
to O
be O
selected O
for O
each O
of O
the O
10 O
picks B
in O
a O
draft B
. O

As O
the O
branching O
factor O
is O
very O
large O
in O
this O
case O
, O
which O
makes O
minimax O
hardly O
a O
feasible O
approach O
, O
we O
propose O
to O
apply O
MCTS O
to O
compute O
the O
optimal O
pick B
for O
the O
current O
player B
. O

Specifically O
, O
we O
propose O
the O
use O
of O
a O
particular O
version O
of O
MCTS O
called B
Upper O
Confidence O
Bound O
applied O
to O
trees O
( O
UCT O
) O
[ O
18 O
] O
for O
this O
Conference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USAZhengxing O
Chen O
, O
Truong O
- O
Huy O
D O
Nguyen O
, O
Yuyu O
Xu O
, O
Christopher O
Amato O
, O
Seth O
Cooper O
, O
Yizhou O
Sun O
, O
and O
Magy O
Seif O
El O
- O
Nasr O
Figure O
1 O
: O
Diagram O
representing O
the O
four O
steps O
of O
UCT O
. O

Diagram O
adapted O
from O
[ O
5 O
] O
and O
[ O
27 O
] O
purpose O
. O

The O
search O
tree O
of O
UCT O
is O
built O
iteratively O
, O
with O
each O
node O
representing O
a O
state O
and O
each O
directed O
edge O
to O
child O
nodes O
representing O
the O
action O
resulting O
to O
a O
next O
state O
. O

It O
starts B
with O
a O
root O
node O
, O
which O
, O
in O
our O
case O
, O
represents O
the O
draft B
state O
s O
right O
before O
the O
current O
player B
is O
to O
pick B
the O
next O
hero B
. O

Then O
, O
the O
search O
tree O
is O
built O
based B
on O
the O
following O
four O
steps O
per O
iteration O
, O
until O
time B
or O
memory O
resource B
allocated O
is O
depleted O
: O
Selection O
. O

In O
this O
step O
, O
the O
algorithm O
starts B
from O
the O
root O
node O
and O
traverses O
down O
the O
tree O
to O
reach O
a O
terminal O
node O
or O
an O
expandable O
node O
. O

A O
node O
is O
expandable O
if O
it O
is O
a O
non O
- O
terminal O
state O
and O
has O
child O
nodes O
unvisited O
. O

The O
tree O
traversal O
is O
done O
by O
successively O
selecting O
child O
nodes O
, O
following O
the O
tree O
policy O
, O
which O
in O
UCT O
is O
based B
on O
the O
Upper O
Confidence O
Bound O
( O
UCB1 O
) O
criterion O
[ O
2 O
] O
: O
πU O
CB1(s O
) O
= O
arg O
max O
a O
n O
w¯ O
+ O
c O
s O
logn(s O
) O
n(s O
, O
a O
) O
o O
, O
( O
2 O
) O
where O
s O
and O
a O
refer O
to O
a O
parent O
node O
and O
an O
action O
available O
at O
that O
node O
, O
respectively O
; O
w¯ O
is O
the O
average O
reward B
received O
after O
taking O
a O
at O
s O
; O
n(s O
, O
a O
) O
is O
the O
number O
of O
times B
a O
is O
sampled O
at O
s O
, O
and O
n(s O
) O
is O
the O
total O
number O
of O
times B
s O
is O
visited O
; O
and O
c O
is O
the O
exploration O
term O
, O
usually O
chosen O
empirically O
. O

What O
is O
implied O
in O
Eqn O
. O

2 O
is O
that O
UCT O
regards O
the O
choice O
of O
actions O
in O
the O
selection O
phase O
as O
multi O
- O
armed O
bandit O
problems O
: O
it O
focuses B
on O
exploiting O
the O
most O
promising O
actions O
to O
expand O
next O
( O
controlled B
by O
w¯ O
) O
, O
while O
balancing O
that O
by O
exploring O
more O
neglected O
branches O
of O
the O
tree O
( O
controlled B
by O
c O
r O
log O
n(s O
) O
n(s O
, O
a O
) O
) O
. O

Expansion O
. O

Unless O
the O
reached O
node O
from O
the O
last O
step O
is O
a O
terminal O
state O
, O
one O
of O
the O
unvisited O
actions O
is O
randomly O
sampled O
and O
applied O
to O
the O
node O
. O

The O
child O
node O
representing O
the O
resulting O
state O
is O
added O
to O
the O
tree O
. O

Simulation O
. O

From O
the O
newly O
expanded O
node O
, O
the O
algorithm O
performs O
a O
roll O
- O
out O
until O
the O
end O
of O
the O
game B
. O

During O
the O
roll O
- O
out O
, O
actions O
are O
performed O
according O
to O
a O
default O
policy O
, O
which O
by O
default O
is O
random O
sampling O
. O

Once O
the O
roll O
- O
out O
reaches O
a O
terminal O
state O
s O
∈ O
ST O
, O
R(s O
) O
are O
calculated O
. O

Backpropagation O
. O

The O
reward B
is O
backpropagated O
from O
the O
expanded O
node O
to O
the O
root O
node O
. O

Statistics O
on O
each O
node O
, O
i.e. O
, O
the O
average O
reward B
and O
the O
number O
of O
visits O
, O
on O
the O
path B
are O
updated B
accordingly O
. O

As O
the O
number O
of O
these O
four O
- O
step O
iterations O
increases B
, O
the O
algorithm O
expands O
the O
search O
tree O
larger O
and O
touches O
on O
more O
states O
and O
actions O
. O

Intuitively O
, O
the O
larger O
the O
search O
tree O
is O
, O
the O
better O
the O
value O
approximation O
of O
state O
nodes O
is O
. O

When O
the O
algorithm O
terminates O
, O
the O
action O
leading B
to O
the O
root O
’s O
most O
rewarding O
child O
node O
, O
i.e. O
, O
highestw¯ O
, O
is O
returned O
. O

The O
tree O
building O
process O
of O
UCT O
is O
sketched O
in O
Figure O
1 O
. O

It O
is O
proven O
that O
when O
proper O
parameters O
are O
configured O
and O
rewards B
are O
bounded O
to O
the O
range B
[ O
0 O
, O
1 O
] O
, O
UCT O
converges O
to O
minimax O
’s O
optimal O
action O
at O
a O
polynomial O
rate O
as O
the O
number O
of O
iterations O
grows O
to O
infinity O
[ O
18 O
] O
. O

This O
implies O
that O
applying O
UCT O
with O
theoretically O
sufficient O
time B
and O
memory O
will O
eventually O
converge O
to O
the O
optimal O
hero B
for O
team B
victory B
. O

There O
are O
two O
benefits O
of O
MCTS O
in O
general O
that O
makes O
it O
suitable O
for O
a O
hero B
pick B
recommendation O
system O
. O

First O
, O
state O
value O
estimation O
could O
solely O
rely O
on O
the O
backpropagation O
of O
the O
reward B
on O
terminal O
states O
without O
needing O
to O
resort O
to O
a O
hand O
authored O
heuristic O
function O
. O

This O
reduces O
the O
amount O
of O
domain O
knowledge O
required O
. O

Second O
, O
MCTS O
is O
an O
anytime O
algorithm O
, O
which O
means O
tree O
building O
can O
be O
interrupted O
when O
a O
given O
time B
or O
memory O
constraint O
is O
exceeded O
and O
estimated O
values O
based B
on O
the O
search O
tree O
built O
so O
far O
can O
be O
returned O
immediately O
. O

This O
makes O
it O
feasible O
for O
our O
MCTS O
- O
based B
method O
to O
be O
deployed O
in B
large O
- I
scale I
online O
matches B
under O
real B
- I
world I
resource B
constraints O
. O

4.3 O
Reward O
Function O
as O
a O
Win O
Rate O
Predictor O
Now O
, O
we O
describe O
how O
the O
reward B
, O
i.e. O
the O
team B
win I
rate O
based B
on O
a O
hero B
line O
- O
up O
, O
is O
calculated O
. O

Following O
the O
previous O
notation O
, O
we O
use O
w(s O
) O
to O
denote O
the O
predicted O
team B
win I
rate O
from O
the O
view O
of O
the O
Radiant O
player B
, O
based I

 I
on O
a O
complete O
draft B
s O
∈ O
ST O
. O

This O
win B
rate I
can O
be O
computed O
by O
a O
machine O
learning O
classification O
model O
trained B
on O
a O
large O
amount O
of O
hero B
line O
- O
ups O
extracted O
from O
historical O
matches B
. O

During O
the O
training O
The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
GamesConference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USA O
of O
such O
a O
model O
, O
the O
input B
feature O
vector O
is O
the O
completed O
draft B
as O
encoded O
in O
Eqn O
. O

3 O
; O
the O
output O
is O
a O
binary O
label O
indicating O
the O
observed O
match B
outcome O
( O
+1 O
for O
a O
win O
or O
0 O
for O
a O
loss O
, O
from O
the O
view B

 O
of I
Radiant I
) I
. O

There O
are O
two O
properties O
making O
a O
model O
desirable O
for O
our O
task O
: O
( O
1 O
) O
it O
should O
return O
the O
class B
probability O
, O
i.e. O
, O
the O
probability O
of O
Radiant O
drafts B
winning O
the O
game B
, O
rather O
than O
just O
a O
binary O
prediction O
; O
and O
( O
2 O
) O
it O
should O
be O
a O
non B
linear O
- I
based I
model O
that O
can O
capture B
the O
interrelationships O
between O
features O
( O
i.e. O
, O
heroes B
) O
. O

5 O
PERFORMANCE O
EVALUATION O
In O
this O
section O
, O
we O
detail O
the O
set B
up O
of I
our I
simulation O
- O
based B
experiments O
and O
demonstrate O
the O
effectiveness O
of O
UCT O
by O
showing O
that O
teams B
following O
our O
algorithm O
to O
pick B
heroes B
can O
form O
stronger O
hero B
line O
- O
ups O
against O
teams B
following O
other O
hero B
pick B
strategies B
. O

5.1 O
Data O
Collection O
We O
choose O
the O
DOTA2 O
match B
dataset O
collected O
between O
February O
11 O
, O
2016 O
to O
March O
2 O
, O
2016 O
by O
Semenov O
et O
al O
. O

[ O
29 O
] O
. O

No O
major B
game I

 O
update I
affecting O
the O
mechanics B
of I
the O
games I
occurred O
during O
the O
data O
collection O
phase O
. O

The O
original O
dataset O
contains O
five O
million O
“ O
Ranked B
All O
Pick O
” O
matches B
, O
with O
each O
match B
containing O
hero B
lineup O
information O
and O
average B
player O
skill I
level B
( O
i.e. O
, O
normal O
, O
high O
, O
and O
very O
high O
) O
. O

To O
further O
reduce O
the O
impact B
introduced O
by O
player B
skill I
, O
we O
extract O
a O
subset O
of O
matches B
played B
by O
gamers B
only O
in O
the O
normal O
skill B
level I
. O

In O
total O
, O
our O
dataset O
has O
3,056,596 O
matches B
, O
containing O
111 O
distinct O
heroes B
. O

The O
dataset O
will O
be O
used O
to O
train B
a O
team B
win I
rate O
predictor O
as O
the O
reward B
function O
, O
as O
well O
as O
provide O
a O
basis O
for O
our O
simulations O
. O

5.2 O
Win O
Rate O
Predictor O
Training O
To O
prepare O
the O
data O
for O
training B
the O
team B
win I
rate O
predictor O
, O
each O
match B
’s O
hero B
line O
- O
up O
is O
encoded O
as O
a O
feature O
vector O
of O
length O
111 O
( O
Eqn O
. O

3 O
) O
while O
the O
match B
outcome O
is O
used O
as O
the O
label O
. O

We O
experiment O
with O
three O
classification O
models O
, O
Gradient O
Boosted B
Decision O
Tree O
( O
GBDT O
) O
[ O
12 O
] O
, O
Neural O
Network O
( O
NN O
) O
[ O
4 O
] O
, O
and O
a O
generalized O
linear O
model O
, O
Logistic O
Regression O
( O
LR O
) O
, O
for O
the O
team B
win I
rate O
predictor O
. O

GBDT O
fits O
the O
logit O
of O
label O
probabilities O
with O
the O
outputs O
of O
a O
collection O
of O
decision O
trees O
learned O
using O
boosting B
[ O
11 O
] O
. O

For O
the O
NN O
model O
, O
we O
use O
one O
input B
layer O
, O
one O
hidden O
layer O
, O
and O
one O
output O
layer O
. O

The O
hidden O
layer O
is O
comprised O
of O
a O
number O
of O
rectified O
linear O
units B
( O
ReLU O
) O
which O
transform O
the O
original O
features O
non O
- O
linearly O
to O
feed O
the O
output O
layer O
. O

The O
output O
layer O
is O
a O
single O
neuron O
with O
the O
sigmoid O
activation O
function O
1 O
1+exp(−x O
) O
. O

LR O
models O
the O
logit O
of O
label O
probabilities O
as O
a O
linear O
combination O
of O
individual O
features O
without O
explicitly O
modeling O
the O
interactions O
among O
features O
. O

Therefore O
, O
although O
all O
the O
three O
models O
can O
predict O
the O
label O
probabilities O
on O
new O
data O
, O
GBDT O
and O
NN O
are O
more O
sophisticated O
and O
able O
to O
capture B
interactions O
among O
features O
. O

We O
also O
test O
a O
naive O
baseline O
model O
which O
always O
outputs O
the O
majority O
class B
( O
MC O
) O
. O

In O
our O
case O
, O
it O
always O
predicts O
a O
win O
for O
Radiant O
because O
there O
are O
53.75 O
% O
matches B
in O
which O
Radiant O
wins O
. O

All O
hyperparameters O
like O
the O
number O
of O
hidden B
units B
of O
NN O
, O
the O
number O
of O
trees O
of O
GBDT O
and O
the O
regularization O
penalty O
of O
LR O
are O
determined O
through O
grid O
search O
on O
a O
10-fold O
cross O
- O
validation O
procedure O
. O

In O
each O
fold O
, O
we O
split B
the O
data O
into O
the O
train B
, O
validate O
and O
test O
set O
in O
an O
8:1:1 O
ratio O
. O

Candidate O
models O
of O
the O
same O
kind O
but O
Table O
1 O
: O
Performance O
of O
Team O
Win O
Rate O
Predictors O
Model O
Accuracy O
AUC O
MC O
0.53779 O
0.5 O
LR O
0.63576 O
0.68767 O
GBDT O
0.64172 O
0.70142 O
NN O
0.65345 O
0.71437 O
with O
different O
hyperparameters O
are O
trained B
on O
the O
train B
dataset O
; O
the O
best O
hyperparameters O
are O
determined O
according O
to O
the O
classification O
accuracy O
on O
the O
validation O
dataset O
. O

The O
classification O
performance O
of O
the O
best O
model O
will O
be O
measured O
on O
the O
test O
dataset O
. O

We O
report B
the O
accuracy O
and O
area B
under O
ROC O
( O
receiver O
operating O
characteristic O
) O
curve O
( O
AUC O
) O
for O
each O
kind O
of O
model O
in O
Table O
1 O
, O
averaged O
over O
10-fold O
cross O
validation O
. O

NN O
achieves O
the O
best O
prediction O
performance O
in O
both O
accuracy O
and O
AUC O
. O

Additionally O
, O
the O
accuracy O
and O
AUC O
of O
NN O
are O
statistically O
higher O
than O
those O
of O
the O
other O
models O
according O
to O
a O
paired O
, O
two O
- O
tailed O
Welch O
’s O
t O
- O
test O
with O
confidence O
level B
0.05 O
. O

Thereby O
we O
decide O
to O
use O
NN O
as O
the O
reward B
function O
in O
our O
simulation O
experiments O
later O
. O

It O
is O
worth B
noting O
that O
, O
the O
absolute O
difference O
between O
LR O
and O
NN O
is O
1.8 O
% O
for O
accuracy O
and O
0.03 O
for O
AUC O
, O
which O
may O
give O
a O
wrong O
impression O
that O
match B
outcomes O
are O
accountable O
by O
the O
sum O
of O
individual O
heroes B
’ O
effects B
and O
hero B
interactions O
are O
not O
as O
important O
as O
we O
want O
to O
emphasize O
. O

We O
find O
a O
possible O
explanation O
for O
this O
phenomenon O
, O
that O
players B
already O
tried O
hard B
to O
build O
closely O
competitive B
hero B
line O
- O
ups O
in O
the O
collected O
matches B
such O
that O
good O
or O
bad O
hero B
interactions O
are O
hard B
to O
stand O
out O
. O

If O
we O
train B
the O
models O
with O
additional O
matches B
in O
which O
players B
are O
forced O
to O
play B
random B

 O
heroes I
, O
NN O
may O
have O
a O
larger O
edge O
over O
LR O
. O

We O
will O
investigate O
such O
an O
issue O
in O
the O
future O
. O

5.3 O
Simulation O
Setup O
We O
design O
four O
strategies B
for O
hero B
pick B
recommendation O
and I
test I

 I
their O
effectiveness O
in O
terms O
of O
team B
victory B
. O

For O
every O
pair O
of O
strategies B
, O
we O
conduct O
1000 O
simulations O
: O
in O
each O
simulation O
, O
two O
teams B
participate O
in O
the O
drafting O
process O
, O
with O
each O
team B
adopting O
one O
of O
the O
two O
strategies B
. O

At O
the O
end O
of O
each O
draft B
, O
we O
collect O
the O
predicted O
win B
rate I
as O
measure O
of O
strength B
of O
the O
built O
hero B
line O
- O
ups O
. O

Finally O
, O
we O
will O
report B
the O
mean O
of O
the O
predicted O
win B
rates I
over O
the O
1000 O
drafts B
for O
each O
pair O
of O
strategies B
. O

The O
procedure O
of O
one O
simulation O
is O
summarized O
in O
Algorithm O
1 O
. O

The O
four O
experimented O
strategies B
include O
: O
• O
UCT O
: O
this O
strategy B
is O
what O
we O
propose O
in O
Section O
4 O
. O

We O
will O
use O
UCTn O
, O
c O
to O
denote O
a O
specific O
UCT O
version O
with O
n O
iterations O
allowed O
and O
an O
exploration O
term O
c. O
• O
Association O
Rules B
( O
AR O
) O
: O
this O
strategy B
was O
proposed O
by O
Hanke O
and O
Chaimowicz O
[ O
15 O
] O
. O

Two O
sets O
of O
association O
rules B
, O
namely O
“ O
ally B
rules B
” O
and O
“ O
enemy B
rules B
” O
, O
are O
extracted O
from O
the O
collected O
matches B
. O

Ally B
rules B
and O
enemy B
rules B
represent O
hero B
subsets O
that O
appear O
frequently O
in O
the O
same O
winning O
team B
or O
in O
the O
opposite B
team I
, O
respectively O
. O

At O
each O
turn B
, O
the O
strategy B
looks O
for O
the O
extracted O
association O
rules B
containing O
both O
the O
heroes B
picked B
already O
and O
the O
heroes B
not O
picked B
yet O
. O

Those O
not O
picked B
yet O
will O
Conference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USAZhengxing O
Chen O
, O
Truong O
- O
Huy O
D O
Nguyen O
, O
Yuyu O
Xu O
, O
Christopher O
Amato O
, O
Seth O
Cooper O
, O
Yizhou O
Sun O
, O
and O
Magy O
Seif O
El O
- O
Nasr O
ALGORITHM O
1 O
: O
Simulation O
of O
one O
match B
Input B
: O
team1 O
, O
team2 O
, O
win B
rate I
predictor O
M O
Output O
: O
w(s O
) O
Initialize O
a O
new O
draft B
s O
: O
= O
s0 O
while O
s O
< O
ST O
do O
team B
= O
ρ(s O
) O
if O
s O
equal O
to O
s0 O
then O
a O
= O
weighted_sample_hero O
( O
) O
else O
a O
= O
team.recommend_hero(s O
) O
end O
s O
= O
f O
( O
s O
, O
a O
) O
end O
w(s):= O
predicted O
by O
M O
with O
input B
s O
be O
selectively O
added O
to O
a O
candidate O
pool O
, O
from O
which O
the O
recommended O
hero B
will O
be O
uniformly O
sampled O
. O

In O
our O
implementation O
, O
we O
adopt O
the O
best O
criteria O
claimed O
by O
the O
authors O
: O
association O
rules B
are O
mined O
with O
0.01 O
% O
minimum O
support B
, O
and O
the O
metrics O
for O
selectively O
adding O
heroes B
to O
the O
candidate O
pool O
are O
“ O
win B
rate I
” O
and O
“ O
confidence O
” O
for O
ally B
rules B
and O
enemy B
rules B
, O
respectively O
. O

Readers O
can O
refer O
to O
the O
original O
paper O
[ O
15 O
] O
for O
more O
details O
on O
the O
approach O
. O

• O
HighestWin O
Rate O
( O
HWR O
) O
: O
each O
time B
, O
pick B
the O
hero B
not O
selected O
yet O
and O
with O
the O
highest O
win B
rate I
, O
based B
on O
frequency O
counts O
on O
our O
dataset O
. O

• O
Random O
( O
RD O
) O
: O
each O
time B
, O
uniformly O
sample O
a O
hero B
not O
yet O
selected O
. O

We O
do O
not O
implement O
the O
strategy B
based B
on O
sequence O
prediction O
models O
as O
proposed O
by O
Summerville O
et O
al O
. O

[ O
35 O
] O
because O
their O
model O
requires O
training O
on O
hero B
pick B
sequences O
, O
which O
are O
not O
readily O
available O
in O
our O
dataset O
. O

In O
fact O
, O
hero B
pick B
sequences O
are O
not O
currently O
downloadable O
from O
the O
official O
APIs O
provided O
by O
the O
game B
and O
need O
to O
resort O
to O
third O
- O
party O
organizations O
. O

We O
would O
implement O
the O
strategy B
in O
the O
future O
should O
we O
have O
access O
to O
such O
data O
of O
hero B
pick B
sequences O
. O

Some O
additional O
implementation O
details O
are O
as O
follows O
. O

For O
each O
simulated O
draft B
, O
regardless O
of O
the O
strategy B
adopted O
, O
the O
first O
hero B
is O
sampled O
following O
the O
probability O
distribution O
reflecting O
how O
frequently O
each O
hero B
is O
picked B
in O
our O
dataset O
. O

This O
helps O
our O
experiments O
cover O
more O
possible O
scenarios O
. O

To O
ensure O
fairness O
when O
comparing O
pairs O
of O
strategies B
, O
among O
the O
1000 O
simulations O
, O
each O
strategy B
is O
set O
to O
start B
first O
for O
half O
of O
the O
simulations O
. O

A O
shared O
random O
seed O
is O
set O
for O
each O
500-simulations O
, O
to O
make O
sure O
that O
randomness O
is O
the O
same O
for O
both O
strategies B
. O

All O
strategies B
follow O
the O
rule B
that O
no O
hero B
can O
be O
picked B
twice O
. O

All O
experiments O
are O
conducted O
on O
a O
PC O
with O
an O
Intel O
i7 O
- O
3632QM O
2.20GHz O
CPU O
. O

5.4 O
Parameter O
Search O
for O
UCT O
We O
first O
run O
simulations O
to O
determine O
the O
exploration O
term O
c O
for O
the O
UCT O
strategy B
. O

We O
chooseUCTn,1 O
as O
benchmarks O
( O
i.e. O
, O
UCT O
with O
the O
exploration O
term O
c O
= O
1 O
) O
, O
where O
n O
= O
{ O
100 O
, O
200 O
, O
400 O
, O
800 O
, O
1600 O
} O
. O

For O
each O
UCTn,1 O
, O
we O
then O
create O
multiple O
UCTn O
, O
c O
strategies B
to O
play B
with O
, O
with O
c O
ranging B
from O
2 O
−6 O
to O
2 O
at O
a O
scaling O
rate O
of O
2 O
. O

The O
results O
are O
shown O
in O
Table O
2 O
. O

Tuning O
c O
is O
not O
helpful O
when O
UCT O
is O
only O
allowed O
with O
100 O
iterations O
. O

This O
is O
because O
c O
controls B
the O
exploration O
strength B
for O
tree O
node O
selection O
in O
the O
selection O
step O
, O
which O
never O
kicks O
off O
within O
100 O
iterations O
due O
to O
the O
large O
number O
of O
available O
actions3 O
. O

We O
can O
infer O
that O
the O
best O
value O
ofc O
to O
couple O
with O
n O
= O
{ O
200 O
, O
400 O
, O
800 O
, O
1600 O
} O
is O
2 O
−5 O
, O
2 O
−2 O
, O
2 O
−1 O
and O
2 O
−1 O
, O
respectively O
, O
because O
they O
force O
UCTn,1 O
to O
have O
the O
lowest O
win B
rate I
( O
indicated O
by O
the O
bold O
cells O
in O
Table O
2 O
) O
. O

Note O
that O
there O
is O
a O
general O
trend O
that O
the O
best O
c O
increases B
as O
n O
increases B
. O

5.5 O
Simulation O
Results O
Based B
on O
the O
results O
from O
the O
parameter O
search O
, O
we O
finally O
choose O
a O
list O
of O
strategies B
to O
compare O
with O
each O
other O
: O
RD O
, O
AR O
, O
HW O
R O
, O
UCT100,1,UCT200,2 O
−5 O
, O
UCT400,2 O
−2 O
, O
UCT800,2 O
−1 O
, O
andUCT1600,2 O
−1 O
. O

The O
simulation O
results O
are O
summarized O
in O
Table O
3 O
, O
in O
which O
each O
cell O
contains O
the O
mean O
predicted O
win B
rate I
of O
the O
strategy B
displayed O
on O
the O
respective O
row O
. O

We O
define O
the O
strength B
of O
a O
strategy B
as O
the O
number O
of O
strategies B
it O
can O
defeat B
with O
more O
than O
50 O
% O
win B
rate I
. O

Therefore O
, O
the O
weakest B
to O
strongest O
strategies B
are O
: O
RD O
, O
AR O
, O
UCT100,1 O
, O
UCT200,2 O
−5 O
, O
HW O
R O
, O
UCT400,2 O
−2 O
, O
UCT800,2 O
−1 O
, O
and O
UCT1600,2 O
−1 O
. O

Given O
a O
sufficient O
number O
of O
iterations O
( O
≥ O
400 O
) O
, O
UCT O
strategies B
can O
outperform O
all O
non O
- O
UCT O
strategies B
, O
which O
proves O
the O
effectiveness O
of O
our O
proposed O
algorithm O
. O

There O
is O
a O
general O
trend O
that O
UCT O
improves O
the O
win B
rate I
as O
the O
number O
of O
iterations O
increases B
. O

Specifically O
, O
UCT1600,2 O
−1 O
can O
beat O
HW O
R O
with O
a O
61.1 O
% O
win B
rate I
, O
highest O
one O
among O
the O
other O
UCTbased O
strategies B
with O
fewer O
iterations O
. O

However O
, O
we O
can O
observe O
the O
phenomenon O
of O
diminishing O
gain O
as O
the O
number O
of O
iterations O
exceeds O
a O
certain O
threshold O
: O
UCT1600,2 O
−1 O
has O
a O
51.3 O
% O
win B
rate I
against O
UCT800,2 O
−1 O
, O
only O
marginally O
better O
than O
50 O
% O
but O
with O
double O
the O
number O
of O
iterations O
. O

Among O
non O
- O
UCT O
strategies B
, O
the O
HW O
R O
strategy B
that O
always O
picks B
the O
highest O
win B
rate I
hero B
achieves O
the O
best O
performance O
, O
which O
can O
defeat B
UCT O
with O
200 O
iterations O
with O
a O
51.6 O
% O
win B
rate I
. O

However O
, O
HW O
R O
can O
not O
prevail O
over O
UCT O
strategies B
that O
are O
allowed O
with O
more O
than O
200 O
iterations O
. O

AR O
adopted O
from O
[ O
15 O
] O
defeats B
RD O
with O
68.2 O
% O
win B
rate I
for O
our O
implementation O
, O
while O
the O
original O
authors O
reported B
a O
76.4 O
% O
win B
rate I
against O
RD O
. O

The O
discrepancy O
may O
be O
due O
to O
different O
datasets O
being O
used O
to O
mine O
association O
rules B
and O
train B
the O
win B
rate I
predictor O
. O

To O
show O
the O
efficiency O
of O
different O
strategies B
, O
we O
report B
the O
average O
wall O
time B
( O
i.e. O
real O
elapsed O
time B
) O
needed O
per O
pick B
in O
Table O
4 O
. O

The O
time B
is O
recorded O
excluding O
the O
first O
pick B
, O
since O
it O
is O
based B
on O
weighted O
random O
sampling O
. O

UCT O
- O
based B
strategies B
take O
1.12 O
seconds O
or O
less O
per O
pick B
, O
which O
is O
a O
small O
fraction O
of O
the O
25 O
second O
time B
limit O
per O
pick B
for O
human O
players B
in O
real O
matches B
[ O
13 O
] O
. O

This O
demonstrates O
the O
feasibility O
of O
applying O
the O
UCT O
- O
based B
hero B
recommendation O
system O
online O
in O
large O
scale O
and O
real O
time B
. O

5.6 O
Extension O
to O
Other O
Match O
Modes O
What O
we O
have O
focused B
on O
so O
far O
in O
this O
paper O
is O
based B
on O
the O
drafting B
rules B
of O
match B
mode O
All O
Ranked B
in O
DOTA O
2 O
. O

However O
, O
our O
MCTS O
- O
based B
algorithm O
is O
not O
limited O
solely O
to O
this O
mode O
and O
can O
be O
adapted O
to O
other O
modes O
, O
whereby O
additional O
drafting O
rules B
and O
mechanics B
are O
deployed O
. O

In O
this O
section O
, O
we O
detail O
how O
our O
3We O
have O
111 O
distinct O
heroes B
and O
10 O
turns B
in O
a O
draft B
, O
which O
means O
applying O
UCT O
at O
any O
turn B
will O
start B
from O
a O
root O
state O
with O
more O
than O
100 O
possible O
child O
nodes O
( O
actions O
) O
to O
expand O
. O

Since O
one O
iteration O
only O
expands O
one O
new O
child O
node O
, O
after O
100 O
iterations O
, O
the O
root O
node O
is O
still O
expandable O
. O

Therefore O
, O
no O
selection O
step O
will O
happen O
. O

The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
GamesConference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USA O
Table O
2 O
: O
Mean O
predicted O
win B
rate I
of O
UCTn,1 O
( O
row O
) O
vs. O
UCTn O
, O
c O
( O
column O
) O
. O

Numbers O
are O
from O
the O
view O
of O
the O
row O
strategy B
. O

Bold O
cells O
indicate O
the O
best O
value O
of O
c O
for O
UCTn O
, O
c O
for O
each O
n O
, O
as O
they O
force O
UCTn,1 O
have O
the O
lowest O
win B
rate I
. O

UCTn,2 O
−6 O
UCTn,2 O
−5 O
UCTn,2 O
−4 O
UCTn,2 O
−3 O
UCTn,2 O
−2 O
UCTn,2 O
−1 O
UCTn,2 O
0 O
UCTn,2 O
1 O
UCT100,1 O
0.5 O
0.5 O
0.5 O
0.5 O
0.5 O
0.5 O
0.5 O
0.5 O
UCT200,1 O
0.459 O
0.435 O
0.442 O
0.444 O
0.447 O
0.464 O
0.5 O
0.540 O
UCT400,1 O
0.497 O
0.476 O
0.469 O
0.469 O
0.457 O
0.469 O
0.5 O
0.534 O
UCT800,1 O
0.561 O
0.544 O
0.527 O
0.509 O
0.488 O
0.485 O
0.5 O
0.525 O
UCT1600,1 O
0.606 O
0.577 O
0.563 O
0.545 O
0.510 O
0.492 O
0.5 O
0.516 O
Table O
3 O
: O
Mean O
predicted O
win B
rate I
of O
row O
strategy B
vs. O
column O
strategy B
. O

Simulations O
are O
based B
on O
the O
drafting B
rules B
of O
match B
mode O
“ O
All O
Ranked B
” O
. O

Numbers O
are O
from O
the O
view O
of O
the O
row O
strategy B
. O

The O
strategies B
are O
sorted O
in O
ascending O
order O
by O
their O
strengths B
, O
from O
top O
to O
bottom B
, O
and O
from O
left O
to O
right O
. O

Win B
rates I
symmetric O
to O
diagonal O
always O
sum O
to O
one O
, O
thus O
half O
of O
them O
are O
omitted O
. O

RD O
AR O
UCT100,1 O
UCT200,2 O
−5 O
HW O
R O
UCT400,2 O
−2 O
UCT800,2 O
−1 O
UCT1600,2 O
−1 O
RD O
0.5 O
AR O
0.682 O
0.5 O
UCT100,1 O
0.783 O
0.663 O
0.5 O
UCT200,2 O
−5 O
0.897 O
0.833 O
0.712 O
0.5 O
HW O
R O
0.883 O
0.846 O
0.715 O
0.516 O
0.5 O
UCT400,2 O
−2 O
0.920 O
0.863 O
0.763 O
0.568 O
0.556 O
0.5 O
UCT800,2 O
−1 O
0.928 O
0.878 O
0.776 O
0.591 O
0.593 O
0.524 O
0.5 O
UCT1600,2 O
−1 O
0.930 O
0.880 O
0.787 O
0.606 O
0.611 O
0.539 O
0.513 O
0.5 O
Table O
4 O
: O
Average O
wall O
time B
per O
pick B
of O
different O
strategies B
( O
unit B
: O
millisecond O
, O
match B
mode O
: O
All O
Ranked B
) O
. O

RD O
AR O
UCT100,1 O
UCT200,2 O
−5 O
HW O
R O
UCT400,2 O
−2 O
UCT800,2 O
−1 O
UCT1600,2 O
−1 O
0.02 O
11 O
43 O
96 O
0.1 O
281 O
562 O
1120 O
Table O
5 O
: O
Mean O
predicted O
win B
rate I
of O
row O
strategy B
vs. O
column O
strategy B
. O

Simulations O
are O
based B
on O
the O
drafting B
rules B
of O
match B
mode O
“ O
Captain O
Mode O
” O
. O

The O
table O
can O
be O
viewed O
in O
a O
similar O
way O
as O
Table O
3 O
. O

RD O
AR O
UCT100,2 O
−5 O
UCT200,2 O
−5 O
HW O
R O
UCT400,2 O
−2 O
UCT800,2 O
−1 O
UCT1600,2 O
−1 O
RD O
0.5 O
AR O
0.693 O
0.5 O
UCT100,2 O
−5 O
0.817 O
0.686 O
0.5 O
UCT200,2 O
−5 O
0.91 O
0.851 O
0.712 O
0.5 O
HW O
R O
0.876 O
0.788 O
0.694 O
0.508 O
0.5 O
UCT400,2 O
−2 O
0.929 O
0.886 O
0.770 O
0.574 O
0.570 O
0.5 O
UCT800,2 O
−1 O
0.941 O
0.896 O
0.788 O
0.598 O
0.607 O
0.536 O
0.5 O
UCT1600,2 O
−1 O
0.942 O
0.903 O
0.797 O
0.620 O
0.627 O
0.552 O
0.519 O
0.5 O
algorithm O
can O
be O
extended O
to O
another O
popular O
match B
mode O
called B
Captain O
Mode O
to O
demonstrate O
our O
algorithm O
’s O
generality O
. O

During O
the O
drafting O
process O
in O
Captain O
Mode O
matches B
, O
there O
is O
another O
type O
of O
action O
, O
called B
hero B
banning O
, O
that O
players B
can O
take O
besides O
hero B
pick B
. O

In O
the O
turns B
of O
banning O
, O
a O
team B
can O
designate O
certain O
heroes B
to O
be O
prohibited O
from O
selection O
by O
either O
team B
. O

The O
drafting B
order O
is O
represented O
in O
Figure O
2 O
: O
two O
teams B
first O
alternate O
to O
ban O
three O
heroes B
, O
then O
alternate O
to O
pick B
two O
heroes B
, O
then O
alternate O
to O
ban O
two O
heroes B
, O
and O
so O
on O
. O

As O
the O
result O
, O
the O
drafting O
is O
22 O
steps O
long O
, O
instead O
of O
just O
10 O
as O
in O
the O
case O
of O
All O
Ranked B
. O

The O
interleaving O
nature O
between O
bans O
and O
picks B
adds O
another O
level B
of O
complexity O
to O
the O
drafting O
, O
as O
players B
need O
to O
consider O
more O
possible O
strategies B
to O
prevent O
opponents B
from O
selecting O
their O
desired O
heroes B
. O

Despite O
being O
more O
complex O
, O
the O
drafting O
in O
Captain O
Mode O
matches B
can O
be O
formulated O
as O
a O
combinatorial O
game B
similar O
to O
that O
in O
All O
Ranked B
matches B
but O
with O
a O
few O
minor O
adjustments O
: O
Conference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USAZhengxing O
Chen O
, O
Truong O
- O
Huy O
D O
Nguyen O
, O
Yuyu O
Xu O
, O
Christopher O
Amato O
, O
Seth O
Cooper O
, O
Yizhou O
Sun O
, O
and O
Magy O
Seif O
El O
- O
Nasr O
Figure O
2 O
: O
Drafting B
order O
in O
match B
mode O
" O
Captain O
Mode O
" O
in O
DOTA O
2 O
. O

The O
green B
and O
red O
cells O
represent O
the O
turns B
of O
two O
different O
teams B
( O
i.e. O
, O
Radiant O
and O
Dire O
) O
, O
assuming O
the O
green B
team B
starts B
first O
. O

Diagram O
is O
adapted O
from O
[ O
13 O
] O
and O
best O
viewed O
in O
color O
. O

• O
The O
components O
of O
a O
game B
state I
s O
can O
take O
an O
additional O
value O
of O
a O
special O
symbol O
Ξ O
denoting O
corresponding O
banned O
heroes B
, O
i.e. O
, O
: O
si O
= O
 O
 O
 O
1 O
, O
hero B
i O
picked B
by O
Radiant O
−1 O
, O
hero B
i O
picked B
by O
Dire O
Ξ O
, O
hero B
i O
banned O
0 O
, O
otherwise O
( O
3 O
) O
The O
terminal O
state O
set O
ST O
will O
also O
change O
accordingly O
. O

A O
terminal O
state O
s O
∈ O
ST O
include O
five O
ones O
, O
five O
negative O
ones O
, O
and O
12 O
special O
symbol O
Ξ. O
• O
An O
action O
can O
be O
either O
a O
ban O
or O
a O
pick B
. O

If O
it O
is O
a O
ban O
, O
it O
changes O
a O
zero O
component O
of O
a O
non O
- O
terminal O
state O
s O
∈ O
S O
\ O
ST O
to O
Ξ. O
• O
The O
turn B
function O
ρ O
will O
also O
be O
updated B
to O
reflect O
the O
drafting B
order O
in O
Figure O
2 O
. O

• O
When O
a O
terminal O
state O
s O
∈ O
ST O
is O
fed O
into O
the O
reward B
function O
R(s O
) O
, O
its O
components O
of O
Ξ O
will O
be O
treated O
as O
zeroes O
. O

Given O
the O
above O
formulation O
of O
the O
Captain O
Mode O
drafting O
, O
in O
the O
form O
of O
a O
combinatorial O
game B
, O
the O
UCT O
algorithm O
can O
be O
applied O
directly O
in O
the O
same O
way O
as O
described O
in O
Section O
4.2 O
. O

Within O
the O
drafting B
rules B
of O
Captain O
Mode O
matches B
, O
we O
conduct O
the O
same O
simulation O
- O
based B
experiments O
to O
compare O
UCT O
with O
the O
baselines O
RD O
, O
AR O
and O
HW O
R. O
When O
taking O
the O
banning O
action O
, O
AR O
is O
executed O
from O
the O
perspective O
of O
the O
opponent B
- O
identifying O
candidate O
heroes B
to O
ban O
as O
if O
the O
opponent B
is O
to O
pick B
; O
HW O
R O
always O
chooses O
among O
the O
available O
heroes B
with O
the O
highest O
win B
rate I
for O
both O
the O
pick B

 O
and I
ban I
actions I
. O

Regarding O
other O
implementation O
details O
, O
the O
selection O
frequency O
- O
weighted O
sampling O
is O
executed O
for O
the O
first O
hero B
to O
ban O
rather O
than O
the O
first O
hero B
to O
pick B
. O

c O
becomes O
effective O
in O
UCT100,c O
, O
since O
the O
simulation O
step O
may O
kick O
off O
after O
the O
starting B
root O
node O
is O
no O
longer O
expandable O
. O

We O
find O
the O
best O
c O
to O
match B
with O
n O
= O
100 O
is O
2 O
−5 O
, O
while O
the O
best O
c O
values O
to O
couple O
with O
other O
n O
= O
{ O
200 O
, O
400 O
, O
800 O
, O
1600 O
} O
remain O
the O
same O
as O
in O
All O
Ranked B
experiments O
. O

The O
evaluation O
results O
are O
shown O
in O
Table O
5 O
. O

First O
, O
we O
observe O
that O
the O
strength B
order O
of I
the I
tested O
strategies B
remains O
the O
same O
, O
with O
RD O
being O
the O
weakest B
and O
UCT1600,2 O
−1 O
being O
the O
strongest O
. O

Second O
, O
the O
win B
rate I
of O
UCT O
strategies B
against O
non O
- O
UCT O
strategies B
usually O
sees O
a O
1 O
- O
3 O
% O
improvement O
in O
absolute O
value O
compared O
to O
the O
counterparts O
in O
All O
Ranked B
experiments O
. O

For O
example O
, O
UCT1600,2 O
−1 O
has O
a O
62.7 O
% O
win B
rate I
against O
HW O
R O
in O
Captain O
Mode O
, O
which O
is O
larger O
than O
the O
win B
rate I
UCT1600,2 O
−1 O
has O
against O
HW O
R O
in O
All O
Ranked B
( O
61.1 O
% O
) O
. O

This O
highlights B
the O
advantage B
of O
employing O
simulation O
- O
based B
approaches O
such O
as O
MCTS O
: O
that O
the O
advanced O
planning O
brought O
by O
the O
MCTS O
algorithm O
could O
further O
widen O
its O
gap O
over O
baseline O
methods O
in O
more O
sophisticated O
settings O
. O

We O
also O
observed O
a O
negligible O
change O
in O
the O
average O
wall O
time B
needed O
per O
pick B
in O
Captain O
Mode O
- O
based B
simulations O
, O
as O
compared O
to O
All O
Ranked B
( O
Table O
4 O
) O
, O
so O
we O
do O
not O
report B
it O
here O
. O

This O
implies O
that O
the O
overhead O
incurred O
by O
the O
deeper O
search O
in O
UCT O
is O
relatively O
smaller O
than O
that O
in O
other O
components O
, O
such O
as O
computing O
the O
reward B
function O
. O

6 O
CONCLUSIONS O
, O
LIMITATIONS O
AND O
FUTURE O
WORKS O
In O
this O
paper O
, O
we O
treat O
the O
drafting O
process O
in O
MOBA O
games B
as O
a O
combinatorial O
game B
. O

Under O
this O
view O
, O
we O
propose O
a O
recommendation O
system O
that O
can O
effectively O
and O
efficiently O
search O
for O
the O
hero B
pick B
optimal O
for O
team B
victory O
based I
on O
Monte O
Carlo O
Tree O
Search O
. O

We O
design O
and O
conduct O
simulation O
- O
based B
experiments O
based B
on O
two O
kinds O
of O
drafting B
rules B
, O
confirming O
that O
MCTS O
- O
based B
recommendations O
can O
lead B
to O
stronger O
hero B
line O
- O
ups O
in O
terms O
of O
predicted O
team B

 O
win I
rates O
, O
as O
compared O
to O
other O
baselines O
. O

One O
limitation O
of O
this O
work O
is O
that O
we O
do O
not O
consider O
playerspecific O
information O
, O
such O
as O
player B
skills I
in O
their O
selected O
heroes B
, O
when O
recommending O
heroes B
. O

It O
is O
possible O
that O
a O
hero B
recommended O
by O
our O
algorithm O
, O
which O
is O
based B
solely O
on O
the O
current O
hero B
lineup O
, O
may O
be O
poorly O
played B
by O
a O
player B
who O
is O
not O
familiar O
with O
it O
. O

Our O
algorithm O
can O
be O
extended O
to O
integrate O
player B
skills I
, O
by O
augmenting O
the O
game B
state I
with O
player B
information O
and O
training O
a O
more O
advanced O
win B
rate I
predictor O
as O
the O
reward B
function O
which O
takes O
as O
input B
both O
hero B
picks B
and O
player B
- O
specific O
information O
. O

This O
limitation O
could O
be O
addressed O
when O
we O
have O
access O
to O
needed O
player B
- O
specific O
data O
. O

There O
are O
some O
other O
promising O
future O
directions O
that O
we O
can O
pursue O
next O
. O

First O
, O
were O
hero B
pick B
sequences O
from O
real O
match B
data O
available O
, O
we O
would O
integrate O
them O
as O
prior O
information O
to O
improve O
the O
tree O
policy O
and O
default O
policy O
in O
MCTS O
, O
thereby O
improving O
capabilities O
to O
build O
search O
trees O
more O
effectively O
and O
efficiently O
[ O
6 O
, O
14 O
] O
. O

Second O
, O
we O
would O
also O
like O
to O
investigate O
how O
our O
recommendation O
systems O
can O
be O
customized O
to O
account O
for O
additional O
drafting O
rules B
and O
extended O
to O
other O
real B
- I
world I
scenarios O
such O
as O
player B
drafting B
in O
sports O
[ O
34 O
] O
. O

REFERENCES O
[ O
1 O
] O
Rakesh O
Agrawal O
, O
Ramakrishnan O
Srikant O
, O
et O
al O
. O

1994 O
. O

Fast O
algorithms O
for O
mining O
association O
rules B
. O

In O
Proceedings O
of O
the O
20th O
International O
Conference O
on O
Very O
Large O
Data O
Bases O
( O
VLDB O
) O
, O
Vol O
. O

1215 O
. O

487–499 O
. O

[ O
2 O
] O
Peter O
Auer O
, O
Nicolo O
Cesa O
- O
Bianchi O
, O
and O
Paul O
Fischer O
. O

2002 O
. O

Finite O
- O
time B
analysis O
of O
the O
multiarmed O
bandit O
problem O
. O

Machine O
learning O
47 O
, O
2 O
- O
3 O
( O
2002 O
) O
, O
235–256 O
. O

[ O
3 O
] O
Radha O
- O
Krishna O
Balla O
and O
Alan O
Fern O
. O

2009 O
. O

UCT O
for O
tactical O
assault O
planning O
in O
real B
- I
time I
strategy O
games I
. O

In O
Proceedings O
of O
the O
21th O
International O
Joint O
Conference O
on O
Artificial O
Intelligence O
( O
IJCAI O
) O
. O

40–45 O
. O

[ O
4 O
] O
Christopher O
M O
Bishop O
. O

2006 O
. O

Pattern O
recognition O
and O
machine O
learning O
. O

Springer O
. O

[ O
5 O
] O
Cameron O
B O
Browne O
, O
Edward O
Powley O
, O
Daniel O
Whitehouse O
, O
Simon O
M O
Lucas O
, O
Peter O
I O
Cowling O
, O
Philipp O
Rohlfshagen O
, O
Stephen O
Tavener O
, O
Diego O
Perez O
, O
Spyridon O
Samothrakis O
, O
and O
Simon O
Colton O
. O

2012 O
. O

A O
survey O
of O
monte O
carlo O
tree O
search O
methods O
. O

IEEE O
Transactions O
on O
Computational O
Intelligence O
and O
AI O
in O
Games O
4 O
, O
1 O
( O
2012 O
) O
, O
1–43 O
. O

[ O
6 O
] O
Guillaume O
Chaslot O
, O
Christophe O
Fiter O
, O
Jean O
- O
Baptiste O
Hoock O
, O
Arpad O
Rimmel O
, O
and O
Olivier O
Teytaud O
. O

2009 O
. O

Adding O
expert O
knowledge O
and O
exploration O
in B
Monte O
- I
Carlo I
Tree O
Search O
. O

In O
Advances O
in O
Computer O
Games O
. O

Springer O
, O
1–13 O
. O

[ O
7 O
] O
Zhengxing O
Chen O
, O
Yizhou O
Sun O
, O
Magy O
Seif O
El O
- O
Nasr O
, O
and O
Truong O
- O
Huy O
D. O
Nguyen O
. O

2016 O
. O

Player B
skill I
decomposition I
in O
Multiplayer O
Online O
Battle O
Arenas O
. O

In O
Meaningful O
Play O
. O

[ O
8 O
] O
Rémi O
Coulom O
. O

2006 O
. O

Efficient O
selectivity O
and O
backup O
operators O
in B
Monte O
- I
Carlo I
tree O
search O
. O

In O
International O
conference O
on O
computers O
and O
games B
. O

Springer O
, O
72–83 O
. O

The O
Art O
of O
Drafting B
: O
A O
Team O
- O
Oriented O
Hero O
Recommendation O
System O
for O
Multiplayer O
Online O
Battle O
Arena O
GamesConference’17 O
, O
July O
2017 O
, O
Washington O
, O
DC O
, O
USA O
[ O
9 O
] O
Anders O
Drachen O
, O
Matthew O
Yancey O
, O
John O
Maguire O
, O
Derrek O
Chu O
, O
Iris O
Yuhui O
Wang O
, O
Tobias O
Mahlmann O
, O
Matthias O
Schubert O
, O
and O
Diego O
Klabajan O
. O

2014 O
. O

Skill O
- O
based B
differences O
in B
spatio O
- I
temporal I
team B
behaviour O
in O
defence O
of O
the O
Ancients O
2 O
( O
DotA O
2 O
) O
. O

In O
2014 O
IEEE O
Games O
Media O
Entertainment O
. O

IEEE O
, O
1–8 O
. O

[ O
10 O
] O
Hilmar O
Finnsson O
and O
Yngvi O
Björnsson O
. O

2008 O
. O

Simulation O
- O
based B
approach O
to O
general O
game B
playing O
. O

In O
Proceedings O
of O
the O
23rd O
National O
Conference O
on O
Artificial O
Intelligence O
( O
AAAI O
) O
. O

[ O
11 O
] O
Jerome O
Friedman O
, O
Trevor O
Hastie O
, O
Robert O
Tibshirani O
, O
et O
al O
. O

2000 O
. O

Additive O
logistic O
regression O
: O
a O
statistical O
view O
of O
boosting O
. O

Annals O
of O
Statistics O
28 O
, O
2 O
( O
2000 O
) O
, O
337–407 O
. O

[ O
12 O
] O
Jerome O
H O
Friedman O
. O

2001 O
. O

Greedy O
function O
approximation O
: O
a O
gradient O
boosting B
machine O
. O

Annals O
of O
statistics O
( O
2001 O
) O
, O
1189–1232 O
. O

[ O
13 O
] O
Gamepedia O
. O

2018 O
. O

DOTA O
2 O
wiki O
- O
game B
modes I
. O

https://dota2.gamepedia.com/Ga O
me_modes O
. O

Online O
; O
accessed O
May O
, O
2018 O
. O

[ O
14 O
] O
Sylvain O
Gelly O
and O
David O
Silver O
. O

2007 O
. O

Combining O
online O
and O
offline B
knowledge O
in O
UCT O
. O

In O
Proceedings O
of O
the O
24th O
International O
Conference O
on O
Machine O
Learning O
( O
ICML O
) O
. O

ACM O
, O
273–280 O
. O

[ O
15 O
] O
Lucas O
Hanke O
and O
Luiz O
Chaimowicz O
. O

2017 O
. O

A O
recommender O
system O
for O
hero B
lineups O
in O
MOBA O
games B
. O

In O
Proceedings O
of O
the O
13th O
AAAI O
Conference O
on O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
( O
AIIDE O
) O
. O

[ O
16 O
] O
Jooyeon O
Kim O
, O
Brian O
C O
Keegan O
, O
Sungjoon O
Park O
, O
and O
Alice O
Oh O
. O

2016 O
. O

The O
proficiency O
- O
congruency O
dilemma O
: O
virtual O
team B
design O
and O
performance O
in O
multiplayer O
online B
games I
. O

In O
Proceedings O
of O
the O
2016 O
CHI O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
. O

ACM O
, O
4351–4365 O
. O

[ O
17 O
] O
Donald O
E O
Knuth O
and O
Ronald O
W O
Moore O
. O

1975 O
. O

An O
analysis O
of O
alpha O
- O
beta O
pruning O
. O

Artificial O
intelligence O
6 O
, O
4 O
( O
1975 O
) O
, O
293–326 O
. O

[ O
18 O
] O
Levente O
Kocsis O
and O
Csaba O
Szepesvári O
. O

2006 O
. O

Bandit O
based B
monte O
- I
carlo I
planning O
. O

In O
European O
Conference O
on O
Machine O
Learning O
. O

Springer O
, O
282–293 O
. O

[ O
19 O
] O
Yubo O
Kou O
and O
Bonnie O
Nardi O
. O

2013 O
. O

Regulating O
anti O
- O
social O
behavior O
on O
the O
Internet O
: O
The O
example O
of O
League O
of O
Legends O
. O

In O
Proceedings O
of O
the O
2013 O
iConference O
. O

iSchools O
. O

[ O
20 O
] O
Haewoon O
Kwak O
, O
Jeremy O
Blackburn O
, O
and O
Seungyeop O
Han O
. O

2015 O
. O

Exploring O
cyberbullying O
and O
other O
toxic O
behavior O
in O
team B
competition I
online O
games I
. O

In O
Proceedings O
of O
the O
33rd O
Annual O
ACM O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
. O

ACM O
, O
3739–3748 O
. O

[ O
21 O
] O
Mike O
Minotti O
. O

2016 O
. O

Comparing O
MOBAs B
: O
League O
of O
Legends O
vs. O
Dota O
2 O
vs. O
Smite O
vs. O
Heroes O
of O
the O
Storm O
. O

http://venturebeat.com/2015/07/15/comparing-mobas-lea O
gue O
- O
of O
- O
legends O
- O
vs O
- O
dota-2-vs O
- O
smite O
- O
vs O
- O
heroes B
- O
of O
- O
the O
- O
storm/. O
Online O
; O
accessed O
May O
, O
2018 O
. O

[ O
22 O
] O
Julia O
Neidhardt O
, O
Yun O
Huang O
, O
and O
Noshir O
Contractor O
. O

2015 O
. O

Team B
vs. O
team B
: O
success B

 O
factors I
in O
a O
Multiplayer O
Online O
Battle O
Arena O
game B
. O

In O
Academy O
of O
Management O
Proceedings O
, O
Vol O
. O

2015 O
. O

Academy O
of O
Management O
, O
18725 O
. O

[ O
23 O
] O
Truong O
- O
Huy O
D. O
Nguyen O
, O
Zhengxing O
Chen O
, O
and O
Magy O
S. O
El O
- O
Nasr O
. O

2015 O
. O

Analyticsbased O
AI O
techniques B
for O
better O
gaming O
experience B
. O

Game B
AI O
Pro O
, O
Vol O
. O

2 O
. O

CRC O
Press O
, O
Boca O
Raton O
, O
Florida O
. O

[ O
24 O
] O
Truong O
- O
Huy O
Dinh O
Nguyen O
, O
Tomi O
Silander O
, O
Wee O
- O
Sun O
Lee O
, O
and O
Tze O
- O
Yun O
Leong O
. O

2014 O
. O

Bootstrapping O
simulation O
- O
based B
algorithms O
with O
a O
suboptimal O
policy O
. O

In O
Proceedings O
of O
the O
24th O
International O
Conference O
on O
Automated O
Planning O
and O
Scheduling O
( O
ICAPS’14 O
) O
. O

AAAI O
Press O
, O
181–189 O
. O

[ O
25 O
] O
Nataliia O
Pobiedina O
, O
Julia O
Neidhardt O
, O
Maria O
del O
Carmen O
Calatrava O
Moreno O
, O
Laszlo O
Grad O
- O
Gyenge O
, O
and O
Hannes O
Werthner O
. O

2013 O
. O

On O
successful O
team B
formation I
: O
statistical O
analysis O
of O
a O
multiplayer O
online B
game I
. O

In O
2013 O
IEEE O
15th O
Conference O
on O
Business O
Informatics O
. O

IEEE O
, O
55–62 O
. O

[ O
26 O
] O
Nataliia O
Pobiedina O
, O
Julia O
Neidhardt O
, O
Maria O
del O
Carmen O
Calatrava O
Moreno O
, O
and O
Hannes O
Werthner O
. O

2013 O
. O

Ranking B
factors O
of O
team B
success O
. O

In O
Proceedings O
of O
the O
22nd O
International O
Conference O
on O
World O
Wide O
Web O
Companion O
. O

International O
World O
Wide O
Web O
Conferences O
Steering O
Committee O
, O
1185–1194 O
. O

[ O
27 O
] O
André O
Santos O
, O
Pedro O
A O
Santos O
, O
and O
Francisco O
S O
Melo O
. O

2017 O
. O

Monte O
Carlo O
tree O
search O
experiments O
in O
hearthstone O
. O

In O
Computational O
Intelligence O
and O
Games O
( O
CIG O
) O
. O

IEEE O
, O
272–279 O
. O

[ O
28 O
] O
Henrik O
Schoenau O
- O
Fog O
. O

2011 O
. O

The O
player B
engagement O
process O
– O
an O
exploration O
of O
continuation O
desire O
in O
digital O
games B
. O

In O
Think O
Design O
Play B
: O
Digital O
Games O
Research O
Conference O
. O

[ O
29 O
] O
Aleksandr O
Semenov O
, O
Peter O
Romov O
, O
Sergey O
Korolev O
, O
Daniil O
Yashkov O
, O
and O
Kirill O
Neklyudov O
. O

2016 O
. O

Performance O
of O
machine O
learning O
algorithms O
in O
predicting O
game B
outcome O
from O
drafts B
in O
Dota O
2 O
. O

In O
Analysis O
of O
Images O
, O
Social O
Networks O
and O
Texts O
. O

Springer O
, O
26–37 O
. O

[ O
30 O
] O
John O
L O
Sherry O
, O
Kristen O
Lucas O
, O
Bradley O
S O
Greenberg O
, O
and O
Ken O
Lachlan O
. O

2006 O
. O

Video O
game B
uses O
and O
gratifications O
as O
predictors O
of O
use O
and O
game B
preference O
. O

Playing B
Video O
Games O
: O
Motives O
, O
Responses O
, O
and O
Consequences O
24 O
( O
2006 O
) O
, O
213–224 O
. O

[ O
31 O
] O
Kenneth O
B O
Shores O
, O
Yilin O
He O
, O
Kristina O
L O
Swanenburg O
, O
Robert O
Kraut O
, O
and O
John O
Riedl O
. O

2014 O
. O

The O
identification O
of O
deviance O
and O
its O
impact B
on O
retention O
in O
a O
multiplayer O
game B
. O

In O
Proceedings O
of O
the O
17th O
ACM O
Conference O
on O
Computer O
Supported O
Cooperative O
Work O
& O
Social O
Computing O
. O

ACM O
, O
1356–1365 O
. O

[ O
32 O
] O
David O
Silver O
, O
Aja O
Huang O
, O
Chris O
J O
Maddison O
, O
Arthur O
Guez O
, O
Laurent O
Sifre O
, O
George O
Van O
Den O
Driessche O
, O
Julian O
Schrittwieser O
, O
Ioannis O
Antonoglou O
, O
Veda O
Panneershelvam O
, O
Marc O
Lanctot O
, O
et O
al O
. O

2016 O
. O

Mastering B
the O
game B
of O
Go O
with O
deep O
neural O
networks O
and O
tree O
search O
. O

Nature O
529 O
, O
7587 O
( O
2016 O
) O
, O
484–489 O
. O

[ O
33 O
] O
David O
Silver O
, O
Julian O
Schrittwieser O
, O
Karen O
Simonyan O
, O
Ioannis O
Antonoglou O
, O
Aja O
Huang O
, O
Arthur O
Guez O
, O
Thomas O
Hubert O
, O
Lucas O
Baker O
, O
Matthew O
Lai O
, O
Adrian O
Bolton O
, O
et O
al O
. O

2017 O
. O

Mastering B
the O
game B
of O
go O
without O
human O
knowledge O
. O

Nature O
550 O
, O
7676 O
( O
2017 O
) O
, O
354 O
. O

[ O
34 O
] O
Barry O
M O
Staw O
and O
Ha O
Hoang O
. O

1995 O
. O

Sunk O
costs B
in O
the O
NBA O
: O
Why O
draft B
order O
affects O
playing O
time B
and O
survival O
in O
professional O
basketball O
. O

Administrative O
Science O
Quarterly O
( O
1995 O
) O
, O
474–494 O
. O

[ O
35 O
] O
Adam O
Summerville O
, O
Michael O
Cook O
, O
and O
Ben O
Steenhuisen O
. O

2016 O
. O

Draft B
- O
analysis B

 O
of I
the I
Ancients I
: I
predicting O
draft B
picks B
in O
DotA O
2 O
using O
machine O
learning O
. O

In O
Proceedings O
of O
the O
12th O
AAAI O
Conference O
on O
Artificial O
Intelligence O
and O
Interactive O
Digital O
Entertainment O
( O
AIIDE O
) O
. O

[ O
36 O
] O
Paul O
Tassi O
. O

2016 O
. O

Riot B
’s O
’ O
League O
of O
Legends O
’ O
reveals O
astonishing O
27 O
million O
daily O
players B
, O
67 O
million O
monthly O
. O

http://www.forbes.com/sites/insertcoin/2014/01/27 O
/riots O
- O
league B
- O
of I
- I
legends O
- O
reveals O
- O
astonishing-27-million O
- O
daily O
- O
players-67-mil O
lion O
- O
monthly/#26ff8e543511 O
. O

Online O
; O
accessed O
May O
, O
2016 O
. O

[ O
37 O
] O
Pu O
Yang O
, O
Harrison O
Brent O
, O
and O
David O
L O
Roberts O
. O

2014 O
. O

Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
MOBA O
games B
. O

In O
Proceedings O
of O
Foundations O
of O
Digital O
Games O
( O
FDG O
) O
. O

[ O
38 O
] O
Nick O
Yee O
. O

2006 O
. O

Motivations O
for O
play B
in O
online B
games I
. O

CyberPsychology O
& O
B]ehavior O
9 O
, O
6 O
( O
2006 O
) O
, O
772–775 O
. O

What O
Makes O
a O
Good O
Team B
? O
A O
Large O
- O
scale O
Study O
on O
the O
Effect O
of O
Team O
Composition O
in O
Honor O
of O
Kings O
Ziqiang O
Cheng O
Zhejiang O
University O
petecheng@zju.edu.cn O
Yang O
Yang O
Zhejiang O
University O
yangya@zju.edu.cn O
Chenhao O
Tan O
University O
of O
Colorado O
Boulder O
chenhao@chenhaot.com O
Denny O
Cheng O
Tencent O
Corporation O
dennycheng@tencent.com O
Yueting O
Zhuang O
Zhejiang O
University O
yzhuang@zju.edu.cn O
Alex O
Cheng O
Tencent O
Corporation O
alexcheng@tencent.com O
ABSTRACT O
Team O
composition O
is O
a O
central O
factor O
in O
determining O
the O
effectiveness O
of O
a O
team B
. O

In O
this O
paper O
, O
we O
present O
a O
large O
- O
scale O
study O
on O
the O
effect B
of O
team B
composition I
on O
multiple O
measures O
of O
team B
effectiveness I
. O

We O
use O
a O
dataset O
from O
the O
largest O
multiplayer O
online O
battle B
arena O
( O
MOBA O
) O
game B
, O
Honor O
of O
Kings O
, O
with O
96 O
million O
matches B
involving O
100 O
million O
players B
. O
We I
measure I
team B
effectiveness O
based I
on O
team B
performance I
( O
whether O
a O
team B
is O
going O
to O
win O
) O
, O
team B
tenacity O
( O
whether O
a O
team B
is O
going O
to O
surrender B
) O
, O
and O
team B
rapport O
( O
whether O
a O
team B
uses O
abusive B
language I
) O
. O

Our O
results O
confirm O
the O
importance O
of O
team B
diversity O
and O
show O
that O
diversity O
has O
varying O
effects B
on O
team B
effectiveness I
: O
although O
diverse O
teams B
perform O
well O
and O
show O
tenacity O
in O
adversity O
, O
they O
are O
more O
likely O
to O
abuse O
when O
losing O
than O
less O
diverse O
teams B
. O

Our O
study O
also O
contributes O
to O
the O
situation O
vs. O
personality O
debate O
and O
show O
that O
abusive O
players B
tend O
to O
choose O
the O
leading O
role B
and O
players B
do O
not O
become O
more O
abusive O
when O
taking O
such O
roles B
. O

We O
further O
demonstrate O
the O
predictive O
power B
of O
features O
based B
on O
team B
composition I
in O
prediction O
experiments O
. O

1 O
CCS O
CONCEPTS O
• O
Applied O
computing O
→ O
Law O
, O
social O
and O
behavioral O
sciences O
. O

KEYWORDS O
team B
composition I
, O
team B
performance I
, O
tenacity O
, O
toxic O
behavior O
, O
MOBA O
1 O
INTRODUCTION O
The O
increasing B
complexity O
and O
scale O
of O
tasks O
in O
modern O
society O
require O
individuals O
to O
work O
together O
as O
a O
team B
[ O
14 O
, O
45 O
] O
. O

Crucial O
to O
the O
effectiveness O
of O
a O
team B
are O
the O
individuals O
in O
the O
team B
, O
i.e. O
, O
team B

 O
composition I
. O

Extensive O
prior O
research O
has O
studied O
team B
roles I
to O
analyze O
team B
composition I
[ O
6 O
, O
7 O
, O
34 O
, O
35 O
, O
41 O
, O
43 O
] O
. O

The O
most O
influential O
work O
is O
Belbin O
’s O
team B
role I
framework O
, O
also O
known O
as O
Belbin O
Team O
Inventory O
. O

A O
central O
hypothesis O
in O
Belbin O
’s O
framework O
is O
that O
balance O
in O
team B
roles I
is O
associated O
with O
team B
performance I
. O

Belbin O
[ O
7 O
] O
defines O
nine O
roles B
2 O
, O
and O
finds O
that O
teams B
with O
certain O
role B
combinations O
result O
in O
poor B
performance O
, O
even O
if O
they O
are O
formed O
by O
members O
with O
the O
sharpest O
mind O
and O
the O
most O
experience B
. O

However O
, O
team B
roles I
are O
usually O
implicit O
in O
real O
life O
and O
it O
is O
difficult O
to O
identify O
such O
nine O
roles B
in O
most O
contexts O
. O

1This O
is O
a O
long O
version O
of O
our O
WWW’19 O
short O
paper O
with O
additional O
analysis O
and O
prediction O
experiments O
: O
https:/doi.org/10.1145/3308558.3312504 O
2 O
See O
https://en.wikipedia.org/wiki/Team_Role_Inventories O
for O
a O
quick O
explanation O
. O

Figure O
1 O
: O
Illustration O
of O
games B
in O
Honor O
of O
Kings O
: O
1 O
) O
a O
matching O
system O
groups B
10 O
players B
into O
two O
similarly O
competitive B
teams B
; O
2 O
) O
players B
in O
each O
team B
choose O
a O
character B
to O
control B
, O
which O
is O
designed O
to O
play B
a O
certain O
role B
( O
note O
that O
multiple O
players B
can O
choose O
the O
same O
role B
) O
; O
3 O
) O
two O
teams B
battle B
to O
destroy O
opponents B
’ O
base I
. O

See O
the O
description O
of O
roles B
in O
Table O
1 O
. O

Another O
challenge O
in O
studying O
the O
effect B
of O
team B
composition I
on O
team B
effectiveness I
arises O
from O
the O
definition O
of O
effectiveness O
. O

Team O
performance O
is O
the O
most O
straightforward O
definition O
and O
has O
been O
studied O
in O
a O
battery O
of O
studies O
[ O
10 O
, O
14 O
, O
17 O
, O
39 O
] O
. O

Example O
measures O
of O
team B
performance I
include O
the O
impact B
of O
published O
papers O
from O
a O
team B
[ O
45 O
] O
, O
winning O
a O
sports O
game B
[ O
4 O
, O
27 O
] O
, O
etc O
. O

However O
, O
the O
effectiveness O
of O
a O
team B
can O
also O
be O
reflected O
by O
the O
tenacity O
in O
face O
of O
adversity O
, O
and O
the O
rapport O
between O
team B
members I
[ O
9 O
] O
. O

The O
effect B
of O
team B
composition I
likely O
depends O
on O
the O
definition O
of O
team B
effectiveness I
. O

Therefore O
, O
it O
is O
important O
to O
understand O
the O
effect B
of O
team B

 O
composition I
on O
the O
effectiveness O
of O
teams B
in O
multiple O
measures O
. O

To O
address O
these O
two O
challenges O
, O
we O
identify O
multiplayer O
online O
battle B
arena O
( O
MOBA O
) O
as O
an O
ideal O
testbed O
and O
provide O
a O
large O
- O
scale O
study O
on O
the O
effect B
of O
team B
compositions I
on O
multiple O
measures O
of O
team B
effectiveness I
. O

We O
use O
a O
dataset O
from O
Honor O
of O
Kings O
, O
the O
largest O
MOBA O
game B
in O
the O
world B
, O
and O
Fig O
. O

1 O
illustrates O
the O
process O
of O
an O
example O
game B
. O

This O
platform O
is O
ideal O
for O
studying O
the O
effect B
of O
team B
composition I
for O
three O
reasons O
. O

First O
, O
there O
are O
defined O
roles B
in O
the O
game B
and O
players B
choose O
their I
roles I
to O
form O
a O
team B
. O

Given O
that O
there O
are O
five O
players B
in O
each O
team B
, O
we O
are O
able O
to O
enumerate O
all O
possible O
team B
compositions I
. O

We O
also O
have O
information O
about O
arXiv:1902.06432v1 O
[ O
cs O
. O

CY O
] O
18 O
Feb O
2019 O
the O
characteristics O
of O
each O
role B
. O

Second O
, O
the O
popularity O
of O
Honor O
of O
Kings O
leads B
to O
digital O
traces O
of O
hundreds O
of O
millions O
of O
players B
in O
hundreds O
of O
millions O
of O
games B
. O

These O
detailed O
game B
records I
allow O
us O
to O
explore O
the O
notion O
of O
team B
“ O
effectiveness I
” O
beyond O
the O
simple O
measure O
of O
team B
performance I
, O
winning O
or O
losing O
. O

For O
instance O
, O
we O
study O
the O
effect B
of O
team B
composition I
on O
abusive B
language I
use O
, O
which O
reflects O
the O
rapport O
in O
a O
team B
. O

This O
research O
question O
naturally O
connects O
to O
the O
literature O
on O
toxic O
behavior O
in O
online B
communities I
and O
gaming B
[ O
12 O
, O
13 O
, O
29 O
] O
. O

Finally O
, O
we O
have O
access O
to O
the O
past O
history O
of O
players B
for O
capturing B
their O
background O
and O
experience B
. O

Historical O
information O
enables O
us O
to O
investigate O
questions O
in O
the O
“ O
situation O
vs. O
personality O
” O
debate O
[ O
18 O
, O
25 O
] O
: O
how O
much O
of O
our O
behavior O
is O
determined O
by O
fixed O
personality O
traits O
or O
by O
the O
specific O
situation O
at O
hand O
? O
We O
will O
explore O
this O
question O
in O
the O
context O
of O
toxic O
behavior O
in O
online O
gaming O
. O

Organization O
and O
paper O
highlights B
. O

In O
this O
paper O
, O
we O
conduct O
a O
large O
- O
scale O
study O
on O
the O
effect B
of O
team B
composition I
in O
the O
context O
of O
online O
gaming O
. O

Our O
dataset O
comes O
from O
a O
popular O
MOBA O
game B
in O
China O
, O
Honor O
of O
Kings O
, O
and O
includes O
96 O
million O
games B
and O
100 O
million O
players B
in O
total O
. O

We O
provide O
details O
of O
the O
dataset O
and O
character B
roles B
in O
§ O
2 O
. O

We O
quantitatively O
study O
the O
effect B
of O
team B
composition I
on O
three O
distinct O
definitions O
of O
team B
effectiveness I
: O
( O
§ O
3 O
) O
team B
performance I
— O
whether O
a O
team B
is O
going O
to O
win O
— O
is O
directly O
based B
on O
the O
result O
/ O
goal B
in O
a O
game B
and O
is O
the O
most O
common O
measure O
in O
prior O
studies O
; O
( O
§ O
4 O
) O
team B
tenacity O
— O
whether O
a O
team B
is O
going O
to O
surrender B
— O
indicates O
a O
team B
’s O
resilience O
to O
adversity O
and O
is O
understudied O
in O
existing O
literature O
as O
most O
studies O
only O
look O
at O
individual O
tenacity O
or O
perseverance O
[ O
5 O
, O
21 O
] O
; O
( O
§ O
5 O
) O
toxic O
behavior O
, O
i.e. O
, O
abusive B
language I
use O
, O
reflects O
the O
rapport O
in O
a O
team B
and O
is O
extracted O
from O
the O
activities O
during O
a O
game B
. O

Our O
results O
on O
team B
effectiveness I
confirm O
the O
theory O
that O
balance O
in O
team B
roles I
is O
important O
for O
effective O
teams B
. O

We O
find O
that O
diverse O
teams B
are O
more O
likely O
to O
win O
. O

Furthermore O
, O
diverse O
teams B
are O
less O
likely O
to O
surrender B
, O
indicating O
a O
higher B
level I
of O
tenacity O
. O

Note O
that O
the O
likelihood O
to O
surrender B
does O
not O
correlate O
with O
winning B
rate I
for O
commonly O
used O
team B
compositions I
. O

However O
, O
we O
observe O
diverging O
effects B
on O
abusive B
language I
use O
: O
diverse O
teams B
are O
more O
likely O
to O
abuse O
when O
losing O
, O
but O
are O
less O
likely O
to O
abuse O
when O
winning O
. O

In O
addition O
to O
exploring O
the O
effect B
of O
team B
composition I
on O
team B

 O
effectiveness I
, O
we O
investigate O
individual O
- O
level B
abusive B
language I
use O
to O
understand O
how O
teamwork B
influences O
individuals O
and O
shed O
light O
on O
the O
“ O
situation O
vs. O
personality O
” O
debate O
[ O
18 O
, O
25 O
] O
. O

We O
find O
that O
players B
who O
choose O
assassins B
( O
the O
leading O
role B
in O
a O
team B
) O
are O
more O
likely O
to O
abuse O
. O

We O
further O
demonstrate O
that O
the O
reason O
is O
that O
abusive O
players B
are O
more O
likely O
to O
choose O
assassins B
, O
instead O
of O
the O
alternative O
explanation O
that O
a O
player B
becomes O
more O
abusive O
when O
playing B
assassins B
. O

In O
a O
similar O
vein O
, O
we O
find O
that O
experienced O
players B
are O
more O
likely O
to O
abuse O
, O
but O
not O
when O
their O
teammates B
are O
also O
experienced O
. O

A O
design O
implication O
is O
that O
matching B
players B
with O
similarly O
experienced O
players B
can O
improve O
the O
rapport O
in O
a O
team B
. O

To O
demonstrate O
the O
effectiveness O
of O
modeling O
team B
composition I
, O
we O
formulate O
predictions O
tasks O
for O
all O
three O
measures O
of O
team B
effectiveness I
in O
§ O
6 O
. O

We O
show O
that O
features O
based B
on O
team B
compositions I
can O
consistently O
outperform O
individual B
player I
information O
. O

In O
fact O
, O
modeling O
single O
roles B
and O
two O
- O
role B
combinations O
provides O
most O
of O
Figure O
2 O
: O
Role B
co O
- O
occurrence O
graph O
. O

Edges O
between O
two O
roles B
represent O
their O
co O
- O
occurrence O
in O
the O
same O
team B
. O

The O
width O
of O
edges O
is O
proportional O
to O
the O
square O
root O
of O
frequency O
. O

The O
three O
most O
common O
role B
pairs O
are O
warrior B
- O
warrior B
( O
- I
, I
50.4 O
% O
) O
, O
warrior B
- I
mage I
( O
- O
, O
47.8 O
% O
) O
, O
and O
mage B
- O
marksman O
( O
- O
, O
43.8 O
% O
) O
. O

the O
predictive O
power B
. O

Finally O
, O
we O
present O
related O
work O
in O
§ O
7 O
and O
offer O
our O
concluding O
thoughts O
in O
§ O
8 O
. O

2 O
DATASET O
Our O
dataset O
is O
provided O
by O
Tencent3 O
and O
comes O
from O
a O
multiplayer O
online O
battle B
arena O
( O
MOBA O
) O
game B
: O
Honor O
of O
Kings O
, O
the O
most O
profitable O
game B
in O
the O
world4 O
. O

Similar O
to O
many O
other O
MOBA O
games B
such O
as O
Dota2 O
and O
League O
of O
Legends O
, O
Honor O
of O
Kingsinvolves O
two O
teams B
to O
battle B
with O
each O
other O
. O

Each O
team B
consists O
of O
five O
players B
and O
the O
success O
of O
a O
team B
depends O
on O
the O
chemistry O
and O
collaboration O
between O
the O
five O
team B
members I
. O

The O
main O
motivation O
of O
our O
study O
is O
that O
players B
are O
expected O
to O
take O
certain O
roles B
when O
they O
select O
characters B
. O

These O
roles B
are O
designed O
to O
complement O
each O
other O
and O
“ O
good O
” O
teams B
usually O
require O
particular O
compositions O
of O
roles B
. O

In O
this O
paper O
, O
we O
aim O
to O
explore O
the O
effect B
of O
team B
composition I
on O
different O
measures O
of O
team B
effectiveness I
. O

There O
are O
multiple O
types O
of O
games B
in O
Honor O
of O
Kings O
. O

Ranked B
games B
are O
used O
to O
estimate O
a O
player B
’s O
rank B
level I
in O
the O
game B
; O
while O
the O
other O
games B
are O
for O
practice O
or O
for O
fun O
. O

Since O
players B
care O
much O
less O
about O
unranked O
games B
than O
ranked B
games B
, O
all O
of O
our O
analyses O
are O
based B
on O
ranked O
games B
. O

Unranked O
games B
are O
only O
used O
to O
analyze O
players B
’ O
role I
preferences O
. O

Dataset O
description O
. O

Our O
dataset O
is O
derived O
from O
daily O
logs O
spanning O
three O
weeks O
in O
the O
August O
of O
2017 O
. O

Specifically O
, O
we O
first O
randomly O
sample O
20 O
K O
games B
in O
each O
day O
of O
the O
third O
week O
, O
thereby O
obtaining O
140 O
K O
games B
and O
1.3 O
M O
players B
participating O
in O
these O
games B
. O

Among O
them O
, O
there O
are O
78 O
K O
ranked B
games B
. O

Our O
prediction O
experiments O
in O
§ O
6 O
are O
based B
on O
these O
ranked B
games B
, O
which O
we O
refer O
to O
as O
prediction O
games B
. O

We O
then O
retrieve O
all O
the O
games B
involving O
any O
of O
these O
1.3 O
M O
players B
in O
the O
first O
two O
weeks O
to O
obtain O
player B
information O
for O
prediction O
. O

In O
total O
, O
we O
obtain O
95.6 O
M O
games B
and O
100.2 O
M O
players B
. O

Among O
these O
games B
, O
there O
are O
54.4 O
M O
ranked B
games B
on O
which O
most O
analyses O
in O
§ O
3 O
, O
§ O
4 O
, O
and O
§ O
5 O
are O
based B
, O
as O
under O
those O
analytical O
settings O
, O
we O
do O
not O
need O
historical O
information O
of O
every O
3Honor O
of O
Kings O
is O
developed O
and O
published O
by O
Tencent O
, O
which O
is O
one O
of O
the O
largest O
Internet O
companies O
and O
also O
the O
largest O
game B
service O
provider O
in O
China O
. O

4 O
See O
https://en.wikipedia.org/wiki/Wangzhe_Rongyao O
for O
an O
introduction O
of O
the O
native O
version O
of O
Honor O
of O
Kings O
( O
Wangzhe O
Rongyao O
) O
. O

2 O
Role O
Description O
Freq.(% O
) O
warrior B
Warriors B
have O
a O
large O
pool O
of O
health O
and O
considerable O
damage B
; O
they O
often O
undertake O
the O
stress O
from O
enemies B
in O
side O
lanes B
. O

27.1 O
mage B
Mages B
have O
heavy B
spell O
damage I
, O
but O
a O
small B

 O
pool I
of I
health I
; O
they O
often O
take O
the O
middle B

 O
lane I
and O
prepare O
to O
assist B
teammates B
. O

25.8 O
marksman O
Marksmen O
have O
a O
small O
pool O
of O
heath B
but O
can O
cause O
heavy B
damage I
; O
they O
need O
protection B
and O
may O
need O
time B
to O
grow O
. O

25.7 O
assassin B
Assassins O
are O
explosive O
and O
control B
the O
pace O
, O
but O
the O
health O
pool O
is O
often O
small O
; O
they O
are O
usually O
the O
leading O
role B
in O
a O
team B
. O

11.5 O
support B
Supports O
always O
aid O
teammates B
; O
they O
absorb O
damage B
while O
disrupting O
opponents B
by O
stunning O
and O
displacing O
them O
. O

9.9 O
Table O
1 O
: O
Description O
and O
frequency O
of O
each O
role B
. O

player B
in O
a O
team B
. O

There O
are O
several O
exceptions O
where O
we O
need O
historical O
statistics O
of O
every O
team B
member I
, O
so O
we O
use O
prediction O
games B
in O
those O
cases O
and O
will O
point B
out O
in O
the O
text O
. O

The O
log O
of O
each O
game B
includes O
its O
start B
time B
, O
end O
time B
, O
result O
( O
which O
team B
wins I
) O
, O
gaming B
stats O
recorded I
when O
the O
game B
ends O
, O
and O
text O
messages B
in O
the O
chat B
room I
with O
abusive O
labels O
. O

In O
addition O
, O
we O
have O
access O
to O
basic O
player B
information O
beyond O
our O
dataset O
, O
e.g. O
, O
number O
of O
winning O
games B
, O
number O
of O
games B
played B
as O
MVP O
( O
Most O
Valuable O
Player O
) O
and O
total O
number O
of O
games B
in O
the O
past O
ten O
ranked B
games B
, O
etc O
. O

See O
Table O
5 O
in O
§ O
A O
for O
details O
. O

Team O
roles B
. O

There O
are O
five O
roles B
in O
Honor O
of O
Kings O
: O
warrior B
( O
) O
, O
mage B
( O
) O
, O
marksman O
( O
) O
, O
assassin B
( O
) O
, O
and O
support B
( O
) O
. O

A O
team B
is O
composed O
of O
different O
roles B
depending O
on O
each O
player B
’s O
choice O
. O

Henceforth O
, O
we O
use O
five O
icons O
to O
represent O
a O
team B
composition I
. O

For O
instance O
, O
( O
, O
, O
, O
, O
) O
stands O
for O
a O
team B
with O
one O
warrior B
, O
two O
mages B
, O
two O
assassins B
, O
and O
no O
marksman O
or O
support B
. O

In O
total O
, O
there O
exist O
126 O
team B
compositions I
. O

Table O
1 O
presents O
the O
characteristics O
and O
frequency O
of O
each O
role5 O
. O

Figure O
2 O
shows O
the O
frequency O
of O
two O
roles B
co O
- O
occurring O
in O
the O
same O
team B
and O
Table O
3 O
shows O
several O
role B
combinations O
with O
its O
winning B
rate I
and O
frequency O
. O

We O
can O
clearly O
see O
that O
team B
compositions I
are O
not O
used O
equally O
by O
players B
: O
single O
roles B
and O
pairs O
of O
roles B
both O
vary O
greatly O
in O
frequency O
. O

Please O
refer O
Table O
2 O
for O
detailed O
statistics O
of O
our O
dataset O
. O

3 O
TEAM O
COMPOSITION O
AND O
WINNING O
We O
first O
investigate O
the O
effect B
of O
team B
composition I
on O
whether O
a O
team B
wins I
or O
not O
, O
a O
direct O
measure O
of O
team B
performance I
. O

Although O
the O
matching O
system O
in O
online O
gaming O
is O
designed O
to O
balance O
the O
ability B
of O
two O
teams B
, O
we O
observe O
that O
the O
winning B
rate I
of O
most O
team B

 O
compositions I
is O
significantly O
lower O
than O
50 O
% O
, O
indicating O
that O
some O
role B
combinations O
can O
not O
effectively O
work O
with O
each O
other O
. O

Although O
two O
teams B
are O
with O
similar B
“ O
skills I
” I
, O
there O
exist O
winning O
and O
losing B
team I
compositions O
( O
Fig O
. O

3a O
) O
. O

Since O
we O
have O
126 O
different O
team B
compositions I
in O
total O
, O
it O
is O
straightforward O
to O
estimate O
the O
winning B
rate I
of O
each O
team B
composition I
using O
the O
fraction O
5The O
boundary O
between O
roles B
can O
sometimes O
be O
blurry O
, O
especially O
considering O
different O
player B
tactics B
. O

Statistics O
Number(fraction O
/ O
SEM O
) O
Overall O
statistics O
# O
ranked B
games B
54 O
, O
434 O
, O
817 O
# O
team B
compositions I
126 O
# O
surrender B
games B
5 O
, O
807 O
, O
005 O
( O
10.7 O
% O
) O
# O
abusing O
games B
30 O
, O
668 O
, O
577 O
( O
56.3 O
% O
) O
# O
early B
games(t O
≤ O
11 O
min O
) O
5 O
, O
049 O
, O
934 O
( O
9.3 O
% O
) O
# O
middle B
games(11 O
< O
t O
≤ O
16 O
min O
) O
23 O
, O
089 O
, O
993 O
( O
42.4 O
% O
) O
# O
late O
games(t O
> O
16 O
min O
) O
26 O
, O
294 O
, O
890 O
( O
48.3 O
% O
) O
Top O
five O
frequent O
combinations O
− O
− O
− O
− O
16 O
, O
186 O
, O
003 O
( O
14.9 O
% O
) O
− O
− O
− O
− O
14 O
, O
516 O
, O
443 O
( O
13.3 O
% O
) O
− O
− O
− O
− O
12 O
, O
630 O
, O
805 O
( O
11.6 O
% O
) O
− O
− O
− O
− O
9 O
, O
124 O
, O
177 O
( O
8.4 O
% O
) O
− O
− O
− O
− O
8 O
, O
929 O
, O
680 O
( O
8.2 O
% O
) O
Personal O
statistics O
# O
target B
players B
1 O
, O
306 O
, O
754 O
# O
target B
players B
who O
have O
played B
ranked B
games B
1 O
, O
173 O
, O
372 O
# O
ranked B
games B
each O
player B
has O
48.4 O
( O
0.05 O
) O
average O
winrate B
52.4 O
% O
( O
1.0 O
× O
10−4 O
) O
average O
abusing O
probability O
9.3 O
% O
( O
1.0 O
× O
10−4 O
) O
average O
surrender B
probability O
5.0 O
% O
( O
6.9 O
× O
10−5 O
) O
# O
target B
players B
who O
have O
played B
at O
least O
20 O
ranked B
games B
758 O
, O
494 O
( O
58.0 O
% O
) O
Table O
2 O
: O
Statistics O
of O
the O
dataset O
. O

of O
winning O
games B
, O
# O
winning O
games B
# O
games B
. O

Fig O
. O

3a O
shows O
that O
the O
winning B

 O
rate I
of O
different O
team B
compositions I
spans O
a O
wide B
range I
, O
from O
8.3 O
% O
to O
53.6 O
% O
. O

It O
seems O
that O
some O
team B
compositions I
are O
doomed O
to O
lose O
. O

In O
addition O
, O
the O
figure O
also O
shows O
the O
cumulative O
distribution O
of O
# O
teams B
for O
each O
team B
composition I
, O
which O
suggests O
that O
most O
losing B

 O
teams I
do O
not O
occur O
frequently O
. O

Notice O
that O
a O
similar O
plot O
will O
also O
be O
made O
for O
surrendering B
and O
abusing O
and O
we O
will O
see O
that O
the O
CDFs O
present O
very O
different O
shapes O
. O

Team O
compositions O
Win O
rate O
Used O
frequency(% O
) O
team B
compositions I
with O
the O
highest O
winning O
rate O
− O
− O
− O
− O
53.6 O
% O
0.3 O
− O
− O
− O
− O
53.2 O
% O
2.0 O
− O
− O
− O
− O
52.6 O
% O
14.9 O
team B
compositions I
with O
the O
lowest B
winning I
rate I
− O
− O
− O
− O
19.7 O
% O
3.1 O
× O
10−3 O
− O
− O
− O
− O
17.4 O
% O
2.2 O
× O
10−2 O
− O
− O
− O
− O
8.3 O
% O
7.0 O
× O
10−4 O
team B
compositions I
that O
are O
most O
used O
− O
− O
− O
− O
52.6 O
% O
14.9 O
− O
− O
− O
− O
52.0 O
% O
13.3 O
− O
− O
− O
− O
48.5 O
% O
11.6 O
Table O
3 O
: O
Example O
team B
compositions I
and O
winning B
rates I
. O

The O
fluctuation O
of O
winning B
rate I
between O
different O
team B
compositions I
among O
all O
games B
may O
not O
due O
to O
the O
effect B
of O
team B
compositions I
; O
another O
possible O
explanation O
is O
the O
ability B
of O
individual O
3 O
20 O
60 O
100 O
team B
composition I
10 O
20 O
30 O
40 O
50 O
winning O
rate.(% O
) O
δ O
= O
0 O
δ O
≤ O
4 O
CDF(#teams)(% O
) O
0 O
20 O
40 O
60 O
80 O
100 O
CDF(#teams)(% O
) O
( O
a O
) O
Sorted O
winning B
rate I
of O
different O
team B
compositions I
. O

−10 O
−5 O
0 O
5 O
10 O
15 O
average O
rank B
gap(δ O
) O
0 O
20 O
40 O
60 O
80 O
100 O
CDF(#games)(% O
) O
δ O
< O
0(28.7 O
% O
) O
δ O
> O
0(59.9 O
% O
) O
CDF(δ O
) O
( O
b O
) O
Distribution O
of O
rank B
gap I
( O
δ O
) O
between O
two O
teams B
. O

1 O
2 O
3 O
4 O
5 O
number O
of O
roles B
20 O
30 O
40 O
50 O
win O
rate(% O
) O
( O
c O
) O
Winning B
rate I
vs. O
team B
diversity O
( O
rank B
gap I
δ O
= O
0 O
) O
. O

1 O
2 O
3 O
4 O
5 O
number O
of O
roles B
0 O
10 O
20 O
30 O
40 O
50 O
60 O
fraction(% O
) O
0.02 O
% O
1.1 O
% O
31.5 O
% O
52.4 O
% O
14.9 O
% O
( O
d O
) O
Frequency O
of O
teams B
vs. O
team B
diversity O
. O

Figure O
3 O
: O
Team B
composition I
and O
winning B
rates I
. O

In O
Fig O
. O

3a O
, O
x O
- O
value O
corresponds O
to O
the O
index O
of O
a O
team B
composition I
, O
and O
is O
sorted O
by O
winning B
rate I
. O

The O
CDF O
of O
# O
teams B
shows O
the O
cumulative O
distribution O
in O
# O
teams B
according O
to O
the O
order O
in O
the O
x O
- O
axis O
. O

Fig O
. O

3b O
shows O
the O
distribution O
of O
the O
rank B
gap I
between O
two O
teams B
. O

Fig O
. O

3c O
shows O
that O
a O
diverse O
team B
with O
more O
role B
categories I
is O
more O
likely O
to O
win O
( O
error O
bars O
represent O
standard O
errors O
) O
. O

Fig O
. O

3d O
shows O
that O
most O
teams B
are O
rationale O
and O
cover O
at O
least O
3 O
roles B
. O

players B
: O
only O
relatively O
bad O
players B
choose O
certain O
bad O
team B
compositions I
. O

Although O
a O
matching O
system O
is O
designed O
to O
balance O
the O
ability B
of O
the O
players B
in O
two O
teams B
, O
it O
is O
difficult O
to O
always O
make O
sure O
that O
two O
teams B
exactly O
have O
the O
same O
ability B
. O

Fig O
. O

3b O
shows O
the O
distribution O
of O
the O
rank B
level O
gap I
( O
δ O
) O
between O
two O
teams6 O
. O

To O
calculate O
δ O
, O
we O
sum O
up O
the O
individual O
rank B
level I
in O
a O
team B
, O
and O
subtract O
the O
result O
of O
losing B
team I
from O
that O
of O
winning O
team B
. O

By O
the O
effect B
of O
the O
matching O
system O
, O
the O
rank B
gaps I
in O
over O
90 O
% O
games B
are O
relatively O
small O
( O
within O
±5 O
) O
. O

These O
minor O
gaps O
, O
however O
, O
are O
still O
associated O
with O
a O
significant O
difference O
in O
winning B
rate I
: O
only O
28.7 O
% O
games B
end O
up O
with O
the O
winning O
of O
the O
lower O
ranked B
team B
. O

Therefore O
, O
to O
demonstrate O
the O
influence O
of O
team B
compositions I
on O
winning B
rates I
and O
to O
exclude O
the O
ability B
factor O
, O
we O
control B
the O
rank B

 O
gap I
between O
teams B
in O
Fig O
. O

3a O
: O
we O
focus B
on O
games B
with O
δ O
= O
0 O
( O
red O
plots O
) O
and O
δ O
≤ O
4 O
( O
gray O
plots O
) O
respectively O
. O

To O
further O
illustrate O
winning O
and O
losing B
team I
compositions O
, O
we O
present O
team B
compositions I
with O
the O
highest O
( O
lowest O
) O
winning B
rate I
in O
Table O
37 O
( O
controlling B
the O
rank B
gap I
δ O
= O
0 O
) O
. O

Upon O
a O
careful O
examination O
, O
these O
losing B
teams I
do O
not O
often O
occur O
, O
reflected O
also O
by O
the O
cumulative O
distribution O
curve O
in O
Fig O
. O

3a O
. O

In O
total O
, O
the O
bottom B
90 O
losing B
team I
compositions O
only O
take O
11.1 O
% O
of O
all O
teams B
. O

This O
suggests O
that O
most O
players B
choose O
a O
reasonable O
team B
composition I
. O

However O
, O
the O
last O
three O
rows O
in O
Table O
3 O
show O
that O
the O
most O
common O
team B

 O
compositions I
still O
vary O
in O
winning B
rates I
, O
from O
48.5 O
% O
to O
52.6 O
% O
. O

Diverse O
teams B
tend O
to O
perform O
well O
( O
Fig O
. O

3c O
, O
3d O
) O
. O

An O
observation O
that O
stands O
out O
in O
Table O
3 O
is O
that O
teams B
with O
the O
highest O
winning B
rate I
consist O
of O
more O
role B
categories I
than O
teams B
with O
the O
lowest B
winning I
rate I
. O

For O
instance O
, O
support B
( O
) O
or O
assassin B
( O
) O
dominates O
the O
two O
compositions O
with O
the O
lowest B
winning I
rate I
, O
while O
all O
the O
top O
winning O
teams B
have O
at O
least O
four O
roles B
. O

We O
further O
explore O
this O
observation O
by O
examining O
how O
winning B

 O
rate I
changes O
as O
the O
number O
of O
roles B
in O
a O
team B
grows O
( O
Fig O
. O

3c O
) O
. O

We O
find O
that O
diverse O
team B
compositions I
are O
much O
more O
likely O
to O
win O
. O

One O
explanation O
is O
that O
these O
roles B
are O
designed O
to O
complement O
each O
other O
and O
teams B
with O
too O
few O
roles B
have O
weaknesses B
to O
be O
exploited O
. O

For O
instance O
, O
a O
team B
with O
no O
mage B
( O
) O
or O
marksman O
( O
) O
can O
not O
cause O
sufficient O
damage B
in O
a O
game B
. O

Fig O
. O

3d O
shows O
that O
most O
teams B
are O
employing O
at O
least O
three O
role B
categories I
, O
which O
6 O
Rank O
level B
indicates O
a O
player B
’s O
gaming O
skill B
and O
ranges B
from O
0 O
to O
26 O
. O

7 O
We O
make O
similar O
tables O
as O
Table O
3 O
for O
surrendering O
and O
abusing O
. O

See O
§ O
A O
for O
reference O
. O

is O
consistent O
with O
the O
observation O
in O
Fig O
. O

3a O
that O
the O
losing B
team I
compositions O
on O
the O
left O
are O
not O
usually O
used O
. O

4 O
TEAM O
COMPOSITION O
AND O
SURRENDERING O
As O
nothing O
worth B
achieving O
is O
going O
to O
be O
easy B
, O
tenacity O
is O
an O
important O
characteristic O
of O
an O
effective O
team B
. O

In O
the O
context O
of O
Honor O
of O
Kings O
, O
team B
tenacity O
can O
be O
reflected O
by O
not O
surrendering B
easily O
. O

The O
surrendering O
procedure O
is O
as O
follows O
: O
a O
player B
can O
propose O
to O
surrender B
and O
if O
at O
least O
four O
of O
five O
team B
members I
agree O
, O
the O
team B
will O
surrender B
as O
a O
whole O
and O
lose O
immediately O
. O

In O
this O
section O
, O
we O
examine O
the O
effect B
of O
team B
composition I
on O
team B
tenacity O
. O

Surrender B
probability O
varies O
across O
team B
compositions I
in O
early B

 O
games I
( O
Fig O
. O

4a O
, O
4b O
) O
. O

Similar O
to O
winning O
, O
surrender B
happens O
at O
the O
end O
of O
a O
game B
. O

But O
surrender B
requires O
extra O
careful O
analysis O
because O
surrender B
may O
not O
always O
reflect O
team B
tenacity O
in O
Honor O
of O
Kings O
and O
other O
MOBA O
games B
. O

For O
example O
, O
when O
the O
opponents B
are O
destroying O
the O
team B
base I
and O
all O
team B
members I
are O
killed B
, O
it O
is O
time B
to O
move B
on O
and O
surrender B
is O
simply O
a O
sign O
of O
“ O
game B
over O
” O
. O

This O
is O
common O
in O
late B
games I
. O

Therefore O
, O
it O
is O
necessary O
to O
distinguish O
surrender B
in O
different O
game B
stages B
. O

We O
examine O
the O
mean O
and O
the O
standard O
deviation O
of O
surrender B
probability O
across O
team B
compositions I
conditioned O
on O
game B
duration O
in O
Fig O
. O

4a O
. O

We O
make O
two O
observations O
. O

First O
, O
the O
surrender B
probability O
monotonically O
decreases O
as O
game B
duration O
increases B
, O
partly O
because O
a O
team B
is O
only O
allowed O
to O
surrender B
starting B
from O
the O
6th O
minute O
and O
games B
end O
in B
6 O
- I
8 I
minutes O
mostly O
because O
a O
team B
surrenders B
. O

Second O
, O
the O
standard O
deviation O
across O
team B
compositions I
first O
increases B
and O
then O
decreases O
as O
game B
duration O
increases B
, O
indicating O
that O
in O
late B
games I
every O
team B
surrenders B
with O
similar O
probability O
to O
signal O
game B
over O
. O

Based B
on O
Fig O
. O

4a O
, O
we O
define O
three O
game B
stages B
: O
early B
games I
( O
t O
≤ O
11 O
) O
, O
where O
the O
surrender B
probability O
has O
great O
mean O
and O
great O
std O
; O
late B
games I
( O
t O
> O
16 O
) O
, O
where O
the O
surrender B
probability O
has O
small O
mean O
and O
small O
std O
; O
middle B
games O
( O
11 O
< O
t O
≤ O
16 O
) O
, O
the O
middle B
part8 O
. O

Proportions O
of O
games B
in O
each O
stages B
are O
shown O
in O
Table O
2 O
. O

8A O
gaming B
related O
reason O
for O
this O
split B
is O
that O
11-minutes O
marks B
a O
watershed O
because O
the O
“ O
Overlord O
” O
and O
“ O
Dark O
Tyrant O
” O
( O
two O
important O
non B
- I
player I
characters B
) O
appear O
at O
the O
10th O
minute O
and O
game B
state I
can O
change O
significantly O
from O
11 O
to O
16 O
minutes O
when O
the O
characters B
get O
killed B
. O

4 O
0 O
8 O
11 O
14 O
17 O
20 O
23 O
26 O
29 O
game B
duration O
t O
( O
min O
) O
0 O
20 O
40 O
60 O
80 O
100 O
surrender B
prob.(% O
) O
std(sur O
- O
prob O
. O

) O
mean(sur O
- O
prob O
. O

) O
0.0 O
0.02 O
0.04 O
0.06 O
0.08 O
std(surrender O
prob O
. O

) O
x=11min O
x=16min O
( O
a O
) O
Surrender B
probability O
vs. O
game B
duration O
. O

20 O
60 O
100 O
team B
composition I
0 O
20 O
40 O
60 O
80 O
surrender B
prob.(% O
) O
t O
≤11min O
11 O
< O
t O
≤16min O
t O
> O
16min O
CDF(#teams)(% O
) O
0 O
20 O
40 O
60 O
80 O
100 O
CDF(#teams)(% O
) O
( O
b O
) O
Surrender O
prob O
. O

vs. O
team B
composition I
. O

1 O
2 O
3 O
4 O
5 O
number O
of O
roles B
10 O
30 O
50 O
surrender B
prob.(% O
) O
t O
≤11min O
11 O
< O
t O
≤16min O
t O
> O
16min O
( O
c O
) O
Surrender O
prob O
. O

vs. O
team B
diversity O
. O

Figure O
4 O
: O
Team B
composition I
and O
surrender B
probability O
. O

Fig O
. O

4a O
shows O
the O
mean O
and O
standard O
deviation O
of O
surrender B
probability O
across O
team B
compositions I
conditioned O
on O
game B
duration O
. O

Fig O
. O

4b O
shows O
that O
team B
compositions I
have O
varying O
surrender B
probabilities O
in O
games B
that O
end O
within O
11 O
minutes O
, O
but O
have O
similar O
ones O
in O
longer O
games B
. O

Fig O
. O

4c O
shows O
that O
diverse O
teams B
are O
less O
likely O
to O
surrender B
in O
early B
games I
( O
error O
bars O
represent O
standard O
errors O
) O
. O

Having O
established O
the O
three O
types O
of O
games B
, O
we O
hypothesize O
that O
surrender B
in O
early B
games I
is O
the O
most O
indicative O
of O
team B
tenacity O
and O
varies O
across O
team B
compositions I
, O
while O
surrender B
in O
middle B
and O
late B
games I
is O
more O
of O
a O
formality O
and O
should O
not O
depend O
on O
team B
compositions I
. O

To O
study O
that O
, O
we O
sort O
team B
compositions I
by O
surrender B
probability O
in O
early B
games I
in O
Fig O
. O

4b O
. O

According O
to O
the O
same O
order O
, O
we O
show O
the O
surrender B
probability O
in O
middle B
games I
and O
late B
games I
, O
as O
well O
as O
the O
cumulative O
distribution O
function O
of O
# O
teams B
. O

Consistent O
with O
our O
hypothesis O
, O
the O
surrender B
probability O
of O
different O
team B
compositions I
span O
a O
wide B
range I
from O
33.6 O
% O
to O
84.6 O
% O
in O
early B
games I
, O
but O
is O
pretty O
stable O
in O
middle B
games I
and O
late B

 O
games I
. O

The O
CDF O
of O
# O
teams B
presents O
a O
different O
shape O
from O
that O
in O
Fig O
. O

3a O
: O
commonly O
used O
team B
compositions I
are O
neither O
the O
most O
tenacious O
nor O
the O
least O
tenacious O
. O

Table O
6 O
shows O
the O
most O
tenacious O
and O
the O
least O
tenacious O
team B
compositions I
. O

Diverse O
teams B
tend O
to O
be O
tenacious O
( O
Fig O
. O

4c O
) O
. O

Role B
diversity O
influences O
surrender B
probability O
in O
early B
games I
, O
especially O
when O
the O
number O
of O
roles B
is O
large O
. O

It O
seems O
that O
diverse O
teams B
with O
five O
roles B
are O
the O
most O
tenacious O
in O
early B
games I
. O

In O
comparison O
, O
the O
influence O
on O
middle B
and O
late B
games I
is O
minimal O
. O

A O
weak B
team B
can O
still O
be O
tenacious O
( O
Fig O
. O

5 O
) O
. O

We O
have O
shown O
that O
diverse O
teams B
tend O
to O
both O
perform O
well O
and O
show O
tenacity O
in O
adversity O
. O

One O
concern O
is O
that O
these O
two O
measures O
are O
correlated O
and O
tenacity O
is O
simply O
a O
side O
effect B
of O
team B
strength B
( O
winning O
) O
. O

This O
hypothesis O
suggests O
that O
winning O
team B
compositions I
should O
be O
less O
likely O
to O
surrender B
. O

To O
further O
understand O
this O
issue O
, O
we O
study O
the O
correlation O
between O
surrender B
probability O
and O
winning B

 O
rate I
for O
different O
team B
compositions I
. O

Fig O
. O

5a O
shows O
that O
there O
is O
indeed O
a O
negative O
correlation O
between O
winning B
rate I
and O
surrender B
probability O
in O
early B
games I
. O

There O
is O
little O
correlation O
in O
middle B
and O
late O
games.9 O
However O
, O
it O
seems O
that O
the O
correlation O
between O
winning B
rate I
and O
surrender B
probability O
in O
early B
games I
is O
dominated O
by O
a O
few O
outliers O
with O
very O
low O
winning B
rates I
. O

Given O
that O
we O
know O
that O
these O
teams B
only O
take O
a O
small O
fraction O
of O
all O
games B
in O
§ O
3 O
, O
we O
filter O
infrequent O
team B
compositions I
that O
appear O
less O
than O
10,000 O
times B
and O
find O
that O
in O
the O
remaining O
team B
compositions I
( O
98.7 O
% O
of O
all O
teams B
) O
, O
surrender B
probability O
is O
not O
correlated O
with O
winning B
rate I
in O
all O
game B
stages B
( O
Fig O
. O

5b O
) O
, O
suggesting O
that O
team B
tenacity O
is O
almost O
independent O
of O
team B
strength B
/ O
performance O
. O

9 O
In O
fact O
, O
there O
is O
a O
small O
significantly O
positive O
coefficient O
in O
late B
games I
, O
which O
is O
another O
evidence O
that O
strength B
does O
not O
entail O
tenacity O
. O

0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
win O
rate(% O
) O
0 O
40 O
80 O
surrender B
prob.(% O
) O
t O
≤ O
11min O
slope O
: O
-0.53 O
* O
p O
- O
value O
: O
6.1×10−6 O
11 O
< O
t O
≤ O
16min O
slope O
: O
0.01 O
p O
- O
val O
: O
0.77 O
t O
> O
16min O
slope O
: O
0.04 O
* O
p O
- O
val O
: O
2.5×10−4 O
( O
a O
) O
All O
team B
compositions I
. O

10 O
20 O
30 O
40 O
50 O
60 O
win O
rate(% O
) O
0 O
20 O
40 O
60 O
80 O
surrender B
prob.(% O
) O
t O
≤ O
11min O
slope O
: O
-0.05 O
p O
- O
value O
: O
0.85 O
11 O
< O
t O
≤ O
16min O
slope O
: O
0.04 O
p O
- O
val O
: O
0.22 O
t O
> O
16min O
slope O
: O
0.04 O
* O
p O
- O
val O
: O
1.5×10−3 O
( O
b O
) O
Filtering O
infrequent O
team B

 O
compositions I
. O

Figure O
5 O
: O
Surrender B
probability O
vs. O
winning B
rate I
. O

In O
both O
figures O
, O
each O
dot B
denotes O
a O
team B
composition I
with O
its O
xvalue O
for O
winning B
rate I
and O
y O
- O
value O
for O
surrender B
probability O
. O

Fig O
. O

5a O
shows O
results O
for O
all O
team B
compositions I
. O

Fig O
. O

5b O
shows O
the O
results O
after O
removing O
infrequent O
compositions O
, O
where O
surrender B
is O
not O
correlated O
with O
winning B
rate I
. O

5 O
TEAM O
COMPOSITION O
AND O
ABUSIVE B

 O
LANGUAGE I
USE O
Our O
final O
measure O
of O
team B
effectiveness I
is O
concerned O
with O
the O
rapport O
in O
a O
team B
during O
the O
game B
. O

We O
use O
abusive B
language I
use O
to O
capture B
the O
rapport O
. O

Toxic O
behavior O
in O
online B
communities I
and O
gaming O
has O
received O
significant O
interests O
from O
our O
research O
community B
recently O
[ O
12 O
, O
13 O
, O
29 O
] O
. O

Here O
we O
provide O
the O
first O
systematic O
study O
on O
the O
effect B
of O
team B
composition I
on O
team B
- O
level B
abusive B
language I
use O
. O

5.1 O
Team O
- O
level B
Abusive O
Language O
Use O
Team O
compositions O
vary O
in O
abusing O
probability O
( O
Fig O
. O

6a O
) O
. O

In O
our O
dataset O
, O
we O
have O
a O
label O
of O
whether O
a O
message B
uses O
abusive B

 O
language I
for O
all O
text O
messages B
based B
on O
a O
dictionary O
- O
based B
method O
officially O
used O
by O
Tencent.10 O
A O
team B
abuses O
if O
any O
player B
in O
that O
team B
abuses O
. O

For O
each O
team B
composition I
, O
we O
define O
abusing O
probability O
as O
the O
fraction O
of O
games B
that O
this O
team B
composition I
abuses O
. O

We O
find O
that O
similar O
to O
winning O
and O
surrendering B
, O
team B
compositions I
vary O
in O
abusing O
probability O
, O
ranging B
from O
28.7 O
% O
to O
56.2 O
% O
. O

The O
cumulative O
distribution O
function O
of O
# O
teams B
looks O
much O
more O
similar O
to O
surrendering B
in O
Fig O
. O

4b O
than O
winning O
in O
Fig O
. O

3a O
: O
most O
commonly O
used O
team B
compositions I
are O
neither O
the O
most O
nor O
the O
least O
abusive O
. O

Table O
7 O
shows O
team B
compositions I
with O
the O
highest O
10Players O
may O
abuse O
using O
voice O
messages B
; O
here O
we O
only O
consider O
text O
messages B
. O

5 O
20 O
60 O
100 O
team B
composition I
30 O
35 O
40 O
45 O
50 O
55 O
abusing O
prob.(% O
) O
abusing O
prob O
. O

CDF(#teams)(% O
) O
0 O
20 O
40 O
60 O
80 O
100 O
CDF(#teams)(% O
) O
( O
a O
) O
Sorted O
abusing O
probability O
. O

10 O
20 O
30 O
40 O
50 O
win O
rate(% O
) O
30 O
40 O
50 O
60 O
abusing O
prob.(% O
) O
slope O
: O
-0.41 O
* O
p O
- O
value O
: O
5.2×10−4 O
( O
b O
) O
Abusing O
probability O
vs. O
winning B
rate I
. O

1 O
2 O
3 O
4 O
5 O
number O
of O
roles B
25 O
35 O
45 O
55 O
abusing O
prob.(% O
) O
win O
team B
lose B
team I
( O
c O
) O
Abusing O
probability O
vs. O
team B
diversity O
. O

Figure O
6 O
: O
Team B
composition I
and O
abusive B
language I
use O
. O

Fig O
. O

6a O
presents O
sorted O
abusing O
probability O
for O
different O
team B
compositions I
and O
its O
corresponding O
cumulative O
distribution O
function O
in O
# O
teams B
. O

In O
Fig O
. O

6b O
, O
each O
dot B
represents O
a O
team B
composition I
after O
filtering O
infrequent O
team B
compositions I
; O
x O
- O
value O
represents O
winning B
rate I
and O
y O
- O
value O
represents O
abusing O
probability O
. O

In O
Fig O
. O

6c O
, O
x O
- O
axis O
represents O
the O
number O
of O
roles B
in O
a O
team B
, O
and O
y O
- O
axis O
represents O
team B
- O
level B
abusing O
probability O
. O

and O
lowest O
abusing O
probability O
. O

It O
is O
striking O
that O
all O
the O
top O
teams B
have O
multiple O
assassins B
( O
) O
and O
all O
the O
bottom B
teams B
have O
multiple O
supports B
( O
) O
. O

We O
will O
investigate O
further O
the O
interaction O
of O
individual O
- O
level B
abusing O
and O
team B
- O
level B
abusing O
in O
§ O
5.2 O
. O

Losing B
teams I
are O
more O
likely O
to O
abuse O
( O
Fig O
. O

6b O
) O
. O

We O
hypothesize O
that O
losing B
teams I
are O
more O
likely O
to O
abuse O
because O
winning O
usually O
brings O
positive O
team B
morale O
, O
while O
losing O
leads B
to O
frustration O
and O
dissatisfaction O
[ O
29 O
] O
. O

This O
is O
indeed O
the O
case O
as O
shown O
in O
Fig O
. O

6b O
. O

Therefore O
, O
it O
is O
important O
to O
distinguish O
abusing O
probability O
between O
winning O
teams B
and O
losing B
teams I
. O

Diverse O
teams B
tend O
to O
abuse O
more O
when O
losing O
and O
abuse O
less O
when O
winning O
( O
Fig O
. O

6c O
) O
. O

We O
further O
explore O
the O
effect B
of O
role B
diversity O
on O
team B
- O
level B
abusing O
. O

We O
observe O
different O
trends O
in O
winning O
teams B
and O
losing B
teams I
. O

When O
a O
team B
wins I
, O
team B
diversity O
is O
associated O
with O
low O
abusing O
probability O
; O
but O
it O
becomes O
the O
other O
way O
around O
if O
a O
team B
loses O
. O

5.2 O
Individual O
- O
level B
Abusive O
Language O
Use O
Different O
from O
winning O
and O
surrendering B
, O
abusive B
language I
use O
is O
an O
individual O
behavior O
. O

It O
provides O
a O
great O
opportunity O
to O
understand O
the O
effect B
of O
team B
composition I
on O
individual O
behavior O
and O
shed O
light O
on O
the O
“ O
situation O
vs. O
personality O
” O
debate O
[ O
18 O
, O
25 O
] O
. O

Note O
that O
we O
conduct O
this O
part O
of O
experiments O
on O
players B
who O
have O
played B
at O
least O
20 O
games B
to O
ensure O
sufficient O
samples O
for O
statistics O
. O

A O
team B
does O
not O
equal O
the O
sum O
of O
individuals O
( O
Fig O
. O

7a O
) O
. O

We O
define O
individual O
abusing O
probability O
based B
on O
the O
fraction O
of O
games O
that O
a O
player B
abuses O
in.11 O
Because O
of O
linearity O
of O
expectation O
, O
the O
total O
number O
of O
expected O
abusing O
players B
in O
a O
team B
is O
simply O
the O
sum O
of O
each O
player B
’s O
abusing O
probability O
: O
E(#abusing O
players B
in O
a O
team B
) O
= O
Õ O
v O
∈team O
Pabuse(v O
) O
( O
1 O
) O
where O
Pabuse O
is O
estimated O
separately O
when O
a O
player B
wins O
or O
loses O
because O
abusing O
is O
associated O
with O
losing O
. O

This O
expectation O
is O
only O
valid O
if O
all O
players B
act O
independently O
in O
a O
team B
, O
but O
studies O
on O
teamwork B
have O
shown O
that O
a O
team B
does O
not O
equal O
the O
sum O
of O
individuals O
11All O
individual O
abusing O
probability O
is O
based B
on O
individual O
level B
samples O
. O

Individual O
samples O
can O
be O
grouped B
based B
on O
criteria O
other O
than O
all O
games B
played B
by O
a O
single O
player B
, O
e.g. O
, O
we O
compute O
individual O
abusing O
probability O
for O
all O
winning O
( O
losing O
) O
games B
of O
a O
player B
in O
Fig O
. O

7a O
and O
for O
all O
players B
who O
choose O
a O
particular O
role B
in O
Fig O
. O

7b O
. O

[ O
7 O
] O
. O

Therefore O
, O
we O
examine O
the O
discrepancy O
between O
observed O
values O
and O
expected O
values O
to O
understand O
how O
team B
composition I
influences O
individual B
players I
. O

This O
analysis O
is O
done O
on O
prediction O
games B
since O
it O
requires O
historical O
information O
of O
every O
team B
member I
. O

Figure O
7a O
shows O
the O
difference O
between O
observed O
and O
expected O
values O
for O
winning O
and O
losing B
teams I
, O
where O
win O
/ O
lose O
diff O
. O

refers O
to O
the O
disparity O
between O
observed O
and O
expected O
number O
of O
abusing O
players(O(#abusing O
players)−E(#abusing O
players B
) O
) O
. O

For O
most O
team B

 O
compositions I
, O
the O
difference O
between O
observed O
values O
and O
expected O
values O
is O
not O
zero O
, O
indicating O
that O
individuals O
abuse O
differently O
depending O
on O
team B
compositions I
. O

In O
particular O
, O
in O
the O
most O
commonly O
used O
teams B
( O
when O
the O
CDF O
grows O
quickly O
on O
the O
right O
of O
the O
plot O
) O
, O
individuals O
are O
more O
likely O
to O
abuse O
than O
expected O
when O
losing O
. O

Players B
who O
prefer O
leading B
roles B
are O
more O
abusive O
( O
Fig O
. O

7b O
, O
7c O
, O
7d O
) O
. O

There O
always O
exists O
a O
leader B
or O
a O
major O
contributor O
in O
a O
team B
. O

In O
Honor O
of O
Kings O
, O
assassins B
usually O
take O
this O
role B
in O
a O
team B
because O
of O
their O
explosiveness O
: O
assassins B
can O
carry B
the O
team B
and O
control B
game B
pace O
. O

A O
failed O
assassin B
may O
lead B
the O
team B
to O
lose O
. O

Fig O
. O

7b O
shows O
that O
assassins B
are O
more O
likely O
to O
abuse O
than O
other O
roles B
. O

We O
call B
this O
the O
“ O
abusive O
assassin B
” O
phenomenon O
. O

One O
natural O
question O
arises O
: O
are O
assassins B
more O
abusive O
because O
abusive O
players B
tend O
to O
choose O
assassins B
, O
or O
players B
become O
more O
abusive O
when O
choosing O
assassins B
? O
To O
answer O
this O
question O
, O
we O
compare O
the O
abusing O
probability O
of O
the O
same O
roles B
chosen O
by O
different O
players B
and O
the O
same O
player B
choosing O
different O
roles B
. O

We O
define O
the O
experienced B
role I
for O
a O
player B
if O
that O
player B
chooses O
a O
role B
frequently O
, O
i.e. O
, O
playing B
in O
more O
than O
50 O
% O
of O
games B
. O

This O
procedure O
identifies O
experienced B
assassin B
players B
, O
experienced O
warrior B
players B
, O
etc O
. O

Fig O
. O

7c O
presents O
the O
abusing O
probability O
of O
experienced O
assassin B
players B
when O
they I
choose I
each O
role B
( O
the O
same O
player B
choosing O
different O
roles B
) O
. O

We O
observe O
that O
these O
experienced B
assassin B
players B
are O
much O
more O
likely O
to O
abuse O
than O
other O
players B
no O
matter O
what O
role B
they O
choose O
. O

Fig O
. O

7d O
examines O
the O
alternative O
hypothesis O
: O
players B
become O
more O
abusive O
when O
choosing O
assassins B
. O

We O
compare O
the O
same O
player B
’s O
abusing O
probability O
when O
they O
choose O
assassins B
with O
choosing O
other O
roles B
, O
grouped B
by O
their O
experienced B
roles I
. O

We O
find O
that O
players B
do O
not O
become O
more O
abusive O
when O
choosing O
assassins B
. O

If O
anything O
, O
experienced B
mages B
and O
experienced O
marksmen O
are O
actually O
more O
abusive O
when O
they O
choose O
roles B
other O
than O
assassins B
. O

6 O
0 O
10 O
20 O
30 O
40 O
team B
composition I
-0.2 O
-0.1 O
0.0 O
0.1 O
diff O
. O

in#(abusing O
players B
) O
CDF(#teams O
) O
lose O
diff O
. O

win O
diff O
. O

0 O
20 O
40 O
60 O
80 O
100 O
CDF(#teams)(% O
) O
( O
a O
) O
Observed O
# O
abusing O
players B
− O
expected O
values O
. O

Assa O
. O

Warr O
. O

Mage O
Mark O
. O

Supp O
. O

role B
14 O
16 O
18 O
20 O
individual O
abusing O
prob(% O
) O
19.0 O
16.0 O
14.0 O
13.9 O
13.6 O
( O
b O
) O
Abusing O
probability O
vs. O
roles B
. O

Assa O
. O

Warr O
. O

Mage O
. O

Mark O
. O

Supp O
. O

chosen O
role B
2 O
6 O
10 O
14 O
individual O
abusing O
prob.(% O
) O
experienced B
assassins B
others O
( O
c O
) O
Abusing O
probability O
of O
the O
same O
roles B
by O
different O
players B
. O

Assa O
. O

Warr O
. O

Mage O
. O

Mark O
. O

Supp O
. O

experienced B
role B
6 O
8 O
10 O
12 O
individual O
abusing O
prob.(% O
) O
choose O
assassin B
choose O
others O
( O
d O
) O
Abusing O
probability O
of O
the O
same O
player B
with O
different O
roles B
. O

Figure O
7 O
: O
Fig O
. O

7a O
shows O
the O
difference O
between O
the O
observed O
number O
of O
abusing O
players B
and O
the O
expected O
value O
based B
on O
individual O
abusing O
probability O
. O

The O
right O
three O
figures O
investigate O
the O
“ O
abusive O
” O
assassin B
phenomenon O
. O

Fig O
. O

7b O
shows O
individual O
abusive O
probability O
grouped B
by O
roles B
. O

Fig O
. O

7c O
compares O
the O
abusing O
probability O
of O
players B
who O
usually O
play B
assassins B
with O
those O
who O
usually O
play B
other O
roles B
, O
grouped B
by O
their O
chosen O
role B
. O

Fig O
. O

7d O
compares O
the O
abusing O
probability O
when O
a O
player B
choose O
assassin B
with O
when O
the O
same O
player B
chooses O
other O
roles B
, O
grouped B
by O
his O
experienced B
role I
. O

Error O
bars O
represent O
standard O
errors O
. O

50 O
% O
60 O
% O
70 O
% O
experienced B
threshold O
2 O
6 O
10 O
14 O
18 O
individual O
abusing O
prob.(% O
) O
inexperienced B
role I
experienced B
role B
( O
a O
) O
Abusing O
probability O
vs. O
( O
in)experienced O
roles B
. O

0 O
1 O
2 O
3 O
4 O
number O
of O
teammates B
8 O
10 O
12 O
14 O
16 O
individual O
abusing O
prob.(% O
) O
threshold O
50 O
% O
threshold O
60 O
% O
threshold O
70 O
% O
( O
b O
) O
Abusing O
probability O
vs. O
# O
experienced B
roles I
in O
teammates B
. O

Figure O
8 O
: O
Individual O
abusing O
probability O
and O
experienced B
roles B
. O

Fig O
. O

8a O
compares O
the O
abusing O
probability O
when O
a O
player B
plays O
his I
experienced I
role B
with O
when O
playing B
inexperienced B
roles I
. O

In O
Fig O
. O

8b O
, O
x O
- O
axis O
denotes O
the O
number O
of O
experienced B
roles I
( O
defined O
by O
given O
threshold O
) O
among O
teammates B
, O
while O
y O
- O
axis O
denotes O
individual O
abusing O
probability O
. O

Overall O
, O
our O
results O
in O
Honor O
of O
Kings O
support B
the O
hypothesis O
that O
abusive O
players B
tend O
to O
choose O
assassins B
, O
the O
leading O
role B
in O
a O
team B
, O
and O
players B
do O
not O
become O
abusive O
when O
choosing O
assassins B
. O

Experienced O
players B
are O
more O
likely O
to O
abuse O
, O
but O
not O
when O
they O
play B
with O
other O
experienced O
players B
( O
Fig O
. O

8) O
. O

Studies O
have O
shown O
that O
experienced O
individuals O
tend O
to O
abuse O
novices B
[ O
23 O
] O
. O

We O
hypothesize O
that O
a O
player B
is O
more O
likely O
to O
abuse O
when O
playing B
his O
experienced B
role I
. O

In O
addition O
to O
using O
50 O
% O
as O
a O
threshold O
to O
define O
experienced B
roles I
, O
we O
also O
use O
70 O
% O
and O
60 O
% O
here O
. O

Fig O
. O

8a O
shows O
that O
players B
are O
more O
abusive O
when O
playing B
their O
experienced B
roles I
no O
matter O
what O
threshold O
is O
. O

We O
next O
further O
explore O
how O
players B
are O
influenced O
by O
teammates B
when O
choosing O
experienced B
roles I
. O

Fig O
. O

8b O
shows O
that O
as O
the O
number O
of O
experienced B
roles I
increases B
among O
a O
player B
’s O
teammates B
, O
his O
individual O
abusing O
probability O
player B
decreases O
( O
since O
this O
analysis O
requires O
historical O
information O
of O
every O
team B
member I
, O
it O
is O
done O
on O
prediction O
games B
) O
. O

This O
observation O
indicates O
that O
a O
player B
becomes O
more O
likely O
to O
abuse O
when O
choosing O
experienced B
roles I
partly O
because O
other O
less O
experienced O
players B
do O
not O
meet O
this O
experienced O
player B
’s O
expectation O
and O
cause O
frustration O
. O

6 O
PREDICTION O
EXPERIMENTS O
In O
this O
section O
, O
we O
set O
up O
three O
prediction O
tasks O
for O
winning O
, O
surrendering B
, O
and O
abusing O
respectively O
to O
further O
examine O
the O
effect B
of O
team B
composition I
in O
a O
predictive O
setting O
. O

Overall O
, O
we O
find O
that O
features O
based B
on O
team B
composition I
consistently O
outperform O
features O
based B
on O
individual B
players I
. O

6.1 O
Experiment O
Setup O
Prediction O
tasks O
and O
evaluation O
metrics O
. O

Our O
goal B
is O
to O
evaluate O
whether O
modeling O
team B
composition I
brings O
extra O
predictive O
power B
to O
modeling O
individual B
players I
. O

Because O
team B
composition I
can O
not O
be O
altered O
after O
a O
game B
starts B
, O
we O
focus B
on O
making O
predictions O
before O
a O
game B
starts B
and O
consider O
the O
following O
three O
prediction O
tasks O
. O

Our O
results O
can O
potentially O
inform O
both O
players B
in O
choosing O
roles B
and O
the O
matching O
- O
making O
system O
to O
match B
players B
. O

• O
Winning O
team B
prediction[46 O
] O
. O

This O
task O
aims O
to O
predict O
which O
team B
will O
win I
before O
the O
game B
starts B
. O

We O
randomly O
swap O
the O
order O
of O
two O
teams B
and O
label O
a O
game B
positive O
if O
the O
first O
team B
wins I
and O
negative O
if O
the O
second O
team B
wins I
. O

This O
leads B
to O
a O
balanced O
dataset O
, O
so O
we O
use O
accuracy O
for O
evaluation O
. O

• O
Surrender B
prediction O
. O

This O
task O
aims O
to O
predict O
whether O
the O
losing B

 O
team I
surrenders B
in O
a O
game B
. O

Only O
8 O
K O
games B
( O
10.5 O
% O
) O
end O
with O
surrender B
, O
so O
we O
use O
F1 O
as O
evaluation O
metric O
in O
this O
task O
. O

• O
Abuse O
prediction O
. O

Before O
a O
game B
starts B
, O
this O
task O
predicts O
whether O
abusive B
language I
use O
happens O
in O
the O
game B
. O

Abusive B
language I
use O
happens O
in O
44 O
K O
games B
( O
57.93 O
% O
) O
, O
so O
we O
use O
accuracy O
for O
evaluation O
in O
this O
task O
. O

Feature O
sets O
. O

We O
consider O
the O
following O
feature O
sets O
. O

• O
Baseline O
. O

People O
’s O
mood O
varies O
over O
time B
and O
is O
associated O
with O
abusive B
language I
use O
[ O
12 O
] O
. O

Our O
baseline O
features O
include O
day O
of O
the O
week O
and O
hour O
of O
the O
day O
encoded O
as O
one O
- O
hot O
vectors O
. O

• O
Player B
information O
. O

Depending O
on O
the O
task O
, O
we O
compute O
each O
player B
’s O
rank B
level I
, O
winning B
rates I
in O
the O
last O
10 O
ranked B
games B
, O
surrender B
probability O
, O
and O
abusing O
probability O
from O
historical O
games B
. O

We O
use O
the O
average O
, O
max O
, O
min O
, O
and O
standard O
deviation O
of O
those O
properties O
of O
all O
players B
in O
a O
team B
as O
features O
. O

7 O
baseline O
player B
info O
. O

role B
prop O
. O

total O
50 O
60 O
70 O
accuracy(% O
) O
49.9 O
% O
67.7 O
% O
68.6 O
% O
70.4 O
% O
( O
a O
) O
Winning O
team B
prediction O
baseline O
player B
info O
. O

role B
prop O
. O

total O
50 O
52 O
54 O
56 O
58 O
accuracy(% O
) O
49.9 O
% O
53.3 O
% O
54.9 O
% O
56.3 O
% O
( O
b O
) O
Winning O
team B
prediction O
on O
games B
where O
the O
rank B
gap I
δ O
= O
0 O
baseline O
player B
info O
. O

role B
prop O
. O

total O
0.26 O
0.28 O
0.3 O
0.32 O
F1 O
score O
0.278 O
0.304 O
0.312 O
0.319 O
( O
c O
) O
Surrender B
prediction O
baseline O
player B
info O
. O

role B
prop O
. O

total O
58 O
62 O
66 O
70 O
accuracy(% O
) O
57.8 O
% O
65.0 O
% O
66.3 O
% O
67.8 O
% O
( O
d O
) O
Abuse O
prediction O
Figure O
9 O
: O
Prediction O
results O
. O

Gray O
lines O
indicate O
the O
performance O
of O
a O
majority O
baseline O
in O
Fig O
. O

9a O
and O
9d O
, O
and O
a O
random O
baseline O
in O
9c O
. O

Features O
that O
incorporate O
role B
combinations O
with O
player B
information O
( O
role B
prop O
. O

) O
consistently O
outperform O
others O
. O

• O
Role B
properties O
. O

A O
role B
combination O
of O
k O
refers O
to O
all O
possible O
combinations O
of O
k O
roles B
. O

For O
instance O
, O
one O
- O
role B
combinations O
refer O
to O
the O
five O
roles B
in O
Table O
1 O
, O
two O
- O
role B
combinations O
refer O
to O
15 O
possible O
combinations O
of O
two O
roles B
( O
5 O
1 O
 O
+ O


 O
5 O
2 O
 O
) O
. O

We O
further O
incorporate O
player B
information O
in O
role B
combinations O
of O
both O
teams B
. O

For O
each O
role B
combination O
in O
a O
team B
, O
we O
compute O
the O
average O
of O
role B
- O
based B
player B
information O
in O
an O
instance O
of O
that O
combination O
and O
then O
use O
the O
average O
, O
max O
, O
min O
, O
and O
std O
of O
all O
instances O
in O
that O
combination O
as O
features O
. O

Taking O
an O
example O
to O
further O
explain O
role B
- O
based B
player B
information O
, O
the O
feature O
winning B
rate I
of O
role B
c O
for O
player B
v O
is O
defined O
as O
the O
probability O
of O
v O
wins O
when O
she O
plays B
c. O
We O
only O
consider O
up O
to O
two O
- O
role B
combinations O
( O
k O
≤ O
2 O
) O
when O
comparing O
prediction O
performances O
. O

Later O
we O
will O
explore O
the O
effect B
of O
considering O
three O
- O
role B
combinations O
up O
to O
five O
- O
role B
combinations O
, O
i.e. O
, O
full B
team B
compositions I
. O

We O
also O
use O
the O
union O
of O
all O
feature O
sets O
( O
“ O
total O
” O
) O
. O

Because O
we O
find O
that O
game B
duration O
plays B
an O
important O
role B
in O
surrender B
, O
we O
add O
game B
duration O
to O
every O
feature O
set O
for O
surrender B
prediction O
. O

In O
abuse O
prediction O
, O
we O
do O
not O
distinguish O
two O
teams B
. O

Prediction O
setup O
. O

For O
each O
task O
, O
we O
conduct O
5-fold O
nested O
crossvalidations O
on O
prediction O
games B
in O
the O
third O
week O
and O
player B
information O
is O
extracted O
from O
historical O
games B
in O
the O
first O
two O
weeks O
. O

We O
use O
Xgboost O
[ O
11 O
] O
since O
it O
is O
often O
the O
winning O
solution O
in O
Kaggle O
competitions B
and O
shows O
strong O
performance O
compared O
to O
other O
methods O
such O
as O
logistic O
regression O
in O
our O
preliminary O
experiments O
. O

We O
grid O
search O
hyperparameters O
in O
max O
depth O
( O
{ O
1 O
, O
3 O
, O
5 O
, O
7 O
, O
9 O
} O
) O
, O
learning O
rate O
( O
{ O
0.1 O
, O
0.2 O
} O
) O
, O
and O
number O
of O
estimators O
( O
{ O
100 O
, O
200 O
, O
300 O
} O
) O
. O

6.2 O
Experiment O
Results O
Prediction O
performance O
( O
Fig O
. O

9a O
, O
9b O
, O
9c O
, O
and O
9d O
) O
. O

In O
all O
prediction O
tasks O
, O
we O
observe O
a O
consistent O
pattern O
that O
total O
> O
role B
property O
> O
player B
information O
> O
baseline O
. O

Player B
information O
achieves O
relatively O
better O
performance O
than O
baselines O
since O
individual O
characteristics O
definitely O
influence O
team B
effectiveness I
in O
some O
degree O
. O

By O
incorporating O
player B
information O
with I
role I
combinations I
, O
role B
properties O
consistently O
outperform O
player B
information O
. O

The O
extra O
predictive O
power B
from O
modeling O
team B
composition I
can O
be O
reflected O
by O
the O
difference O
between O
total O
and O
player B
information O
: O
e.g. O
, O
70.4 O
% O
vs O
67.7 O
% O
in O
winning O
prediction O
for O
all O
games B
, O
and O
67.8 O
% O
vs O
65.0 O
% O
in O
abusing O
prediction O
. O

In O
winning O
prediction O
, O
the O
effect B
of O
team B
1 O
2 O
3 O
4 O
5 O
k O
- O
role B
prop O
. O

67.5 O
68.5 O
69.5 O
70.5 O
winrate B
accuracy(% O
) O
baseline+player O
info O
. O

role B
prop O
. O

( O
a O
) O
Winning O
team B
prediction O
. O

1 O
2 O
3 O
4 O
5 O
k O
- O
role B
prop O
. O

65.0 O
66.0 O
67.0 O
68.0 O
abusing O
accuracy(% O
) O
baseline+player O
info O
. O

role B
prop O
. O

( O
b O
) O
Abuse O
prediction O
. O

Figure O
10 O
: O
Effect B
of O
role B
combinations O
. O

It O
shows O
that O
modeling O
two O
- O
role B
combinations O
is O
sufficient O
to O
capture B
the O
effect B
of O
team B
composition I
. O

composition O
is O
further O
amplified O
when O
the O
rank B
gap I
being O
controlled B
: O
56.3 O
% O
vs O
53.3 O
% O
. O

The O
difference O
is O
the O
least O
prominent O
in O
the O
prediction O
of O
surrender B
, O
as O
this O
behavior O
is O
often O
highly O
influenced O
by O
teammates B
and O
various O
situations O
during O
games B
, O
while O
role B
properties O
still O
improve O
the O
performance O
by O
1.5 O
% O
in O
surrender B
prediction O
. O

Recall O
that O
Fig O
. O

3b O
shows O
a O
clear B
influence O
of O
rank B
level I
on O
winning B
rate I
. O

Therefore O
, O
to O
further O
understand O
the O
effect B
of O
team B
compositions I
, O
we O
report B
the O
performance O
on O
games B
where O
the O
rank B

 O
level I
gap O
δ O
= O
0 O
in O
Fig O
. O

9b O
. O

We O
find O
that O
after O
the O
rank B
level O
gap I
is O
controlled B
, O
this O
task O
becomes O
much O
more O
challenging O
and O
the O
performance O
of O
all O
feature O
sets O
drop B
significantly O
compared O
to O
Fig O
. O

9a O
. O

However O
, O
the O
difference O
between O
role B
properties O
and O
player B
information O
almost O
doubles O
( O
1.6 O
% O
vs. O
0.9 O
% O
) O
, O
demonstrating O
the O
effectiveness O
of O
incorporating O
team B
compositions I
, O
especially O
when O
the O
two O
teams B
are O
similarly O
skilled B
. O

In O
contrast O
, O
rank B
level I
does O
not O
affect O
the O
performance O
when O
predicting O
surrender B
or O
abuse O
. O

Modeling O
two O
- O
role B
combinations O
is O
sufficient O
to O
capture B
the O
effect B
of O
team B
composition I
( O
Fig O
. O

10 O
) O
. O

To O
further O
illustrate O
the O
effect B
of O
team B
compositions I
, O
we O
examine O
the O
performance O
of O
adding O
k O
- O
role B
combinations O
to O
player B
information O
and O
baseline O
one O
by O
one O
: O
we O
first O
add O
single O
- O
role B
features O
, O
then O
add O
features O
based B
on O
two O
- O
role B
combinations O
, O
until O
adding O
features O
based B
on O
entire O
team B

 O
compositions I
. O

Taking O
winning O
team B
( O
on O
all O
games B
) O
and O
abuse O
prediction O
as O
examples O
, O
Fig O
. O

10 O
shows O
that O
the O
performance O
improvement O
rank B
features O
importance(% O
) O
# O
7 O
minimum O
winrate B
of O
− O
1.1 O
# O
8 O
minimum O
winrate B
of O
− O
1.1 O
# O
9 O
minimum O
winrate B
of O
− O
1.0 O
# O
10 O
std(rank O
level B
) O
of I
1.0 I
# O
11 O
average O
rank B
level I
of O
0.9 O
Table O
4 O
: O
Top O
five O
role B
property O
features O
in O
winning O
team B
prediction O
. O

Feature O
importance O
is O
defined O
as O
the O
frequency O
of O
a O
feature O
being O
used O
in O
a O
tree O
. O

saturates O
after O
adding O
two O
- O
role B
combinations O
. O

It O
suggests O
that O
modeling O
single O
roles B
and O
two O
- O
role B
combinations O
provides O
most O
of O
the O
predictive O
power B
. O

Feature O
analysis O
. O

We O
finally O
explore O
the O
importance O
of O
features O
by O
using O
winning O
team B
prediction O
task O
as O
an O
example O
. O

When O
combining O
role B
properties O
and O
player B
information O
, O
average O
rank B
levels I
of O
two O
teams B
are O
the O
most O
important O
features O
( O
ranked B
top-2 O
) O
, O
as O
well O
as O
average O
individual O
winning O
probability O
( O
in O
top-5 O
) O
. O

To O
understand O
the O
key O
features O
in O
role B
properties O
, O
we O
show O
the O
top O
five O
features O
in O
role B
properties O
in O
Table O
4 O
. O

Consistent O
with O
the O
characteristics O
of O
roles B
where O
mage B
( O
) O
, O
marksman O
( O
) O
and O
warrior B
( O
) O
are O
most O
commonly O
used O
, O
the O
combinations O
of O
these O
roles B
are O
more O
important O
in O
role B
properties O
. O

And O
as O
marksman O
( O
) O
and O
mage B
( O
) O
are O
the O
key O
roles B
to O
cause O
damages B
, O
role B
combinations O
related O
with O
these O
roles B
are O
particularly O
significant O
. O

The O
first O
three O
of O
the O
top O
five O
are O
all O
two O
- O
role B
features O
, O
which O
again O
indicates O
that O
modeling O
two O
- O
role B
combinations O
is O
effective O
to O
capture B
the O
team B
performance I
. O

7 O
RELATED O
WORK O
In O
addition O
to O
studies O
mentioned O
throughout O
the O
paper O
, O
we O
discuss O
additional O
related O
work O
in O
three O
strands O
. O

Team O
formation O
. O

Researchers O
from O
the O
data O
mining O
community B
have O
formulated O
the O
problem O
of O
team B
formulation O
as O
a O
constrained O
optimization O
problem O
based B
on O
each O
individual B
’s O
skills I
and I
their O
social O
networks O
[ O
2 O
, O
3 O
, O
30 O
, O
31 O
] O
. O

For O
instance O
, O
Anagnostopoulos O
et O
al O
. O

[ O
3 O
] O
take O
into O
account O
of O
the O
task O
requirements O
, O
individual O
workloads O
, O
and O
team B
communication B
costs B
. O

Li O
et O
al O
. O

[ O
31 O
] O
study O
a O
family O
of O
problems O
in O
team B
enhancement O
including O
team B
member I
replacement O
, O
team B
expansion O
, O
and O
team B
shrinkage O
. O

Social O
/ O
team B
roles I
. O

Digitalization O
of O
human O
traces O
have O
increasingly O
made O
implicit O
social O
/ O
team B
roles I
explicit O
and O
enabled O
largescale O
study O
on O
theories O
of O
social O
/ O
team B
roles I
[ O
6–8 O
, O
47 O
] O
. O

Yang O
et O
al O
. O

[ O
49 O
] O
study O
how O
social O
roles B
influence O
the O
process O
of O
online O
information O
diffusion O
. O

Danescu O
- O
Niculescu O
- O
Mizil O
et O
al O
. O

[ O
16 O
] O
demonstrate O
changes O
in O
language O
use O
after O
a O
Wikipedia O
user B
becomes O
an O
administrator O
. O

Yang O
et O
al O
. O

[ O
47 O
] O
infer O
editor O
roles B
on O
Wikipedia O
based B
on O
behavior O
traces O
. O

In O
the O
context O
of O
gaming O
, O
Stetina O
et O
al O
. O

[ O
42 O
] O
examined O
problematic O
gaming O
behavior O
and O
depressive O
tendencies O
among O
people O
who O
play B
different O
types O
of O
online B
- O
games I
. O

Online O
gaming O
. O

Many O
researchers O
have O
recognized O
that O
online O
gaming O
, O
especially O
MOBA O
and O
MMOGs O
( O
Massively O
Multiplayer O
Online O
Games O
) O
, O
can O
serve O
as O
a O
platform O
for O
studying O
individual O
and O
team B
behavior O
, O
group B
norms O
, O
and O
communities B
[ O
20 O
, O
27 O
] O
. O

In O
such O
games B
, O
players B
not O
only O
develop O
individual B
skills I
, O
but O
also O
coordinate O
and O
communicate O
with O
others O
. O

The O
existence O
, O
significance O
, O
and O
influence O
of O
such O
social O
interactions O
have O
been O
examined O
in O
many O
studies O
[ O
19 O
, O
22 O
, O
24 O
, O
32 O
, O
44 O
] O
. O

Team O
performance O
is O
the O
most O
heavily O
studied O
topic O
among O
our O
three O
measures O
of O
effectiveness O
. O

Pobiedina O
et O
al O
. O

[ O
37 O
, O
38 O
] O
explore O
factors O
that O
influence O
player B
’s O
performance O
in O
Dota O
2 O
and O
reveal O
that O
team B
- O
level B
interactions O
, O
especially O
the O
number O
of O
friends O
in O
a O
team B
, O
relate O
to O
team B
performance I
. O

Similar O
to O
our O
work O
, O
prior O
studies O
such O
as O
[ O
1 O
, O
15 O
] O
demonstrate O
that O
different O
role B
combinations O
may O
greatly O
influence O
team B
performance I
. O

Deep O
learning O
based B
recommendation O
systems O
are O
also O
built O
to O
recommend O
heroes B
or O
roles B
to O
players[40 O
] O
. O

Furthermore O
, O
gamification O
can O
also O
potentially O
play B
an O
important O
role B
in O
education O
and O
scientific O
discovery O
[ O
26 O
] O
. O

In O
particular O
, O
Petter O
[ O
36 O
] O
suggests O
that O
researchers O
should O
embrace O
the O
growing O
popularity O
of O
online O
gaming O
and O
seek O
opportunities O
in O
studying O
online O
gaming O
in O
the O
context O
of O
education O
, O
individual O
, O
and O
team B
behavior O
. O

In O
addition O
to O
these O
exciting O
opportunities O
, O
online O
gaming O
leads B
to O
issues O
such O
as O
addiction O
and O
depression O
[ O
28 O
, O
42 O
] O
. O

8 O
CONCLUSION O
In O
this O
paper O
, O
we O
study O
the O
effect B
of O
team B
composition I
in O
the O
largest O
multiplayer O
online O
battle B
arena O
( O
MOBA O
) O
game B
: O
Honor O
of O
Kings O
. O

We O
quantitatively O
show O
the O
varying O
effects B
of O
team B
composition I
on O
team B
performance I
( O
winning O
) O
, O
team B
tenacity O
( O
surrender B
) O
, O
and O
team B
rapport O
( O
abusive B
language I
use O
) O
: O
although O
diverse O
teams B
tend O
to O
perform O
well O
and O
show O
tenacity O
in O
adversity O
, O
they O
are O
more O
likely O
to O
abuse O
when O
losing O
. O

The O
double O
- O
edged O
influence O
of O
team B
diversity O
suggests O
the O
importance O
of O
balancing O
team B
composition I
. O

We O
also O
examine O
how O
team B
composition I
influences O
individual O
behavior O
in O
abusive B
language I
use O
. O

In O
addition O
to O
showing O
that O
a O
team B
is O
not O
the O
sum O
of O
independent O
individuals O
, O
we O
contribute O
to O
the O
“ O
situation O
vs. O
personality O
” O
debate O
and O
show O
that O
assassins B
abuse O
more O
because O
abusive O
players B
tend O
to O
choose O
assassins B
instead O
of O
players B
becoming O
abusive O
when O
choosing O
assassins B
. O

Our O
work O
suggests O
that O
the O
gaming O
environment O
may O
be O
improved O
by O
adjusting O
team B
matching O
and O
preventing O
players B
from O
using O
team B
compositions I
that O
correlate O
with O
increased B
abusive B
language I
use O
. O

Limitations O
. O

Although O
we O
control B
for O
roles B
in O
the O
analyses O
and O
further O
validate O
our O
results O
through O
prediction O
experiments O
, O
our O
study O
is O
observational O
and O
can O
be O
strengthened O
through O
experimental O
studies O
. O

We O
would O
like O
to O
highlight B
that O
experimental O
studies O
on O
team B
compositions I
are O
non O
- O
trivial O
because O
of O
the O
great O
number O
of O
possible O
combinations O
. O

Our O
work O
is O
also O
limited O
by O
the O
data O
that O
we O
have O
access O
to O
. O

Due O
to O
privacy O
issues O
, O
we O
do O
not O
have O
sensitive O
individual O
information O
like O
location B
and O
consumption O
record O
. O

Moreover O
, O
although O
Honor O
of O
Kings O
is O
the O
largest O
MOBA O
game B
, O
the O
selection O
bias O
in O
our O
data O
may O
limit O
the O
generalizability O
of O
our O
findings O
. O

Future O
directions O
. O

Several O
promising O
directions O
arise O
from O
our O
work O
. O

Our O
prediction O
experiments O
show O
that O
it O
is O
important O
to O
incorporate O
individual B
player I
information O
with O
role B
combinations O
. O

It O
remains O
challenging O
to O
develop O
a O
holistic O
model O
that O
captures B
the O
interaction O
between O
individual O
members O
, O
learns O
novel O
representations O
for O
role B
combinations O
, O
and O
makes O
even O
more O
accurate O
predictions O
. O

Furthermore O
, O
although O
MOBA O
games B
provide O
an O
ideal O
environment O
for O
understanding O
the O
effect B
of O
team B
composition I
, O
it O
is O
important O
to O
validate O
our O
findings O
in O
other O
scenarios O
, O
e.g. O
, O
with O
implicit O
roles B
beyond O
gaming O
. O

9 O
REFERENCES O
[ O
1 O
] O
Atish O
Agarwala O
and O
Michael O
Pearce O
. O

2014 O
. O

Learning O
Dota O
2 O
team B
compositions I
. O

Technical O
Report O
. O

Technical O
report B
, O
Stanford O
University O
. O

[ O
2 O
] O
Aris O
Anagnostopoulos O
, O
Luca O
Becchetti O
, O
Carlos O
Castillo O
, O
Aristides O
Gionis O
, O
and O
Stefano O
Leonardi O
. O

2010 O
. O

Power B
in O
unity O
: O
forming O
teams B
in B
large O
- I
scale I
community B
systems O
. O

In O
CIKM O
. O

[ O
3 O
] O
Aris O
Anagnostopoulos O
, O
Luca O
Becchetti O
, O
Carlos O
Castillo O
, O
Aristides O
Gionis O
, O
and O
Stefano O
Leonardi O
. O

2012 O
. O

Online B
team I
formation I
in O
social O
networks O
. O

In O
WWW O
. O

[ O
4 O
] O
Rick O
Audas O
, O
Stephen O
Dobson O
, O
and O
John O
Goddard O
. O

2002 O
. O

The O
impact B
of O
managerial O
change O
on O
team B
performance I
in O
professional O
sports O
. O

Journal O
of O
Economics O
and O
Business O
54 O
, O
6 O
( O
2002 O
) O
, O
633–650 O
. O

[ O
5 O
] O
J O
Robert O
Baum O
and O
Edwin O
A O
Locke O
. O

2004 O
. O

The O
relationship O
of O
entrepreneurial O
traits O
, O
skill B
, O
and O
motivation O
to O
subsequent O
venture O
growth O
. O

Journal O
of O
applied O
psychology O
89 O
, O
4 O
( O
2004 O
) O
, O
587 O
. O

[ O
6 O
] O
R O
Meredith O
Belbin O
. O

2011 O
. O

Management O
teams B
: O
Why O
they O
succeed O
or O
fail O
. O

Human O
Resource O
Management O
International O
Digest O
19 O
, O
3 O
( O
2011 O
) O
. O

[ O
7 O
] O
R O
Meredith O
Belbin O
. O

2012 O
. O

Team B
roles I
at O
work O
. O

Routledge O
. O

[ O
8 O
] O
Bruce O
J O
Biddle O
. O

1986 O
. O

Recent O
developments O
in O
role B
theory O
. O

Annual O
review O
of O
sociology O
12 O
, O
1 O
( O
1986 O
) O
, O
67–92 O
. O

[ O
9 O
] O
Erin O
Bradner O
, O
Gloria O
Mark O
, O
and O
Tammie O
D O
Hertel O
. O

2005 O
. O

Team B
size O
and O
technology O
fit O
: O
Participation O
, O
awareness O
, O
and O
rapport O
in O
distributed O
teams B
. O

IEEE O
Transactions O
on O
Professional O
Communication O
48 O
, O
1 O
( O
2005 O
) O
, O
68–77 O
. O

[ O
10 O
] O
Michael O
T. O
Brannick O
, O
Regina O
M. O
Roach O
, O
and O
Eduardo O
Salas O
. O

1993 O
. O

Understanding O
Team O
Performance O
: O
A O
Multimethod O
Study O
. O

Human O
Performance O
6 O
, O
4 O
( O
1993 O
) O
, O
287 O
– O
308 O
. O

[ O
11 O
] O
Tianqi O
Chen O
and O
Carlos O
Guestrin O
. O

2016 O
. O

Xgboost O
: O
A O
scalable O
tree O
boosting B
system O
. O

In O
KDD O
. O

[ O
12 O
] O
Justin O
Cheng O
, O
Michael O
Bernstein O
, O
Cristian O
Danescu O
- O
Niculescu O
- O
Mizil O
, O
and O
Jure O
Leskovec O
. O

2017 O
. O

Anyone O
can O
become O
a O
troll B
: O
Causes O
of O
trolling B
behavior O
in O
online O
discussions O
. O

In O
CSCW O
. O

[ O
13 O
] O
Justin O
Cheng O
, O
Cristian O
Danescu O
- O
Niculescu O
- O
Mizil O
, O
and O
Jure O
Leskovec O
. O

2015 O
. O

Antisocial O
Behavior O
in O
Online O
Discussion O
Communities O
. O

In O
ICWSM O
. O

[ O
14 O
] O
Susan O
G. O
Cohen O
and O
Diane O
E. O
Bailey O
. O

1997 O
. O

What O
Makes O
Teams B
Work O
: O
Group O
Effectiveness O
Research O
from O
the O
Shop O
Floor O
to O
the O
Executive O
Suite O
. O

Journal O
of O
Management O
23 O
, O
3 O
( O
1997 O
) O
, O
239–290 O
. O

[ O
15 O
] O
Kevin O
Conley O
and O
Daniel O
Perry O
. O

2013 O
. O

How O
does O
he O
saw O
me O
? O
A O
recommendation O
engine O
for O
picking B
heroes B
in O
Dota O
2 O
. O

Np O
, O
nd O
Web O
7 O
( O
2013 O
) O
. O

[ O
16 O
] O
Cristian O
Danescu O
- O
Niculescu O
- O
Mizil O
, O
Lillian O
Lee O
, O
Bo O
Pang O
, O
and O
Jon O
Kleinberg O
. O

2012 O
. O

Echoes O
of O
Power B
: O
Language O
Effects O
and O
Power O
Differences O
in O
Social O
Interaction O
. O

In O
WWW O
. O

[ O
17 O
] O
Kurt O
T O
Dirks O
. O

1999 O
. O

The O
effects B
of O
interpersonal O
trust O
on O
work O
group B
performance O
. O

Journal O
of O
applied O
psychology O
84 O
, O
3 O
( O
1999 O
) O
, O
445 O
. O

[ O
18 O
] O
M. O
Brent O
Donellan O
, O
Richard O
E. O
Lucas O
, O
and O
William O
Fleeson O
( O
Eds O
. O

) O
. O

2009 O
. O

Personality O
and O
Assessment O
at O
Age O
40 O
: O
Reflections O
on O
the O
Past O
Person O
- O
Situation O
Debate O
and O
Emerging O
Directions O
of O
Future O
Person O
- O
Situation O
Integration O
[ O
Special O
Issue O
] O
. O

[ O
19 O
] O
Nicolas O
Ducheneaut O
, O
Nicholas O
Yee O
, O
Eric O
Nickell O
, O
and O
Robert O
J O
Moore O
. O

2006 O
. O

Alone O
together O
? O
: O
exploring O
the O
social O
dynamics O
of O
massively O
multiplayer O
online B
games I
. O

In O
Proceedings O
of O
the O
SIGCHI O
conference O
on O
Human O
Factors O
in O
computing O
systems O
. O

ACM O
, O
407–416 O
. O

[ O
20 O
] O
Nicolas O
Ducheneaut O
, O
Nicholas O
Yee O
, O
Eric O
Nickell O
, O
and O
Robert O
J. O
Moore O
. O

2007 O
. O

The O
Life O
and O
Death O
of O
Online O
Gaming O
Communities O
. O

In O
CHI O
. O

[ O
21 O
] O
Angela O
L O
Duckworth O
, O
Christopher O
Peterson O
, O
Michael O
D O
Matthews O
, O
and O
Dennis O
R O
Kelly O
. O

2007 O
. O

Grit O
: O
perseverance O
and O
passion O
for O
long O
- O
term O
goals B
. O

Journal O
of O
personality O
and O
social O
psychology O
92 O
, O
6 O
( O
2007 O
) O
, O
1087 O
. O

[ O
22 O
] O
Guo O
Freeman O
. O

2018 O
. O

Multiplayer O
Online O
Games O
: O
Origins O
, O
Players O
, O
and O
Social O
Dynamics O
. O

AK O
Peters O
/ O
CRC O
Press O
. O

[ O
23 O
] O
Courtenay O
Honeycutt O
. O

2005 O
. O

Hazing O
as O
a O
process O
of O
boundary O
maintenance O
in O
an O
online B
community I
. O

Journal O
of O
computer O
- O
mediated O
communication B
10 O
, O
2 O
( O
2005 O
) O
, O
JCMC1021 O
. O

[ O
24 O
] O
Matthew O
Hudson O
and O
Paul O
Cairns O
. O

2014 O
. O

Measuring O
social O
presence O
in O
teambased O
digital O
games B
. O

Interacting O
with O
Presence O
: O
HCI O
and O
the O
Sense O
of O
Presence O
in O
Computer O
- O
mediated O
Environments O
( O
2014 O
) O
, O
83 O
. O

[ O
25 O
] O
Douglas O
T O
Kenrick O
and O
David O
C O
Funder O
. O

1988 O
. O

Profiting O
from O
controversy O
: O
Lessons O
from O
the O
person O
- O
situation O
debate O
. O

American O
Psychologist O
43 O
, O
1 O
( O
1988 O
) O
, O
23 O
. O

[ O
26 O
] O
Firas O
Khatib O
, O
Seth O
Cooper O
, O
Michael O
D O
Tyka O
, O
Kefan O
Xu O
, O
Ilya O
Makedon O
, O
Zoran O
Popović O
, O
and O
David O
Baker O
. O

2011 O
. O

Algorithm O
discovery O
by O
protein O
folding O
game B
players B
. O

the O
National O
Academy O
of O
Sciences O
108 O
, O
47 O
( O
2011 O
) O
, O
18949–18953 O
. O

[ O
27 O
] O
Young O
Ji O
Kim O
, O
David O
Engel O
, O
Anita O
Williams O
Woolley O
, O
Jeffrey O
Yu O
- O
Ting O
Lin O
, O
Naomi O
McArthur O
, O
and O
Thomas O
W. O
Malone O
. O

2017 O
. O

What O
Makes O
a O
Strong O
Team B
? O
: O
Using O
Collective O
Intelligence O
to O
Predict O
Team O
Performance O
in O
League O
of O
Legends O
. O

In O
CSCW O
. O

[ O
28 O
] O
Daria O
J O
Kuss O
, O
Jorik O
Louws O
, O
and O
Reinout O
W O
Wiers O
. O

2012 O
. O

Online O
gaming O
addiction O
? O
Motives O
predict O
addictive O
play B
behavior O
in O
massively O
multiplayer O
online B
roleplaying O
games I
. O

Cyberpsychology O
, O
Behavior O
, O
and O
Social O
Networking O
15 O
, O
9 O
( O
2012 O
) O
, O
480–485 O
. O

[ O
29 O
] O
Haewoon O
Kwak O
, O
Jeremy O
Blackburn O
, O
and O
Seungyeop O
Han O
. O

2015 O
. O

Exploring O
Cyberbullying O
and O
Other O
Toxic O
Behavior O
in O
Team O
Competition O
Online O
Games O
. O

In O
CHI O
. O

[ O
30 O
] O
Theodoros O
Lappas O
, O
Kun O
Liu O
, O
and O
Evimaria O
Terzi O
. O

2009 O
. O

Finding O
a O
team B
of O
experts O
in O
social O
networks O
. O

In O
KDD O
. O

[ O
31 O
] O
Liangyue O
Li O
, O
Hanghang O
Tong O
, O
Nan O
Cao O
, O
Kate O
Ehrlich O
, O
Yu O
- O
Ru O
Lin O
, O
and O
Norbou O
Buchler O
. O

2017 O
. O

Enhancing O
Team O
Composition O
in O
Professional O
Networks O
: O
Problem O
Definitions O
and O
Fast O
Solutions O
. O

IEEE O
Trans O
Knowl O
Data O
Eng O
29 O
, O
3 O
( O
2017 O
) O
, O
613–626 O
. O

[ O
32 O
] O
Alexandru O
Losup O
, O
Ruud O
Van O
De O
Bovenkamp O
, O
Siqi O
Shen O
, O
Adele O
Lu O
Jia O
, O
and O
Fernando O
Kuipers O
. O

2014 O
. O

Analyzing O
implicit O
social O
networks O
in O
multiplayer O
online B
games I
. O

IEEE O
Internet O
Computing O
3 O
( O
2014 O
) O
, O
36–44 O
. O

[ O
33 O
] O
Dwight O
R O
Norms O
and O
Robert O
E O
Niebuhr O
. O

1980 O
. O

Group B
variables O
and O
gaming B
success O
. O

Simulation O
& O
Games O
11 O
, O
3 O
( O
1980 O
) O
, O
301–312 O
. O

[ O
34 O
] O
Glenn O
M O
Parker O
. O

1990 O
. O

Team B
players B
and O
teamwork B
. O

Jossey O
- O
Bass O
San O
Francisco O
, O
CA O
. O

[ O
35 O
] O
David O
Partington O
and O
Hilary O
Harris O
. O

1999 O
. O

Team B
role I
balance O
and O
team B
performance I
: O
an O
empirical O
study O
. O

Journal O
of O
Management O
Development O
18 O
, O
8 O
( O
1999 O
) O
, O
694–705 O
. O

[ O
36 O
] O
Stacie O
Petter O
. O

2017 O
. O

More O
Than O
Child O
’s O
Play B
: O
Embracing O
the O
Study O
of O
Online O
Gaming O
in O
Information O
Systems O
Research O
. O

SIGMIS O
Database O
48 O
, O
4 O
( O
Nov. O
2017 O
) O
, O
9–13 O
. O

[ O
37 O
] O
Nataliia O
Pobiedina O
, O
Julia O
Neidhardt O
, O
Maria O
del O
Carmen O
Calatrava O
Moreno O
, O
and O
Hannes O
Werthner O
. O

2013 O
. O

Ranking B
factors O
of O
team B
success O
. O

In O
Proceedings O
of O
the O
22nd O
International O
Conference O
on O
World O
Wide O
Web O
. O

ACM O
, O
1185–1194 O
. O

[ O
38 O
] O
Nataliia O
Pobiedina O
, O
Julia O
Neidhardt O
, O
Maria O
del O
Carmen O
Calatrava O
Moreno O
, O
Laszlo O
Grad O
- O
Gyenge O
, O
and O
Hannes O
Werthner O
. O

2013 O
. O

On O
successful O
team B
formation I
: O
Statistical O
analysis O
of O
a O
multiplayer O
online B
game I
. O

In O
Business O
Informatics O
( O
CBI O
) O
, O
2013 O
IEEE O
15th O
Conference O
on O
. O

IEEE O
, O
55–62 O
. O

[ O
39 O
] O
Eduardo O
Salas O
, O
Nancy O
J O
Cooke O
, O
and O
Michael O
A O
Rosen O
. O

2008 O
. O

On O
teams B
, O
teamwork B
, O
and O
team B
performance I
: O
Discoveries O
and O
developments O
. O

Human O
factors O
50 O
, O
3 O
( O
2008 O
) O
, O
540–547 O
. O

[ O
40 O
] O
Anna O
Sapienza O
, O
Palash O
Goyal O
, O
and O
Emilio O
Ferrara O
. O

2018 O
. O

Deep O
Neural O
Networks O
for O
Optimal O
Team O
Composition O
. O

arXiv O
preprint O
arXiv:1805.03285 O
( O
2018 O
) O
. O

[ O
41 O
] O
Barbara O
Senior O
. O

1997 O
. O

Team B
roles I
and O
team B
performance I
: O
Is O
there O
’ O
really O
’ O
a O
link O
? O
Journal O
of O
Occupational O
and O
Organizational O
Psychology O
70 O
, O
3 O
( O
1997 O
) O
, O
241–258 O
. O

[ O
42 O
] O
Birgit O
U. O
Stetina O
, O
Oswald O
D. O
Kothgassner O
, O
Mario O
Lehenbauer O
, O
and O
Ilse O
KryspinExner O
. O

2011 O
. O

Beyond O
the O
fascination O
of O
online B
- O
games I
: O
Probing O
addictive O
behavior O
and O
depression O
in O
the O
world B
of O
online O
- O
gaming O
. O

Computers O
in O
Human O
Behavior O
27 O
, O
1 O
( O
2011 O
) O
, O
473 O
– O
479 O
. O

Current O
Research O
Topics O
in O
Cognitive O
Load O
Theory O
. O

[ O
43 O
] O
Greg O
L. O
Stewart O
, O
Ingrid O
S. O
Fulmer O
, O
and O
Murray O
R. O
Barrick O
. O

2005 O
. O

An O
exploration O
of O
member O
roles B
as O
a O
multilevel O
linking O
mechanism B
for O
individual O
traits O
and O
team B
outcomes O
. O

Personnel O
Psychology O
58 O
, O
2 O
( O
2005 O
) O
, O
343–365 O
. O

[ O
44 O
] O
Michael O
Szell O
and O
Stefan O
Thurner O
. O

2010 O
. O

Measuring O
social O
dynamics O
in O
a O
massive O
multiplayer O
online B
game I
. O

Social O
networks O
32 O
, O
4 O
( O
2010 O
) O
, O
313–329 O
. O

[ O
45 O
] O
Stefan O
Wuchty O
, O
Benjamin O
F O
Jones O
, O
and O
Brian O
Uzzi O
. O

2007 O
. O

The O
Increasing O
Dominance O
of O
Teams O
in O
Production O
of O
Knowledge O
. O

Science O
316 O
, O
5827 O
( O
2007 O
) O
, O
1036–1039 O
. O

[ O
46 O
] O
Bang O
Xia O
, O
Huiwen O
Wang O
, O
and O
Ronggang O
Zhou O
. O

2017 O
. O

What O
Contributes O
to O
Success O
in O
MOBA O
Games O
? O
An O
Empirical O
Study O
of O
Defense O
of O
the O
Ancients O
2 O
. O

Games O
and O
Culture O
( O
2017 O
) O
, O
1555412017710599 O
. O

[ O
47 O
] O
Diyi O
Yang O
, O
Aaron O
Halfaker O
, O
Robert O
Kraut O
, O
and O
Eduard O
Hovy O
. O

2016 O
. O

Who O
Did O
What O
: O
Editor O
Role O
Identification O
in O
Wikipedia O
. O

In O
ICWSM O
. O

[ O
48 O
] O
Pu O
Yang O
, O
Brent O
E O
Harrison O
, O
and O
David O
L O
Roberts O
. O

2014 O
. O

Identifying O
patterns O
in O
combat B
that O
are O
predictive O
of O
success O
in O
MOBA O
games B
.. O
In O
FDG O
. O

[ O
49 O
] O
Yang O
Yang O
, O
Jie O
Tang O
, O
Cane O
Wing O
- O
ki O
Leung O
, O
Yizhou O
Sun O
, O
Qicong O
Chen O
, O
Juanzi O
Li O
, O
and O
Qiang O
Yang O
. O

2015 O
. O

RAIN O
: O
Social O
Role O
- O
Aware O
Information O
Diffusion O
. O

In O
AAAI’15 O
. O

367–373 O
. O

10 O
Team O
compositions O
Abusing O
prob O
. O

Used O
frequency(% O
) O
team B
compositions I
with O
the O
highest O
abusing O
rates O
− O
− O
− O
− O
56.2 O
% O
2.2 O
× O
10−3 O
− O
− O
− O
− O
55.9 O
% O
2.6 O
× O
10−3 O
− O
− O
− O
− O
55.6 O
% O
7.2 O
× O
10−3 O
team B
compositions I
with O
the O
lowest O
abusing O
rates O
− O
− O
− O
− O
30.6 O
% O
1.7 O
× O
10−3 O
− O
− O
− O
− O
29.7 O
% O
1.5 O
× O
10−5 O
− O
− O
− O
− O
28.7 O
% O
8.6 O
× O
10−5 O
Table O
7 O
: O
Team B
compositions I
with O
the O
highest O
and O
lowest O
abusing O
probability O
. O

A O
APPENDIX O
A.1 O
Game O
information O
Table O
5 O
shows O
the O
detailed O
information O
contained O
in O
the O
game B

 O
records I
. O

Table O
5 O
: O
The O
list O
of O
available O
information O
for O
each O
game B
. O

Tag O
Description O
Player O
’s O
basic O
information O
# O
grade O
- O
of O
- O
rank B
Player O
’s O
rank B
level I
, O
0 O
- O
26 O
. O

# O
heroid O
Hero O
that O
player B
uses O
in O
this O
game B
. O

Game O
’s O
basic O
information O
# O
game B
- O
time I
UTC O
timestamp O
, O
game B
start O
time I
. O

# O
duration O
Time O
duration O
of O
the O
game B
. O

# O
gameresult O
Win O
/ O
lose O
result O
for O
the O
player B
. O

# O
surrender B
- O
tag O
The O
tag O
of O
whether O
the O
game B
ends O
with O
surrendering O
. O

In B
- I
game I
information O
# O
kill B
- O
cnt O
Number O
of O
kills B
. O

# O
dead O
- O
cnt O
Number O
of O
deaths B
. O

# O
assist B
- O
cnt O
Number O
of O
assistance O
kills B
. O

coin O
- O
cnt O
Amount O
of O
coins O
. O

Player O
’s O
historical O
records O
rank B
- O
mvp O
Number O
of O
mvp O
he O
/ O
she O
got O
in O
his O
/ O
her O
last O
10 O
ranked B
games B
. O

# O
ten O
- O
rank B
- O
win O
The O
number O
of O
winning O
in O
his O
/ O
her O
last O
10 O
ranked B
games B
. O

# O
ten O
- O
rank B
- O
total O
Total O
number O
of O
previous O
10 O
ranked B
games B
. O

Chat O
information(if O
exists O
) O
# O
timestamp O
UTC O
timestamp O
, O
the O
time B
of O
one O
chat B
record O
. O

# O
abuse O
- O
tag O
The O
tag O
of O
whether O
this O
chat B
record O
includes O
abusing O
words O
. O

A.2 O
Detailed O
Statistics O
We O
present O
team B
compositions I
with O
the O
highest O
( O
lowest O
) O
surrendering B
probability O
in O
Table O
6 O
, O
and O
those O
with O
the O
highest O
( O
lowest O
) O
abusing O
probability O
in O
Table O
7 O
. O

Team O
compositions O
Surrender O
prob O
. O

Used O
frequency(% O
) O
team B
compositions I
with O
the O
highest O
surrendering O
rates O
− O
− O
− O
− O
84.6 O
% O
3.0 O
× O
10−4 O
− O
− O
− O
− O
82.4 O
% O
6.6 O
× O
10−4 O
− O
− O
− O
− O
79.8 O
% O
9.7 O
× O
10−5 O
team B
compositions I
with O
the O
lowest O
surrendering O
rates O
− O
− O
− O
− O
34.8 O
% O
4.6 O
× O
10−4 O
− O
− O
− O
− O
34.3 O
% O
2.1 O
× O
10−5 O
− O
− O
− O
− O
33.6 O
% O
1.5 O
× O
10−4 O
Table O
6 O
: O
Team B
compositions I
with O
the O
highest O
and O
lowest O
surrender B
probability O
( O
t O
≤ O
11min O
) O
. O

11 O
Personality O
and O
Behavior O
in B
Role O
- I
based I
Online O
Games O
Zhao O
Wang1 O
, O
Anna O
Sapienza2 O
, O
Aron O
Culotta1 O
, O
Emilio O
Ferrara2 O
1Department O
of O
Computer O
Science O
, O
Illinois O
Institute O
of O
Technology O
, O
Chicago O
, O
USA O
2 O
Information O
Sciences O
Institute O
, O
University O
of O
Southern O
California O
, O
Los O
Angeles O
, O
USA O
{ O
zwang185 O
, O
aculotta}@hawk.iit.edu O
, O
{ O
annas O
, O
ferrarae}@isi.edu O
Abstract O
— O
Both O
offline B
and O
online O
human O
behaviors O
are O
affected O
by O
personality O
. O

Of O
special O
interests O
are O
online B
games I
, O
where O
players B
have O
to O
impersonate O
specific B
roles I
and O
their O
behaviors O
are O
extensively O
tracked O
by O
the O
game B
. O

In O
this O
paper O
, O
we O
propose O
to O
study O
the O
relationship O
between O
players B
’ O
personality O
and O
game B
behavior O
in O
League O
of O
Legends O
( O
LoL O
) O
, O
one O
of O
the O
most O
popular O
Multiplayer O
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
. O

We O
use O
linear O
mixed O
effects B
( O
LME O
) O
models O
to O
describe O
relationships O
between O
players B
’ O
personality O
traits O
( O
measured O
by O
the O
Five O
Factor O
Model O
) O
and O
two O
major O
aspects O
of O
the O
game B
: O
the O
impersonated O
roles B
and O
in B
- I
game I
actions O
. O

On O
the O
one O
hand O
, O
we O
study O
relationships O
within O
the O
game B
environment O
by O
modeling O
role B
attributes I
from O
match B
behaviors O
and O
vice O
versa O
. O

On O
the O
other O
hand O
, O
we O
analyze O
the O
relationship O
between O
a O
player B
’s O
five O
personality O
traits O
and O
their O
game B
behavior O
by O
showing O
significant O
correlations O
between O
each O
personality O
trait O
and O
the O
set O
of O
corresponding O
behaviors O
. O

Our O
findings O
suggest O
that O
personality O
and O
behavior O
are O
highly O
entangled O
and O
provide O
a O
new O
perspective O
to O
understand O
how O
personality O
can O
affect O
behavior O
in B
role O
- I
based I
online B
games I
. O

Index O
Terms O
— O
personality O
, O
game B
behavior O
, O
online O
role B
- O
based B
I. O
INTRODUCTION O
Numerous O
studies O
from O
cognitive O
science O
and O
social O
science O
have O
shown O
strong O
connections B
between O
personality O
and O
human O
behavior O
[ O
1]–[3 O
] O
. O

As O
an O
important O
part O
of O
human O
activity O
, O
digital O
footprints O
are O
found O
to O
be O
closely O
related O
to O
user B
personality O
. O

For O
instance O
, O
extroverts O
and O
emotionally O
stable O
people O
are O
shown O
to O
be O
popular O
and O
influential O
on O
Twitter O
[ O
4 O
] O
. O

Meanwhile O
, O
personality O
can O
be O
successfully O
inferred O
from O
usergenerated O
contents O
in O
social O
media O
( O
e.g. O
, O
Facebook O
, O
Twitter O
, O
and O
YouTube O
) O
[ O
5 O
] O
. O

As O
a O
modern O
entertainment O
, O
online B
games I
are O
becoming O
increasingly O
popular O
, O
with O
an O
average O
time B
spent O
in O
playing O
of O
more O
than O
20 O
hours O
per O
week O
[ O
2 O
] O
. O

Given O
the O
popularity O
achieved O
, O
these O
games B
constitute O
a O
desirable O
source O
of O
data O
, O
providing O
valuable O
opportunities O
for O
researchers O
to O
investigate O
potential O
connections B
between O
personality O
and O
behavior O
. O

Of O
special O
interests O
are O
Multi O
- O
players B
Online O
Battle O
Arena O
( O
MOBA O
) O
games B
, O
which O
include O
several O
social O
aspects O
that O
could O
be O
influenced O
by O
personality O
, O
such O
as O
impersonating O
a O
role B
and O
cooperating O
with O
teammates B
. O

Here O
, O
we O
study O
one O
of O
the O
most O
popular O
MOBA O
games B
: O
League O
of O
Legends O
( O
LoL B
) O
. O

In O
LoL B
game B
scenario O
, O
each O
player B
This O
work O
is O
partly O
supported B
by O
DARPA O
( O
grant O
# O
D16AP00115 O
) O
. O

It O
does O
not O
necessarily O
reflect O
the O
position O
/ O
policy O
of O
the O
Government O
; O
no O
official O
endorsement O
should O
be O
inferred O
. O

Approved O
for O
public O
release O
; O
unlimited O
distribution O
. O

selects O
a O
specific B
role I
by O
impersonating O
a O
champion B
, O
i.e. O
a O
character B
of O
the O
game B
, O
to O
conduct O
a O
series O
of O
actions O
during O
the O
match B
. O

Each O
role B
is O
designed O
from O
the O
prototype O
of O
a O
champion B
in O
the O
fairy O
tale O
and O
characterized O
by O
a O
set O
of O
unique O
abilities B
[ O
6 O
] O
. O

As O
humans O
have O
personalities O
that O
describe O
their O
stable O
behavioral O
patterns O
, O
champions B
also O
have O
attributes O
that O
characterize O
their O
abilities B
in O
the O
game B
scenario O
. O

Players B

 O
with I
different I
personalities I
would O
have O
different O
preferences O
for O
specific B
roles I
, O
which O
subsequently O
lead B
to O
different O
game B
behaviors O
. O

Numerous O
researches O
have O
been O
conducted O
to O
investigate O
the O
relationship O
between O
personality O
and O
game B
behavior O
by O
profiling O
players B
’ O
personality O
through O
their O
actions O
in O
the O
game B
[ O
7 O
] O
, O
by O
conducting O
surveys O
to O
track O
both O
personality O
and O
selfreported O
in B
- I
game I
behaviors O
[ O
2 O
] O
, O
and O
by O
taking O
demographic O
effects B
( O
e.g. O
, O
gender O
and O
age O
) O
into O
account O
[ O
8]–[10 O
] O
. O

However O
, O
these O
works O
do O
not O
consider O
the O
different O
roles B
chosen O
by O
players B
, O
and O
often O
rely O
on O
self O
- O
reported B
game B
actions O
, O
which O
could O
lead B
to O
biases O
and O
inaccuracies O
in O
the O
data O
. O

By O
extending O
the O
previous O
research O
in O
this O
field B
, O
we O
aim O
at O
both O
studying O
how O
players B
’ O
personalities O
are O
related O
to O
their O
preferences O
in O
selecting O
game B
characters B
and O
understanding O
how O
personalities O
influence O
game B
behavior O
. O

We O
first O
conduct O
surveys O
to O
collect O
the O
BIG-5 O
personality O
traits O
of O
players B
, O
as O
discussed O
in O
Section O
III O
. O

Then O
, O
we O
combine O
data O
of O
players B
’ O
impersonated O
characters B
and O
actions O
provided O
by O
the O
official O
Riot O
Games O
API.1 O
Our O
main O
goal B
is O
to O
understand O
: O
first O
, O
how O
a O
player B
’s O
impersonated O
role B
and O
their O
match B
behavior O
are O
related O
; O
and O
second O
, O
whether O
knowing O
a O
player B
’s O
role I
and O
match B
behavior O
helps O
to O
understand O
their O
personality O
. O

To O
this O
aim O
, O
we O
apply O
Linear O
Mixed O
Effects O
( O
LME O
) O
models O
( O
Section O
IV O
) O
to O
find O
the O
underlying O
relations O
between O
LoL O
champions B
( O
i.e. O
game B
characters B
) O
and O
the O
actions O
performed O
during O
the O
match B
( O
Section O
V O
) O
. O

In O
Section O
VI O
, O
we O
analyze O
players B
’ O
personality O
traits O
according O
to O
the O
attributes O
of O
impersonated O
champions B
and O
match B
behaviors O
. O

Finally O
, O
in O
Section O
VII O
, O
we O
report B
the O
conclusions O
, O
limitations O
, O
and O
future O
work O
. O

II O
. O

RELATED O
WORK O
Previous O
studies O
focused B
on O
exploring O
the O
relationship O
between O
personality O
and O
game B
behaviors O
. O

In O
[ O
2 O
] O
, O
the O
authors O
conduct O
a O
study O
based B
on O
205 O
players B
of O
World O
of O
Warcraft O
and O
1https://developer.riotgames.com/ O
978 O
- O
1 O
- O
7281 O
- O
1884 O
- O
0/19/$31.00 O
© O
2019 O
IEEE O
arXiv:1905.08418v1 O
[ O
cs O
. O

HC O
] O
21 O
May O
2019 O
find O
that O
personality O
traits O
strongly O
influence O
game B
behaviors O
. O

However O
, O
this O
work O
collects O
game B
behaviors O
through O
surveys O
, O
which O
might O
suffer O
from O
inadequacy O
and O
reliability O
. O

Authors O
in O
[ O
7 O
] O
develop O
a O
framework O
to O
automatically O
collect O
playing B
behaviors O
in O
Neverwinter O
Nights O
, O
and O
then O
use O
regression O
analysis O
to O
profile O
44 O
players B
through O
the O
collected O
behaviors O
. O

Results O
suggest O
significant O
correlations O
between O
personality O
and O
behavior O
in B
role O
- I
playing I
game B
, O
nevertheless O
, O
this O
work O
does O
not O
take O
into O
account O
possible O
effects B
caused O
by O
different O
roles B
. O

Additionally O
, O
players B
with O
different O
personalities O
are O
shown O
to O
differ O
in O
: O
motivations O
for O
playing B
[ O
11 O
] O
, O
preference O
of O
game B
genres O
[ O
12 O
] O
, O
selection O
of O
game B
characters B
[ O
13 O
] O
, O
and O
engagement O
of O
match B
behaviors O
[ O
14 O
] O
. O

Demographic O
variables O
( O
e.g. O
, O
gender O
, O
age O
) O
were O
also O
found O
to O
influence O
these O
aspects O
of O
the O
game B
[ O
8 O
] O
. O

A O
great O
amount O
of O
work O
has O
been O
also O
devoted O
to O
study O
the O
connections B
between O
personality O
and O
offline B
behavior O
. O

However O
, O
it O
is O
not O
clear B
how O
to O
transfer O
the O
related O
findings O
to O
have O
better O
insights O
into O
online O
behavior O
. O

On O
the O
one O
hand O
, O
some O
studies O
found O
consistencies O
between O
personality O
and O
offline B
behaviors O
as O
well O
as O
personality O
and O
online O
behaviors O
[ O
15]–[18 O
] O
. O

For O
instance O
, O
Agreeableness O
and O
Extraversion O
are O
shown O
to O
be O
connected O
to O
playing O
motivations O
. O

On O
the O
other O
hand O
, O
there O
are O
also O
works O
reporting B
inconsistencies O
between O
personality O
traits O
and O
behaviors O
, O
such O
as O
inconsistency O
of O
Conscientiousness O
and O
motivations O
in O
[ O
19 O
] O
. O

In O
the O
present O
work O
, O
we O
aim O
at O
shedding O
light O
on O
the O
underlying O
connections B
between O
personality O
and O
game B
behavior O
by O
taking O
big-5 O
personality O
traits O
, O
role B
attributes I
, O
actions O
during O
matches B
, O
and O
demographic O
factors O
all O
into O
account O
. O

III O
. O

DATA O
In O
this O
paper O
, O
we O
study O
the O
relationship O
between O
personality O
and O
game B
behavior O
of O
League O
of O
Legends O
( O
LoL B
) O
players I
. O

We O
first O
collect O
personality O
traits O
data O
through O
a O
survey O
containing O
personality O
test O
and O
demographic O
questions.2 O
Then O
, O
for O
each O
player B
participating O
in O
the O
survey O
, O
we O
collect O
their O
game B
information O
( O
e.g. O
, O
selected O
characters B
and O
match B
history O
) O
via O
the O
official O
Riot O
Games O
API.1 O
A. O
League O
of O
Legends O
( O
LoL B
) O
As O
one O
of O
the O
most O
popular O
MOBA O
games B
, O
LoL O
was O
first O
released O
in O
2009 O
and O
updated B
for O
nine O
seasons B
by O
the O
end O
of O
2018 O
[ O
21 O
] O
. O

It O
attracts O
millions O
of O
players B
of O
all O
ages O
, O
nationalities O
, O
and O
occupations O
. O

What O
’s O
more O
, O
LoL O
is O
reported B
to O
have O
over O
80 O
million O
active B
players I
per O
month O
and O
over O
27 O
million O
players B
every O
single O
day O
[ O
22 O
] O
. O

LoL O
offers O
a O
variety O
of O
options O
allowing O
players B
to O
interact O
with O
each O
other O
in O
a O
virtual O
environment O
full B
of O
activities O
and O
diverse O
game B
modes I
[ O
23 O
] O
. O

There O
are O
141 O
unique O
characters B
( O
i.e. O
champions B
in O
LoL O
) O
, O
each O
defined O
by O
a O
set O
of O
special B

 O
abilities I
. O

We O
summarize O
these O
abilities B
into O
six O
main O
attributes O
to O
characterize O
each O
champion:3 O
2An O
anonymized O
version O
of O
the O
dataset O
used O
in O
this O
study O
is O
available O
upon O
request O
to O
the O
contact O
author O
. O

3 O
Champions O
and O
attributes O
definitions O
( O
copy O
the O
link O
into O
your O
browser O
) O
: O
https://leagueoflegends.fandom.com/wiki/List O
of O
champions B
/ O
Ratings O
TABLE O
I O
MEAN O
AND O
STANDARD O
DEVIATION O
FOR O
BIG-5 O
PERSONALTY O
TRAITS O
OF O
811 O
VALID O
PARTICIPANTS O
Agr O
Con O
Emo O
Ext O
Ope O
Mean O
4.42 O
4.70 O
4.60 O
3.53 O
4.95 O
Std O
1.24 O
1.31 O
1.46 O
1.52 O
1.20 O
aWe O
use O
first O
3 O
letters O
to O
represent O
trait O
names O
. O

• O
Damage B
: O
“ O
ability B
to O
deal B
damage I
” O
. O

• O
Toughness O
: O
“ O
ability B
to O
survive O
being O
focused B
” O
. O

• O
Mobility B
: O
“ O
ability B
to O
move B
quickly O
, O
blink O
or O
dash B
” O
. O

• O
Utility B
: O
“ O
ability B
to O
grant O
beneficial O
effects B
on O
allies B
, O
or O
provide O
vision O
” O
. O

• O
Control B
: O
“ O
ability B
to O
disable O
enemies B
” O
. O

• O
Difficulty O
: O
“ O
a O
champion B
’s O
mechanical O
difficulty O
” O
. O

Each O
attribute O
has O
a O
rating O
that O
ranges B
from O
0 O
to O
3 O
, O
with O
0 O
representing O
the O
weakest B
level B
and O
3 O
representing O
the O
strongest O
. O

Fig.1 O
shows O
examples O
of O
five O
different O
champions B
’ O
attributes I
. O

A O
single O
match B
in O
the O
Summoner O
’s O
Rift O
( O
i.e. O
the O
most O
popular O
game B
mode I
of O
LoL B
) O
requires O
two O
teams B
of O
five O
players B
that O
compete O
to O
destroy O
the O
enemy B
base I
, O
and O
each O
player B
controls B
one O
of O
the O
141 O
champions B
. O

Therefore O
, O
the O
champion B
selection O
and O
playing O
style O
are O
critical O
to O
the O
outcome O
of O
a O
match B
, O
both O
of O
which O
can O
be O
strongly O
influenced O
by O
players B
’ O
personalities O
. O

B. O
Survey O
We O
conducted O
a O
survey O
to O
collect O
players B
’ O
personality O
traits O
and O
demographic O
information O
. O

A O
sample O
of O
our O
survey O
is O
available O
online.4 O
The O
questionnaire O
contains O
three O
categories O
of O
questions O
: O
personality O
test O
, O
demographic O
questions O
, O
and O
player B
identification O
questions O
. O

Among O
various O
personality O
models O
, O
the O
BIG-5 O
framework O
has O
emerged O
as O
the O
most O
widely O
accepted O
model O
to O
study O
personality O
traits O
[ O
24]–[26 O
] O
. O

In O
our O
setting O
, O
we O
choose O
BIG5 O
Ten O
Item O
Personality O
Inventory O
( O
TIPI O
) O
as O
our O
personality O
questionnaire O
, O
which O
is O
brief O
but O
proven O
to O
be O
reliable O
[ O
24 O
] O
. O

We O
obtain O
the O
TIPI O
questionnaire O
from O
PsyToolkit:5 O
a O
free B
survey O
library O
that O
provides O
cognitive O
- O
psychological O
tests O
[ O
27 O
] O
, O
[ O
28 O
] O
. O

Each O
of O
the O
five O
personality O
dimensions O
, O
i.e. O
agreeableness O
, O
conscientiousness O
, O
emotional O
stability,6 O
, O
extraversion O
, O
and O
openness O
is O
measured O
according O
to O
two O
questions O
of O
the O
TIPI O
test O
[ O
29 O
] O
. O

Participants O
are O
asked O
to O
rate O
their O
answers O
to O
each O
question O
on O
a O
seven O
- O
point B
scale O
ranging B
from O
1 O
( O
i.e. O
strongly O
disagree O
) O
to O
7 O
( O
i.e. O
strongly O
agree O
) O
. O

TABLE O
I O
shows O
the O
mean O
and O
standard O
deviation O
for O
personality O
scores O
of O
valid O
participants O
. O

Fig.2 O
shows O
examples O
of O
five O
different O
players B
’ O
personality O
traits O
. O

Detailed O
definitions O
of O
the O
big-5 O
personality O
traits O
are O
provided O
in O
Section O
VI O
- O
B. O
To O
control B
for O
the O
potential O
effects B
that O
demographic O
factors O
have O
on O
personality O
[ O
2 O
] O
, O
[ O
8]–[10 O
] O
, O
we O
also O
ask O
for O
players B
’ O
demographic O
information O
( O
i.e. O
age O
, O
gender O
, O
region O
of O
the O
game B
) O
and O
translate O
the O
survey O
into O
several O
languages O
( O
i.e. O
Korean O
, O
Japanese O
, O
Turkish O
, O
Spanish O
, O
and O
Chinese O
) O
to O
reach O
out O
players B
of O
different O
nationalities O
. O

4https://www.psytoolkit.org/cgi-bin/psy2.5.1/survey?s=LUPWZ O
5https://www.psytoolkit.org/survey-library/big5-tipi.html O
6Also O
known O
as O
an O
antonym O
of O
neuroticism O
[ O
24 O
] O
. O

Fig O
. O

1 O
. O

Example O
of O
radar O
plots O
for O
five O
different O
champions B
( O
i.e. O
, O
Alistar O
, O
Aatrox O
, O
Akali O
, O
Ahri O
, O
Bard O
) O
, O
each O
defined O
by O
six O
attributes O
( O
we O
use O
the O
first O
three O
letters O
as O
abbreviations O
of O
attribute O
names O
) O
. O

We O
observe O
that O
every O
champion B
has O
its O
weaknesses B
and O
strengths B
regarding O
the O
six O
attributes O
and O
no O
champion B
is O
designed O
to O
be O
perfect O
[ O
20 O
] O
, O
so O
that O
champions B
in O
a O
team B
need O
to O
cooperate O
to O
win O
. O

Fig O
. O

2 O
. O

Example O
of O
radar O
plots O
for O
big-5 O
personality O
traits O
of O
five O
different O
players B
( O
we I
use I
the O
first O
three O
letters O
as O
abbreviations O
of O
personality O
trait O
names O
) O
. O

Players B
’ O
big-5 O
personality O
traits O
are O
analogous O
to O
champion B
attributes I
: O
each O
player B
has O
its I
weaknesses I
and O
strengths B
regarding O
the O
five O
dimensions O
of O
personality O
traits O
. O

Players B
with O
different O
personalities O
will O
have O
different O
preferences O
for O
champions B
. O

Moreover O
, O
we O
ask O
the O
following O
identification O
questions O
: O
the O
summoner B
name7 O
( O
i.e. O
the O
unique O
identification O
name O
of O
each O
player B
) O
; O
three O
favorite O
champions8 O
( O
i.e. O
characters B
a O
player B
prefers O
to O
impersonate O
during O
the O
game B
) O
; O
and O
the O
highest O
champion B
level I
. O

These O
questions O
are O
used O
to O
verify O
that O
the O
participant O
is O
an O
actual O
LoL B
player I
. O

Finally O
, O
we O
also O
record O
players B
’ O
response B
time I
for O
each O
question O
. O

Specifically O
, O
for O
the O
10 O
personality O
questions O
, O
the O
mean O
response B
time I
is O
50s O
with O
a O
standard O
deviation O
of O
25s O
. O

We O
post O
the O
survey O
on O
several O
platforms O
, O
such O
as O
LoL O
forums,9 O
popular O
websites O
( O
e.g. O
, O
Reddit O
) O
, O
and O
Amazon O
Mechanical O
Turk O
( O
AMT O
) O
. O

However O
, O
despite O
the O
great O
interest O
shown O
by O
volunteers O
when O
the O
survey O
was O
released O
, O
the O
overall O
attention O
dramatically O
decayed O
after O
a O
few O
days O
. O

We O
then O
focused B
on O
AMT O
, O
where O
we O
boost B
participation O
by O
providing O
a O
0.5 O
$ O
reward B
for O
each O
valid O
response O
. O

We O
conducted O
the O
survey O
on O
AMT O
for O
three O
months O
( O
June O
to O
September O
2018 O
) O
. O

We O
use O
the O
following O
criteria O
to O
validate O
the O
authenticity O
of O
participants O
. O

First O
, O
we O
remove O
duplicate O
responses O
having O
the O
same O
summoner B
name O
or O
AMT O
worker O
ID O
. O

Second O
, O
we O
check O
if O
a O
participant O
’s O
answers O
to O
the O
identification O
questions O
match B
the O
information O
we O
get O
from O
the O
official O
API O
of O
LoL. O
Since O
the O
identification O
information O
is O
only O
visible O
to O
a O
player B
on O
their O
own O
account O
, O
we O
can O
identify O
false O
LoL B
players I
through O
this O
process O
. O

Finally O
, O
we O
require O
a O
participant O
to O
have O
played B
at O
least O
10 O
matches B
to O
filter O
possible O
biases O
introduced O
by O
players B
who O
only O
play B
the O
game B
a O
few O
times B
and O
have O
insufficient O
information O
to O
assess O
their O
game B
behavior O
. O

7https://na.leagueoflegends.com/en/game-info/summoners/ O
8https://na.leagueoflegends.com/en/game-info/champions/ O
9https://boards.na.leagueoflegends.com O
/ O
en/ O
TABLE O
II O
DEMOGRAPHIC O
STATISTICS O
OF O
811 O
VALID O
SURVEY O
PARTICIPANTS O
Gender O
Age O
Region O
M:682 O
13∼20 O
: O
195 O
NA1 O
( O
North O
America O
) O
: O
580 O
F:129 O
21∼30 O
: O
496 O
EUW1 O
( O
EU O
West O
) O
: O
107 O
31∼40 O
: O
100 O
others O
: O
124 O
> O
40 O
: O
20 O
aMinimum O
age O
of O
a O
LoL B
player I
is O
required O
to O
be O
1310 O
bRegions O
are O
defined O
by O
server B
locations B
( O
check O
full B
list O
of O
regions O
here11 O
) O
At O
the O
end O
of O
this O
procedure O
, O
we O
finally O
get O
811 O
valid O
responses O
out O
of O
2785 O
in O
total O
. O

TABLE O
II O
shows O
a O
summary O
of O
the O
demographic O
statistics O
of O
verified O
survey O
participants O
. O

Note O
that O
, O
since O
we O
are O
focusing B
on O
AMTs O
, O
the O
participants O
’ O
demographics O
are O
affected O
by O
AMT O
demographics O
. O

C. O
Game O
Behavior O
Data O
The O
Riot O
Games O
API O
provides O
easy B
access O
to O
LoL O
game B
data O
in O
a O
secure O
and O
reliable O
way O
. O

For O
the O
811 O
valid O
players B
, O
we I

 I
utilize O
the O
API O
to O
collect O
the O
record O
of O
champions B
a O
player B
has O
impersonated O
, O
the O
timeline O
of O
their O
matches B
, O
and O
the O
specific O
actions O
performed O
during O
each O
match B
( O
e.g. O
, O
the O
number O
of O
kills B
, O
deaths B
, O
and O
assists B
) O
. O

The O
statistics O
of O
collected O
champions B
and O
matches B
are O
shown O
in O
the O
histograms O
of O
Fig O
. O

3 O
. O

IV O
. O

METHODS O
In O
this O
section O
, O
we O
first O
explain O
the O
representative O
features O
of O
players B
. O

Then O
, O
we O
describe O
the O
technical O
background O
of O
linear O
mixed O
effects B
models O
and O
show O
how O
we O
apply O
them O
to O
explore O
potential O
relationships O
between O
personality O
and O
game B
behavior O
. O

10https://na.leagueoflegends.com/en/legal/termsofuse O
11https://leagueoflegends.fandom.com/wiki/Servers O
A. O
Features O
We O
extracted O
four O
sets O
of O
features O
from O
the O
collected O
data O
: O
demographic O
factors O
, O
big-5 O
personality O
traits O
, O
champion B
features O
, O
and O
match B
behaviors O
. O

Demographic O
factors O
: O
age O
, O
gender O
, O
and O
region O
. O

Big-5 O
personality O
traits O
: O
agreeableness O
, O
conscientiousness O
, O
emotional O
stability O
, O
extraversion O
, O
and O
openness O
. O

Champion O
features O
: O
we O
consider O
the O
following O
features O
to O
describe O
the O
champions B
impersonated O
by O
each O
player B
: O
• O
n O
champs O
: O
the O
total O
number O
of O
unique O
champions B
used O
by O
each O
player B
, O
as O
the O
horizontal O
axis O
shows O
in O
Fig.3 O
; O
• O
n O
level6 O
: O
the O
number O
of O
champions B
ranked B
no O
lower O
than O
level B
six O
( O
champion B
level I
is O
determined O
by O
the O
frequency O
and O
proficiency O
of O
its O
usage O
by O
a O
player B
) O
; O
This O
is O
a O
complementary O
feature O
of O
n O
champs O
, O
which O
helps O
to O
identify O
a O
player B
’s O
performance O
level B
and O
characterize O
the O
expertise O
of O
a O
player B
’s O
skill I
. O

• O
Average O
attribute O
ratings O
of O
a O
player B
’s O
three O
favorite B
champions I
( O
used O
to O
measure O
a O
player B
’s O
champion B
preference O
) O
; O
• O
Weighted O
attribute O
ratings O
of O
a O
player B
’s O
top O
three O
ranked B
champions B
( O
weight O
by O
the O
number O
of O
matches B
being O
played B
with O
each O
specific B
champion I
) O
, O
which O
is O
used O
to O
measure O
a O
player B
’s O
champion B
skill B
. O

Match B
behavior O
: O
We O
select O
the O
actions O
of O
top O
three O
ranked B
champions B
of O
a O
player B
to O
represent O
their O
overall O
match B
behavior O
. O

The O
official O
API O
records O
more O
than O
100 O
behavioral O
features O
for O
each O
match B
, O
including O
kills B
, O
deaths B
, O
and O
assists B
( O
the O
complete O
set O
of O
features O
can O
be O
found O
at O
official O
API1 O
webpage O
) O
. O

However O
, O
some O
features O
are O
either O
not O
representative O
of O
a O
player B
’s O
game B
behavior O
( O
e.g. O
, O
visionScore O
) O
or O
correlated O
with O
other O
features O
( O
e.g. O
, O
kills O
, O
doubleKills B
, O
tripleKills B
, O
and O
quadraKills O
are O
highly O
correlated O
) O
, O
or O
not O
recorded O
for O
most O
players B
( O
e.g. O
, O
combatPlayerScore O
) O
. O

To O
focus B
on O
features O
that O
are O
most O
relevant O
to O
the O
playing O
behavior O
, O
we O
remove O
nonrepresentative O
features O
and O
highly O
correlated O
features O
, O
and O
only O
keep O
features O
that O
are O
recorded O
for O
most O
players B
. O

By O
doing O
so O
, O
we O
extract O
11 O
significant O
features O
to O
describe O
important O
aspects O
of O
the O
playing B
behavior O
: O
the O
total O
number O
of O
matches B
( O
n O
matches B
, O
as O
the O
vertical O
axis O
shows O
in O
Fig.3 O
) O
, O
number O
of O
kills B
, O
number O
of O
deaths B
, O
number O
of O
assists B
, O
winning B
rate I
, O
average O
match B
duration O
, O
gold B
earned O
, O
gold B
spent O
, O
total B
damage I
dealt O
, O
total B
damage I
taken O
, O
total O
heal O
. O

B. O
Linear O
Mixed O
Effects O
Models O
( O
LME O
) O
Previous O
studies O
have O
shown O
that O
both O
playing B
behaviors O
and O
personality O
traits O
are O
affected O
by O
demographics O
( O
e.g. O
, O
gender O
, O
age O
) O
[ O
2 O
] O
, O
[ O
9 O
] O
. O

For O
example O
, O
female O
players B
are O
more O
engaged O
in O
assisting B
behaviors O
than O
male O
players B
, O
and O
young O
players B
tend O
to O
be O
less O
emotionally O
stable O
than O
old O
ones O
[ O
14 O
] O
. O

However O
, O
mixing O
demographic O
effects B
with O
playing B
behaviors O
and O
personality O
traits O
would O
impede O
the O
identification O
of O
possible O
relationships O
between O
personality O
and O
game B
behavior O
. O

To O
deal O
with O
this O
issue O
, O
we O
apply O
LME O
models O
, O
which O
allow O
us O
to O
consider O
both O
fixed O
and O
random O
effects B
. O

Fixed O
effects B
refer O
to O
variations O
that O
could O
be O
explained O
by O
the O
independent O
variables O
( O
like O
in O
linear O
models O
) O
and O
random O
effects B
refer O
to O
variations O
that O
could O
not O
be O
explained O
by O
independent O
variables O
. O

By O
following O
the O
syntax O
provided O
in O
[ O
30 O
] O
, O
we O
can O
write O
the O
equations O
for O
mixed O
effects B
models O
as O
follows O
: O
outcome O
∼ O
1 O
+ O
fixed O
effects B
+ O
( O
random O
effects|group O
) O
( O
1 O
) O
where O
an O
outcome O
( O
dependent O
) O
variable O
is O
explained O
by O
using O
an O
intercept B
equal O
to O
1 O
, O
one O
or O
more O
fixed O
effects B
, O
and O
one O
or O
more O
random O
effects B
allowing O
for O
variations O
between O
groups B
( O
e.g. O
, O
gender O
group B
: O
male O
, O
female O
) O
. O

We O
use O
the O
R O
implementation O
of O
lme4 O
to O
perform O
our O
linear O
mixed O
effects B
analysis O
in O
all O
the O
experiments O
[ O
31 O
] O
, O
[ O
32 O
] O
. O

To O
model O
each O
of O
the O
three O
aspects O
: O
big-5 O
personality O
traits O
, O
champion B
attributes I
, O
and O
match B
behavioral O
features O
, O
we O
refer O
to O
the O
one O
being O
modeled O
as O
the O
outcome O
variable O
, O
while O
the O
remaining O
aspects O
are O
considered O
to O
be O
the O
candidate O
fixed O
effects B
( O
which O
are O
constant O
across O
demographic O
groups B
[ O
33 O
] O
) O
. O

Finally O
, O
we O
control B
for O
different O
demographic O
groups B
by O
considering O
them O
as O
random O
effects B
. O

To O
assess O
the O
relative O
fits O
of O
the O
models O
, O
on O
the O
one O
hand O
, O
we O
check O
their O
linearity O
, O
homoscedasticity O
, O
and O
normality O
of O
the O
residuals O
. O

On O
the O
other O
hand O
, O
we O
compute O
the O
Bayesian O
Information O
Criterion O
( O
BIC O
) O
[ O
34 O
] O
, O
which O
accounts O
for O
both O
the O
complexity O
and O
likelihood O
of O
a O
model O
. O

For O
each O
outcome O
variable O
, O
we O
start B
with O
a O
null O
hypothesis O
which O
only O
explains O
the O
outcome O
by O
the O
intercept B
and O
random O
effects B
of O
gender O
, O
age O
, O
and O
region O
. O

Then O
, O
we O
incrementally O
add O
fixed O
effects B
and O
select O
those O
that O
decrease O
the O
BIC O
score O
of O
the O
model O
. O

Finally O
, O
we O
use O
ANOVA O
tests O
to O
compare O
the O
differences O
between O
the O
null O
model O
and O
the O
one O
with O
the O
lowest O
BIC O
score O
. O

According O
to O
the O
interpretation O
table O
of O
BIC O
score O
differences O
in O
[ O
35 O
] O
, O
we O
then O
determine O
the O
significance O
of O
each O
model O
and O
report B
the O
one O
that O
is O
the O
most O
significant O
( O
i.e. O
the O
model O
whose O
BIC O
score O
difference O
with O
the O
null O
hypothesis O
model O
is O
greater O
than O
10 O
and O
has O
the O
lowest O
BIC O
score O
) O
. O

V. O
RELATIONSHIP O
BETWEEN O
ROLE B
AND O
MATCH O
BEHAVIOR O
Players B
choose O
champions B
to O
conduct O
a O
series O
of O
actions O
during O
the O
game B
and O
the O
champion B
’s O
abilities B
would O
directly O
affect O
players B
’ O
match B
behaviors O
[ O
36 O
] O
. O

We O
start B
by O
showing O
the O
relationship O
between O
champion B
usage O
and O
matches B
. O

Then O
, O
we O
model O
champion B
attributes I
from O
match B
behavior O
as O
well O
as O
infer O
specific O
match B
behavior O
according O
to O
champion B
attributes I
. O

A. O
Relationship O
between O
Champions O
and O
Matches O
To O
understand O
how O
the O
number O
of O
different O
champions B
used O
by O
players B
varies O
with I
the I
number I
of O
matches B
being O
played B
, O
we O
first O
compute O
the O
Spearman O
correlations O
between O
these O
two O
components O
and O
results O
show O
that O
they O
are O
strongly O
correlated O
( O
0.813 O
with O
p O
value O
< O
0.01 O
) O
. O

Fig O
. O

3 O
shows O
the O
scatter O
plot O
of O
the O
number O
of O
champions B
and O
matches B
for O
811 O
valid O
players B
. O

Histograms O
on O
two O
axes O
represent O
the O
density O
of O
champions B
and O
matches B
in O
each O
interval O
. O

We O
can O
observe O
that O
the O
number O
of O
different O
champions B
each O
player B
tries O
in O
their O
gaming O
history O
is O
positively O
correlated O
with O
the O
number O
of O
matches B
s O
/ O
he O
plays B
. O

Moreover O
, O
this O
relation O
TABLE O
III O
INFERRED O
CHAMPION B
ATTRIBUTES I
FROM O
MATCH O
BEHAVIOR O
Control B
Damage O
Difficulty O
Mobility O
Toughness O
Utility O
Random O
Effects O
gender O
X O
X O
age O
X O
X O
X O
X O
region O
X O
X O
X O
Fixed O
Effects O
kills B
-0.34 O
0.79 O
1.05 O
0.81 O
-0.55 O
assists B
0.62 O
-0.91 O
0.11 O
-0.45 O
0.24 O
0.53 O
deaths B
-0.25 O
0.75 O
-1.56 O
0.45 O
win O
0.31 O
-0.49 O
-0.17 O
0.41 O
average O
duration O
0.38 O
-0.41 O
0.51 O
0.30 O
champLevel B
0.61 O
-0.18 O
0.71 O
goldEarned O
-1.18 O
1.35 O
-1.56 O
-1.27 O
-1.04 O
goldSpent O
0.83 O
0.78 O
-0.54 O
totalDamageDealt O
-0.29 O
totalDamageTaken B
0.19 O
-0.75 O
2.00 O
-0.49 O
totalHeal O
-0.20 O
-0.26 O
0.21 O
-1.04 O
0.44 O
a O
Notes O
apply O
to O
all O
the O
following O
tables O
: O
( O
1 O
) O
Results O
should O
be O
interpreted O
by O
column O
; O
Feature O
values O
are O
pre O
- O
processed O
by O
applying O
min O
- O
max O
normalization O
; O
( O
2 O
) O
For O
random O
effects B
, O
we O
use O
check O
marks B
to O
indicate O
which O
plays B
an O
important O
role B
; O
( O
3 O
) O
For O
fixed O
effects B
, O
we O
only O
show O
those O
with O
significance O
p<0.05 O
; O
Fig O
. O

3 O
. O

Relation O
between O
number O
of O
champions B
and O
matches B
for O
811 O
players B
, O
each O
point B
represents O
one O
player B
. O

follows O
an O
exponential O
trend O
, O
which O
suggests O
that O
at O
the O
start B
of O
their O
gaming O
history O
, O
players B
take O
few O
champions B
to O
learn O
how O
to O
play B
the O
game B
, O
and O
later O
on O
, O
they O
are O
more O
inclined O
to O
explore O
new O
characters B
. O

Despite O
the O
overall O
trend O
, O
we O
find O
cases O
of O
players B
who O
play B
most O
of O
their O
matches B
with O
a O
small O
number O
of O
champions B
, O
and O
cases O
of O
players B
who O
explore O
the O
use O
of O
different O
champions B
in O
the O
early B
stages B
of O
their O
gaming O
history O
. O

The O
former O
case O
is O
indicative O
of O
players B
who O
tend O
to O
have O
extremely O
limited O
interest O
in O
champions B
. O

The O
latter O
case O
is O
instead O
indicative O
of O
players B
with O
a O
broad O
interest O
in O
exploring O
new O
options O
. O

B. O
Model O
Champion O
Attributes O
from O
Match O
Behavior O
Each O
champion B
has O
specific O
abilities B
and O
these O
abilities B
would O
subsequently O
affect O
match B
behaviors O
, O
thus O
we O
are O
interested O
in O
understanding O
how O
do O
match B
behaviors O
reflect O
champion B
abilities B
. O

We O
focus B
on O
the O
six O
main O
attributes O
that O
characterize O
champion B
abilities B
: O
control B
, O
damage B
, O
difficulty O
, O
mobility B
, O
toughness O
, O
and O
utility B
. O

We O
then O
formulate O
this O
problem O
as O
LME O
models O
for O
champion B
attributes I
. O

Here O
, O
each O
attribute O
is O
modeled O
with O
the O
11 O
match B
features O
as O
candidate O
fixed O
effects B
and O
three O
demographic O
factors O
as O
candidate O
random O
effects B
. O

TABLE O
III O
shows O
the O
relationship O
between O
champion B
attributes I
and O
the O
set O
of O
most O
relevant O
match B
behaviors O
. O

We O
observe O
that O
: O
( O
1 O
) O
demographic O
features O
play B
an O
important O
role B
as O
random O
effects B
to O
model O
attributes O
of O
damage B
, O
difficulty O
, O
mobility B
, O
and O
utility B
; O
( O
2 O
) O
the O
11 O
match B
behavior O
features O
are O
reflective O
of O
champion B
attributes I
, O
which O
can O
be O
backed O
up O
by O
the O
statistics O
of O
matched B
games B
from O
Champion.gg,12 O
a O
website O
that O
provides O
in O
- O
depth O
and O
accurate O
statistics O
about O
the O
overall O
performance O
of O
each O
champion B
in O
LoL. O
Take O
the O
attribute O
“ O
Damage B
” O
for O
an O
example O
, O
TABLE O
III O
shows O
that O
a O
large O
number O
of O
kills B
and O
deaths B
but O
a O
small B
number I
of I

 O
assists I
are O
indicative O
of O
high B
damage I
rating I
of O
the O
champion B
. O

According O
to O
the O
statistics O
in O
Champion.gg O
, O
we O
find O
that O
the O
top O
five O
champions B
conducting O
a O
large O
number O
of O
kills B
and O
a O
small B
number I
of I
assists I
are O
indeed O
champions B
with O
high B

 O
damage I
ratings O
. O

For O
instance O
, O
the O
champion B
Quinn O
, O
having O
damage B
rating O
of O
3 O
( O
the O
highest B
level I
) O
, O
being O
ranked B
2nd O
for O
average B
number I
of I
kills I
, O
and O
being O
in O
the O
lowest O
ranks B
for O
number O
of O
assists.13 O
C. O
Infer O
Match O
Behavior O
according O
to O
Champion O
Attributes O
After O
modeling O
champion B
attributes I
from O
match B
behavior O
, O
we O
are O
then O
interested O
in O
understanding O
how O
the O
choice O
of O
certain O
champions B
affect O
playing B
behaviors O
in O
the O
game B
. O

We O
answer O
this O
question O
by O
fitting O
a O
LME O
model O
for O
each O
match B
feature O
. O

In O
this O
model O
, O
we O
use O
the O
six O
champion B
attributes I
as O
candidate O
fixed O
effects B
and O
the O
three O
demographic O
factors O
as O
candidate O
random O
effects B
. O

12https://champion.gg O
13https://champion.gg/champion/Quinn/Top?league=gold O
TABLE O
IV O
INFERRED O
MATCH O
BEHAVIORS O
ACCORDING O
TO O
CHAMPION B
ATTRIBUTES I
Kills O
Assists O
TotalDamageTaken B
TotalHeal O
Random O
Effects O
gender O
X O
X O
age O
X O
X O
X O
X O
region O
X O
X O
X O
X O
Fixed O
Effects O
control B
-0.09 O
-0.17 O
-0.16 O
damage B
0.11 O
-0.16 O
-0.15 O
difficulty O
-0.05 O
mobility B
-0.08 O
toughness O
-0.10 O
0.25 O
0.09 O
utility B
-0.06 O
0.08 O
-0.07 O
0.05 O
We O
fit O
one O
LME O
model O
for O
each O
of O
the O
11 O
match B
features O
and O
identified O
significant O
LME O
models O
for O
kills B
, O
assists B
, O
totalDamageTaken B
and O
totalHeal O
. O

TABLE O
IV O
shows O
the O
relationship O
between O
match B
features O
and O
champion B
attributes I
indicated O
by O
corresponding O
LME O
models O
. O

Results O
show O
that O
demographic O
features O
also O
play B
an O
important O
role B
in O
understanding O
how O
champion B
choice O
affects O
match B
behaviors O
. O

Moreover O
, O
the O
partial O
combination O
of O
champion B
attributes I
is O
indicative O
of O
specific O
match B
behaviors O
. O

For O
instance O
, O
champions B
with O
low B
control I

 O
rating I
, O
high B
damage I
rating I
, O
and O
low O
utility B
rating O
are O
expected O
to O
have O
more O
kills B
; O
champions B
with O
low B
damage I
, O
low O
mobility B
, O
low O
toughness O
, O
and O
high O
utility B
ratings O
are O
expected O
to O
have O
more O
assists B
. O

These O
relationships O
also O
reflect O
the O
statistics O
of O
matched B
games B
from O
Champion.gg.12 O
In O
particular O
, O
the O
champion B
Master O
Yi O
has O
the O
following O
attribute O
ratings O
: O
Damage:3 O
, O
Control:0 O
, O
Utility:0 O
, O
Mobility:2 O
, O
Toughness:1 O
, O
Difficulty:1 O
, O
and O
is O
ranked B
5th O
on O
the O
basis O
of O
their O
number O
of O
kills B
. O

Given O
these O
results O
, O
we O
found O
that O
there O
exists O
an O
intrinsic O
relation O
between O
impersonated O
champions B
and O
their O
actions O
during O
the O
game B
. O

We O
are O
now O
interested O
in O
making O
a O
step O
further O
to O
check O
whether O
a O
relation O
exists O
between O
the O
impersonated O
champion B
and O
personality O
. O

This O
will O
allow O
us O
to O
understand O
how O
personality O
influences O
a O
player B
’s O
choice O
of O
roles B
as O
well O
as O
to O
directly O
link O
their O
personality O
with O
the O
actions O
performed O
during O
the O
game B
. O

VI O
. O

UNDERSTANDING O
PERSONALITY O
FROM O
GAME O
BEHAVIOR O
In O
this O
section O
, O
we O
first O
show O
an O
interesting O
relationship O
between O
players B
’ O
favorite B
champions I
and O
most O
skilled B
champions I
. O

Then O
, O
we O
examine O
how O
do O
big-5 O
personality O
traits O
align O
with O
champion B
choices O
and O
corresponding O
match B
behaviors O
in O
the O
online B
game I
environment O
. O

A. O
Favorite O
vs O
Skilled O
Champions O
To O
understand O
whether O
players B
are O
good O
at O
playing B
with O
their O
favorite B
champions I
, O
we O
compare O
players B
’ O
favorite O
three O
champions B
( O
collected O
in O
the O
survey O
) O
with O
their O
top O
three O
ranked B
champions B
( O
collected O
via O
the O
official O
API O
) O
. O

Results O
show O
that O
for O
83 O
% O
of O
players B
, O
at O
least O
one O
champion B
is O
both O
their O
favorite O
and O
skilled B
, O
while O
for O
17 O
% O
players B
, O
their I
favorite I
champions B
are O
totally O
different O
from O
the O
skilled B
ones O
. O

This O
finding O
suggests O
that O
players B
’ O
skilled B
champions I
are O
not O
always O
exactly O
their O
favorite O
ones O
. O

In O
particular O
, O
players B
might O
have O
the O
tendency O
of O
using O
champions B
that O
are O
easy B
to O
control B
and O
win O
despite O
personal O
preference O
. O

B. O
Understanding O
Personality O
from O
Game O
Behavior O
As O
personality O
could O
affect O
champion B
choices O
and O
champion B
abilities B
would O
subsequently O
influence O
playing O
behaviors O
. O

Here O
, O
we O
focus B
on O
understanding O
personality O
traits O
from O
impersonated O
roles B
and O
corresponding O
playing O
behaviors O
, O
which O
would O
corroborate O
the O
previous O
results O
showing O
the O
existence O
of O
associations O
between O
players B
’ O
personality O
and O
playing O
behaviors O
in O
online B
game I
environments O
[ O
2 O
] O
, O
[ O
17 O
] O
. O

To O
explore O
such O
associations O
, O
we O
formulate O
the O
problem O
as O
LME O
modeling O
tasks O
for O
the O
big-5 O
personality O
traits O
. O

Specifically O
, O
with O
champion B
attributes I
and O
match B
features O
as O
candidate O
fixed O
effects B
and O
demographic O
factors O
as O
candidate O
random O
effects B
, O
our O
goal B
is O
to O
explore O
relationships O
between O
personality O
traits O
and O
game B
aspects O
( O
including O
champion B
choices O
and O
match B
behaviors O
) O
. O

We O
first O
conduct O
the O
analysis O
by O
using O
all O
of O
811 O
samples O
, O
but O
results O
show O
weak B
correlations O
between O
personality O
and O
game B
aspects O
, O
which O
might O
be O
caused O
by O
either O
noisy O
samples O
or O
neutral O
personality O
traits O
( O
e.g. O
, O
some O
players B
show O
neutral O
ratings O
for O
agreeableness O
) O
. O

On O
the O
one O
hand O
, O
to O
limit O
possible O
noisy O
samples O
, O
which O
is O
a O
general O
problem O
in B
survey O
- I
based I
studies O
, O
we O
follow O
the O
criteria O
of O
response B
time I
reported B
in O
[ O
17 O
] O
and O
limit O
our O
samples O
to O
those O
whose O
response B
time I
for O
the O
personality O
test O
is O
greater O
than O
two O
minutes O
. O

On O
the O
other O
hand O
, O
we O
label O
each O
trait O
according O
to O
its O
first O
and O
third O
quantiles O
, O
and O
only O
keep O
those O
whose O
personality O
score O
is O
smaller O
than O
the O
first O
quantile O
( O
label O
as O
low O
) O
or O
greater O
than O
the O
third O
quantile O
( O
label O
as O
high O
) O
. O

This O
allows O
us O
to O
study O
players B
with O
obvious B
differences O
in O
personalities O
and O
avoid O
confusions O
introduced O
by O
those O
with O
neutral O
personality O
traits O
. O

We O
then O
re O
- O
fit O
our O
LME O
models O
on O
the O
obtained O
sub O
- O
samples O
regarding O
each O
trait O
and O
get O
more O
consistent O
and O
significant O
relationships O
. O

TABLE O
V O
illustrates O
the O
correlations O
between O
each O
personality O
trait O
and O
selected O
game B
aspects O
( O
including O
champion B
attributes I
and O
match B
features O
) O
. O

We O
will O
explain O
the O
relationship O
regarding O
each O
personality O
trait O
in O
turn B
below O
. O

Agreeableness O
is O
the O
tendency O
of O
getting O
along O
with O
others O
in O
pleasant O
, O
satisfying O
relationships O
[ O
29 O
] O
. O

TABLE O
V O
shows O
that O
the O
relationship O
between O
agreeableness O
and O
game B
behavior O
is O
mainly O
affected O
by O
gender O
and O
region O
. O

In O
our O
samples O
, O
males O
show O
lower O
agreeableness O
than O
females O
; O
players B
in O
region O
TABLE O
V O
UNDERSTANDING O
BIG-5 O
PERSONALITY O
TRAITS O
FROM O
CHAMPION B
ATTRIBUTES I
AND O
MATCH O
BEHAVIOR O
Agreeableness O
Conscientiousness O
Emotional O
stability O
Extraversion O
Openness O
Random O
Effects O
gender O
X O
X O
X O
X O
age O
X O
X O
X O
X O
region O
X O
X O
Fixed O
Effects O
control B
-1.34 O
1.03 O
3.28 O
-0.72 O
1.59 O
damage B
-1.04 O
-3.82 O
1.47 O
1.45 O
difficulty O
-1.47 O
-1.49 O
-1.49 O
-2.34 O
mobility B
0.76 O
-2.06 O
4.35 O
-4.30 O
toughness O
-1.28 O
-2.46 O
-1.61 O
-5.24 O
-6.37 O
utility B
-0.54 O
1.49 O
1.27 O
-4.88 O
2.35 O
kills B
9.22 O
-5.26 O
8.71 O
17.28 O
5.64 O
deaths B
-8.10 O
-4.04 O
-11.9 O
-4.33 O
assists B
-2.92 O
-5.63 O
-4.36 O
-7.43 O
win O
15.6 O
2.56 O
3.05 O
11.92 O
15.31 O
average O
duration O
-0.01 O
-0.01 O
0.09 O
-0.03 O
2.53 O
n O
level6 O
0.06 O
-0.04 O
0.07 O
0.12 O
-1.73 O
EUW1 O
( O
i.e. O
, O
European O
West O
) O
show O
lower O
agreeableness O
than O
players B
from O
other O
regions O
. O

Results O
also O
show O
that O
agreeableness O
is O
negatively O
correlated O
with O
utility B
in O
LoL O
, O
which O
is O
different O
from O
the O
case O
in O
a O
real O
- O
life O
scenario O
[ O
5 O
] O
. O

Additionally O
, O
since O
people O
with O
high O
agreeableness O
are O
expected O
to O
cooperate O
well O
with O
teammates B
, O
it O
is O
reasonable O
to O
find O
that O
they O
are O
characterized O
by O
a O
strong O
killing O
behavior O
and O
fewer O
deaths B
, O
which O
is O
related O
to O
good O
cooperation B
with O
teammates B
and O
a O
key O
component O
to O
achieve O
a O
high O
winning O
rate O
. O

Conscientiousness O
is O
the O
personality O
trait O
of O
being O
careful O
and O
having O
the O
desire O
to O
positively O
fulfill O
a O
task O
[ O
14 O
] O
, O
[ O
37 O
] O
. O

According O
to O
TABLE O
V O
, O
the O
relationship O
between O
conscientiousness O
and O
game B
behavior O
is O
mainly O
affected O
by O
age O
, O
where O
older O
players B
tend O
to O
have O
higher O
conscientiousness O
than O
younger O
ones O
. O

Results O
also O
show O
that O
conscientious O
players B
prefer O
champions B
with I
good I
control I
ability B
which O
allow O
for O
better O
management O
in O
dangerous O
situations O
. O

The O
nature O
of O
being O
careful O
and O
keep O
things O
under O
control B
make O
players B
more O
likely O
to O
win O
. O

We O
also O
notice O
that O
conscientious O
players B
have O
a O
relatively O
small O
number O
of O
skilled B
champions I
, O
which O
suggests O
that O
conscientious O
players B
mainly O
focus B
on O
specific B
champions I
and O
are O
not O
exploring O
new O
possibilities O
of O
different O
champions B
. O

Emotional O
stability O
is O
the O
ability B
of O
calmly O
handling O
difficult O
situations O
[ O
18 O
] O
, O
[ O
24 O
] O
. O

Results O
in O
TABLE O
V O
show O
that O
the O
relationship O
between O
emotional O
stability O
and O
game B
behavior O
is O
mainly O
affected O
by O
gender O
and O
age O
. O

According O
to O
our O
samples O
, O
males O
tend O
to O
be O
more O
emotionally O
stable O
than O
females O
; O
players B
between O
21 O
∼ O
30 O
are O
reported B
to O
be O
more O
emotionally O
stable O
than O
others O
. O

Moreover O
, O
emotionally O
stable O
players B
are O
able O
to O
take O
champions B
with I
high I
control I
and O
utility B
ratings O
as O
well O
as O
performing O
more O
kills B
while O
keeping O
a O
low B
death I
rate I
. O

Additionally O
, O
we O
observe O
that O
the O
average O
match B
durations O
for O
emotionally O
stable O
players B
are O
relatively O
longer O
. O

All O
these O
behaviors O
correspond O
to O
their O
ability B
of O
being O
stable O
and O
productive O
in O
tough O
situations O
. O

Extraversion O
is O
the O
trait O
of O
being O
outgoing O
and O
social O
[ O
1 O
] O
, O
[ O
2 O
] O
. O

Its O
relationship O
with O
game B
behavior O
is O
affected O
by O
gender O
, O
age O
, O
and O
region O
. O

In O
our O
sample O
, O
males O
show O
lower O
extraversion O
than O
females O
, O
older O
players B
tend O
to O
be O
more O
extrovert O
than O
younger O
ones O
, O
and O
players B
in O
EUW1 O
show O
lower O
extraversion O
than O
those O
in O
other O
regions O
. O

TABLE O
V O
also O
indicates O
that O
extrovert O
players B
prefer O
champions B
with O
high B
damage I
and O
mobility B
ratings O
, O
and O
thus O
are O
prone O
to O
kill B
more O
and O
have O
a O
low B
death I
rate I
, O
which O
leads B
to O
a O
high O
winning O
rate O
. O

It O
also O
shows O
that O
extrovert O
players B
have O
a O
relatively O
large O
number O
of O
expert O
champions B
, O
which O
means O
these O
players B
have O
a O
broad O
interest O
in O
trying O
different O
champions B
. O

These O
behaviors O
support B
the O
outgoing O
, O
social O
and O
optimistic O
aspects O
of O
being O
extrovert O
. O

Openness O
is O
being O
characterized O
by O
high O
curiosity O
and O
creativity O
. O

Its O
relationship O
with O
game B
behavior O
is O
slightly O
affected O
by O
gender O
and O
age O
. O

Players B
with O
high O
openness O
tend O
to O
choose O
champions B
with I
high I
control I
, O
damage B
, O
and O
utility B
ratings O
, O
leading B
to O
relatively O
more O
killing B
behaviors O
, O
fewer O
deaths B
, O
and O
thus O
higher O
winning O
rate O
. O

However O
, O
we O
notice O
that O
players B

 O
with I
high I
openness I
do O
not O
have O
a O
large O
number O
of O
assisting B
behaviors O
and O
expert O
champions B
, O
which O
differs O
from O
the O
reallife O
behavior O
of O
people O
with O
the O
nature O
of O
being O
open B
. O

VII O
. O

CONCLUSIONS O
AND O
LIMITATIONS O
In O
this O
paper O
, O
we O
explore O
the O
relationship O
between O
personality O
and O
behavior O
in O
LoL. O
We O
collect O
data O
of O
players B
’ O
ingame B
behavior O
from O
the O
official O
Riot O
Games O
API O
and O
adopt O
the O
Five O
- O
Factor O
Model O
to O
get O
survey O
- O
based B
personality O
traits O
of O
LoL O
players B
. O
We I
applied I
linear O
mixed O
effects B
models O
to O
fit O
our O
data O
and O
describe O
the O
entangled O
relationships O
between O
personality O
and O
playing O
behaviors O
while O
taking O
demographic O
random O
effects B
into O
consideration O
. O

First O
, O
we O
highlight B
the O
exponential O
relationship O
between O
the O
number O
of O
champions B
being O
used O
and O
matches B
being O
played B
. O

We O
then O
study O
the O
relationship O
between O
champions B
and O
match B
behaviors O
by O
modeling O
champion B
attributes I
and O
playing B
behaviors O
from O
each O
other O
. O

For O
instance O
, O
results O
show O
that O
champions B
with O
low B
control I
rating I
and O
utility B
rating O
but O
high B
damage I
rating I
are O
expected O
to O
have O
more O
killing B
behaviors O
. O

Second O
, O
we O
investigate O
the O
relations O
between O
a O
player B
’s O
personality O
and O
game B
aspects O
( O
including O
impersonated O
champions B
and O
corresponding O
match B
behaviors O
) O
. O

Results O
show O
significant O
associations O
among O
these O
factors O
. O

For O
example O
, O
we O
observed O
that O
conscientious O
players B
prefer O
champions B
with I

 O
good I
control I
ability B
to O
face O
dangerous O
situations O
, O
and O
that O
the O
nature O
of O
being O
careful O
makes O
these O
players B
more O
likely O
to O
win O
. O

Future O
work O
will O
be O
devoted O
to O
overcoming O
some O
limitations O
of O
the O
current O
study O
. O

We O
plan O
to O
collect O
more O
data O
as O
the O
current O
analysis O
is O
based B
on O
the O
data O
of O
811 O
players B
. O

Despite O
this O
amount O
of O
participants O
is O
sufficient O
from O
the O
psychological O
perspective O
of O
personality O
analysis O
, O
it O
limits O
us O
from O
applying O
complex O
machine O
learning O
models O
as O
well O
as O
conducting O
more O
nuanced O
analysis O
( O
e.g. O
, O
considering O
the O
change O
of O
match B
statistics O
when O
champions B
fill O
different O
roles B
, O
comparing O
performance O
of O
low O
- O
rank B
and O
high B
- I
rank I
players B
) O
. O

A O
bigger O
and O
more O
diverse O
dataset O
will O
also O
allow O
overcoming O
problems O
due O
to O
misbehavior O
of O
survey O
participants O
. O

LoL O
is O
a O
worldwide O
game B
, O
but O
we O
mainly O
spread O
our O
personality O
questionnaires O
in O
US O
websites O
and O
Amazon O
Mechanical O
Turk O
. O

Thus O
the O
demographics O
of O
participants O
are O
not O
universally O
balanced O
. O

For O
future O
work O
, O
we O
will O
improve O
the O
recruitment O
of O
participants O
to O
have O
broader O
samples O
. O

In O
conclusion O
, O
we O
firmly O
believe O
that O
the O
results O
of O
our O
work O
could O
benefit O
both O
companies O
and O
players B
. O

The O
former O
could O
develop O
customized O
game B
characters B
or O
carry B
out O
personalized O
recommendations O
based B
on O
players B
’ O
historical O
match B
behaviors O
. O

The O
latter O
would O
have O
access O
to O
a O
way O
of O
understanding O
how O
their O
personalities O
affect O
game B
behaviors O
, O
thus O
having O
insights O
of O
how O
to O
successfully O
assemble O
teams B
and O
develop O
reasonable O
strategies B
during O
the O
match B
. O

REFERENCES O
[ O
1 O
] O
Shoshannah O
Tekofsky O
, O
Pieter O
Spronck O
, O
Aske O
Plaat O
, O
and O
Plaat@uvt O
. O

Psyops O
: O
Personality O
assessment O
through O
gaming O
behavior O
. O

In O
the O
8th O
International O
Conference O
on O
the O
Foundations O
of O
Digital O
Games O
, O
2013 O
. O

[ O
2 O
] O
Narnia O
C. O
Worth O
and O
Angela O
S. O
Book O
. O

Personality O
and O
behavior O
in O
a O
massively O
multiplayer O
online O
role B
- O
playing B
game B
. O

Computers O
in O
Human O
Behavior O
, O
38:322 O
– O
330 O
, O
2014 O
. O

[ O
3 O
] O
P.G. O
Schrader O
Randy O
Boone O
Michael O
P. O
McCreery O
, O
S. O
Kathleen O
Krach O
. O

Defining O
the O
virtual O
self O
: O
Personality O
, O
behavior O
, O
and O
the O
psychology O
of O
embodiment O
. O

Computers O
in O
Human O
Behavior O
, O
28(3):976 O
– O
983 O
, O
2012 O
. O

[ O
4 O
] O
Daniele O
Quercia O
, O
Michal O
Kosinski O
, O
David O
Stillwell O
, O
and O
Jon O
Crowcroft O
. O

Our O
twitter O
profiles O
, O
our O
selves O
: O
Predicting O
personality O
with O
twitter O
. O

pages O
180–185 O
, O
10 O
2011 O
. O

[ O
5 O
] O
Golnoosh O
Farnadi O
, O
Geetha O
Sitaraman O
, O
Shanu O
Sushmita O
, O
Fabio O
Celli O
, O
Michal O
Kosinski O
, O
David O
Stillwell O
, O
Sergio O
Davalos O
, O
Marie O
- O
Francine O
Moens O
, O
and O
Martine O
De O
Cock O
. O

Computational O
personality O
recognition O
in O
social O
media O
. O

User O
Modeling O
and O
User O
- O
Adapted O
Interaction O
, O
26 O
, O
02 O
2016 O
. O

[ O
6 O
] O
League O
of O
Legends O
. O

The O
art O
of O
league B
of I
legends I
. O

https://universe O
. O

leagueoflegends.com/en O
US O
/ O
champions/. O
[ O
7 O
] O
G. O
van O
Lankveld O
, O
P. O
Spronck O
, O
J. O
van O
den O
Herik O
, O
and O
A. O
Arntz O
. O

Games O
as O
personality O
profiling O
tools O
. O

In O
2011 O
IEEE O
Conference O
on O
Computational O
Intelligence O
and O
Games O
( O
CIG’11 O
) O
, O
pages O
197–202 O
, O
Aug O
2011 O
. O

[ O
8 O
] O
Nick O
Yee O
. O

The O
demographics O
, O
motivations O
, O
and O
derived O
experiences B
of O
users B
of O
massively O
multi O
- O
user B
online O
graphical O
environments O
. O

Presence O
: O
Teleoper O
. O

Virtual O
Environ O
. O

, O
15(3):309–329 O
, O
June O
2006 O
. O

[ O
9 O
] O
Lewis O
R. O
Goldberg O
, O
Dennis O
Sweeney O
, O
Peter O
F. O
Merenda O
, O
and O
John O
Edward O
Hughes O
. O

Demographic O
variables O
and O
personality O
: O
the O
effects B
of O
gender O
, O
age O
, O
education O
, O
and O
ethnic O
/ O
racial O
status B
on O
self O
- O
descriptions O
of O
personality O
attributes O
. O

Personality O
and O
Individual O
Differences O
, O
24(3):393 O
– O
403 O
, O
1998 O
. O

[ O
10 O
] O
Mark O
Griffiths O
, O
Mark O
Davies O
, O
and O
Darren O
Chappell O
. O

Demographic O
factors O
and O
playing B
variables O
in O
online O
computer O
gaming O
. O

Cyberpsychology O
& O
behavior O
: O
the O
impact B
of O
the O
Internet O
, O
multimedia O
and O
virtual O
reality O
on O
behavior O
and O
society O
, O
7:479–87 O
, O
09 O
2004 O
. O

[ O
11 O
] O
John O
Suler O
. O

The O
online O
disinhibition O
effect B
. O

CyberPsychology O
& O
Behavior O
, O
7(3):321–326 O
, O
2004 O
. O

PMID O
: O
15257832 O
. O

[ O
12 O
] O
Joseph O
B O
Borders O
. O

Relationship O
between O
personality O
and O
video B
game I
preferences O
. O

PhD O
thesis O
, O
2012 O
. O

[ O
13 O
] O
Martin O
Delhove O
and O
Tobias O
Greitemeyer O
. O

The O
relationship O
between O
video B

 O
game I
character B
preferences O
and O
aggressive O
and O
prosocial O
personality O
traits O
. O

Psychology O
of O
Popular O
Media O
Culture O
, O
11 O
2018 O
. O

[ O
14 O
] O
Nick O
Yee O
, O
Nicolas O
Ducheneaut O
, O
Lester O
Nelson O
, O
and O
Peter O
Likarish O
. O

Introverted O
elves O
& O
conscientious O
gnomes O
: O
the O
expression O
of O
personality O
in O
world B
of O
warcraft O
. O

In O
CHI O
, O
2011 O
. O

[ O
15 O
] O
Lindsay O
T. O
Graham O
and O
Samuel O
D. O
Gosling O
. O

Personality O
profiles O
associated O
with O
different O
motivations O
for O
playing B
world B
of O
warcraft O
. O

Cyberpsychology O
, O
Behavior O
, O
and O
Social O
Networking O
, O
16(3):189–193 O
, O
2013 O
. O

PMID O
: O
23438267 O
. O

[ O
16 O
] O
Daniel O
Johnson O
and O
John O
Gardner O
. O

Personality O
, O
motivation O
and O
video B

 O
games I
. O

In O
Proceedings O
of O
the O
22Nd O
Conference O
of O
the O
Computer O
- O
Human O
Interaction O
Special O
Interest O
Group O
of O
Australia O
on O
Computer O
- O
Human O
Interaction O
, O
OZCHI O
’ O
10 O
, O
pages O
276–279 O
, O
New O
York O
, O
NY O
, O
USA O
, O
2010 O
. O

ACM O
. O

[ O
17 O
] O
Giel O
Lankveld O
, O
Sonny O
Schreurs O
, O
and O
Pieter O
Spronck O
. O

Psychologically O
verified O
player B
modelling O
. O

pages O
12–19 O
, O
01 O
2009 O
. O

[ O
18 O
] O
Wei O
Peng O
, O
Ming O
Liu O
, O
and O
Yi O
Mou O
. O

Do O
aggressive B
people O
play I
violent O
computer O
games B
in O
a O
more O
aggressive O
way O
? O
individual O
difference O
and O
idiosyncratic O
game B
- O
playing O
experience B
. O

CyberPsychology O
& O
Behavior O
, O
11(2):157–161 O
, O
2008 O
. O

PMID O
: O
18422407 O
. O

[ O
19 O
] O
Jowon O
Park O
, O
Yosep O
Song O
, O
and O
Ching O
- O
I O
Teng O
. O

Exploring O
the O
links O
between O
personality O
traits O
and O
motivations O
to O
play B
online B
games I
. O

Cyberpsychology O
, O
Behavior O
, O
and O
Social O
Networking O
, O
14(12):747–751 O
, O
2011 O
. O

PMID O
: O
21780935 O
. O

[ O
20 O
] O
A. O
Sapienza O
, O
H. O
Peng O
, O
and O
E. O
Ferrara O
. O

Performance O
dynamics O
and O
success O
in O
online B
games I
. O

In O
2017 O
IEEE O
International O
Conference O
on O
Data O
Mining O
Workshops O
( O
ICDMW O
) O
, O
pages O
902–909 O
, O
Nov O
2017 O
. O

[ O
21 O
] O
LOLESPORTS O
STAFF O
. O

2018 O
mid B
- I
season I
. O

https://www.lolesports.com/ O
en O
US O
/ O
articles/2018-mid O
- O
season B
- O
invitational O
- O
numbers O
. O

[ O
22 O
] O
Ian O
Sheer O
. O

Player B
tally O
for O
’ O
league O
of O
legends O
’ O
surges O
. O

https://blogs.wsj O
. O

com O
/ O
digits/2014/01/27 O
/ O
player B
- O
tally O
- O
for O
- O
league B
- O
of I
- I
legends O
- O
surges/. O
[ O
23 O
] O
LoL B
new O
player I
guide O
. O

New O
player B
guide O
( O
lol B
) O
. O

https://na O
. O

leagueoflegends.com/en/game-info/get-started/new-player-guide/. O
[ O
24 O
] O
Samuel O
D O
Gosling O
, O
Peter O
J. O
Rentfrow O
, O
and O
William O
B O
Swann O
. O

A O
very O
brief O
measure O
of O
the O
big O
- O
five O
personality O
domains O
. O

Journal O
of O
Research O
in O
Personality O
, O
37(6):504–528 O
, O
1 O
2003 O
. O

[ O
25 O
] O
J O
M O
Digman O
. O

Personality O
structure O
: O
Emergence O
of O
the O
five O
- O
factor O
model O
. O

Annual O
Review O
of O
Psychology O
, O
41(1):417–440 O
, O
1990 O
. O

[ O
26 O
] O
Heather O
E. O
P. O
Cattell O
. O

The O
original O
big O
five O
: O
A O
historical O
perspective O
. O

European O
Review O
of O
Applied O
Psychology O
/ O
Revue O
Europenne O
de O
Psychologie O
Applique O
, O
46:5–14 O
, O
01 O
1996 O
. O

[ O
27 O
] O
Gijsbert O
Stoet O
. O

Psytoolkit O
: O
A O
software O
package O
for O
programming O
psychological O
experiments O
using O
linux O
. O

Behavior O
Research O
Methods O
, O
42(4):1096–1104 O
, O
Nov O
2010 O
. O

[ O
28 O
] O
Gijsbert O
Stoet O
. O

Psytoolkit O
: O
A O
novel O
web O
- O
based B
method O
for O
running O
online O
questionnaires O
and O
reaction B
- O
time I
experiments O
. O

Teaching O
of O
Psychology O
, O
44(1):24–31 O
, O
2017 O
. O

[ O
29 O
] O
Warren O
T. O
Norman O
. O

Toward O
an O
adequate O
taxonomy O
of O
personality O
attributes O
: O
Replicated O
factor O
structure O
in O
peer O
nomination O
personality O
ratings O
. O

Journal O
of O
abnormal O
and O
social O
psychology O
, O
66:574–83 O
, O
07 O
1963 O
. O

[ O
30 O
] O
Douglas O
Bates O
, O
Martin O
Mchler O
, O
Ben O
Bolker O
, O
and O
Steve O
Walker O
. O

Fitting O
linear O
mixed O
- O
effects B
models O
using O
lme4 O
. O

Journal O
of O
Statistical O
Software O
, O
Articles O
, O
67(1):1–48 O
, O
2015 O
. O

[ O
31 O
] O
Douglas O
Bates O
, O
Martin O
Mchler O
, O
and O
Bin O
Dai O
. O

lme4 O
: O
Linear O
Mixed O
- O
Effects B
Models O
Using O
S4 O
Classes B
, O
volume O
0.999375 O
- O
33 O
. O

01 O
2011 O
. O

[ O
32 O
] O
R O
Development O
Core O
Team O
. O

R O
: O
A O
language O
and O
environment O
for O
statistical O
computing O
. O

R. O
Found O
. O

Stat B
. O

Comput O
. O

, O
1 O
, O
01 O
2011 O
. O

[ O
33 O
] O
Andrew O
Gelman O
. O

Analysis O
of O
variance O
– O
why O
it O
is O
more O
important O
than O
ever O
. O

Ann O
. O

Statist O
. O

, O
33(1):1–53 O
, O
02 O
2005 O
. O

[ O
34 O
] O
Gideon O
Schwarz O
. O

Estimating O
the O
dimension O
of O
a O
model O
. O

Ann O
. O

Statist O
. O

, O
6(2):461–464 O
, O
03 O
1978 O
. O

[ O
35 O
] O
Robert O
E. O
Kass O
and O
Adrian O
E. O
Raftery O
. O

Bayes O
factors O
. O

Journal O
of O
the O
American O
Statistical O
Association O
, O
90(430):773–795 O
, O
1995 O
. O

[ O
36 O
] O
Dr. O
Anthony O
Bean O
. O

Video O
gamers B
and O
personality O
: O
A O
five O
- O
factor O
model O
to O
understand O
game B
playing O
style I
. O

Psychology O
of O
Popular O
Media O
Culture O
, O
Online O
First O
Publication O
, O
03 O
2014 O
. O

[ O
37 O
] O
Arthur O
Poropat O
. O

A O
meta B
- O
analysis O
of O
the O
five O
- O
factor O
model O
of O
personality O
and O
academic O
performance O
. O

Psychological O
bulletin O
, O
135:322–38 O
, O
04 O
2009 O
. O

